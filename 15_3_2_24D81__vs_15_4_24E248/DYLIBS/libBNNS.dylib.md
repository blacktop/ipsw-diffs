## libBNNS.dylib

> `/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBNNS.dylib`

```diff

-1361.60.15.0.0
-  __TEXT.__text: 0x94c70c
-  __TEXT.__auth_stubs: 0x11e0
-  __TEXT.__gcc_except_tab: 0x270e0
-  __TEXT.__const: 0x1276f
-  __TEXT.__cstring: 0x2afda
-  __TEXT.__oslogstring: 0x303
-  __TEXT.__unwind_info: 0xa288
-  __TEXT.__eh_frame: 0x8b28
+1497.100.141.0.0
+  __TEXT.__text: 0xa22adc
+  __TEXT.__auth_stubs: 0x1250
+  __TEXT.__gcc_except_tab: 0x2b46c
+  __TEXT.__const: 0x1532f
+  __TEXT.__cstring: 0x2ffd6
+  __TEXT.__oslogstring: 0x36b
+  __TEXT.__unwind_info: 0xb968
+  __TEXT.__eh_frame: 0xbe90
   __DATA_CONST.__got: 0xf8
-  __DATA_CONST.__const: 0x3610
-  __AUTH_CONST.__auth_got: 0x8f8
-  __AUTH_CONST.__const: 0xe718
+  __DATA_CONST.__const: 0x3838
+  __AUTH_CONST.__auth_got: 0x930
+  __AUTH_CONST.__const: 0xfb28
   __AUTH_CONST.__cfstring: 0x200
-  __AUTH.__data: 0x4e0
+  __AUTH.__data: 0x4e8
   __DATA.__data: 0x1c
   __DATA.__bss: 0x1c0
   - /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib

   - /System/Library/PrivateFrameworks/MIL.framework/Versions/A/MIL
   - /usr/lib/libSystem.B.dylib
   - /usr/lib/libc++.1.dylib
-  UUID: 0A0EA04E-3AE2-3734-9E57-D13FBD17FA9A
-  Functions: 11026
-  Symbols:   602
-  CStrings:  4146
+  UUID: A1EEA1FE-1693-35BC-89D6-0EE849B9C68F
+  Functions: 11607
+  Symbols:   611
+  CStrings:  4640
 
Symbols:
+ _BNNSGraphCompileOptionsGetShapeBounding
+ _BNNSGraphCompileOptionsSetShapeBounding
+ _BNNSGraphContextSetAdvancedWorkspaceReuse
+ _BNNSGraphContextSetStreamingAdvanceCount
+ ___sme_memcpy
+ ___sme_memset
+ __os_feature_enabled_impl
+ _abort
+ _atoi
+ _dispatch_apply_f
+ _getenv
+ _strlcat
- _BNNSRecommendDataLayoutsContraction
- _BNNSRecommendDataLayoutsMultiHeadAttention
- _strncmp
CStrings:
+ "  - function attr table "
+ "  .shape_workspace_size = %llu\n"
+ "  attributes = {"
+ " -A %u"
+ " -B %zu"
+ " -B %zu -IS %zu -OS %zu"
+ " -BD "
+ " -Bdelta"
+ " -CDS 2"
+ " -DS %zu %zu"
+ " -Idelta"
+ " -LPA %u"
+ " -P %zu %zu"
+ " -PA %zu %zu %zu %zu"
+ " -S %zu %zu"
+ " -SD "
+ " -WO %zu"
+ " -Wdelta"
+ " -axis "
+ " -axis 255 "
+ " -bi"
+ " -bias"
+ " -bias "
+ " -bias -BD %s"
+ " -clientptr=off"
+ " -clientptr=on"
+ " -dequantize "
+ " -groups %zu "
+ " -inplace"
+ " -low-mem"
+ " -no-cell"
+ " -no-hidden"
+ " -nt %zu"
+ " -peephole"
+ " -pw %u"
+ " -quantize "
+ " -scale"
+ " -sum_input_delta"
+ " -train-ptr"
+ " -training"
+ " -trans"
+ " -wl IOHW"
+ " -wl IOHrWr"
+ " -wl OIHrWr"
+ " GEMM: %s %c%c %zu x %zu x %zu "
+ " [SHAPE PASS]"
+ " ] stripe_width=%u stripe_align=%u\n"
+ " ] stripe_width=%u stripe_align=%u interleave=%u\n"
+ " gemm_intermediate"
+ " matmul bias repack for "
+ " matmul weights repack for "
+ " rot repack for "
+ " sd_available"
+ " shape_workspace_offset=%llu"
+ " } "
+ " } { "
+ " }, .data_type = %d } "
+ " }, .stride = {"
+ "%s -IO %zu"
+ "%s -OO %zu"
+ "%s%s%s%s"
+ "%s: exec op %llu of %llu  %s\n"
+ "%s: skipped op %llu of %llu (it was performed during shape pass)\n"
+ "%sIS %zu "
+ "%sRS %zu "
+ "%zu "
+ "%{name=layer}s %{name=direction}s BS:%{name=batch_size}zu %{name=summary}s"
+ "(unsupported fn %d) "
+ "+="
+ ", .input = "
+ ", .output = "
+ ", query="
+ ", transX"
+ ", transY"
+ "-A %d "
+ "-A %u "
+ "-AC "
+ "-B %zu %s"
+ "-B %zu %s%s"
+ "-B %zu -IABS %zu -IBBS %zu -OS %zu "
+ "-Bdelta "
+ "-C %zu -I %zu %zu %s%s-O %zu %zu %s%s-K %zu %zu -P %zu %zu -S %zu %zu %s%s -D %s %s "
+ "-I"
+ "-I %zu %zu %zu %s%s-O %zu %zu %zu %s%s-K %zu %zu -P %zu %zu -S %zu %zu %s-ID %s -OD %s -WD %s%s "
+ "-I %zu %zu %zu %s%s-O %zu %zu %zu %s%s-K %zu %zu%s%s%s%s%s%s -ID %s -OD %s -WD %s%s "
+ "-I %zu -O %zu %s-ID %s -OD %s -WD %s%s%s%s"
+ "-I %zu -O %zu -B %zu -seq %zu -stack %zu%s%s%s%s%s%s%s"
+ "-I %zu -O %zu -B %zu -seq %zu -stack %zu%s%s%s%s%s%s%s -grad"
+ "-I %zu -O %zu -K %u %u -LPA %u -A %u -ID %s -OD %s -WD %s%s "
+ "-I1"
+ "-I1_type %d "
+ "-I2"
+ "-I2_type %d "
+ "-IAdelta "
+ "-IBdelta "
+ "-M %d "
+ "-O"
+ "-O_type %d "
+ "-R %d "
+ "-T"
+ "-W"
+ "-Wdelta "
+ "-add_zero_attn"
+ "-avgE "
+ "-avgI "
+ "-beta %f "
+ "-eps %e "
+ "-epsilon %f "
+ "-gamma %f "
+ "-in_place1 "
+ "-in_place2 "
+ "-max "
+ "-momentum %f "
+ "-num_groups %zu "
+ "-op %d "
+ "-op %s Indexing: %s Contracting:%s%s A:%s B:%s C:%s"
+ "-unmax "
+ "0"
+ "1"
+ "2"
+ "2D conv_transpose: bias data type must match output data type"
+ "3"
+ "4"
+ "5"
+ "6"
+ "7"
+ "8"
+ "9"
+ "<No summary info id %u>"
+ "="
+ "A"
+ "AB"
+ "ACCELERATE_416DE66E47C9"
+ "ACCELERATE_7B09D8C42EF4"
+ "ACCELERATE_83A752B02172"
+ "ACCELERATE_965A44538097E"
+ "ACCELERATE_B4EB444453EC"
+ "ACCELERATE_DB267B4233C11"
+ "ACCELERATE_DF5AD80B280E"
+ "AffineGridSample"
+ "Attempted to execute unsupported data type for MLA binary arithmetic op."
+ "B"
+ "BA"
+ "BNNS %s CONVOLUTION PARAMETERS: -one_line %s%s%s"
+ "BNNS => NORM %s"
+ "BNNS ACTIVATION PARAMETERS: -I %zu %zu %zu %s%s-O %zu %zu %zu %s%s-ID %s -OD %s%s%s%s%s%s"
+ "BNNS AFFINE GRID SAMPLE %s %s %s %s"
+ "BNNS ARITHMETIC (%s) BWD %s%s%s"
+ "BNNS ARITHMETIC (%s) FWD %s%s%s"
+ "BNNS Activation Apply Backward: unable to allocate memory to compute activation, using slower compute path"
+ "BNNS Arithmetic Filter Warning: Batch size is 0, nothing to do"
+ "BNNS Arithmetic Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic"
+ "BNNS Arithmetic Filter: input type is BNNSConstant, no gradient to compute"
+ "BNNS Batchnorm Apply Backward: activation allocation failed"
+ "BNNS Batchnorm Apply Backward: failed to apply activation backward"
+ "BNNS Binary Arithmetic received null pointer for either one of the operands or results data."
+ "BNNS BroadcastMatMul FWD %s -A %s -B %s -C %s%s"
+ "BNNS CONTRACTION %s%s%s"
+ "BNNS CONVOLUTION A1: problem doesn't fit in memory limit\n"
+ "BNNS CONVOLUTION ACCELERATE1: weights aren't contiguous, using pack weights general\n"
+ "BNNS CONVOLUTION CUSTOMIZED: -one_line %s%s%s"
+ "BNNS CONVOLUTION PARAMETERS: -one_line %s%s%s"
+ "BNNS CONVOLUTION VERS2: -one_line %s%s%s"
+ "BNNS CONVOLUTIONS CUSTOMIZED: layer param is NULL\n"
+ "BNNS CONVOLUTIONS VERSION2: 2D conv doesn't fit in memory limit"
+ "BNNS CONVOLUTIONS VERSION2: BFloat16 weights are only allowed for OIHW layout"
+ "BNNS CONVOLUTIONS VERSION2: cannot compute nonzero output channels for zero input channels"
+ "BNNS CONVOLUTIONS VERSION2: empty input spatial dimensions without padding is not allowed"
+ "BNNS CONVOLUTIONS VERSION2: empty kernel or output spatial dimensions not allowed"
+ "BNNS CONVOLUTIONS VERSION2: int/out/weight/bias descriptor element stride (stride[0]) must be 1"
+ "BNNS CONVOLUTIONS VERSION2: layer param is NULL\n"
+ "BNNS CONVOLUTIONS VERSION2: output bias or scale is not supported\n"
+ "BNNS CONVOLUTIONS VERSION2: unsupported activation\n"
+ "BNNS CONVOLUTIONS VERSION2: unsupported bias data type\n"
+ "BNNS CONVOLUTIONS VERSION2: unsupported input data type\n"
+ "BNNS CONVOLUTIONS VERSION2: unsupported output data type\n"
+ "BNNS CONVOLUTIONS VERSION2: unsupported weight data type\n"
+ "BNNS COPYSUM %s%s%s"
+ "BNNS Convolution Create: convolution doesn't support indexed weights"
+ "BNNS Convolution Create: input data type is not supported"
+ "BNNS Convolution Create: int16/uint16 output is not supported"
+ "BNNS Convolution Create: int8/uint8 output supported only with int8/uint8 inputs and weights"
+ "BNNS Convolution Create: not allowed to delay allocation to apply if weights ptr isn't maintained by Client\n"
+ "BNNS Convolution Create: output data type is not supported"
+ "BNNS Convolution Create: uint32 output is not supported"
+ "BNNS Convolution Create: weight data type is not supported"
+ "BNNS Convolution: Winograd weights memory doesn't fit in memory\n"
+ "BNNS Convolution: unable to create Winograd that fit in memory\n"
+ "BNNS Convolutions A1   FP Engine: kernel with greater than 16 isn't supported (-K %zu %zu)"
+ "BNNS Convolutions A1  FP Engine: kernel with greater than 16 isn't supported (-K %zu %zu), kernel width after dilation: %zu"
+ "BNNS Convolutions A1: 2D conv doesn't fit in memory limit"
+ "BNNS Convolutions A1: asymmetric strides aren't supported (-S %zu %zu)"
+ "BNNS Convolutions A1: dilation is only supported for 1x1 stride (-DS %zu %zu) (-S %zu %zu)"
+ "BNNS Convolutions A1: input is 0, no computation (-I %zu %zu %zu)"
+ "BNNS Convolutions A1: strides larger than 2, aren't supported (-S %zu %zu)"
+ "BNNS Copy DEST %s SRC %s %s"
+ "BNNS DEPTHWISE CONVOLUTION %s"
+ "BNNS DROPOUT %s%s%s"
+ "BNNS EMBEDDING %s"
+ "BNNS Embedding Apply: Input value %lld is out of range [0, %zu)\n"
+ "BNNS FIRST FILTER => QUANT %s"
+ "BNNS FIRST FILTER:"
+ "BNNS FULLY CONNECTED PARAMETERS: %s%s%s"
+ "BNNS FUSED %s%s"
+ "BNNS FUSED PADDING/CONVOLUTION %s"
+ "BNNS Fully Connected Backward: no bias, bias delta ignored"
+ "BNNS Fully Connected Choose: inputs, weights, outputs and bias must be contigeous in memory (stride[0] <= 1)"
+ "BNNS Fully Connected Sparse COO: probably wrong stride size %zu"
+ "BNNS Fully Connected Sparse Info: fp16 batch multi %zu * %zu * %zu, num nonzero %zu, number of mla vecfp %zu, number of mla matfp+lut %zu, number of mla matfp %zu, sparsify size in bytes: %zu"
+ "BNNS Fully Connected Sparse Info: fp16 batch single %zu * %zu * %zu, num nonzero %zu, number of mla vecfp %zu, number of nop vecfp %zu, sparsify size in bytes: %zu"
+ "BNNS Fully Connected Sparse Info: fp16 batch single %zu * %zu, num non-zero %zu, number of neon fma %zu, sparsify size in bytes %zu"
+ "BNNS Fully Connected Sparse Info: fp16 batch single %zu * %zu, num nonzero %zu, number of neon fma %zu, sparsify size in bytes: %zu"
+ "BNNS Fully Connected Sparse Info: fp16 batch single 4x4 %zu * %zu, num nonzero %zu, number of neon fma %zu, sparsify size in bytes: %zu"
+ "BNNS Fully Connected Sparse Info: fp16 batch single structured 12_16 mla2 %zu * %zu, num non-zero %zu, number of mla2 fma %zu, sparsify size in bytes %zu\n"
+ "BNNS Fully Connected Sparse Info: fp16 batch single structured 12_16 mla3 %zu * %zu, num non-zero %zu, number of mla3 fma %zu, sparsify size in bytes %zu\n"
+ "BNNS Fully Connected Sparse Info: fp16 batch single structured 14_16 mla2 %zu * %zu, num non-zero %zu, number of mla2 fma %zu, sparsify size in bytes %zu\n"
+ "BNNS Fully Connected Sparse Info: fp16 batch single structured 14_16 mla3 %zu * %zu, num non-zero %zu, number of mla3 fma %zu, sparsify size in bytes %zu\n"
+ "BNNS Fully Connected Sparse Info: fp16 batch single structured 5_8 %zu * %zu, num non-zero %zu, number of neon fma %zu, sparsify size in bytes %zu"
+ "BNNS Fully Connected Sparse Info: fp16 batch single structured 6_8 %zu * %zu, num non-zero %zu, number of neon fma %zu, sparsify size in bytes %zu"
+ "BNNS Fully Connected Sparse Info: fp32 batch multi %zu * %zu * %zu, num nonzero %zu, number of mla vecfp %zu, number of mla matfp+lut %zu, number of mla matfp %zu, sparsify size in bytes: %zu"
+ "BNNS Fully Connected Sparse Info: fp32 batch single %zu * %zu, num nonzero %zu, number of neon fma %zu, sparsify size in bytes: %zu"
+ "BNNS Fully Connected Sparse: using batch single compute for batch_size %zu, will be probably better to use batch_multi code path"
+ "BNNS Fully Connected Sparsify: not enough memory in scratch memory to encode sparsity"
+ "BNNS Fully Connected Sparsify: tried structured 12:16 but number of non zeros is vey low %zu"
+ "BNNS Fully Connected Sparsify: tried structured 13:16 but number of non zeros is vey low %zu"
+ "BNNS Fully Connected Sparsify: tried structured 14:16 but number of non zeros is vey low %zu"
+ "BNNS Fully Connected Sparsify: tried structured 5:8 but number of non zeros is vey low %zu"
+ "BNNS Fully Connected Sparsify: tried structured 6:8 but number of non zeros is vey low %zu"
+ "BNNS Fully Connected: attempting to convert %zu x %zu x %zu to Sparse"
+ "BNNS Fully Connected: using generic code to convert data 1 element each time\n"
+ "BNNS Fully Convolution Sparse COO: probably wrong stride size %zu"
+ "BNNS Fused Filter Backward Multi-Input Warning: Batch size is 0, nothing to do"
+ "BNNS Fused Filter Backward Multi-Input: Error - only fused arithmetic and normalization is supported."
+ "BNNS Fused Filter Backward Multi-Input: Error normalization backward failed"
+ "BNNS Fused Filter Backward Multi-Input: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. "
+ "BNNS Fused Filter Backward Multi-Input: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. "
+ "BNNS Fused Filter Backward Warning: Batch size is 0, nothing to do"
+ "BNNS Fused Filter Backward: Error - fused compute and quantization gradient is not support "
+ "BNNS Fused Filter Backward: Error Weight delta pointer is NULL. backward computation of all active parameters must be done in a single function call. "
+ "BNNS Fused Filter Backward: Error bias delta pointer is NULL. backward computation of all active parameters must be done in a single function call. "
+ "BNNS Fused Filter Backward: Error normalization backward failed"
+ "BNNS Fused Filter Backward: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. "
+ "BNNS Fused Filter Backward: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. "
+ "BNNS Fused Filter Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer"
+ "BNNS Fused Filter Multi-Input Warning: Batch size is 0, nothing to do"
+ "BNNS Fused Filter Multi-Input: Error - only fused arithmetic and normalization is supported."
+ "BNNS Fused Filter Multi-Input: Error arithmetic filter apply failed"
+ "BNNS Fused Filter Warning: Batch size is 0, nothing to do"
+ "BNNS Fused Filter: Error first filter apply failed"
+ "BNNS Fused Filter: Error malloc failed"
+ "BNNS Fused Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer"
+ "BNNS GARPH Attention: size not supported"
+ "BNNS GRAPTH ATTENTION: Data Type not supported"
+ "BNNS Graph: layer norm encountered NaN, execution failed."
+ "BNNS Graph: layer norm op values changed, execution failed."
+ "BNNS Graph: wrong parallel size"
+ "BNNS Group Norm Apply Backward: activation allocation failed"
+ "BNNS Group Norm Apply Backward: failed to apply activation backward"
+ "BNNS Grpah: Pattern failed"
+ "BNNS IR describes a graph with dynamic sizes, BNNSGraphContextGetWorkspaceSize() must be used instead of BNNSGraphGetWorkspaceSize()."
+ "BNNS Instance Norm Apply Backward: activation allocation failed"
+ "BNNS Instance Norm Apply Backward: failed to apply activation backward"
+ "BNNS LOSS %s"
+ "BNNS LSTM %s"
+ "BNNS LSTM APPLY BACKWARD: forward pass intermediate results weren't cached, recomputing forward pass"
+ "BNNS Layer Norm Apply Backward: activation allocation failed"
+ "BNNS Layer Norm Apply Backward: failed to apply activation backward"
+ "BNNS Log Filter: invalid one input filter id %u"
+ "BNNS Log Filter: invalid two input filter id %u"
+ "BNNS Loss Backward Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax/sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber"
+ "BNNS Loss Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber"
+ "BNNS Loss Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax"
+ "BNNS Loss Warning: reduction BNNSLossReductionWeightedMean sum of weights is zero"
+ "BNNS Loss Warning: weight_size==0 but weight pointer is not NULL. Weight pointer is ignored."
+ "BNNS Loss: Error yolo weight_size value must be 0, use yolo specific weight factors during filter create"
+ "BNNS Loss: Warning weight_size==0 but weight pointer is not NULL. Weight pointer is ignored."
+ "BNNS MATVEC: attempted to invoke matvec with accumulation type not supported on hardware"
+ "BNNS MATVEC: attempted to invoke matvec with bf16 accumulation type. Please use fp32 accumulation instead"
+ "BNNS MULTIHEAD ATTENTION PARAMETERS: -multihead %zu -TSL %zu -SSL %zu -dm %zu -dk %zu -dv %zu%s"
+ "BNNS NORM -batchnorm %s%s%s"
+ "BNNS NORM -groupnorm %s%s%s"
+ "BNNS NORM -instancenorm %s%s%s"
+ "BNNS NORM -layernorm %s%s%s"
+ "BNNS Normalization Backward Warning: Batch size is 0, nothing to do"
+ "BNNS Normalization Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization"
+ "BNNS Normalization Warning: epsilon is zero, it may cause division by zero"
+ "BNNS Normalization Warning: momentum is zero"
+ "BNNS PADDING %s"
+ "BNNS PADDING: ndim %u lpad[%u,%u,%u,%u,%u,%u,%u,%u] OrigTensor[%llu,%llu,%llu,%llu,%llu,%llu,%llu,%llu] rpad[%u,%u,%u,%u,%u,%u,%u,%u] -> DstTensor[%llu,%llu,%llu,%llu,%llu,%llu,%llu,%llu]\n"
+ "BNNS POOLING PARAMETERS: %s%s%s"
+ "BNNS Permute Filter Warning: Batch size is 0, nothing to do"
+ "BNNS Pooling Filter Warning: Batch size is 0, nothing to do"
+ "BNNS Pooling: optimized code supports kernel width/height up to 16"
+ "BNNS REDUCTION %s%s%s"
+ "BNNS RESIZE %s%s%s"
+ "BNNS SPARSE CONVOLUTION: -one_line %s%s%s"
+ "BNNS SPARSE FULLY CONNECTED PARAMETERS: %s%s%s"
+ "BNNS Sparse Fully Connected: dense mla codepath available, better to use dense mla than sparse neon"
+ "BNNS Sparse Fully Connected: out of scratch memory to encode"
+ "BNNS TRANSPOSED CONVOLUTION: -one_line -trans %s%s%s"
+ "BNNS VECTOR TRANSPOSED CONVOLUTION: -one_line -trans %s%s%s"
+ "BNNS VIO Pose: Unexpected Tensor Sizes\n"
+ "BNNS VIO Pose: missing %s"
+ "BNNS VIO Pose: wrong size"
+ "BNNS, Warning, Trying to generate IR for a zero-sized input"
+ "BNNS: BNNS: Bad Kernel ID."
+ "BNNS: BNNSGraphContextExecute called before streaming advance size has been set through call to BNNSGraphContextSetStreamingAdvanceCount."
+ "BNNS: DEBUG INTERMEDIATE TYPE %u DOES NOT MATCH TENSOR TYPE %u\n"
+ "BNNS: FAILED TO COMPARE TENSOR OF TYPE %u\n"
+ "BNNS: Your BNNS-IR file contains a bug and must be recompiled with a newer compiler"
+ "BNNS: input/output data type not supported"
+ "BNNS: wrong varaitn chosen"
+ "BNNSGraphCompileOptionsGetShapeBounding"
+ "BNNSGraphCompileOptionsSetShapeBounding"
+ "BNNSGraphContextSetAdvancedWorkspaceReuse"
+ "BNNSGraphContextSetStreamingAdvanceCount"
+ "BNNSGraphContextSetStreamingAdvanceCount may only be used with context created using BNNSGraphContextMakeStreaming"
+ "BNNS_GRAPH: error with workspace"
+ "BNNS_GRAPH: too many offsets"
+ "BNNS_GRAPH: unexpected number of threads"
+ "BasicNeuralNetworkSubroutines-1497.100.141~155"
+ "C %s %.2f * %sA%s %s%s%s"
+ "COPY %zu %zu %zu %zu (%zu %zu %zu %zu) <-- %zu %zu %zu %zu (%zu %zu %zu %zu)"
+ "ConvTranspose Dynamic: Shape deduction received insufficient number of output tensors."
+ "Convolution weight delta compute warning - batch size is 0, nothing to do"
+ "Copy"
+ "D"
+ "D "
+ "DFT"
+ "Failed to allocate workspace of size %zu for subgraph %u\n"
+ "I:"
+ "I: %zu %zu %zu O: %zu %zu %zu K: %zu %zu TYPE: %s %s %s"
+ "I: %zu %zu %zu O: %zu %zu %zu K: %zu %zu TYPE: %s %s %s DILATION: %zu %zu"
+ "I: %zu O: %zu TYPE: %s %s %s TRANS: %c"
+ "KERNEL_ARITHMETIC_BINARY_DYNAMIC_MLA2"
+ "KERNEL_ARITHMETIC_BINARY_DYNAMIC_MLA3"
+ "KERNEL_ARITHMETIC_BINARY_DYNAMIC_SME2"
+ "KERNEL_ARITHMETIC_SVADD_BF16_MLA3"
+ "KERNEL_ARITHMETIC_SVADD_BF16_SME2"
+ "KERNEL_ARITHMETIC_SVADD_FP16_MLA2"
+ "KERNEL_ARITHMETIC_SVADD_FP16_MLA3"
+ "KERNEL_ARITHMETIC_SVADD_FP32_MLA2"
+ "KERNEL_ARITHMETIC_SVADD_FP32_MLA3"
+ "KERNEL_ARITHMETIC_SVADD_FP32_SME2"
+ "KERNEL_ARITHMETIC_SVMUL_BF16_MLA3"
+ "KERNEL_ARITHMETIC_SVMUL_BF16_SME2"
+ "KERNEL_ARITHMETIC_SVMUL_FP16_MLA2"
+ "KERNEL_ARITHMETIC_SVMUL_FP16_MLA3"
+ "KERNEL_ARITHMETIC_SVMUL_FP32_MLA2"
+ "KERNEL_ARITHMETIC_SVMUL_FP32_MLA3"
+ "KERNEL_ARITHMETIC_SVMUL_FP32_SME2"
+ "KERNEL_ARITHMETIC_UPCONVERT_ADD"
+ "KERNEL_ARITHMETIC_VADD_BF16_MLA3"
+ "KERNEL_ARITHMETIC_VADD_BF16_SME2"
+ "KERNEL_ARITHMETIC_VADD_FP16_MLA2"
+ "KERNEL_ARITHMETIC_VADD_FP16_MLA3"
+ "KERNEL_ARITHMETIC_VADD_FP32_MLA2"
+ "KERNEL_ARITHMETIC_VADD_FP32_MLA3"
+ "KERNEL_ARITHMETIC_VADD_FP32_SME2"
+ "KERNEL_ARITHMETIC_VMUL_BF16_MLA3"
+ "KERNEL_ARITHMETIC_VMUL_BF16_SME2"
+ "KERNEL_ARITHMETIC_VMUL_FP16_MLA2"
+ "KERNEL_ARITHMETIC_VMUL_FP16_MLA3"
+ "KERNEL_ARITHMETIC_VMUL_FP32_MLA2"
+ "KERNEL_ARITHMETIC_VMUL_FP32_MLA3"
+ "KERNEL_ARITHMETIC_VMUL_FP32_SME2"
+ "KERNEL_ARITHMETIC_VSADD_BF16_MLA3"
+ "KERNEL_ARITHMETIC_VSADD_BF16_SME2"
+ "KERNEL_ARITHMETIC_VSADD_FP16_MLA2"
+ "KERNEL_ARITHMETIC_VSADD_FP16_MLA3"
+ "KERNEL_ARITHMETIC_VSADD_FP32_MLA2"
+ "KERNEL_ARITHMETIC_VSADD_FP32_MLA3"
+ "KERNEL_ARITHMETIC_VSADD_FP32_SME2"
+ "KERNEL_ARITHMETIC_VSMUL_BF16_MLA3"
+ "KERNEL_ARITHMETIC_VSMUL_BF16_SME2"
+ "KERNEL_ARITHMETIC_VSMUL_FP16_MLA3"
+ "KERNEL_ARITHMETIC_VSMUL_FP32_MLA2"
+ "KERNEL_ARITHMETIC_VSMUL_FP32_MLA3"
+ "KERNEL_ARITHMETIC_VSMUL_FP32_SME2"
+ "KERNEL_ATTENTION"
+ "KERNEL_CAST_SME2_0"
+ "KERNEL_CHANNELNORM_V2_SME2"
+ "KERNEL_CHANNELNORM_V2_ST_SME2"
+ "KERNEL_CONV_3D_SME2_0"
+ "KERNEL_CONV_KERNELPOOL_FACETIME_VOICE_ISOLATION_MLA1_Mx1x32OC_x_Mx1x8IC_FP16_NON_CONTIG_INOUT"
+ "KERNEL_CONV_KERNELPOOL_FACETIME_VOICE_ISOLATION_MLA1_Mx1x64OC_x_Mx1x8IC_FP16_NON_CONTIG_INOUT"
+ "KERNEL_CONV_KERNELPOOL_FACETIME_VOICE_ISOLATION_MLA2_Mx1x32OC_x_Mx1x8IC_FP16_NON_CONTIG_INOUT"
+ "KERNEL_CONV_KERNELPOOL_FACETIME_VOICE_ISOLATION_MLA2_Mx1x64OC_x_Mx1x8IC_FP16_NON_CONTIG_INOUT"
+ "KERNEL_CONV_KERNELPOOL_FACETIME_VOICE_ISOLATION_NEON_32x1x64OC_x_32x1x8IC_FP16_NON_CONTIG_INOUT"
+ "KERNEL_CONV_SME2"
+ "KERNEL_CONV_SME2_DYNAMIC"
+ "KERNEL_CONV_TRANS_1D_FP32_FP32_ACCUM_SME2_0"
+ "KERNEL_CONV_TRANS_2D_FP32_FP32_ACCUM_SME2_0"
+ "KERNEL_CONV_TRANS_3D_INTEL"
+ "KERNEL_CONV_TRANS_3D_SME2_0"
+ "KERNEL_COPY_CONCAT_DYNAMIC_MLA_OR_NEON"
+ "KERNEL_COPY_CONCAT_DYNAMIC_SME2_0_OR_NEON"
+ "KERNEL_COPY_SLICE_UPDATE_ALIAS_DYNAMIC"
+ "KERNEL_COPY_SLICE_UPDATE_SME2_0"
+ "KERNEL_COPY_SME2_0"
+ "KERNEL_EINSUM_STRIDED_MATMUL_SME2_DYNAMIC"
+ "KERNEL_GRU_SME"
+ "KERNEL_LSTM_MERGED_WEIGHTS_SME"
+ "KERNEL_LSTM_SME"
+ "KERNEL_MATMUL_ELEMENTWISE_ACTIVATION_MATMUL"
+ "KERNEL_MATMUL_SME2"
+ "KERNEL_MATMUL_SME2_DYNAMIC"
+ "KERNEL_MATMUL_SOFTMAX_MATMUL"
+ "KERNEL_MATVEC_MLA1"
+ "KERNEL_MATVEC_MLA1_DYNAMIC"
+ "KERNEL_MATVEC_MLA2"
+ "KERNEL_MATVEC_MLA2_DYNAMIC"
+ "KERNEL_MATVEC_MLA3"
+ "KERNEL_MATVEC_MLA3_DYNAMIC"
+ "KERNEL_MATVEC_NEON"
+ "KERNEL_MATVEC_NEON_DYNAMIC"
+ "KERNEL_MATVEC_SME2"
+ "KERNEL_MATVEC_SME2_DYNAMIC"
+ "KERNEL_REDUCE_FP_SUM"
+ "KERNEL_REDUCE_FP_SUM_SME"
+ "KERNEL_REDUCE_MIN_MAX"
+ "KERNEL_REDUCE_MIN_MAX_SME"
+ "KERNEL_RNN_MERGED_WEIGHTS_SME"
+ "KERNEL_RNN_SME"
+ "KERNEL_ROTATION"
+ "KERNEL_SDPA_MLA2"
+ "KERNEL_SDPA_MLA3"
+ "KERNEL_TRANSPOSE2D_MLA1"
+ "KERNEL_TRANSPOSE2D_MLA1_DYNAMIC"
+ "KERNEL_TRANSPOSE2D_MLA2"
+ "KERNEL_TRANSPOSE2D_MLA2_DYNAMIC"
+ "KERNEL_TRANSPOSE2D_MLA3"
+ "KERNEL_TRANSPOSE2D_MLA3_DYNAMIC"
+ "Key %s not found in user data."
+ "LSTM BWD %s%s"
+ "LSTM FWD %s%s"
+ "M:%d I: %llu %llu %llu %llu P: [%u %u] [%u %u] [%u %u] [%u %u]"
+ "MatMul"
+ "Pooling layer filter running slow path: stride=%zu,%zu kernel=%zu,%zu"
+ "S "
+ "SME 1D conv_transpose did not receive needed scratch space."
+ "SME 1D conv_transpose received insufficient scratch space."
+ "SME 1D conv_transpose: Unsupported data layout for weights tensor."
+ "SME 1D conv_transpose: bias data type must match output data type"
+ "SME 2D conv_transpose did not receive needed scratch space."
+ "SME 2D conv_transpose received insufficient scratch space."
+ "SME 2D conv_transpose: Unsupported data layout for weights tensor."
+ "SME Binary Arithmetic Kernels received invalid data pointers."
+ "SME svadd_fp32 kernel was called on problem size that doesn't merit SME usage."
+ "SME svmul_fp32 kernel was called on problem size that doesn't merit SME usage."
+ "SME vadd_fp32 kernel was called on problem size that doesn't merit SME usage."
+ "SME vmul_fp32 kernel was called on problem size that doesn't merit SME usage."
+ "SME vsadd_fp32 kernel was called on problem size that doesn't merit SME usage."
+ "SME vsmul_fp32 kernel was called on problem size that doesn't merit SME usage."
+ "StateMode=Streaming cannot determine streaming size and/or axis"
+ "StateMode=Streaming does not support dynamically sized states"
+ "Unsupported binary arithmetic kernel routine."
+ "Unsupported datatype for transpose2d_sme2"
+ "WINOGRAD"
+ "^T"
+ "attention params for "
+ "attention_mask"
+ "attn_mask"
+ "attn_wt"
+ "binary_arithmetic"
+ "bnns"
+ "bnns_internal.binary_arithmetic"
+ "bnns_internal.matmul"
+ "bnns_internal.softmax"
+ "combo_attention"
+ "combo_matmul_elementwise_activation_matmul"
+ "combo_matmul_softmax_matmul"
+ "combo_op_matmul_repacked("
+ "combo_rotation"
+ "comobo_rotation"
+ "copy_constant"
+ "currently sdpa does not support bool"
+ "dead_code_elimination"
+ "div_no_nan"
+ "error while executing dynamic concat."
+ "exec op %llu of %llu  %s\n"
+ "for(op = "
+ "gemm_intermediate"
+ "h16c"
+ "h16s"
+ "internal.combo_attention"
+ "internal.combo_matmul_elementwise_activation_matmul"
+ "internal.combo_matmul_softmax_matmul"
+ "internal.combo_rotation"
+ "internal.copy_constant"
+ "internal.read_state"
+ "invalid concurrent access to bnns_graph_context_t"
+ "inverse_with_eps"
+ "key"
+ "layout_workspace"
+ "layout_workspace(for: while op_id="
+ "log2"
+ "log_with_eps"
+ "lva(of:"
+ "lva_permute(of:"
+ "lva_reshape(of:"
+ "matmul_0_bias"
+ "matmul_0_weights"
+ "matmul_1_bias"
+ "matmul_1_weights"
+ "matmul_concat_bias("
+ "matvec: model compiled for hardware not available on this device"
+ "mul_no_nan"
+ "negate"
+ "number_of_heads"
+ "op%5zu%s %s: {"
+ "op:%s I:"
+ "operand type not supported by permute"
+ "out_bias"
+ "params"
+ "placeholder"
+ "qkt"
+ "qkv_bias"
+ "qkv_weights"
+ "query"
+ "recip"
+ "rms norm"
+ "rms_scale"
+ "rot_cos"
+ "rot_sin"
+ "rsqrt_with_eps"
+ "scaled_dot_product_attention"
+ "scaling_tensor"
+ "scrtach for elementwise activation matmul "
+ "scrtach for matmul softmax matmul "
+ "sdpa got different data types for k,q,v"
+ "sdpa only supports fp16, bf16, and fp32 operands"
+ "sdpa requires input and output types to match"
+ "sdpa(key="
+ "skipped op %llu of %llu (it was performed during shape pass)\n"
+ "sme"
+ "soft_wt"
+ "sqkt"
+ "subgraph planning: failed to execute shape shape pass op for subgraph %u on op %llu\n"
+ "transpose for matmul softmax matmul "
+ "trunc_div"
+ "trunc_remainder"
+ "tts nlp mha number of heads("
+ "tts_nlp_transposed("
+ "v32@?0Q8{?=^vQ}16"
+ "{ .control = %d, .size { %zu, %zu, %zu, %zu }, .rate = %f, .gain = %f } "
+ "{ .ndim = %zu, .size = {"
+ "{ .opstr = \"%s\", .zero_output = %c, .alpha = %f, .beta = %f, .apply_fn_type = %d, .sum = "
+ "{reshape for matmul output}"
+ "{reshape for matmul: "
+ "} param_offset=%llu param_size=%llu\n"
- " = modified_version_of("
- "%s: exec op %llu of %llu\n"
- ", for=matmul_bias)"
- ",style=filled,color=\".7 .3 1.0\""
- "BNNS Activation: Error NULL input/output pointer"
- "BNNS Arithmetic Filter Backward Internal: unsupported arithmetic function"
- "BNNS BNNSGraphContextMakeExperimentalStateSupport failed to allocate memory"
- "BNNS BNNSGraphContextMakeExperimentalStateSupport passed graph with unsupported ir_version %u"
- "BNNS BNNSGraphContextMakeExperimentalStateSupport passed invalid graph"
- "BNNS Graph: layer norm encountered NaN, execution failed"
- "BNNS Graph: layer norm op values changed, execution failed"
- "BNNS MHA: layer_params->key.weights.layout=%u is not compatible with low memory flag."
- "BNNS MHA: layer_params->query.weights.layout=%u is not compatible with low memory flag."
- "BNNS MHA: layer_params->value.weights.layout=%u is not compatible with low memory flag."
- "BNNS Optimizer: unsupported optimizer function"
- "BNNS Quantize: wrong call"
- "BNNS SPMPR APPLY: failed to calculate Fully Connected\n"
- "BNNS: BNNSGraphExecute does not support inout tensors"
- "BNNS: Bad Kernel ID\n"
- "BNNS: matvec: repack: couldn't sparsify weights"
- "BNNSCopy: Cannot perform type conversion as part of opaque repack"
- "BNNSCopy: Unsupported repack operation from layout=%u to layout=%u"
- "BNNSCopy: src->size[%zu]=%zu is not equal to dest->size[%zu]=%zu"
- "BNNSGraphContextMake: does not support graphs with intent inout tensors"
- "BNNSGraphContextMakeExperimentalStateSupport"
- "BasicNeuralNetworkSubroutines-1361.60.15~1228"
- "Encountered invalid data type for begin."
- "Encountered invalid data type for size."
- "KERNEL_CONV_CREATE_APPLY"
- "KERNEL_CONV_CUSTOMIZED_VOICE_ISOLATION_FP32_S1x1_O12x1x16N_I12x1xM_K1x1_P0x0x0x0_GK"
- "KERNEL_CONV_FC"
- "KERNEL_CONV_LOW_MEM"
- "KERNEL_COPY_CONCAT_DYNAMIC"
- "KERNEL_MATMUL_MLA3_GRP_INT4"
- "KERNEL_MGLM_MHA_PART0_V1_BF16_FP32_MLA3"
- "KERNEL_MGLM_MHA_PART1_V1"
- "KERNEL_MGLM_MHA_PART1_V1_BF16_FP32_1x32x64_MLA3"
- "NEAREST_NEIGHBOR"
- "Specified opaque layout is not supported for use with specified contraction operation\n"
- "StateMode must match on all functions (or be disabled)"
- "Unsupported repack operation from layout=%u to layout=%u\n"
- "Unsupported shape deduction for tensor with count of selected boxes."
- "_train"
- "add_bias(of:"
- "contiguous_scratch_size(for:"
- "exec op %llu of %llu\n"
- "ijk"
- "jki"
- "literal(for:"
- "mtmul_concat_bias("
- "mutable_dep"
- "op%5zu %s: {"
- "permute only supports fp16, bf16 and fp32 operands"
- "probs"
- "temp_desc"
- "valid"
- "} param_offset=%llu param_size=%llu"

```
