## VisageHRTF

> `/System/Library/PrivateFrameworks/VisageHRTF.framework/VisageHRTF`

```diff

-1.9.14.0.0
-  __TEXT.__text: 0x127608
-  __TEXT.__auth_stubs: 0x1a40
-  __TEXT.__objc_methlist: 0x1f90
-  __TEXT.__gcc_except_tab: 0x105ec
-  __TEXT.__cstring: 0x850e
-  __TEXT.__const: 0x12f72
-  __TEXT.__oslogstring: 0x127c
-  __TEXT.__unwind_info: 0x6db8
+1.9.19.0.0
+  __TEXT.__text: 0x12a7bc
+  __TEXT.__auth_stubs: 0x19b0
+  __TEXT.__objc_methlist: 0x1f7c
+  __TEXT.__cstring: 0x869e
+  __TEXT.__const: 0x13035
+  __TEXT.__oslogstring: 0x10bb
+  __TEXT.__gcc_except_tab: 0xffbc
+  __TEXT.__unwind_info: 0x59c0
   __TEXT.__eh_frame: 0x88
-  __TEXT.__objc_classname: 0x5e9
-  __TEXT.__objc_methname: 0x4b62
-  __TEXT.__objc_methtype: 0x1421
-  __TEXT.__objc_stubs: 0x38a0
-  __DATA_CONST.__got: 0x3d8
-  __DATA_CONST.__const: 0xdd0
-  __DATA_CONST.__objc_classlist: 0x1d8
+  __TEXT.__objc_classname: 0x565
+  __TEXT.__objc_methname: 0x437b
+  __TEXT.__objc_methtype: 0x1794
+  __TEXT.__objc_stubs: 0x3120
+  __DATA_CONST.__got: 0x3c8
+  __DATA_CONST.__const: 0xd88
+  __DATA_CONST.__objc_classlist: 0x1b0
   __DATA_CONST.__objc_catlist: 0x8
-  __DATA_CONST.__objc_protolist: 0x28
+  __DATA_CONST.__objc_protolist: 0x18
   __DATA_CONST.__objc_imageinfo: 0x8
-  __DATA_CONST.__objc_const: 0x3af8
-  __DATA_CONST.__objc_selrefs: 0x1138
-  __DATA_CONST.__objc_arraydata: 0x70
-  __AUTH_CONST.__const: 0x9210
-  __AUTH_CONST.__cfstring: 0x1740
-  __AUTH_CONST.__objc_floatobj: 0x30
-  __AUTH_CONST.__objc_const: 0x1558
+  __DATA_CONST.__objc_const: 0x35a0
+  __DATA_CONST.__objc_selrefs: 0xef8
   __AUTH_CONST.__auth_ptr: 0x10
+  __AUTH_CONST.__const: 0x93c8
+  __AUTH_CONST.__cfstring: 0x1400
   __AUTH_CONST.__objc_intobj: 0xc0
-  __AUTH_CONST.__objc_doubleobj: 0x10
-  __AUTH_CONST.__objc_dictobj: 0x28
-  __AUTH_CONST.__auth_got: 0xd38
+  __AUTH_CONST.__objc_floatobj: 0x30
+  __AUTH_CONST.__objc_const: 0x14c8
+  __AUTH_CONST.__auth_got: 0xcf0
   __AUTH.__data: 0x10
-  __AUTH.__objc_data: 0x1270
+  __AUTH.__objc_data: 0x10e0
   __DATA.__got_weak: 0x8
-  __DATA.__objc_classrefs: 0x248
-  __DATA.__objc_superrefs: 0xf8
-  __DATA.__objc_ivar: 0x374
-  __DATA.__data: 0x13d0
+  __DATA.__objc_classrefs: 0x210
+  __DATA.__objc_superrefs: 0xe0
+  __DATA.__objc_ivar: 0x378
+  __DATA.__data: 0x1310
   __DATA.__crash_info: 0x40
-  __DATA.__bss: 0x218
+  __DATA.__bss: 0x220
+  __DATA.__common: 0x18
   - /System/Library/Frameworks/Accelerate.framework/Accelerate
   - /System/Library/Frameworks/CoreFoundation.framework/CoreFoundation
   - /System/Library/Frameworks/CoreGraphics.framework/CoreGraphics

   - /usr/lib/libSystem.B.dylib
   - /usr/lib/libc++.1.dylib
   - /usr/lib/libobjc.A.dylib
-  Functions: 5370
-  Symbols:   661
-  CStrings:  2138
+  Functions: 4245
+  Symbols:   643
+  CStrings:  1942
 
Symbols:
+ _MGGetSInt32Answer
+ _OBJC_CLASS_$_VNGeneratePersonSegmentationRequest
+ __DefaultRuneLocale
+ __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE6appendEmc
+ __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE6insertEmPKcm
+ __ZNSt3__19to_stringEd
+ __ZNSt3__19to_stringEf
+ ___maskrune
+ __os_feature_enabled_impl
+ _kCVAFaceTracking_Deterministic
+ _malloc_type_aligned_alloc
+ _strtod
+ _strtof
+ _vDSP_dotpr
+ _vDSP_meanv
+ _vImageHistogramCalculation_Planar8
- _CFEqual
- _CGAffineTransformMakeScale
- _CGImageRelease
- _NSLog
- _OBJC_CLASS_$_NSConstantDictionary
- _OBJC_CLASS_$_NSConstantDoubleNumber
- _OBJC_CLASS_$_NSDate
- _OBJC_CLASS_$_VNClassifyFaceAttributesRequest
- _OBJC_CLASS_$_VNDetectFaceCaptureQualityRequest
- _OBJC_CLASS_$_VNDetectFaceLandmarksRequest
- _OBJC_CLASS_$_VNDetectFaceRectanglesRequest
- _OBJC_CLASS_$_VNSequenceRequestHandler
- _VNFaceAttributeEyesClosed
- _VNFaceAttributeSmiling
- __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE6appendEPKc
- __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE6assignEPKc
- __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE6insertEmPKc
- __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE7reserveEm
- __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE9__grow_byEmmmmmm
- __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEC1ERKS5_
- __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEC1ERKS5_mmRKS4_
- __ZNSt3__113basic_ostreamIcNS_11char_traitsIcEEElsEPKv
- _espresso_buffer_pack_tensor_shape
- _espresso_network_bind_cvpixelbuffer
- _kCVPixelBufferPixelFormatDescriptionKey
- _kIOSurfaceCreationProperties
- _objc_opt_respondsToSelector
- _objc_storeWeak
- _os_release
- _os_workgroup_attr_set_flags
- _os_workgroup_cancel
- _os_workgroup_create
- _vImageConvert_PlanarFtoPlanar16F
- _vImagePermuteChannels_ARGB8888
CStrings:
+ "\x02\x11\xf0\x81\xf0\x81\xf0\xa1\xf0\x81\xf0\x81"
+ " Failed to create earPoseDetector. "
+ " Frame#%zu Face yaw for ear pose %f "
+ " Frame#%zu rejected because face too far, current distance: %f "
+ " Frame#%zu rejected because of low contrast "
+ " Frame#%zu rejected by pose estimator because of box out of image bounds. "
+ " Frame#%zu rejected by pose estimator because of front facing view, yaw = %f, limit = %f "
+ " Pixel format: %@ not supported by isLowContrast function "
+ " face confidence %f yaw %f pitch %f "
+ " vImageHistogramCalculation_Planar8 failed in isLowContrast function with error: %ld "
+ "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Essentials/Thread/src/DispatchQueue.cpp"
+ "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Essentials/Thread/src/DispatchQueueTypeUtil.cpp"
+ "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Visage/EarFrameSelection/src/LandmarkPCA.mm"
+ "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Visage/EarFrameSelection/src/VGEarPCAFrameSelector.mm"
+ "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Visage/EarFrameSelection/src/VGEarPCAPoseDetector.mm"
+ "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Visage/HRTFModel/src/ImagePreprocessing.mm:311"
+ "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Visage/HRTFModel/src/ImagePreprocessing.mm:456"
+ "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Visage/VHRTF/src/HRTFAPI/EarPCACapture.mm"
+ "/ear_detection_mlmodels/landmark_pca/lm_pca_mean.bin"
+ "/ear_detection_mlmodels/landmark_pca/lm_pca_weights.bin"
+ "4"
+ "@\"VGEarPCACaptureOptions\""
+ "@\"VGEarPCAPoseDetector\""
+ "CameraIMUDistanceType"
+ "CreationProperties"
+ "EarPCACapture"
+ "Face not at center of camera view"
+ "Face too far"
+ "Failed to get person segmentation map from Vision."
+ "GraphicsFeatureSetClass"
+ "GraphicsFeatureSetFallbacks"
+ "HeadphoneFeatures"
+ "IOSurface CreationProperties are nil"
+ "Input image size (%u x %u) Frame count threshold %u Ear bbox detection visibility threshold %f Ear landmark detection visibility threshold %f Use motion blur filter %@ Motion blur filter threshold %fFace yaw limit %f"
+ "Invalid input data size for PCA matrix."
+ "Invalid input data size for PCA mean vector."
+ "PSAEnrollmentSingleStep"
+ "TB,N,V_enableAggressorDetection"
+ "TB,N,V_leftEarCompleted"
+ "TB,N,V_rightEarCompleted"
+ "TI,N,V_frameCountThreshold"
+ "TI,N,V_leftFrameCount"
+ "TI,N,V_rightFrameCount"
+ "TQ,N,V_status"
+ "Tf,N,V_confidence"
+ "Tf,N,V_faceYawLimit"
+ "Ti,N,V_selectionError"
+ "T{VGEarPose=i{array<float, 3UL>=[3f]}{optional<float>=(?=cf)B}{?=[4]}ff{BoundingBox=ffff}{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}},N,V_pose"
+ "VGCoderUtilities.mm"
+ "VGEarPCACaptureOptions"
+ "VGEarPCAFrameSelector"
+ "VGEarPCAPoseDetector"
+ "Vision request handler failed to instantiate."
+ "_confidence"
+ "_enableAggressorDetection"
+ "_faceYawLimit"
+ "_frameCountThreshold"
+ "_leftEarCompleted"
+ "_leftEarGroup"
+ "_lm_pca_model"
+ "_rightEarCompleted"
+ "_rightEarGroup"
+ "_selectionError"
+ "_status"
+ "c09110f5b0dd11a4cb87402ac66909fe3b056b75"
+ "cannot use push_back() with "
+ "color_height > 0"
+ "color_width > 0"
+ "com.apple.VisageHRTF"
+ "com.apple.cv3d.test"
+ "defaultFaceYawLimit"
+ "dispatch_queue"
+ "earLandmarkVisibility"
+ "earPose && \"Unallocated earPose.\""
+ "ear_pca_capture_expected && \"Failed to create ear capture object.\""
+ "enableAggressorDetection"
+ "faceYawLimit"
+ "faceYawLimitForEarPose"
+ "frameCountThreshold"
+ "isLowContrast:"
+ "leftEarCompleted"
+ "leftFrameCount"
+ "numberOfEarFrame"
+ "posesFromGroup:"
+ "rightEarCompleted"
+ "rightFrameCount"
+ "selectionError"
+ "setConfidence:"
+ "setEnableAggressorDetection:"
+ "setFaceYawLimit:"
+ "setFrameCountThreshold:"
+ "setLeftEarCompleted:"
+ "setLeftFrameCount:"
+ "setOutputPixelFormat:"
+ "setQualityLevel:"
+ "setRightEarCompleted:"
+ "setRightFrameCount:"
+ "setSelectionError:"
+ "setStatus:"
+ "size[0] > 0"
+ "size[1] > 0"
+ "status"
+ "v192@0:8{VGEarPose=i{array<float, 3UL>=[3f]}{optional<float>=(?=cf)B}{?=[4]}ff{BoundingBox=ffff}{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}}16"
+ "{EarFrameGroup=\"frames_\"{array<VGEarFrame, 3UL>=\"__elems_\"[3{VGEarFrame=\"captureData\"@\"VGCaptureData\"\"pose\"{VGEarPose=\"side\"i\"lm_pca\"{array<float, 3UL>=\"__elems_\"[3f]}\"faceYawAngle\"{optional<float>=\"\"(?=\"__null_state_\"c\"__val_\"f)\"__engaged_\"B}\"pose\"{?=\"columns\"[4]}\"yawAngle\"f\"pitchAngle\"f\"box\"{BoundingBox=\"xmin\"f\"ymin\"f\"xmax\"f\"ymax\"f}\"landmark_points\"{vector<float, std::allocator<float>>=\"__begin_\"^f\"__end_\"^f\"__end_cap_\"{__compressed_pair<float *, std::allocator<float>>=\"__value_\"^f}}\"landmark_conf\"{vector<float, std::allocator<float>>=\"__begin_\"^f\"__end_\"^f\"__end_cap_\"{__compressed_pair<float *, std::allocator<float>>=\"__value_\"^f}}}}]}\"area_\"f}"
+ "{VGEarPose=\"side\"i\"lm_pca\"{array<float, 3UL>=\"__elems_\"[3f]}\"faceYawAngle\"{optional<float>=\"\"(?=\"__null_state_\"c\"__val_\"f)\"__engaged_\"B}\"pose\"{?=\"columns\"[4]}\"yawAngle\"f\"pitchAngle\"f\"box\"{BoundingBox=\"xmin\"f\"ymin\"f\"xmax\"f\"ymax\"f}\"landmark_points\"{vector<float, std::allocator<float>>=\"__begin_\"^f\"__end_\"^f\"__end_cap_\"{__compressed_pair<float *, std::allocator<float>>=\"__value_\"^f}}\"landmark_conf\"{vector<float, std::allocator<float>>=\"__begin_\"^f\"__end_\"^f\"__end_cap_\"{__compressed_pair<float *, std::allocator<float>>=\"__value_\"^f}}}"
+ "{VGEarPose=i{array<float, 3UL>=[3f]}{optional<float>=(?=cf)B}{?=[4]}ff{BoundingBox=ffff}{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}}16@0:8"
+ "{unique_ptr<cv3d::visage::ear_frame_selection::EarLandmarkPCAModel, std::default_delete<cv3d::visage::ear_frame_selection::EarLandmarkPCAModel>>=\"__ptr_\"{__compressed_pair<cv3d::visage::ear_frame_selection::EarLandmarkPCAModel *, std::default_delete<cv3d::visage::ear_frame_selection::EarLandmarkPCAModel>>=\"__value_\"^{EarLandmarkPCAModel}}}"
+ "{vector<VGEarFrame, std::allocator<VGEarFrame>>=^{VGEarFrame}^{VGEarFrame}{__compressed_pair<VGEarFrame *, std::allocator<VGEarFrame>>=^{VGEarFrame}}}16@0:8"
+ "{vector<VGEarFrame, std::allocator<VGEarFrame>>=^{VGEarFrame}^{VGEarFrame}{__compressed_pair<VGEarFrame *, std::allocator<VGEarFrame>>=^{VGEarFrame}}}24@0:8r^v16"
- "\x01A"
- "\x04\x81"
- "\x11"
- "\x12\x11"
- "\x16"
- " Espresso error: %s "
- " Face detection: confidence %f, quality %f "
- " Face detection: highConfidence %d, highQuality %d, expression detection %d, smiling %d, eyesClosed %d, mouthOpen %d "
- " Failed create CVPixelBuffer for direct binding "
- " Failed direct bind IOSurface "
- " Failed to write DTF debug data, %d "
- " Failed to write DTFwithMetadata  debug data, %d "
- " IOSurface channels %zu != %zu "
- " IOSurface height %zu != %zu "
- " IOSurface provided for VGMLEspresso buffer output is empty. "
- " IOSurface width %zu != %zu "
- " IOSurface(s) not consistent with the espresso output buffer. "
- " No modelpath with info %@ "
- " VGFaceEnrollment completed "
- " VGFaceEnrollment started "
- " VNImageRequestHandler : performRequests error %@ "
- " error face detection : %@ "
- " error face landmarks : %@ "
- " error face quality : %@ "
- " image rejected : confidence %f quality %f smiling %d eyes closed %d mouth open %d "
- " image written %@ "
- " processPixelBuffer : pixelBuffer == nil "
- " roll %@ yaw %@ BB %@ image %@ "
- " yaw %f pitch %f "
- "#16@0:8"
- "$"
- "%@/%@/"
- "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Essentials/Thread/src/ThreadGroup.cpp"
- "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Essentials/Thread/src/WorkQueue.cpp"
- "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Essentials/Thread/src/WorkQueueTypeUtil.cpp"
- "/Library/Caches/com.apple.xbs/Sources/VisageHRTF/library/Visage/HRTFModel/src/ImagePreprocessing.mm:450"
- "/dtf.ir"
- "/dtf_ir_coeffs.data"
- "/dtf_usr_metadata.ir"
- "63899c5395cc79f61a2e04fbdfc9f78059405765"
- "<"
- "@\"<VGFaceDetectionDelegate>\""
- "@\"NSMutableArray\""
- "@\"NSString\"16@0:8"
- "@\"NSUserDefaults\""
- "@\"VGFaceDetection\""
- "@\"VNClassifyFaceAttributesRequest\""
- "@\"VNDetectFaceCaptureQualityRequest\""
- "@\"VNDetectFaceLandmarksRequest\""
- "@\"VNDetectFaceRectanglesRequest\""
- "@\"VNFaceObservation\""
- "@\"VNSequenceRequestHandler\""
- "@24@0:8:16"
- "@32@0:8:16@24"
- "@32@0:8@16^@24"
- "@32@0:8@16^{__CVBuffer=}24"
- "@36@0:8^{?=^vi}16@24i32"
- "@40@0:8:16@24@32"
- "B24@0:8#16"
- "B24@0:8:16"
- "B24@0:8@\"Protocol\"16"
- "B32@0:8@16^{__CVBuffer=}24"
- "CVPixelBufferCreate Failed"
- "CVPixelBufferRef createCVPixelBuffer32BGRAFrom32ARGB(const CVPixelBufferRef)"
- "Empty ThreadGroup name."
- "Failed find person segmentation mask."
- "Failed to add user data to dtf ir data."
- "Failed to create dtf ir data."
- "Failed to create segmentation model."
- "NSObject"
- "Only supports IOSurface backed pixel buffers"
- "Pixel format not supported!"
- "T#,R"
- "T@\"NSDictionary\",R,N,V_outputLayersDictionary"
- "T@\"NSString\",R,C"
- "T@\"NSString\",R,N,V_networkFilePath"
- "TIFFRepresentation"
- "TQ,R"
- "TQ,R,N"
- "TQ,R,N,V_inputImageHeight"
- "TQ,R,N,V_inputImageWidth"
- "TQ,R,N,V_numInputs"
- "TQ,R,N,V_numOutputs"
- "TQ,R,N,V_outputSegmentationMapHeight"
- "TQ,R,N,V_outputSegmentationMapWidth"
- "T{VGEarPose=i{?=[4]}ff{BoundingBox=ffff}{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}},N,V_pose"
- "URLByAppendingPathComponent:"
- "VGFaceDetection"
- "VGFaceDetectionDelegate"
- "VGFaceEnrollment"
- "VGMLEspressoBuffer"
- "VGMLEspressoBuffer.mm"
- "VGMLEspressoModel"
- "VGMLFloatsToFloatsModel"
- "VGMLImageToFloatsModel"
- "VGMLImageToMultiFloatsModel"
- "VGMLSegmentationModel"
- "Vv16@0:8"
- "[5Q]"
- "\\u%04x\\u%04x"
- "\\ufffd"
- "^v"
- "^{?=^vi}"
- "^{_NSZone=}16@0:8"
- "^{__CVBuffer=}24@0:8@16"
- "__internal_DebugWriteImageFileOnDisk"
- "_buffer"
- "_compareFaceObservations:observation:"
- "_context"
- "_croppedImageWithFaceObservation:pixelBuffer:"
- "_currentStateToFetch"
- "_defaults"
- "_delegate"
- "_dimensions"
- "_engine"
- "_faceAttributesRequest"
- "_faceDetection"
- "_faceDetectionRequest"
- "_faceLandmarksRequest"
- "_faceObservation"
- "_faceQualityRequest"
- "_inputBuffers"
- "_inputImageHeight"
- "_inputImageWidth"
- "_inputLayerNames"
- "_isMouthOpen:"
- "_lastReason"
- "_layerName"
- "_mode"
- "_network"
- "_networkFilePath"
- "_numInputs"
- "_numOutputs"
- "_orientationForFaceObservation:"
- "_outputBuffers"
- "_outputLayerNameRemap"
- "_outputLayerNames"
- "_outputLayersDictionary"
- "_outputSegmentationMapHeight"
- "_outputSegmentationMapWidth"
- "_performFaceDetectionOnPixelBuffer:"
- "_performFaceRequestsIfNeededOnPixelBuffer:"
- "_plan"
- "_plan_flags"
- "_requestHandler"
- "_runModel:"
- "_state"
- "_storageFormat"
- "_updateEnrollementStatePixelBuffer:"
- "_validFaceObservation:pixelBuffer:"
- "_validImages"
- "_writeImageFileOnDisk"
- "arrayWithCapacity:"
- "autorelease"
- "bindIOSurface:"
- "bindImage:"
- "bindManagedBuffer"
- "bindTensor %zu != %zu"
- "bindTensor:"
- "boundingBox"
- "buffer has no data."
- "checkBufferAndIOSurfaceConsistency:"
- "class"
- "color_"
- "common_mlmodels/affogato"
- "config"
- "config.plist"
- "conformsToProtocol:"
- "context"
- "copyBufferIntoIOSurface:"
- "createCGImage:fromRect:"
- "dataWithLength:"
- "data_container_->group_ && \"Dispatch group is not initialized\""
- "data_container_->queue_ && \"Dispatch queue is not initialized\""
- "date"
- "debugDescription"
- "dimensions[3] (batch) %zu != %zu"
- "engine"
- "extent"
- "eyesCategory"
- "faceAttributes"
- "faceAttributesRequest"
- "faceCaptureQuality"
- "faceDetection:faceExpression:"
- "faceDetection:faceObservation:pixelBuffer:"
- "faceDetectionFailed:reason:"
- "faceDetectionRequest"
- "faceLandmarksRequest"
- "faceQualityRequest"
- "facial_hair"
- "failure"
- "firstObject"
- "getDimensions"
- "getInputDimensions:"
- "getInputSize"
- "getOutputDimensions:"
- "getOutputSize"
- "glasses"
- "glasses:0"
- "hair"
- "hair:0"
- "hasGlasses:"
- "hash"
- "identifier"
- "imageByApplyingTransform:"
- "imageHeight %zu != %zu"
- "imageWidth %zu != %zu"
- "imageWithCGImage:"
- "images"
- "img_%@_%lu.tiff"
- "inferInput:toOutput:"
- "inferInputImage:toOutput:"
- "inferInputs:"
- "inferInputs:toOutputs:"
- "inferModel"
- "initWithCVPixelBuffer:"
- "initWithCompletionHandler:"
- "initWithDelegate:"
- "initWithModelInfo:"
- "initWithModelPath:"
- "initWithModelPath:error:"
- "initWithNetwork:withLayerName:withMode:"
- "innerLips"
- "inputImageHeight"
- "inputImageWidth"
- "input_name"
- "isKindOfClass:"
- "isMemberOfClass:"
- "isProxy"
- "label"
- "landmarks"
- "model_path"
- "networkFilePath"
- "normalizedPoints"
- "nullopt"
- "numInputs"
- "numOutputs"
- "outputData"
- "outputLayersDictionary"
- "outputSegmentationMapHeight"
- "outputSegmentationMapWidth"
- "output_name"
- "performSelector:"
- "performSelector:withObject:"
- "performSelector:withObject:withObject:"
- "person"
- "person:0"
- "pointCount"
- "processPixelBuffer:"
- "processPixelBuffer:completion:"
- "q32@0:8@16@24"
- "r^Q16@0:8"
- "r^Q24@0:8Q16"
- "r^v16@0:8"
- "release"
- "respondsToSelector:"
- "ret == 0 && \"Failed to set workgroup flags.\""
- "retain"
- "retainCount"
- "roll"
- "segmentationMaps:"
- "self"
- "semantics-estimator/split_channels_1__2"
- "setFaceCoreExtractBlink:"
- "setFaceCoreExtractSmile:"
- "setFaceCoreMinFaceSize:"
- "setInputFaceObservations:"
- "setObject:forKey:"
- "setState:"
- "skin"
- "skin:0"
- "sky"
- "sky:0"
- "smilingCategory"
- "state"
- "superclass"
- "teeth"
- "teeth:0"
- "temporaryDirectory"
- "unsignedLongValue"
- "v144@0:8{VGEarPose=i{?=[4]}ff{BoundingBox=ffff}{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}}16"
- "v24@?0@\"VNRequest\"8@\"NSError\"16"
- "v32@0:8@\"VGFaceDetection\"16Q24"
- "v32@0:8@16@24"
- "v32@0:8@16Q24"
- "v32@0:8^{__CVBuffer=}16@24"
- "v32@0:8^{__CVBuffer=}16@?24"
- "v40@0:8@\"VGFaceDetection\"16@\"VNFaceObservation\"24^{__CVBuffer=}32"
- "v40@0:8@16@24^{__CVBuffer=}32"
- "vImagePermuteChannels_ARGB8888 Failed"
- "valueForKey:"
- "valueForKeyPath:"
- "vg_createDecodedPixelBufferForKey:"
- "vg_decodePixelBufferForKey:"
- "vg_encodeArrayOfSurfaces:forKey:"
- "vg_encodePixelBuffer:forKey:"
- "vg_encodeSurface:forKey:"
- "w %f h %f"
- "work_group_ != nullptr && \"Failed to create workgroup.\""
- "writeToURL:atomically:"
- "zone"
- "{?=\"data\"^v\"reserved\"^v\"dim\"[4Q]\"stride\"[4Q]\"width\"Q\"height\"Q\"channels\"Q\"batch_number\"Q\"sequence_length\"Q\"stride_width\"Q\"stride_height\"Q\"stride_channels\"Q\"stride_batch_number\"Q\"stride_sequence_length\"Q\"storage_type\"i}"
- "{?=\"plan\"^v\"network_index\"i}"
- "{VGEarPose=\"side\"i\"pose\"{?=\"columns\"[4]}\"yawAngle\"f\"pitchAngle\"f\"box\"{BoundingBox=\"xmin\"f\"ymin\"f\"xmax\"f\"ymax\"f}\"landmark_points\"{vector<float, std::allocator<float>>=\"__begin_\"^f\"__end_\"^f\"__end_cap_\"{__compressed_pair<float *, std::allocator<float>>=\"__value_\"^f}}}"
- "{VGEarPose=i{?=[4]}ff{BoundingBox=ffff}{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}}16@0:8"

```
