## CorePhotogrammetry

> `/System/Library/PrivateFrameworks/CorePhotogrammetry.framework/CorePhotogrammetry`

```diff

-3.8.0.0.0
-  __TEXT.__text: 0xf203c8
-  __TEXT.__auth_stubs: 0x3870
-  __TEXT.__init_offsets: 0x8
-  __TEXT.__gcc_except_tab: 0xa8c18
-  __TEXT.__const: 0x565bc
-  __TEXT.__cstring: 0x799ed
+3.9.1.0.0
+  __TEXT.__text: 0xf45d94
+  __TEXT.__auth_stubs: 0x3860
+  __TEXT.__init_offsets: 0xc
+  __TEXT.__gcc_except_tab: 0xaacac
+  __TEXT.__const: 0x5773c
+  __TEXT.__cstring: 0x7c75a
   __TEXT.__oslogstring: 0x487
-  __TEXT.__unwind_info: 0x30df0
+  __TEXT.__unwind_info: 0x31610
   __TEXT.__eh_frame: 0xfe0
-  __TEXT.__objc_methname: 0x179e
-  __TEXT.__objc_stubs: 0x1c40
-  __DATA_CONST.__got: 0x6e0
-  __DATA_CONST.__const: 0x1230
+  __TEXT.__objc_methname: 0x18f6
+  __TEXT.__objc_stubs: 0x1d60
+  __DATA_CONST.__got: 0x6f8
+  __DATA_CONST.__const: 0x12b0
   __DATA_CONST.__objc_imageinfo: 0x8
-  __DATA_CONST.__objc_selrefs: 0x710
-  __AUTH_CONST.__auth_got: 0x1c48
-  __AUTH_CONST.__const: 0x302f0
+  __DATA_CONST.__objc_selrefs: 0x758
+  __AUTH_CONST.__auth_got: 0x1c40
+  __AUTH_CONST.__const: 0x31310
   __AUTH_CONST.__cfstring: 0x700
   __AUTH_CONST.__objc_intobj: 0x60
   __AUTH.__data: 0x38
   __AUTH.__thread_vars: 0x60
   __AUTH.__thread_bss: 0x8038
-  __DATA.__data: 0x4064
+  __DATA.__data: 0x4084
   __DATA.__llvm_prf_cnts: 0x408
   __DATA.__llvm_prf_data: 0x1500
   __DATA.__llvm_prf_names: 0x1890
   __DATA.__crash_info: 0x148
-  __DATA.__bss: 0x7710
-  __DATA.__common: 0x1c0
+  __DATA.__bss: 0x77a8
+  __DATA.__common: 0x1d8
   __DATA_DIRTY.__common: 0x1d0
   __LLVM_COV.__llvm_covfun: 0xf318
   __LLVM_COV.__llvm_covmap: 0x448

   - /System/Library/PrivateFrameworks/Espresso.framework/Espresso
   - /System/Library/PrivateFrameworks/GraphComputeRT.framework/GraphComputeRT
   - /System/Library/PrivateFrameworks/LearnedFeatures.framework/LearnedFeatures
+  - /System/Library/PrivateFrameworks/MobileAsset.framework/MobileAsset
   - /System/Library/PrivateFrameworks/SignpostCollection.framework/SignpostCollection
   - /System/Library/PrivateFrameworks/SignpostSupport.framework/SignpostSupport
   - /usr/lib/libMobileGestalt.dylib

   - /usr/lib/libc++.1.dylib
   - /usr/lib/libobjc.A.dylib
   - /usr/lib/usd/libusd_ms.dylib
-  UUID: 41D258DC-B690-31AB-8343-968FAC9D90CB
-  Functions: 33310
-  Symbols:   3238
-  CStrings:  5124
+  UUID: 849F2D12-447A-3179-AA29-6280D29100F7
+  Functions: 33669
+  Symbols:   3240
+  CStrings:  5211
 
Symbols:
+ _OBJC_CLASS_$_MAAutoAsset
+ _OBJC_CLASS_$_MAAutoAssetPolicy
+ _OBJC_CLASS_$_MAAutoAssetSelector
- __ZNSt3__113shared_futureIvED1Ev
CStrings:
+ "\n\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT = int64_t,\n    int N_READS = REDUCE_N_READS>\n[[kernel]] void all_reduce(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& in_size [[buffer(2)]],\n    const constant size_t& row_size [[buffer(3)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  threadgroup U shared_vals[simd_size];\n  U total = Op::init;\n  IdxT start_idx = gid.y * IdxT(row_size);\n  IdxT actual_row =\n      (start_idx + row_size <= in_size) ? row_size : in_size - start_idx;\n  IdxT blocks = actual_row / (lsize.x * N_READS);\n  int extra = actual_row - blocks * (lsize.x * N_READS);\n  extra -= lid.x * N_READS;\n  start_idx += lid.x * N_READS;\n  in += start_idx;\n  if (extra >= N_READS) {\n    blocks++;\n    extra = 0;\n  }\n  for (IdxT b = 0; b < blocks; b++) {\n    for (int i = 0; i < N_READS; i++) {\n      total = op(static_cast<U>(in[i]), total);\n    }\n    in += lsize.x * N_READS;\n  }\n  if (extra > 0) {\n    for (int i = 0; i < extra; i++) {\n      total = op(static_cast<U>(in[i]), total);\n    }\n  }\n  total = op.simd_reduce(total);\n  if (simd_per_group > 1) {\n    if (simd_lane_id == 0) {\n      shared_vals[simd_group_id] = total;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    total = lid.x < simd_per_group ? shared_vals[lid.x] : op.init;\n    total = op.simd_reduce(total);\n  }\n  if (lid.x == 0) {\n    out[gid.y] = total;\n  }\n}\ntemplate <typename T, typename U, typename Op, typename IdxT, int NDIMS>\n[[kernel]] void col_reduce_small(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant int64_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant int64_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant int64_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]]) {\n  constexpr int n_reads = 4;\n  Op op;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  U totals[n_reads];\n  for (int i = 0; i < n_reads; i++) {\n    totals[i] = Op::init;\n  }\n  IdxT column = IdxT(gid.x) * lsize.x * n_reads + lid.x * n_reads;\n  if (column >= reduction_stride) {\n    return;\n  }\n  bool safe = column + n_reads <= reduction_stride;\n  IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n  IdxT in_idx = elem_to_loc<IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + column;\n  IdxT total_rows = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(lid.y, reduce_shape, reduce_strides);\n  for (IdxT r = lid.y; r < total_rows; r += lsize.y) {\n    row = in + loop.location();\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(static_cast<U>(row[i]), totals[i]);\n      }\n    } else {\n      U vals[n_reads];\n      for (int i = 0; i < n_reads; i++) {\n        vals[i] =\n            (column + i < reduction_stride) ? static_cast<U>(row[i]) : op.init;\n      }\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(vals[i], totals[i]);\n      }\n    }\n    loop.next(lsize.y, reduce_shape, reduce_strides);\n  }\n  if (lsize.y > 1) {\n    threadgroup U shared_vals[32 * 8 * n_reads];\n    for (int i = 0; i < n_reads; i++) {\n      shared_vals[lid.y * lsize.x * n_reads + lid.x * n_reads + i] = totals[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (lid.y == 0) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = shared_vals[lid.x * n_reads + i];\n      }\n      for (uint j = 1; j < lsize.y; j++) {\n        for (int i = 0; i < n_reads; i++) {\n          totals[i] =\n              op(shared_vals[j * lsize.x * n_reads + lid.x * n_reads + i],\n                 totals[i]);\n        }\n      }\n    }\n  }\n  if (lid.y == 0) {\n    out += out_idx * IdxT(reduction_stride) + column;\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        out[i] = totals[i];\n      }\n    } else {\n      for (int i = 0; column + i < reduction_stride; i++) {\n        out[i] = totals[i];\n      }\n    }\n  }\n}\ntemplate <typename T, typename U, typename Op, typename IdxT, int NDIMS>\n[[kernel]] void col_reduce_longcolumn(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant int64_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant int64_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    const constant size_t& out_size [[buffer(11)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]]) {\n  Op op;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  IdxT out_idx = gid.x + gsize.x * IdxT(gid.y);\n  IdxT in_idx = elem_to_loc<IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + lid.x;\n  U total = Op::init;\n  IdxT total_rows = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(gid.z * lsize.y + lid.y, reduce_shape, reduce_strides);\n  for (IdxT r = gid.z * lsize.y + lid.y; r < total_rows;\n       r += lsize.y * gsize.z) {\n    row = in + loop.location();\n    total = op(static_cast<U>(*row), total);\n    loop.next(lsize.y * gsize.z, reduce_shape, reduce_strides);\n  }\n  threadgroup U shared_vals[32 * 32];\n  shared_vals[lid.y * lsize.x + lid.x] = total;\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  if (lid.y == 0) {\n    for (uint i = 1; i < lsize.y; i++) {\n      total = op(total, shared_vals[i * lsize.x + lid.x]);\n    }\n    out[gid.z * IdxT(out_size) + out_idx * IdxT(reduction_stride) + lid.x] =\n        total;\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int BM,\n    int BN>\n[[kernel]] void col_reduce_looped(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant int64_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant int64_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant int64_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  constexpr int n_simdgroups = 8;\n  constexpr short tgp_size = n_simdgroups * simd_size;\n  constexpr short n_reads = (BM * BN) / tgp_size;\n  constexpr short n_read_blocks = BN / n_reads;\n  threadgroup U shared_vals[BN * BM];\n  U totals[n_reads];\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  for (int i = 0; i < n_reads; i++) {\n    totals[i] = Op::init;\n  }\n  short lid = simd_group_id * simd_size + simd_lane_id;\n  short2 offset((lid % n_read_blocks) * n_reads, lid / n_read_blocks);\n  IdxT column = BN * gid.x + offset.x;\n  bool safe = column + n_reads <= reduction_stride;\n  IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n  IdxT in_idx = elem_to_loc<IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + column;\n  IdxT total = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(offset.y, reduce_shape, reduce_strides);\n  for (IdxT r = offset.y; r < total; r += BM) {\n    row = in + loop.location();\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(static_cast<U>(row[i]), totals[i]);\n      }\n    } else {\n      U vals[n_reads];\n      for (int i = 0; i < n_reads; i++) {\n        vals[i] =\n            (column + i < reduction_stride) ? static_cast<U>(row[i]) : op.init;\n      }\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(vals[i], totals[i]);\n      }\n    }\n    loop.next(BM, reduce_shape, reduce_strides);\n  }\n  if (BM == 32) {\n    constexpr int n_outputs = BN / n_simdgroups;\n    static_assert(\n        BM != 32 || n_outputs == n_reads,\n        \"The tile should be selected such that n_outputs == n_reads\");\n    for (int i = 0; i < n_reads; i++) {\n      shared_vals[offset.y * BN + offset.x + i] = totals[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    short2 out_offset(simd_group_id * n_outputs, simd_lane_id);\n    for (int i = 0; i < n_outputs; i++) {\n      totals[i] =\n          op.simd_reduce(shared_vals[out_offset.y * BN + out_offset.x + i]);\n    }\n    if (simd_lane_id == 0) {\n      IdxT out_column = BN * gid.x + out_offset.x;\n      out += out_idx * IdxT(reduction_stride) + out_column;\n      if (out_column + n_outputs <= reduction_stride) {\n        for (int i = 0; i < n_outputs; i++) {\n          out[i] = totals[i];\n        }\n      } else {\n        for (int i = 0; out_column + i < reduction_stride; i++) {\n          out[i] = totals[i];\n        }\n      }\n    }\n  }\n  else {\n    short x_block = offset.x / n_reads;\n    for (int i = 0; i < n_reads; i++) {\n      shared_vals[x_block * BM * n_reads + i * BM + offset.y] = totals[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (offset.y == 0) {\n      for (int i = 0; i < n_reads; i++) {\n        for (int j = 1; j < BM; j++) {\n          totals[i] =\n              op(shared_vals[x_block * BM * n_reads + i * BM + j], totals[i]);\n        }\n      }\n    }\n    if (offset.y == 0) {\n      out += out_idx * IdxT(reduction_stride) + column;\n      if (safe) {\n        for (int i = 0; i < n_reads; i++) {\n          out[i] = totals[i];\n        }\n      } else {\n        for (int i = 0; column + i < reduction_stride; i++) {\n          out[i] = totals[i];\n        }\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int BM,\n    int BN>\n[[kernel]] void col_reduce_2pass(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant int64_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant int64_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant int64_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    const constant size_t& out_size [[buffer(11)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  constexpr int n_simdgroups = 8;\n  constexpr short tgp_size = n_simdgroups * simd_size;\n  constexpr short n_reads = (BM * BN) / tgp_size;\n  constexpr short n_read_blocks = BN / n_reads;\n  constexpr int n_outputs = BN / n_simdgroups;\n  constexpr short outer_blocks = 32;\n  static_assert(BM == 32, \"BM should be equal to 32\");\n  threadgroup U shared_vals[BN * BM];\n  U totals[n_reads];\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  for (int i = 0; i < n_reads; i++) {\n    totals[i] = Op::init;\n  }\n  short lid = simd_group_id * simd_size + simd_lane_id;\n  short2 offset((lid % n_read_blocks) * n_reads, lid / n_read_blocks);\n  IdxT column = BN * gid.x + offset.x;\n  bool safe = column + n_reads <= reduction_stride;\n  IdxT full_idx = gid.y + gsize.y * IdxT(gid.z);\n  IdxT block_idx = full_idx / IdxT(out_size);\n  IdxT out_idx = full_idx % IdxT(out_size);\n  IdxT in_idx = elem_to_loc<IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + column;\n  IdxT total = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(offset.y + block_idx * BM, reduce_shape, reduce_strides);\n  for (IdxT r = offset.y + block_idx * BM; r < total; r += outer_blocks * BM) {\n    row = in + loop.location();\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(static_cast<U>(row[i]), totals[i]);\n      }\n    } else {\n      U vals[n_reads];\n      for (int i = 0; i < n_reads; i++) {\n        vals[i] =\n            (column + i < reduction_stride) ? static_cast<U>(row[i]) : op.init;\n      }\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(vals[i], totals[i]);\n      }\n    }\n    loop.next(outer_blocks * BM, reduce_shape, reduce_strides);\n  }\n  for (int i = 0; i < n_reads; i++) {\n    shared_vals[offset.y * BN + offset.x + i] = totals[i];\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  short2 out_offset(simd_group_id * n_outputs, simd_lane_id);\n  for (int i = 0; i < n_outputs; i++) {\n    totals[i] =\n        op.simd_reduce(shared_vals[out_offset.y * BN + out_offset.x + i]);\n  }\n  if (simd_lane_id == 0) {\n    IdxT out_column = BN * gid.x + out_offset.x;\n    out += full_idx * IdxT(reduction_stride) + out_column;\n    if (out_column + n_outputs <= reduction_stride) {\n      for (int i = 0; i < n_outputs; i++) {\n        out[i] = totals[i];\n      }\n    } else {\n      for (int i = 0; out_column + i < reduction_stride; i++) {\n        out[i] = totals[i];\n      }\n    }\n  }\n}\ntemplate <typename T, typename Op>\n[[kernel]] void init_reduce(\n    device T* out [[buffer(0)]],\n    uint tid [[thread_position_in_grid]]) {\n  out[tid] = Op::init;\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void per_thread_row_reduce(\n    thread U totals[N_WRITES],\n    const device T* inputs[N_WRITES],\n    int blocks,\n    int extra,\n    uint lsize_x,\n    uint lid_x) {\n  Op op;\n  for (int i = 0; i < N_WRITES; i++) {\n    totals[i] = Op::init;\n  }\n  for (int i = 0; i < blocks; i++) {\n    for (int j = 0; j < N_WRITES; j++) {\n      for (int i = 0; i < N_READS; i++) {\n        totals[j] = op(static_cast<U>(inputs[j][i]), totals[j]);\n      }\n      inputs[j] += lsize_x * N_READS;\n    }\n  }\n  int index = lid_x * N_READS;\n  if (index + N_READS <= extra) {\n    for (int j = 0; j < N_WRITES; j++) {\n      for (int i = 0; i < N_READS; i++) {\n        totals[j] = op(static_cast<U>(inputs[j][i]), totals[j]);\n      }\n    }\n  } else {\n    for (int j = 0; j < N_WRITES; j++) {\n      for (int i = 0; index + i < extra; i++) {\n        totals[j] = op(static_cast<U>(inputs[j][i]), totals[j]);\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void per_thread_row_reduce(\n    thread U totals[N_WRITES],\n    const device T* in,\n    const constant size_t& reduction_size,\n    int blocks,\n    int extra,\n    uint lsize_x,\n    uint lid_x) {\n  const device T* inputs[N_WRITES];\n  inputs[0] = in + lid_x * N_READS;\n  for (int i = 1; i < N_READS; i++) {\n    inputs[i] = inputs[i - 1] + reduction_size;\n  }\n  per_thread_row_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, inputs, blocks, extra, lsize_x, lid_x);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void per_thread_row_reduce(\n    thread U totals[N_WRITES],\n    const device T* in,\n    const int64_t row_idx,\n    int blocks,\n    int extra,\n    const constant int* shape,\n    const constant int64_t* strides,\n    const constant int& ndim,\n    uint lsize_x,\n    uint lid_x) {\n  const device T* inputs[N_WRITES];\n  in += lid_x * N_READS;\n  for (int i = 0; i < N_READS; i++) {\n    inputs[i] = in + elem_to_loc(row_idx + i, shape, strides, ndim);\n  }\n  per_thread_row_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, inputs, blocks, extra, lsize_x, lid_x);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void threadgroup_reduce(\n    thread U totals[N_WRITES],\n    threadgroup U* shared_vals,\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  for (int i = 0; i < N_WRITES; i++) {\n    totals[i] = op.simd_reduce(totals[i]);\n  }\n  if (simd_per_group > 1) {\n    if (simd_lane_id == 0) {\n      for (int i = 0; i < N_WRITES; i++) {\n        shared_vals[simd_group_id * N_WRITES + i] = totals[i];\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    U values[N_WRITES];\n    for (int i = 0; i < N_WRITES; i++) {\n      values[i] = (lid.x < simd_per_group) ? shared_vals[lid.x * N_WRITES + i]\n                                           : op.init;\n    }\n    for (int i = 0; i < N_WRITES; i++) {\n      totals[i] = op.simd_reduce(values[i]);\n    }\n  }\n}\ntemplate <typename T, typename U, typename Op, int N_READS = REDUCE_N_READS>\nMETAL_FUNC void\nthread_reduce(thread U& total, const device T* row, int blocks, int extra) {\n  Op op;\n  for (int i = 0; i < blocks; i++) {\n    U vals[N_READS];\n    for (int j = 0; j < N_READS; j++) {\n      vals[j] = row[j];\n    }\n    for (int j = 0; j < N_READS; j++) {\n      total = op(vals[j], total);\n    }\n    row += N_READS;\n  }\n  for (int i = 0; i < extra; i++) {\n    total = op(*row++, total);\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int N_READS = REDUCE_N_READS>\n[[kernel]] void row_reduce_small(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant int64_t& row_size [[buffer(2)]],\n    const constant int64_t& non_row_reductions [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant int64_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant int64_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 tid [[thread_position_in_grid]],\n    uint3 tsize [[threads_per_grid]]) {\n  Op op;\n  U total_val = Op::init;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  int blocks = IdxT(row_size) / N_READS;\n  int extra = IdxT(row_size) % N_READS;\n  if ((non_row_reductions < 32 && row_size <= 8) || non_row_reductions <= 8) {\n    IdxT out_idx = tid.x + tsize.y * IdxT(tid.y);\n    in += elem_to_loc<IdxT>(out_idx, shape, strides, ndim);\n    for (uint r = 0; r < non_row_reductions; r++) {\n      row = in + loop.location();\n      thread_reduce<T, U, Op, N_READS>(total_val, row, blocks, extra);\n      loop.next(reduce_shape, reduce_strides);\n    }\n    out[out_idx] = total_val;\n  } else {\n    IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n    in += elem_to_loc<IdxT>(out_idx, shape, strides, ndim);\n    loop.next(simd_lane_id, reduce_shape, reduce_strides);\n    for (uint r = simd_lane_id; r < non_row_reductions; r += simd_size) {\n      row = in + loop.location();\n      thread_reduce<T, U, Op, N_READS>(total_val, row, blocks, extra);\n      loop.next(simd_size, reduce_shape, reduce_strides);\n    }\n    total_val = op.simd_reduce(total_val);\n    if (simd_lane_id == 0) {\n      out[out_idx] = total_val;\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT = int64_t,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\n[[kernel]] void row_reduce_simple(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant int64_t& out_size [[buffer(3)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  threadgroup U shared_vals[simd_size * N_WRITES];\n  U totals[N_WRITES];\n  IdxT out_idx = N_WRITES * (gid.y + gsize.y * IdxT(gid.z));\n  if (out_idx + N_WRITES > out_size) {\n    out_idx = out_size - N_WRITES;\n  }\n  in += out_idx * IdxT(reduction_size);\n  out += out_idx;\n  int blocks = IdxT(reduction_size) / (lsize.x * N_READS);\n  int extra = reduction_size - blocks * (lsize.x * N_READS);\n  per_thread_row_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, in, reduction_size, blocks, extra, lsize.x, lid.x);\n  threadgroup_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, shared_vals, lid, simd_lane_id, simd_per_group, simd_group_id);\n  if (lid.x == 0) {\n    for (int i = 0; i < N_WRITES; i++) {\n      out[i] = totals[i];\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int N_READS = REDUCE_N_READS>\n[[kernel]] void row_reduce_looped(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant int64_t& row_size [[buffer(2)]],\n    const constant int64_t& non_row_reductions [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant int64_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant int64_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  threadgroup U shared_vals[simd_size];\n  U total = Op::init;\n  IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n  in += elem_to_loc<IdxT>(out_idx, shape, strides, ndim) + lid.x * N_READS;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  int blocks = IdxT(row_size) / (lsize.x * N_READS);\n  int extra = row_size - blocks * (lsize.x * N_READS);\n  for (IdxT i = 0; i < non_row_reductions; i++) {\n    row = in + loop.location();\n    U row_total;\n    per_thread_row_reduce<T, U, Op, N_READS, 1>(\n        &row_total, &row, blocks, extra, lsize.x, lid.x);\n    total = op(total, row_total);\n    loop.next(reduce_shape, reduce_strides);\n  }\n  threadgroup_reduce<T, U, Op, N_READS, 1>(\n      &total, shared_vals, lid, simd_lane_id, simd_per_group, simd_group_id);\n  if (lid.x == 0) {\n    out[out_idx] = total;\n  }\n}\n"
+ "\n#if (__METAL_VERSION__ >= 310)\nusing namespace metal;\ntypedef bfloat bfloat16_t;\ninline uint16_t bfloat16_to_uint16(const bfloat16_t x) {\n  return as_type<uint16_t>(x);\n}\ninline bfloat16_t uint16_to_bfloat16(const uint16_t x) {\n  return as_type<bfloat16_t>(x);\n}\n\n #else\nusing namespace metal;\nconstexpr METAL_FUNC uint16_t float_to_bfloat_bits(float x) {\n  if ((as_type<uint32_t>(x) & ~_fp_encoding_traits<float>::sign_mask) >\n      _fp_encoding_traits<float>::inf_mask) {\n    return uint16_t(as_type<uint32_t>(0x7FC0));\n  }\n  uint32_t float_bits = as_type<uint32_t>(x);\n  float_bits += ((float_bits >> 16) & 1) + as_type<uint32_t>(0x7FFF);\n  return float_bits >> 16;\n}\nconstexpr METAL_FUNC float bfloat_bits_to_float(uint16_t x) {\n  return as_type<float>((uint32_t)x << 16);\n}\nstruct _MLX_BFloat16;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_to_bfloat =\n    !is_same_v<T, _MLX_BFloat16> && is_convertible_v<T, float>;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_from_bfloat =\n    !is_same_v<T, _MLX_BFloat16> && is_convertible_v<float, T>;\nstruct _MLX_BFloat16 {\n  uint16_t bits_;\n  _MLX_BFloat16() thread = default;\n  _MLX_BFloat16() threadgroup = default;\n  _MLX_BFloat16() device = default;\n  _MLX_BFloat16() constant = default;\n  struct bits_to_bfloat_struct {};\n  static constexpr METAL_FUNC bits_to_bfloat_struct bits_to_bfloat() {\n    return bits_to_bfloat_struct();\n  }\n  constexpr METAL_FUNC _MLX_BFloat16(uint16_t bits, bits_to_bfloat_struct)\n      : bits_(bits) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) thread\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) threadgroup\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) device\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) constant\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const thread {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const threadgroup {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const device {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const constant {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n};\nconstexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 x) {\n  return -static_cast<float>(x);\n}\nconstexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC float operator+(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC float operator+(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC float operator+(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC float operator+(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); };;\nconstexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC float operator-(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC float operator-(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC float operator-(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC float operator-(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); };;\nconstexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC float operator*(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC float operator*(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC float operator*(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC float operator*(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); };;\nconstexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC float operator/(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC float operator/(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC float operator/(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC float operator/(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator+=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator+=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator+=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator-=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator-=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator-=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator*=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator*=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator*=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator/=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator/=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator/=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator+=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator+=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator+=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator-=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator-=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator-=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator*=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator*=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator*=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator/=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator/=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator/=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator+=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator+=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator+=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator-=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator-=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator-=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator*=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator*=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator*=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator/=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator/=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator/=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator+=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator+=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator+=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator-=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator-=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator-=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator*=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator*=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator*=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator/=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator/=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator/=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator+=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator+=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator+=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator-=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator-=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator-=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator*=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator*=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator*=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator/=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator/=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator/=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator+=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator+=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator+=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator-=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator-=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator-=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator*=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator*=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator*=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator/=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator/=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator/=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator+=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator+=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator+=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator-=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator-=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator-=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator*=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator*=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator*=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator/=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator/=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator/=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator+=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator+=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator+=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator-=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator-=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator-=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator*=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator*=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator*=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator/=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator/=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator/=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;\ntypedef struct _MLX_BFloat16 bfloat16_t;\n#pragma METAL internals : enable\nnamespace metal {\ntemplate <>\nstruct _numeric_limits_impl<bfloat16_t> : _fp_numeric_limits_impl_base {\n  static constexpr constant int digits = 8;\n  static constexpr constant int digits10 = 2;\n  static constexpr constant int max_digits10 = 4;\n  static constexpr constant int radix = 2;\n  static constexpr constant int min_exponent = -125;\n  static constexpr constant int min_exponent10 = -37;\n  static constexpr constant int max_exponent = 128;\n  static constexpr constant int max_exponent10 = 38;\n  static constexpr bfloat16_t min() {\n    return _MLX_BFloat16(0x0080, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t lowest() {\n    return _MLX_BFloat16(0xFF7F, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t max() {\n    return _MLX_BFloat16(0x7F7F, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t epsilon() {\n    return _MLX_BFloat16(0x3C00, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t round_error() {\n    return _MLX_BFloat16(0x3F00, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t infinity() {\n    return _MLX_BFloat16(0x7F80, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t quiet_NaN() {\n    return _MLX_BFloat16(0x7FC0, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t signaling_NaN() {\n    return _MLX_BFloat16(0x7F80, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t denorm_min() {\n    return _MLX_BFloat16(0x0001, _MLX_BFloat16::bits_to_bfloat());\n  }\n};\nMETAL_FUNC bool isnan(_MLX_BFloat16 x) {\n  return x != x;\n}\n}\n#pragma METAL internals : disable\ninline uint16_t bfloat16_to_uint16(const bfloat16_t x) {\n  return x.bits_;\n}\ninline bfloat16_t uint16_to_bfloat16(const uint16_t x) {\n  return _MLX_BFloat16(x, _MLX_BFloat16::bits_to_bfloat());\n}\n\n #endif\n\nnamespace metal {\nMETAL_FUNC bfloat16_t abs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t acos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acos(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t acosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acosh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t asin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asin(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t asinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asinh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t atan(bfloat16_t y_over_x) { return static_cast<bfloat16_t>( __metal_atan(static_cast<float>(y_over_x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t atan2(bfloat16_t y, bfloat16_t x) { return static_cast<bfloat16_t>( __metal_atan2(static_cast<float>(y), static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t atanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_atanh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t ceil(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_ceil(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t cos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cos(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t cosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cosh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t cospi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cospi(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t divide(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_divide(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t exp(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t exp10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp10(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t exp2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp2(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fabs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fdim(bfloat16_t x, bfloat16_t y) { float t = static_cast<float>(x - y); return static_cast<bfloat16_t>(select(t, float(0), t < float(0) || x == y)); } METAL_FUNC bfloat16_t floor(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_floor(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fma(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fma( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z))); } METAL_FUNC bfloat16_t fmax(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmax3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmedian3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmod(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmod(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fract(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fract(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t frexp(bfloat16_t x, thread int& exp) { return static_cast<bfloat16_t>(__metal_frexp(static_cast<float>(x), &exp)); } METAL_FUNC bfloat16_t ldexp(bfloat16_t x, int k) { return static_cast<bfloat16_t>(__metal_ldexp(static_cast<float>(x), k, __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t log(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t log10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log10(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t log2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log2(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t max(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t max3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t median3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t min(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t min3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t nextafter(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_nextafter(static_cast<float>(x), static_cast<float>(y))); } METAL_FUNC bfloat16_t pow(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_pow(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t powr(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_powr(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t rint(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rint(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t round(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_round(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t rsqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rsqrt(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sin(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sinpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinpi(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sqrt(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t tan(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tan(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t tanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t tanpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanpi(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t trunc(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_trunc(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); };\nnamespace fast {\nMETAL_FUNC bfloat16_t abs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t acos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acos(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t acosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acosh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t asin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asin(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t asinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asinh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t atan(bfloat16_t y_over_x) { return static_cast<bfloat16_t>( __metal_atan(static_cast<float>(y_over_x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t atan2(bfloat16_t y, bfloat16_t x) { return static_cast<bfloat16_t>( __metal_atan2(static_cast<float>(y), static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t atanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_atanh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t ceil(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_ceil(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t cos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cos(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t cosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cosh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t cospi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cospi(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t divide(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_divide(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t exp(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t exp10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp10(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t exp2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp2(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fabs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fdim(bfloat16_t x, bfloat16_t y) { float t = static_cast<float>(x - y); return static_cast<bfloat16_t>(select(t, float(0), t < float(0) || x == y)); } METAL_FUNC bfloat16_t floor(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_floor(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fma(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fma( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z))); } METAL_FUNC bfloat16_t fmax(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmax3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmedian3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmod(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmod(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fract(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fract(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t frexp(bfloat16_t x, thread int& exp) { return static_cast<bfloat16_t>(__metal_frexp(static_cast<float>(x), &exp)); } METAL_FUNC bfloat16_t ldexp(bfloat16_t x, int k) { return static_cast<bfloat16_t>(__metal_ldexp(static_cast<float>(x), k, __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t log(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t log10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log10(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t log2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log2(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t max(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t max3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t median3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t min(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t min3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t nextafter(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_nextafter(static_cast<float>(x), static_cast<float>(y))); } METAL_FUNC bfloat16_t pow(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_pow(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t powr(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_powr(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t rint(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rint(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t round(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_round(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t rsqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rsqrt(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sin(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sinpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinpi(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sqrt(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t tan(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tan(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t tanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t tanpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanpi(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t trunc(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_trunc(static_cast<float>(x), __METAL_FAST_MATH__)); };\n}\nnamespace precise {\nMETAL_FUNC bfloat16_t abs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t acos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acos(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t acosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acosh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t asin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asin(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t asinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asinh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t atan(bfloat16_t y_over_x) { return static_cast<bfloat16_t>( __metal_atan(static_cast<float>(y_over_x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t atan2(bfloat16_t y, bfloat16_t x) { return static_cast<bfloat16_t>( __metal_atan2(static_cast<float>(y), static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t atanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_atanh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t ceil(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_ceil(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t cos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cos(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t cosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cosh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t cospi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cospi(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t divide(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_divide(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t exp(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t exp10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp10(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t exp2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp2(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fabs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fdim(bfloat16_t x, bfloat16_t y) { float t = static_cast<float>(x - y); return static_cast<bfloat16_t>(select(t, float(0), t < float(0) || x == y)); } METAL_FUNC bfloat16_t floor(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_floor(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fma(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fma( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z))); } METAL_FUNC bfloat16_t fmax(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmax3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmedian3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmin(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmin3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmod(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmod(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fract(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fract(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t frexp(bfloat16_t x, thread int& exp) { return static_cast<bfloat16_t>(__metal_frexp(static_cast<float>(x), &exp)); } METAL_FUNC bfloat16_t ldexp(bfloat16_t x, int k) { return static_cast<bfloat16_t>(__metal_ldexp(static_cast<float>(x), k, __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t log(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t log10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log10(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t log2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log2(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t max(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t max3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t median3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t min(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t min3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t nextafter(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_nextafter(static_cast<float>(x), static_cast<float>(y))); } METAL_FUNC bfloat16_t pow(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_pow(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t powr(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_powr(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t rint(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rint(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t round(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_round(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t rsqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rsqrt(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sin(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sinpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinpi(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sqrt(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t tan(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tan(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t tanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t tanpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanpi(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t trunc(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_trunc(static_cast<float>(x), __METAL_PRECISE_MATH__)); };\n}\n}\nnamespace metal {\nMETAL_FUNC bfloat16_t simd_broadcast(bfloat16_t data, ushort broadcast_lane_id) { return uint16_to_bfloat16( __metal_simd_broadcast(bfloat16_to_uint16(data), broadcast_lane_id)); } METAL_FUNC bfloat16_t simd_shuffle(bfloat16_t data, ushort simd_lane_id) { return uint16_to_bfloat16( __metal_simd_shuffle(bfloat16_to_uint16(data), simd_lane_id)); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_down( bfloat16_t data, bfloat16_t filling_data, ushort delta, ushort modulo) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_down( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, modulo)); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_down( bfloat16_t data, bfloat16_t filling_data, ushort delta) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_down( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, __metal_get_simdgroup_size(ushort()))); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_up( bfloat16_t data, bfloat16_t filling_data, ushort delta, ushort modulo) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_up( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, modulo)); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_up( bfloat16_t data, bfloat16_t filling_data, ushort delta) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_up( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, __metal_get_simdgroup_size(ushort()))); } METAL_FUNC bfloat16_t simd_shuffle_down(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_down(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_rotate_down(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_rotate_down(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_rotate_up(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_rotate_up(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_up(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_up(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_xor(bfloat16_t data, ushort mask) { return uint16_to_bfloat16( __metal_simd_shuffle_xor(bfloat16_to_uint16(data), mask)); };\nMETAL_FUNC bfloat16_t simd_max(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_max(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_min(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_min(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_exclusive_product(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_exclusive_product(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_exclusive_sum(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_exclusive_sum(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_inclusive_product(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_inclusive_product(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_inclusive_sum(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_inclusive_sum(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_product(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_product(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_sum(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_sum(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_xor(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_xor(static_cast<float>(data))); };\n}\nusing namespace metal;\nstruct complex64_t;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_to_complex64 =\n    !is_same_v<T, complex64_t> && is_convertible_v<T, float>;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_from_complex64 =\n    !is_same_v<T, complex64_t> &&\n    (is_convertible_v<float, T> || is_convertible_v<bfloat16_t, T>);\nstruct complex64_t {\n  float real;\n  float imag;\n  constexpr complex64_t(float real, float imag) : real(real), imag(imag) {};\n  constexpr complex64_t() : real(0), imag(0) {};\n  constexpr complex64_t() threadgroup : real(0), imag(0) {};\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) thread : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) threadgroup : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) device : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) constant : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const thread {\n    return static_cast<T>(real);\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const threadgroup {\n    return static_cast<T>(real);\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const device {\n    return static_cast<T>(real);\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const constant {\n    return static_cast<T>(real);\n  }\n};\nconstexpr complex64_t operator-(complex64_t x) {\n  return {-x.real, -x.imag};\n}\nconstexpr bool operator>=(complex64_t a, complex64_t b) {\n  return (a.real > b.real) || (a.real == b.real && a.imag >= b.imag);\n}\nconstexpr bool operator>(complex64_t a, complex64_t b) {\n  return (a.real > b.real) || (a.real == b.real && a.imag > b.imag);\n}\nconstexpr bool operator<=(complex64_t a, complex64_t b) {\n  return operator>=(b, a);\n}\nconstexpr bool operator<(complex64_t a, complex64_t b) {\n  return operator>(b, a);\n}\nconstexpr bool operator==(complex64_t a, complex64_t b) {\n  return a.real == b.real && a.imag == b.imag;\n}\nconstexpr complex64_t operator+(complex64_t a, complex64_t b) {\n  return {a.real + b.real, a.imag + b.imag};\n}\nconstexpr complex64_t operator-(complex64_t a, complex64_t b) {\n  return {a.real - b.real, a.imag - b.imag};\n}\nconstexpr complex64_t operator*(complex64_t a, complex64_t b) {\n  return {a.real * b.real - a.imag * b.imag, a.real * b.imag + a.imag * b.real};\n}\nconstexpr complex64_t operator/(complex64_t a, complex64_t b) {\n  auto denom = b.real * b.real + b.imag * b.imag;\n  auto x = a.real * b.real + a.imag * b.imag;\n  auto y = a.imag * b.real - a.real * b.imag;\n  return {x / denom, y / denom};\n}\nconstexpr complex64_t operator%(complex64_t a, complex64_t b) {\n  auto real = a.real - (b.real * static_cast<int64_t>(a.real / b.real));\n  auto imag = a.imag - (b.imag * static_cast<int64_t>(a.imag / b.imag));\n  if (real != 0 && (real < 0 != b.real < 0)) {\n    real += b.real;\n  }\n  if (imag != 0 && (imag < 0 != b.imag < 0)) {\n    imag += b.imag;\n  }\n  return {real, imag};\n}\nstatic constant constexpr int MAX_REDUCE_SPECIALIZED_DIMS = 4;\nstatic constant constexpr int REDUCE_N_READS = 4;\nstatic constant constexpr int REDUCE_N_WRITES = 4;\nstatic constant constexpr int SOFTMAX_N_READS = 4;\nstatic constant constexpr int RMS_N_READS = 4;\nstatic constant constexpr int RMS_LOOPED_LIMIT = 4096;\n\ntypedef half float16_t;\ntemplate <typename U>\nstruct Limits {\n  static const constant U max = metal::numeric_limits<U>::max();\n  static const constant U min = metal::numeric_limits<U>::min();\n  static const constant U finite_max = metal::numeric_limits<U>::max();\n  static const constant U finite_min = metal::numeric_limits<U>::min();\n};\ntemplate <> struct Limits<uint8_t> { static constexpr constant uint8_t max = metal::numeric_limits<uint8_t>::max(); static constexpr constant uint8_t min = metal::numeric_limits<uint8_t>::min(); static constexpr constant uint8_t finite_max = metal::numeric_limits<uint8_t>::max(); static constexpr constant uint8_t finite_min = metal::numeric_limits<uint8_t>::min(); };;\ntemplate <> struct Limits<uint16_t> { static constexpr constant uint16_t max = metal::numeric_limits<uint16_t>::max(); static constexpr constant uint16_t min = metal::numeric_limits<uint16_t>::min(); static constexpr constant uint16_t finite_max = metal::numeric_limits<uint16_t>::max(); static constexpr constant uint16_t finite_min = metal::numeric_limits<uint16_t>::min(); };;\ntemplate <> struct Limits<uint32_t> { static constexpr constant uint32_t max = metal::numeric_limits<uint32_t>::max(); static constexpr constant uint32_t min = metal::numeric_limits<uint32_t>::min(); static constexpr constant uint32_t finite_max = metal::numeric_limits<uint32_t>::max(); static constexpr constant uint32_t finite_min = metal::numeric_limits<uint32_t>::min(); };;\ntemplate <> struct Limits<uint64_t> { static constexpr constant uint64_t max = metal::numeric_limits<uint64_t>::max(); static constexpr constant uint64_t min = metal::numeric_limits<uint64_t>::min(); static constexpr constant uint64_t finite_max = metal::numeric_limits<uint64_t>::max(); static constexpr constant uint64_t finite_min = metal::numeric_limits<uint64_t>::min(); };;\ntemplate <> struct Limits<int8_t> { static constexpr constant int8_t max = metal::numeric_limits<int8_t>::max(); static constexpr constant int8_t min = metal::numeric_limits<int8_t>::min(); static constexpr constant int8_t finite_max = metal::numeric_limits<int8_t>::max(); static constexpr constant int8_t finite_min = metal::numeric_limits<int8_t>::min(); };;\ntemplate <> struct Limits<int16_t> { static constexpr constant int16_t max = metal::numeric_limits<int16_t>::max(); static constexpr constant int16_t min = metal::numeric_limits<int16_t>::min(); static constexpr constant int16_t finite_max = metal::numeric_limits<int16_t>::max(); static constexpr constant int16_t finite_min = metal::numeric_limits<int16_t>::min(); };;\ntemplate <> struct Limits<int32_t> { static constexpr constant int32_t max = metal::numeric_limits<int32_t>::max(); static constexpr constant int32_t min = metal::numeric_limits<int32_t>::min(); static constexpr constant int32_t finite_max = metal::numeric_limits<int32_t>::max(); static constexpr constant int32_t finite_min = metal::numeric_limits<int32_t>::min(); };;\ntemplate <> struct Limits<int64_t> { static constexpr constant int64_t max = metal::numeric_limits<int64_t>::max(); static constexpr constant int64_t min = metal::numeric_limits<int64_t>::min(); static constexpr constant int64_t finite_max = metal::numeric_limits<int64_t>::max(); static constexpr constant int64_t finite_min = metal::numeric_limits<int64_t>::min(); };;\ntemplate <> struct Limits<half> { static constexpr constant half max = metal::numeric_limits<half>::infinity(); static constexpr constant half min = -metal::numeric_limits<half>::infinity(); static constexpr constant half finite_max = metal::numeric_limits<half>::max(); static constexpr constant half finite_min = -metal::numeric_limits<half>::max(); };;\ntemplate <> struct Limits<float> { static constexpr constant float max = metal::numeric_limits<float>::infinity(); static constexpr constant float min = -metal::numeric_limits<float>::infinity(); static constexpr constant float finite_max = metal::numeric_limits<float>::max(); static constexpr constant float finite_min = -metal::numeric_limits<float>::max(); };;\ntemplate <> struct Limits<bfloat16_t> { static constexpr constant bfloat16_t max = metal::numeric_limits<bfloat16_t>::infinity(); static constexpr constant bfloat16_t min = -metal::numeric_limits<bfloat16_t>::infinity(); static constexpr constant bfloat16_t finite_max = metal::numeric_limits<bfloat16_t>::max(); static constexpr constant bfloat16_t finite_min = -metal::numeric_limits<bfloat16_t>::max(); };;\ntemplate <>\nstruct Limits<bool> {\n  static constexpr constant bool max = true;\n  static constexpr constant bool min = false;\n};\ntemplate <>\nstruct Limits<complex64_t> {\n  static constexpr constant complex64_t max = complex64_t(\n      metal::numeric_limits<float>::infinity(),\n      metal::numeric_limits<float>::infinity());\n  static constexpr constant complex64_t min = complex64_t(\n      -metal::numeric_limits<float>::infinity(),\n      -metal::numeric_limits<float>::infinity());\n};\ntemplate <typename IdxT = int64_t>\nMETAL_FUNC IdxT elem_to_loc(\n    IdxT elem,\n    constant const int* shape,\n    constant const int64_t* strides,\n    int ndim) {\n  IdxT loc = 0;\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    loc += (elem % shape[i]) * IdxT(strides[i]);\n    elem /= shape[i];\n  }\n  return loc;\n}\ntemplate <typename IdxT = int64_t>\nMETAL_FUNC IdxT elem_to_loc(\n    uint3 elem,\n    constant const int* shape,\n    constant const int64_t* strides,\n    int ndim) {\n  IdxT loc =\n      elem.x * IdxT(strides[ndim - 1]) + elem.y * IdxT(strides[ndim - 2]);\n  for (int d = ndim - 3; d >= 0; --d) {\n    loc += (elem.z % shape[d]) * IdxT(strides[d]);\n    elem.z /= shape[d];\n  }\n  return loc;\n}\ntemplate <typename IdxT = int64_t>\nMETAL_FUNC IdxT elem_to_loc_1(uint elem, constant const int64_t& stride) {\n  return elem * IdxT(stride);\n}\ntemplate <typename IdxT = int64_t>\nMETAL_FUNC IdxT elem_to_loc_2(uint2 elem, constant const int64_t strides[2]) {\n  return elem.x * IdxT(strides[1]) + elem.y * IdxT(strides[0]);\n}\ntemplate <typename IdxT = int64_t>\nMETAL_FUNC IdxT elem_to_loc_3(uint3 elem, constant const int64_t strides[3]) {\n  return elem.x * IdxT(strides[2]) + elem.y * IdxT(strides[1]) +\n      elem.z * IdxT(strides[0]);\n}\ntemplate <typename IdxT = int64_t>\nMETAL_FUNC vec<IdxT, 2> elem_to_loc_2_nd(\n    uint3 elem,\n    constant const int* shape,\n    constant const int64_t* a_strides,\n    constant const int64_t* b_strides,\n    int ndim) {\n  vec<IdxT, 2> loc = {\n      IdxT(\n          elem.x * IdxT(a_strides[ndim - 1]) +\n          IdxT(elem.y) * IdxT(a_strides[ndim - 2])),\n      IdxT(\n          elem.x * IdxT(b_strides[ndim - 1]) +\n          elem.y * IdxT(b_strides[ndim - 2]))};\n  for (int d = ndim - 3; d >= 0; --d) {\n    uint l = elem.z % shape[d];\n    loc.x += l * IdxT(a_strides[d]);\n    loc.y += l * IdxT(b_strides[d]);\n    elem.z /= shape[d];\n  }\n  return loc;\n}\ntemplate <typename IdxT = int64_t>\nMETAL_FUNC vec<IdxT, 3> elem_to_loc_3_nd(\n    uint3 elem,\n    constant const int* shape,\n    constant const int64_t* a_strides,\n    constant const int64_t* b_strides,\n    constant const int64_t* c_strides,\n    int ndim) {\n  vec<IdxT, 3> loc = {\n      IdxT(elem.x * IdxT(a_strides[ndim - 1])) +\n          IdxT(elem.y * IdxT(a_strides[ndim - 2])),\n      IdxT(elem.x * IdxT(b_strides[ndim - 1])) +\n          IdxT(elem.y * IdxT(b_strides[ndim - 2])),\n      IdxT(elem.x * IdxT(c_strides[ndim - 1])) +\n          IdxT(elem.y * IdxT(c_strides[ndim - 2]))};\n  for (int d = ndim - 3; d >= 0; --d) {\n    uint l = elem.z % shape[d];\n    loc.x += l * IdxT(a_strides[d]);\n    loc.y += l * IdxT(b_strides[d]);\n    loc.z += l * IdxT(c_strides[d]);\n    elem.z /= shape[d];\n  }\n  return loc;\n}\ntemplate <int DIM, typename OffsetT = size_t, bool General = true>\nstruct LoopedElemToLoc {\n  int dim;\n  LoopedElemToLoc<DIM - 1, OffsetT, General> inner_looper;\n  OffsetT offset{0};\n  int index{0};\n  LoopedElemToLoc(int dim) : dim(dim), inner_looper(dim - 1) {}\n  void next(const constant int* shape, const constant int64_t* strides) {\n    if (dim == 0) {\n      return;\n    }\n    index++;\n    offset += OffsetT(strides[dim - 1]);\n    if (index >= shape[dim - 1]) {\n      index = 0;\n      inner_looper.next(shape, strides);\n      offset = inner_looper.offset;\n    }\n  }\n  void next(int n, const constant int* shape, const constant int64_t* strides) {\n    if (dim == 0) {\n      return;\n    }\n    index += n;\n    offset += n * OffsetT(strides[dim - 1]);\n    if (index >= shape[dim - 1]) {\n      int extra = index - shape[dim - 1];\n      if (extra >= shape[dim - 1]) {\n        inner_looper.next(1 + extra / shape[dim - 1], shape, strides);\n        extra = extra % shape[dim - 1];\n      } else {\n        inner_looper.next(shape, strides);\n      }\n      index = 0;\n      offset = inner_looper.offset;\n      if (extra > 0) {\n        next(extra, shape, strides);\n      }\n    }\n  }\n  OffsetT location() {\n    return offset;\n  }\n};\ntemplate <typename OffsetT>\nstruct LoopedElemToLoc<1, OffsetT, true> {\n  int dim;\n  OffsetT offset{0};\n  uint index{0};\n  LoopedElemToLoc(int dim) : dim(dim) {}\n  void next(const constant int* shape, const constant int64_t* strides) {\n    index++;\n    if (dim > 1) {\n      offset = elem_to_loc<OffsetT>(index, shape, strides, dim);\n    } else {\n      offset += OffsetT(strides[0]);\n    }\n  }\n  void next(int n, const constant int* shape, const constant int64_t* strides) {\n    index += n;\n    if (dim > 1) {\n      offset = elem_to_loc<OffsetT>(index, shape, strides, dim);\n    } else {\n      offset = index * OffsetT(strides[0]);\n    }\n  }\n  OffsetT location() {\n    return offset;\n  }\n};\ntemplate <typename OffsetT>\nstruct LoopedElemToLoc<1, OffsetT, false> {\n  OffsetT offset{0};\n  LoopedElemToLoc(int) {}\n  void next(const constant int*, const constant int64_t* strides) {\n    offset += OffsetT(strides[0]);\n  }\n  void next(int n, const constant int*, const constant int64_t* strides) {\n    offset += n * OffsetT(strides[0]);\n  }\n  OffsetT location() {\n    return offset;\n  }\n};\ntemplate <typename T, typename U>\ninline T ceildiv(T N, U M) {\n  return (N + M - 1) / M;\n}\ninline float log1p(float x) {\n  float xp1 = 1.0f + x;\n  if (xp1 == Limits<float>::max) {\n    return Limits<float>::max;\n  }\n  if (xp1 == 1.0f) {\n    return x;\n  }\n  return x * (metal::log(xp1) / (xp1 - 1.0f));\n}\ninline bfloat16_t log1p(bfloat16_t x) {\n  float xp1 = 1.0f + static_cast<float>(x);\n  if (xp1 == Limits<float>::max) {\n    return Limits<bfloat16_t>::max;\n  }\n  if (xp1 == 1.0f) {\n    return x;\n  }\n  return bfloat16_t(x * (metal::log(xp1) / (xp1 - 1.0f)));\n}\ninline uint64_t simd_shuffle_down(uint64_t data, uint16_t delta) {\n  return as_type<uint64_t>(\n      metal::simd_shuffle_down(as_type<uint2>(data), delta));\n}\ninline int64_t simd_shuffle_down(int64_t data, uint16_t delta) {\n  return as_type<int64_t>(\n      metal::simd_shuffle_down(as_type<uint2>(data), delta));\n}\ninline bool simd_shuffle_down(bool data, uint16_t delta) {\n  return simd_shuffle_down(static_cast<uint32_t>(data), delta);\n}\ninline complex64_t simd_shuffle_down(complex64_t data, uint16_t delta) {\n  return complex64_t(\n      simd_shuffle_down(data.real, delta), simd_shuffle_down(data.imag, delta));\n}\ninline uint64_t simd_shuffle_up(uint64_t data, uint16_t delta) {\n  return as_type<uint64_t>(metal::simd_shuffle_up(as_type<uint2>(data), delta));\n}\ninline int64_t simd_shuffle_up(int64_t data, uint16_t delta) {\n  return as_type<int64_t>(metal::simd_shuffle_up(as_type<uint2>(data), delta));\n}\ninline bool simd_shuffle_up(bool data, uint16_t delta) {\n  return simd_shuffle_up(static_cast<uint32_t>(data), delta);\n}\ninline complex64_t simd_shuffle_up(complex64_t data, uint16_t delta) {\n  return complex64_t(\n      simd_shuffle_up(data.real, delta), simd_shuffle_up(data.imag, delta));\n}\ninline uint64_t\nsimd_shuffle_and_fill_up(uint64_t data, uint64_t filling, uint16_t delta) {\n  return as_type<uint64_t>(metal::simd_shuffle_and_fill_up(\n      as_type<uint2>(data), as_type<uint2>(filling), delta));\n}\ninline int64_t\nsimd_shuffle_and_fill_up(int64_t data, int64_t filling, uint16_t delta) {\n  return as_type<int64_t>(metal::simd_shuffle_and_fill_up(\n      as_type<uint2>(data), as_type<uint2>(filling), delta));\n}\ninline bool simd_shuffle_and_fill_up(bool data, bool filling, uint16_t delta) {\n  return simd_shuffle_and_fill_up(\n      static_cast<uint32_t>(data), static_cast<uint32_t>(filling), delta);\n}\ninline complex64_t simd_shuffle_and_fill_up(\n    complex64_t data,\n    complex64_t filling,\n    uint16_t delta) {\n  return complex64_t(\n      simd_shuffle_and_fill_up(data.real, filling.real, delta),\n      simd_shuffle_and_fill_up(data.imag, filling.imag, delta));\n}\ninline uint64_t simd_shuffle(uint64_t data, uint16_t lane) {\n  return as_type<uint64_t>(metal::simd_shuffle(as_type<uint2>(data), lane));\n}\ninline int64_t simd_shuffle(int64_t data, uint16_t lane) {\n  return as_type<int64_t>(metal::simd_shuffle(as_type<uint2>(data), lane));\n}\ninline bool simd_shuffle(bool data, uint16_t lane) {\n  return simd_shuffle(static_cast<uint32_t>(data), lane);\n}\ninline complex64_t simd_shuffle(complex64_t data, uint16_t lane) {\n  return complex64_t(\n      simd_shuffle(data.real, lane), simd_shuffle(data.imag, lane));\n}\ntemplate <bool condition, typename T, typename U>\nstruct ConditionalType {\n  using type = U;\n};\ntemplate <typename T, typename U>\nstruct ConditionalType<true, T, U> {\n  using type = T;\n};\n"
+ "\nMETAL_FUNC ulong2 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const int64_t* a_strides,\n    constant const int64_t* b_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n  }\n  return ulong2(loc_a, loc_b);\n}\nMETAL_FUNC ulong3 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const int64_t* a_strides,\n    constant const int64_t* b_strides,\n    constant const int64_t* c_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  ulong loc_c{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n    loc_c += pos_in_dim * c_strides[i];\n  }\n  return ulong3(loc_a, loc_b, loc_c);\n}\n\ntemplate <int NDIM>\nstruct MLXConvParams {\n  const int N;\n  const int C;\n  const int O;\n  const int iS[NDIM];\n  const int wS[NDIM];\n  const int oS[NDIM];\n  const int str[NDIM];\n  const int pad[NDIM];\n  const int kdil[NDIM];\n  const int idil[NDIM];\n  const int64_t in_strides[NDIM + 2];\n  const int64_t wt_strides[NDIM + 2];\n  const int64_t out_strides[NDIM + 2];\n  const int groups;\n  const bool flip;\n};\nnamespace mlx {\nnamespace steel {\nstruct ImplicitGemmConv2DParams {\n  const int M;\n  const int N;\n  const int K;\n  const int gemm_k_iterations;\n  const int inp_jump_w;\n  const int inp_jump_h;\n  const int inp_jump_c;\n  const int tiles_n;\n  const int tiles_m;\n  const int swizzle_log;\n};\nstruct Conv2DGeneralJumpParams {\n  const int f_wgt_jump_h;\n  const int f_wgt_jump_w;\n  const int f_out_jump_h;\n  const int f_out_jump_w;\n  const int adj_out_h;\n  const int adj_out_w;\n  const int adj_out_hw;\n  const int adj_implicit_m;\n};\nstruct Conv2DGeneralBaseInfo {\n  int weight_base;\n  int weight_size;\n};\n}\n}\nnamespace mlx {\nnamespace steel {\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderLargeFilter {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant ImplicitGemmConv2DParams* gemm_params;\n  short weight_h;\n  short weight_w;\n  const device T* src[n_rows];\n  int read_n[n_rows];\n  int read_ih[n_rows];\n  int read_iw[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderLargeFilter(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        gemm_params(gemm_params_),\n        weight_h(0),\n        weight_w(0) {\n    int out_n_pixels = params->oS[0] * params->oS[1];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / out_n_pixels;\n      int hw = offset_nhw % out_n_pixels;\n      int oh = hw / params->oS[1];\n      int ow = hw % params->oS[1];\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n      if (params->flip) {\n        ih += (params->wS[0] - 1) * params->kdil[0];\n        iw += (params->wS[1] - 1) * params->kdil[1];\n      }\n      src[i] = src_ + n * params->in_strides[0] + ih * params->in_strides[1] +\n          iw * params->in_strides[2] + bj;\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      int n = read_n[i];\n      int ih = read_ih[i] + weight_h * params->kdil[0];\n      int iw = read_iw[i] + weight_w * params->kdil[1];\n      if ((n < params->N) && (ih >= 0 && ih < params->iS[0]) &&\n          (iw >= 0 && iw < params->iS[1])) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = src[i][j];\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    if (++weight_w < params->wS[1]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_w;\n      }\n      return;\n    }\n    weight_w = 0;\n    if (++weight_h < params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_h;\n      }\n      return;\n    }\n    weight_h = 0;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; i++) {\n      src[i] += gemm_params->inp_jump_c;\n    }\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderSmallFilter {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  using mask_t = short;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant ImplicitGemmConv2DParams* gemm_params;\n  short weight_h;\n  short weight_w;\n  const device T* src[n_rows];\n  mask_t mask_h[n_rows];\n  mask_t mask_w[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderSmallFilter(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        gemm_params(gemm_params_),\n        weight_h(0),\n        weight_w(0) {\n    int out_n_pixels = params->oS[0] * params->oS[1];\n    int read_n[n_rows];\n    int read_ih[n_rows];\n    int read_iw[n_rows];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / out_n_pixels;\n      int hw = offset_nhw % out_n_pixels;\n      int oh = hw / params->oS[1];\n      int ow = hw % params->oS[1];\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n      if (params->flip) {\n        ih += (params->wS[0] - 1) * params->kdil[0];\n        iw += (params->wS[1] - 1) * params->kdil[1];\n      }\n      src[i] = src_ + n * params->in_strides[0] + ih * params->in_strides[1] +\n          iw * params->in_strides[2] + bj;\n    }\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      mask_h[i] = 0;\n      mask_w[i] = 0;\n    }\n    for (short kh = 0; kh < params->wS[0]; kh++) {\n      short flip_h = params->flip ? params->wS[0] - kh - 1 : kh;\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; ++i) {\n        int n = read_n[i];\n        int ih = read_ih[i] + flip_h * params->kdil[0];\n        bool in_bounds = n < params->N && ih >= 0 && ih < params->iS[0];\n        mask_h[i] |= (in_bounds << kh);\n      }\n    }\n    for (short kw = 0; kw < params->wS[1]; kw++) {\n      short flip_w = params->flip ? params->wS[1] - kw - 1 : kw;\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; ++i) {\n        int iw = read_iw[i] + flip_w * params->kdil[1];\n        bool in_bounds = iw >= 0 && iw < params->iS[1];\n        mask_w[i] |= (in_bounds << kw);\n      }\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n    mask_t h_mask = mask_t(1) << weight_h;\n    mask_t w_mask = mask_t(1) << weight_w;\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      if ((mask_h[i] & h_mask) && (mask_w[i] & w_mask)) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = src[i][j];\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    if (++weight_w < params->wS[1]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_w;\n      }\n      return;\n    }\n    weight_w = 0;\n    if (++weight_h < params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_h;\n      }\n      return;\n    }\n    weight_h = 0;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; i++) {\n      src[i] += gemm_params->inp_jump_c;\n    }\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DWeightBlockLoader {\n  static constant constexpr const short BROWS = BN;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size =\n      (BN == 8) ? 1 : (tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4);\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const int src_ld;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  const constant MLXConvParams<2>* params;\n  int weight_hw;\n  const int read_n;\n  const bool do_read;\n  METAL_FUNC Conv2DWeightBlockLoader(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(params_->wt_strides[0]),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld + bj),\n        params(params_),\n        weight_hw(0),\n        read_n(offsets.y + bi),\n        do_read(read_n + n_rows * TROWS <= gemm_params_->N) {}\n  METAL_FUNC void load_unsafe() const {\n    if (BN != 8 || do_read) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BN; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = src[i * src_ld + j];\n        }\n      }\n    } else {\n      for (short i = 0; i < BN; i += TROWS) {\n        if ((read_n + i) < params->O) {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = src[i * src_ld + j];\n          }\n        } else {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    if (++weight_hw < (params->wS[1] * params->wS[0])) {\n      src += params->wt_strides[2];\n      return;\n    }\n    weight_hw = 0;\n    src += BK - (params->wS[1] * params->wS[0] - 1) * params->wt_strides[2];\n  }\n};\n}\n}\nnamespace mlx {\nnamespace steel {\ntemplate <short n_channels_>\nstruct ChannelHelper {\n  static constant constexpr const short n_channels = n_channels_;\n  static constant constexpr const short vec_size = n_channels_ <= 4 ? 4 : 8;\n  static constant constexpr const short excess = vec_size - n_channels_;\n};\ntemplate <>\nstruct ChannelHelper<1> {\n  static constant constexpr const short n_channels = 1;\n  static constant constexpr const short vec_size = 1;\n  static constant constexpr const short excess = 0;\n};\ntemplate <>\nstruct ChannelHelper<2> {\n  static constant constexpr const short n_channels = 2;\n  static constant constexpr const short vec_size = 2;\n  static constant constexpr const short excess = 0;\n};\ntemplate <>\nstruct ChannelHelper<3> {\n  static constant constexpr const short n_channels = 3;\n  static constant constexpr const short vec_size = 4;\n  static constant constexpr const short excess = 1;\n};\ntemplate <>\nstruct ChannelHelper<4> {\n  static constant constexpr const short n_channels = 4;\n  static constant constexpr const short vec_size = 4;\n  static constant constexpr const short excess = 0;\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short n_channels,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderSmallChannels {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = ChannelHelper<n_channels>::vec_size;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant ImplicitGemmConv2DParams* gemm_params;\n  short weight_hw;\n  const device T* src[n_rows];\n  int read_n[n_rows];\n  int read_ih[n_rows];\n  int read_iw[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderSmallChannels(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        gemm_params(gemm_params_),\n        weight_hw(thread_idx % TCOLS) {\n    int out_n_pixels = params->oS[0] * params->oS[1];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / out_n_pixels;\n      int hw = offset_nhw % out_n_pixels;\n      int oh = hw / params->oS[1];\n      int ow = hw % params->oS[1];\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      src[i] = src_ + n * params->in_strides[0] + ih * params->in_strides[1] +\n          iw * params->in_strides[2];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n    if (weight_hw >= params->wS[1] * params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n      return;\n    }\n    int wh = (weight_hw / params->wS[1]);\n    int ww = (weight_hw % params->wS[1]);\n    int flip_h = params->flip ? params->wS[0] - wh - 1 : wh;\n    int flip_w = params->flip ? params->wS[1] - ww - 1 : ww;\n    int weight_h = flip_h * params->kdil[0];\n    int weight_w = flip_w * params->kdil[1];\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      int n = read_n[i];\n      int ih = read_ih[i] + weight_h;\n      int iw = read_iw[i] + weight_w;\n      if ((n < params->N) && (ih >= 0 && ih < params->iS[0]) &&\n          (iw >= 0 && iw < params->iS[1])) {\n        const device T* curr_src = src[i] + weight_h * params->in_strides[1] +\n            weight_w * params->in_strides[2];\n#pragma clang loop unroll(full)\n        for (short j = 0; j < n_channels; ++j) {\n          dst[is * dst_ld + j] = curr_src[j];\n        }\n#pragma clang loop unroll(full)\n        for (short j = n_channels; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    weight_hw += TCOLS;\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short n_channels,\n    short tgp_padding = 0>\nstruct Conv2DWeightBlockLoaderSmallChannels {\n  static constant constexpr const short BROWS = BN;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = ChannelHelper<n_channels>::vec_size;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const int src_ld;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  const constant MLXConvParams<2>* params;\n  int weight_hw;\n  const int read_n;\n  const bool do_read;\n  METAL_FUNC Conv2DWeightBlockLoaderSmallChannels(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(params_->wt_strides[0]),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld),\n        params(params_),\n        weight_hw(thread_idx % TCOLS),\n        read_n(offsets.y + bi),\n        do_read(read_n + BN <= gemm_params_->N) {}\n  METAL_FUNC void load_unsafe() const {\n    if (bi >= BROWS || bj >= BCOLS)\n      return;\n    if (read_n >= params->O || weight_hw >= params->wS[1] * params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n      return;\n    }\n    const device T* curr_src = src + weight_hw * params->wt_strides[2];\n    if (BN != 8 || do_read) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < n_channels; j++) {\n          dst[i * dst_ld + j] = curr_src[i * src_ld + j];\n        }\n#pragma clang loop unroll(full)\n        for (short j = n_channels; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n    } else {\n      for (short i = 0; i < BROWS; i += TROWS) {\n        if (((read_n + i) < params->O)) {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < n_channels; j++) {\n            dst[i * dst_ld + j] = curr_src[i * src_ld + j];\n          }\n#pragma clang loop unroll(full)\n          for (short j = n_channels; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        } else {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    weight_hw += TCOLS;\n  }\n};\n}\n}\n\nnamespace mlx {\nnamespace steel {\ntemplate <typename OutT, typename InT>\nstruct TransformNone {\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT) {\n    return static_cast<OutT>(x);\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAdd {\n  TransformAdd(const float, const float) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT c) {\n    return static_cast<OutT>(x) + c;\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAxpby {\n  const float alpha;\n  const float beta;\n  TransformAxpby(const float alpha_, const float beta_)\n      : alpha(alpha_), beta(beta_) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  METAL_FUNC OutT apply(InT x, OutT c) const {\n    return static_cast<OutT>(x * alpha + (beta * c));\n  }\n};\ntemplate <typename T>\nstruct AccumHelper {\n  typedef float accum_type;\n};\nstruct BlockSwizzle {\n  static METAL_FUNC int2\n  swizzle(uint3 tid [[threadgroup_position_in_grid]], const int swizzle_log) {\n    const int tid_x = (tid.x) >> swizzle_log;\n    const int tid_y =\n        ((tid.y) << swizzle_log) + ((tid.x) & ((1 << swizzle_log) - 1));\n    return int2(tid_x, tid_y);\n  }\n};\n}\n}\n#pragma METAL internals : enable\nnamespace metal {\ntemplate <typename T>\nstruct is_empty : metal::bool_constant<__is_empty(T)> {};\ntemplate <typename... Ts>\nstruct make_void {\n  typedef void type;\n};\ntemplate <typename... Ts>\nusing void_t = typename make_void<Ts...>::type;\ntemplate <class T>\nstruct is_static : metal::bool_constant<is_empty<remove_cv_t<T>>::value> {};\ntemplate <typename T>\nstruct pointer_element {};\ntemplate <typename T>\nstruct pointer_element<thread T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<device T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<constant T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<threadgroup T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nusing pointer_element_t = typename pointer_element<remove_cv_t<T>>::type;\n}\n#pragma METAL internals : disable\n\n#pragma METAL internals : enable\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, T v>\nstruct integral_constant {\n  static constexpr constant T value = v;\n  using value_type = T;\n  using type = integral_constant;\n  METAL_FUNC constexpr operator value_type() const noexcept {\n    return value;\n  }\n};\ntemplate <bool B>\nusing bool_constant = integral_constant<bool, B>;\nusing true_type = bool_constant<true>;\nusing false_type = bool_constant<false>;\ntemplate <class T>\nstruct is_integral : bool_constant<metal::is_integral<T>::value> {};\ntemplate <class T, T v>\nstruct is_integral<integral_constant<T, v>>\n    : bool_constant<metal::is_integral<T>::value> {};\ntemplate <typename T>\nconstexpr constant bool is_integral_v = is_integral<T>::value;\ntemplate <int val>\nusing Int = integral_constant<int, val>;\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator+( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv + uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator-( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv - uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator*( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv * uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator/( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv / uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator==( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv == uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator!=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv != uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv < uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv > uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv <= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv >= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator&&( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv && uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator||( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv || uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T>\nMETAL_FUNC constexpr T sum(T x) {\n  return x;\n}\ntemplate <typename T, typename... Us>\nMETAL_FUNC constexpr auto sum(T x, Us... us) {\n  return x + sum(us...);\n}\n}\n}\n#pragma METAL internals : disable\n\nusing namespace metal;\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, int kFragRows_, int kFragCols_>\nstruct BaseMMAFrag {\n  static_assert(\n      kFragRows_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n  static_assert(\n      kFragCols_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n};\ntemplate <typename T>\nstruct BaseMMAFrag<T, 8, 8> {\n  static constant constexpr const int kFragRows = 8;\n  static constant constexpr const int kFragCols = 8;\n  static constant constexpr const int kElemsPerFrag = (kFragRows * kFragCols) / 32;\n  static constant constexpr const int kElemRows = 1;\n  static constant constexpr const int kElemCols = 2;\n  static_assert(\n      kElemRows * kElemCols == kElemsPerFrag,\n      \"MMAFrag shape is not consistent with MMAFrag size\");\n  typedef metal::simdgroup_matrix<T, kFragRows, kFragCols> mat_type;\n  typedef metal::vec<T, kElemsPerFrag> frag_type;\n  METAL_FUNC static constexpr short2 get_coord(ushort simd_lane_id\n                                               [[thread_index_in_simdgroup]]) {\n    const short qid = simd_lane_id / 4;\n    const short fm = (qid & 4) + ((simd_lane_id / 2) % 4);\n    const short fn = (qid & 2) * 2 + (simd_lane_id % 2) * 2;\n    return short2{fn, fm};\n  }\n  template <typename SrcPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  load(thread frag_type& dst, SrcPtrType src, StrX str_x, StrY str_y) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * kElemCols + j] = static_cast<T>(src[i * str_x + j * str_y]);\n      }\n    }\n  }\n  template <\n      typename SrcPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void load_safe(\n      thread frag_type& dst,\n      SrcPtrType src,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[i * kElemCols + j] =\n              static_cast<T>(src[(off_x + i) * str_x + (off_x + j) * str_y]);\n        } else {\n          dst[i * kElemCols + j] = T(0);\n        }\n      }\n    }\n  }\n  template <typename DstPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  store(const thread frag_type& src, DstPtrType dst, StrX str_x, StrY str_y) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * str_x + j * str_y] = static_cast<U>(src[i * kElemCols + j]);\n      }\n    }\n  }\n  template <\n      typename DstPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void store_safe(\n      const thread frag_type& src,\n      DstPtrType dst,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[(off_x + i) * str_x + (off_y + j) * str_y] =\n              static_cast<U>(src[i * kElemCols + j]);\n        }\n      }\n    }\n  }\n  template <\n      typename DstPtrType,\n      typename StrX,\n      typename StrY,\n      typename StartX,\n      typename StopX,\n      typename StartY,\n      typename StopY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void store_slice(\n      const thread frag_type& src,\n      DstPtrType dst,\n      StrX str_x,\n      StrY str_y,\n      StartX start_x,\n      StopX stop_x,\n      StartY start_y,\n      StopY stop_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < stop_x && (off_x + i) >= start_x &&\n            (off_y + j) < stop_y && (off_y + j) >= start_y) {\n          dst[(off_x + i) * str_x + (off_y + j) * str_y] =\n              static_cast<U>(src[i * kElemCols + j]);\n        }\n      }\n    }\n  }\n  METAL_FUNC static constexpr void mma(\n      thread frag_type& D,\n      thread frag_type& A,\n      thread frag_type& B,\n      thread frag_type& C) {\n    mat_type D_mat;\n    mat_type A_mat;\n    mat_type B_mat;\n    mat_type C_mat;\n    reinterpret_cast<thread frag_type&>(A_mat.thread_elements()) = A;\n    reinterpret_cast<thread frag_type&>(B_mat.thread_elements()) = B;\n    reinterpret_cast<thread frag_type&>(C_mat.thread_elements()) = C;\n    mma(D_mat, A_mat, B_mat, C_mat);\n    D = reinterpret_cast<thread frag_type&>(D_mat.thread_elements());\n  }\n  METAL_FUNC static constexpr void mma(\n      thread mat_type& D,\n      thread mat_type& A,\n      thread mat_type& B,\n      thread mat_type& C) {\n    simdgroup_multiply_accumulate(D, A, B, C);\n  }\n};\ntemplate <\n    typename T,\n    int kTileRows_,\n    int kTileCols_,\n    class MMAFrag_ = BaseMMAFrag<T, 8, 8>>\nstruct MMATile {\n  using MMAFrag_t = MMAFrag_;\n  using elem_type = T;\n  static constant constexpr const int kFragRows = MMAFrag_t::kFragRows;\n  static constant constexpr const int kFragCols = MMAFrag_t::kFragCols;\n  static constant constexpr const int kElemsPerFrag = MMAFrag_t::kElemsPerFrag;\n  static constant constexpr const int kTileRows = kTileRows_;\n  static constant constexpr const int kTileCols = kTileCols_;\n  static constant constexpr const int kRows = kTileRows * kFragRows;\n  static constant constexpr const int kCols = kTileCols * kFragCols;\n  static constant constexpr const int kNumFrags = kTileRows * kTileCols;\n  static constant constexpr const int kElemsPerTile = kNumFrags * kElemsPerFrag;\n  typedef typename MMAFrag_t::mat_type mat_type;\n  typedef typename MMAFrag_t::frag_type frag_type;\n  frag_type val_frags[kNumFrags] = {frag_type(0)};\n  METAL_FUNC MMATile() thread {}\n  METAL_FUNC constexpr void clear() {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kNumFrags; ++i) {\n      val_frags[i] = frag_type(0);\n    }\n  }\n  METAL_FUNC constexpr thread frag_type& frag_at(const short i, const short j) {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC constexpr const thread frag_type& frag_at(\n      const short i,\n      const short j) const {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC mat_type mat_at(const short i, const short j) {\n    mat_type val_mat;\n#pragma clang loop unroll(full)\n    for (short ii = 0; ii < kElemsPerFrag; ++ii) {\n      val_mat.thread_elements()[ii] = frag_at(i, j)[ii];\n    }\n    return val_mat;\n  }\n  METAL_FUNC thread elem_type* elems() {\n    return reinterpret_cast<thread elem_type*>(val_frags);\n  }\n  METAL_FUNC const thread elem_type* elems() const {\n    return reinterpret_cast<const thread elem_type*>(val_frags);\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void load(const threadgroup U* src) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(\n                src[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void store(threadgroup U* dst) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(\n                dst[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void load(const device U* src, const int ld) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(src[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void store(device U* dst, const int ld) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(dst[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  load_safe(const device U* src, const int ld, const short2 src_tile_dims) {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load_safe(\n            frag_at(i, j),\n            src,\n            ld,\n            Int<1>{},\n            src_tile_dims.y,\n            src_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  store_safe(device U* dst, const int ld, const short2 dst_tile_dims) const {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store_safe(\n            frag_at(i, j),\n            dst,\n            ld,\n            Int<1>{},\n            dst_tile_dims.y,\n            dst_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void store_slice(\n      device U* dst,\n      const int ld,\n      const short2 start,\n      const short2 stop) const {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store_slice(\n            frag_at(i, j),\n            dst,\n            ld,\n            Int<1>{},\n            start.y,\n            stop.y,\n            start.x,\n            stop.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n};\ntemplate <typename T, typename U, int M, int N, int K>\nMETAL_FUNC void tile_matmad(\n    thread MMATile<T, M, N>& D,\n    thread MMATile<U, M, K>& A,\n    thread MMATile<U, K, N>& B,\n    thread MMATile<T, M, N>& C) {\n#pragma clang loop unroll(full)\n  for (short m = 0; m < M; ++m) {\n#pragma clang loop unroll(full)\n    for (short n = 0; n < N; ++n) {\n      short n_serp = (m % 2) ? (N - 1 - n) : n;\n#pragma clang loop unroll(full)\n      for (short k = 0; k < K; ++k) {\n        MMATile<T, M, N>::MMAFrag_t::mma(\n            D.frag_at(m, n_serp),\n            A.frag_at(m, k),\n            B.frag_at(k, n_serp),\n            C.frag_at(m, n_serp));\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    short lda_tgp,\n    short ldb_tgp,\n    typename AccumType = float,\n    typename Epilogue = TransformNone<U, AccumType>>\nstruct BlockMMA {\n  static constant constexpr const short kFragSize = 8;\n  using MMAFrag_acc_t = BaseMMAFrag<AccumType, kFragSize, kFragSize>;\n  static constant constexpr const short TM_stride = kFragSize * WM;\n  static constant constexpr const short TN_stride = kFragSize * WN;\n  static constant constexpr const short TM = BM / (kFragSize * WM);\n  static constant constexpr const short TN = BN / (kFragSize * WN);\n  static constant constexpr const short A_str_m = transpose_a ? 1 : lda_tgp;\n  static constant constexpr const short A_str_k = transpose_a ? lda_tgp : 1;\n  static constant constexpr const short B_str_k = transpose_b ? 1 : ldb_tgp;\n  static constant constexpr const short B_str_n = transpose_b ? ldb_tgp : 1;\n  static constant constexpr const short tile_stride_a = kFragSize * A_str_k;\n  static constant constexpr const short tile_stride_b = kFragSize * B_str_k;\n  MMATile<AccumType, TM, 1, MMAFrag_acc_t> Atile;\n  MMATile<AccumType, 1, TN, MMAFrag_acc_t> Btile;\n  MMATile<AccumType, TM, TN, MMAFrag_acc_t> Ctile;\n  short sm;\n  short sn;\n  short As_offset;\n  short Bs_offset;\n  METAL_FUNC BlockMMA(\n      ushort simd_group_id [[simdgroup_index_in_threadgroup]],\n      ushort simd_lane_id [[thread_index_in_simdgroup]]) {\n    short tm = kFragSize * (simd_group_id / WN);\n    short tn = kFragSize * (simd_group_id % WN);\n    short2 simd_coord = MMAFrag_acc_t::get_coord(simd_lane_id);\n    sm = simd_coord.y;\n    sn = simd_coord.x;\n    As_offset = (tm + sm) * A_str_m + (sn)*A_str_k;\n    Bs_offset = (sm)*B_str_k + (tn + sn) * B_str_n;\n    sm += tm;\n    sn += tn;\n  }\n  METAL_FUNC void mma(const threadgroup T* As, const threadgroup T* Bs) {\n    As += As_offset;\n    Bs += Bs_offset;\n#pragma clang loop unroll(full)\n    for (short kk = 0; kk < BK; kk += kFragSize) {\n      simdgroup_barrier(mem_flags::mem_none);\n      Atile.template load<T, WM, 1, A_str_m, A_str_k>(As);\n      simdgroup_barrier(mem_flags::mem_none);\n      Btile.template load<T, 1, WN, B_str_k, B_str_n>(Bs);\n      simdgroup_barrier(mem_flags::mem_none);\n      tile_matmad(Ctile, Atile, Btile, Ctile);\n      As += tile_stride_a;\n      Bs += tile_stride_b;\n    }\n  }\n  METAL_FUNC void store_result(device U* D, const int ldd) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    Ctile.template store<U, WM, WN>(D, ldd);\n  }\n  METAL_FUNC void\n  store_result_slice(device U* D, const int ldd, short2 start, short2 stop) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    start -= short2(sn, sm);\n    stop -= short2(sn, sm);\n    if (stop.y <= 0 || stop.x <= 0) {\n      return;\n    }\n    Ctile.template store_slice<U, WM, WN>(D, ldd, start, stop);\n  }\n  METAL_FUNC void\n  store_result_safe(device U* D, const int ldd, short2 dst_tile_dims) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    Ctile.template store_safe<U, WM, WN>(D, ldd, dst_tile_dims);\n  }\n  template <typename UnaryEpilogue>\n  METAL_FUNC void apply_epilogue(thread const UnaryEpilogue& epilogue_op) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = epilogue_op.apply(Ctile.elems()[i]);\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n#pragma clang loop unroll(full)\n        for (short k = 0; k < decltype(Ctile)::kElemsPerFrag; k++) {\n          accum[k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue_safe(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n        U c_elems[kelems] = {0};\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          if ((j * TN_stride + k) < dst_tile_dims.x) {\n            c_elems[k] = C[offset_c + k * fdc];\n          }\n        }\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          accum[k] = epilogue_op.apply(accum[k], c_elems[k]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread const auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          D[offset_d + k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result_safe(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (int i = 0; i < TM; i++) {\n      if (i * TM_stride < dst_tile_dims.y) {\n#pragma clang loop unroll(full)\n        for (int j = 0; j < TN; j++) {\n          thread const auto& accum = Ctile.frag_at(i, j);\n          int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n          int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n          for (short k = 0; k < kelems; k++) {\n            if ((j * TN_stride + k) < dst_tile_dims.x) {\n              D[offset_d + k] =\n                  epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n            }\n          }\n        }\n      }\n    }\n  }\n};\n}\n}\n\nusing namespace metal;\nusing namespace mlx::steel;\n"
+ "\n[[kernel]] void gather{0}_{3}_{6}_{7}(\n    const device {1}* src [[buffer(0)]],\n    device {1}* out [[buffer(1)]],\n    const constant int* src_shape [[buffer(2)]],\n    const constant int64_t* src_strides [[buffer(3)]],\n    const constant size_t& src_ndim [[buffer(4)]],\n    const constant int* slice_sizes [[buffer(5)]],\n    const constant int* axes [[buffer(6)]],\n    const constant int* idx_shapes [[buffer(7)]],\n    const constant int64_t* idx_strides [[buffer(8)]],\n    const constant bool* idx_contigs [[buffer(9)]],\n    const constant int& idx_ndim [[buffer(10)]],\n    {4}\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {{\n  Indices<{2}, {3}> idxs{{\n    {{ {5} }}, idx_shapes, idx_strides, idx_contigs, idx_ndim}};\n\n  return gather_impl<{1}, {2}, {3}, {6}, {7}>(\n      src,\n      out,\n      src_shape,\n      src_strides,\n      src_ndim,\n      slice_sizes,\n      axes,\n      idxs,\n      index,\n      grid_dim);\n}}\n"
+ "\n[[kernel]] void scatter{0}_{4}_updc_{7}_nwork{8}_{9}(\n    const device {1}* updates [[buffer(1)]],\n    device mlx_atomic<{1}>* out [[buffer(2)]],\n    const constant int* upd_shape [[buffer(3)]],\n    const constant int64_t* upd_strides [[buffer(4)]],\n    const constant size_t& upd_ndim [[buffer(5)]],\n    const constant size_t& upd_size [[buffer(6)]],\n    const constant int* out_shape [[buffer(7)]],\n    const constant int64_t* out_strides [[buffer(8)]],\n    const constant size_t& out_ndim [[buffer(9)]],\n    const constant int* axes [[buffer(10)]],\n    const constant int* idx_shapes [[buffer(11)]],\n    const constant int64_t* idx_strides [[buffer(12)]],\n    const constant bool* idx_contigs [[buffer(13)]],\n    const constant int& idx_ndim [[buffer(14)]],\n    const constant size_t& idx_size [[buffer(15)]],\n    {5}\n    uint2 gid [[thread_position_in_grid]]) {{\n  Indices<{2}, {4}> idxs{{ {{ {6} }}, idx_shapes, idx_strides, idx_contigs, idx_ndim}};\n\n  return scatter_impl<{1}, {2}, {3}, {4}, {7}, {8}, {9}>(\n      updates,\n      out,\n      upd_shape,\n      upd_strides,\n      upd_ndim,\n      upd_size,\n      out_shape,\n      out_strides,\n      out_ndim,\n      axes,\n      idx_size,\n      idxs,\n      gid);\n}}\n"
+ "\nfloat erf(float a) {\n  float r, s, t, u;\n  t = metal::abs(a);\n  s = a * a;\n  if (t > 0.927734375f) {\n    r = metal::fma(\n        -1.72853470e-5f, t, 3.83197126e-4f);\n    u = metal::fma(\n        -3.88396438e-3f, t, 2.42546219e-2f);\n    r = metal::fma(r, s, u);\n    r = metal::fma(r, t, -1.06777877e-1f);\n    r = metal::fma(r, t, -6.34846687e-1f);\n    r = metal::fma(r, t, -1.28717512e-1f);\n    r = metal::fma(r, t, -t);\n    r = 1.0f - metal::exp(r);\n    r = metal::copysign(r, a);\n  } else {\n    r = -5.96761703e-4f;\n    r = metal::fma(r, s, 4.99119423e-3f);\n    r = metal::fma(r, s, -2.67681349e-2f);\n    r = metal::fma(r, s, 1.12819925e-1f);\n    r = metal::fma(r, s, -3.76125336e-1f);\n    r = metal::fma(r, s, 1.28379166e-1f);\n    r = metal::fma(r, a, a);\n  }\n  return r;\n}\nfloat erfinv(float a) {\n  auto t = metal::fma(a, 0.0f - a, 1.0f);\n  t = metal::log(t);\n  float p;\n  if (metal::abs(t) > 6.125f) {\n    p = 3.03697567e-10f;\n    p = metal::fma(p, t, 2.93243101e-8f);\n    p = metal::fma(p, t, 1.22150334e-6f);\n    p = metal::fma(p, t, 2.84108955e-5f);\n    p = metal::fma(p, t, 3.93552968e-4f);\n    p = metal::fma(p, t, 3.02698812e-3f);\n    p = metal::fma(p, t, 4.83185798e-3f);\n    p = metal::fma(p, t, -2.64646143e-1f);\n    p = metal::fma(p, t, 8.40016484e-1f);\n  } else {\n    p = 5.43877832e-9f;\n    p = metal::fma(p, t, 1.43285448e-7f);\n    p = metal::fma(p, t, 1.22774793e-6f);\n    p = metal::fma(p, t, 1.12963626e-7f);\n    p = metal::fma(p, t, -5.61530760e-5f);\n    p = metal::fma(p, t, -1.47697632e-4f);\n    p = metal::fma(p, t, 2.31468678e-3f);\n    p = metal::fma(p, t, 1.15392581e-2f);\n    p = metal::fma(p, t, -2.32015476e-1f);\n    p = metal::fma(p, t, 8.86226892e-1f);\n  }\n  return a * p;\n}\nfloat expm1f_scaled_unchecked(float a, float b) {\n  float f, j, r, s, t, u, v, x, y;\n  int i;\n  j = fma(1.442695f, a, 12582912.f);\n  j = j - 12582912.0f;\n  i = (int)j;\n  f = fma(j, -6.93145752e-1f, a);\n  s = f * f;\n  if (a == 0.0f)\n    s = a;\n  r = 1.97350979e-4f;\n  r = fma(r, f, 1.39309070e-3f);\n  r = fma(r, f, 8.33343994e-3f);\n  r = fma(r, f, 4.16668020e-2f);\n  r = fma(r, f, 1.66666716e-1f);\n  r = fma(r, f, 4.99999970e-1f);\n  u = (j == 1) ? (f + 0.5f) : f;\n  v = fma(r, s, u);\n  s = 0.5f * b;\n  t = ldexp(s, i);\n  y = t - s;\n  x = (t - y) - s;\n  r = fma(v, t, x) + y;\n  r = r + r;\n  if (j == 0)\n    r = v;\n  if (j == 1)\n    r = v + v;\n  return r;\n}\nfloat expm1f(float a) {\n  float r;\n  r = expm1f_scaled_unchecked(a, 1.0f);\n  if (abs(a - 1.0f) > 88.0f) {\n    r = pow(2, a);\n    r = fma(r, r, -1.0f);\n  }\n  return r;\n}\n\nnamespace {\nconstant float inf = metal::numeric_limits<float>::infinity();\n}\nstruct Abs {\n  template <typename T>\n  T operator()(T x) {\n    return metal::abs(x);\n  };\n  template <>\n  uint8_t operator()(uint8_t x) {\n    return x;\n  };\n  template <>\n  uint16_t operator()(uint16_t x) {\n    return x;\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x;\n  };\n  template <>\n  uint64_t operator()(uint64_t x) {\n    return x;\n  };\n  template <>\n  bool operator()(bool x) {\n    return x;\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {metal::precise::sqrt(x.real * x.real + x.imag * x.imag), 0};\n  };\n};\nstruct ArcCos {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::acos(x);\n  };\n};\nstruct ArcCosh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::acosh(x);\n  };\n};\nstruct ArcSin {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::asin(x);\n  };\n};\nstruct ArcSinh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::asinh(x);\n  };\n};\nstruct ArcTan {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::atan(x);\n  };\n};\nstruct ArcTanh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::atanh(x);\n  };\n};\nstruct BitwiseInvert {\n  template <typename T>\n  T operator()(T x) {\n    return ~x;\n  };\n};\nstruct Ceil {\n  template <typename T>\n  T operator()(T x) {\n    return metal::ceil(x);\n  };\n  template <>\n  int8_t operator()(int8_t x) {\n    return x;\n  };\n  template <>\n  int16_t operator()(int16_t x) {\n    return x;\n  };\n  template <>\n  int32_t operator()(int32_t x) {\n    return x;\n  };\n  template <>\n  int64_t operator()(int64_t x) {\n    return x;\n  };\n  template <>\n  uint8_t operator()(uint8_t x) {\n    return x;\n  };\n  template <>\n  uint16_t operator()(uint16_t x) {\n    return x;\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x;\n  };\n  template <>\n  uint64_t operator()(uint64_t x) {\n    return x;\n  };\n  template <>\n  bool operator()(bool x) {\n    return x;\n  };\n};\nstruct Cos {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::cos(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::cos(x.real) * metal::precise::cosh(x.imag),\n        -metal::precise::sin(x.real) * metal::precise::sinh(x.imag)};\n  };\n};\nstruct Cosh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::cosh(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::cosh(x.real) * metal::precise::cos(x.imag),\n        metal::precise::sinh(x.real) * metal::precise::sin(x.imag)};\n  };\n};\nstruct Conjugate {\n  complex64_t operator()(complex64_t x) {\n    return complex64_t{x.real, -x.imag};\n  }\n};\nstruct Erf {\n  template <typename T>\n  T operator()(T x) {\n    return static_cast<T>(erf(static_cast<float>(x)));\n  };\n};\nstruct ErfInv {\n  template <typename T>\n  T operator()(T x) {\n    return static_cast<T>(erfinv(static_cast<float>(x)));\n  };\n};\nstruct Exp {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::exp(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    auto m = metal::precise::exp(x.real);\n    return {m * metal::precise::cos(x.imag), m * metal::precise::sin(x.imag)};\n  }\n};\nstruct Expm1 {\n  template <typename T>\n  T operator()(T x) {\n    return static_cast<T>(expm1f(static_cast<float>(x)));\n  };\n};\nstruct Floor {\n  template <typename T>\n  T operator()(T x) {\n    return metal::floor(x);\n  };\n  template <>\n  int8_t operator()(int8_t x) {\n    return x;\n  };\n  template <>\n  int16_t operator()(int16_t x) {\n    return x;\n  };\n  template <>\n  int32_t operator()(int32_t x) {\n    return x;\n  };\n  template <>\n  int64_t operator()(int64_t x) {\n    return x;\n  };\n  template <>\n  uint8_t operator()(uint8_t x) {\n    return x;\n  };\n  template <>\n  uint16_t operator()(uint16_t x) {\n    return x;\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x;\n  };\n  template <>\n  uint64_t operator()(uint64_t x) {\n    return x;\n  };\n  template <>\n  bool operator()(bool x) {\n    return x;\n  };\n};\nstruct Imag {\n  template <typename T>\n  T operator()(T x) {\n    return x.imag;\n  };\n};\nstruct Log {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::log(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    auto r = metal::precise::log(Abs{}(x).real);\n    auto i = metal::precise::atan2(x.imag, x.real);\n    return {r, i};\n  };\n};\nstruct Log2 {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::log2(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    auto y = Log{}(x);\n    return {y.real / M_LN2_F, y.imag / M_LN2_F};\n  };\n};\nstruct Log10 {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::log10(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    auto y = Log{}(x);\n    return {y.real / M_LN10_F, y.imag / M_LN10_F};\n  };\n};\nstruct Log1p {\n  template <typename T>\n  T operator()(T x) {\n    return log1p(x);\n  };\n};\nstruct LogicalNot {\n  template <typename T>\n  T operator()(T x) {\n    return !x;\n  };\n};\nstruct Negative {\n  template <typename T>\n  T operator()(T x) {\n    return -x;\n  };\n};\nstruct Real {\n  template <typename T>\n  T operator()(T x) {\n    return x.real;\n  };\n};\nstruct Round {\n  template <typename T>\n  T operator()(T x) {\n    return metal::rint(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {metal::rint(x.real), metal::rint(x.imag)};\n  };\n};\nstruct Sigmoid {\n  template <typename T>\n  T operator()(T x) {\n    auto y = 1 / (1 + metal::exp(-metal::abs(x)));\n    return (x < 0) ? 1 - y : y;\n  }\n};\nstruct Sign {\n  template <typename T>\n  T operator()(T x) {\n    return (x > T(0)) - (x < T(0));\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x != 0;\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    if (x == complex64_t(0)) {\n      return x;\n    }\n    return x /\n        (complex64_t)metal::precise::sqrt(x.real * x.real + x.imag * x.imag);\n  };\n};\nstruct Sin {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::sin(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::sin(x.real) * metal::precise::cosh(x.imag),\n        metal::precise::cos(x.real) * metal::precise::sinh(x.imag)};\n  };\n};\nstruct Sinh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::sinh(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::sinh(x.real) * metal::precise::cos(x.imag),\n        metal::precise::cosh(x.real) * metal::precise::sin(x.imag)};\n  };\n};\nstruct Square {\n  template <typename T>\n  T operator()(T x) {\n    return x * x;\n  };\n};\nstruct Sqrt {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::sqrt(x);\n  };\n};\nstruct Rsqrt {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::rsqrt(x);\n  };\n};\nstruct Tan {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::tan(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    float tan_a = metal::precise::tan(x.real);\n    float tanh_b = metal::precise::tanh(x.imag);\n    float t1 = tan_a * tanh_b;\n    float denom = 1. + t1 * t1;\n    return {(tan_a - tanh_b * t1) / denom, (tanh_b + tan_a * t1) / denom};\n  };\n};\nstruct Tanh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::tanh(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    float tanh_a = metal::precise::tanh(x.real);\n    float tan_b = metal::precise::tan(x.imag);\n    float t1 = tanh_a * tan_b;\n    float denom = 1. + t1 * t1;\n    return {(tanh_a + tan_b * t1) / denom, (tan_b - tanh_a * t1) / denom};\n  };\n};\n"
+ "\nnamespace mlx {\nnamespace steel {\ntemplate <\n    typename T,\n    short BROWS,\n    short BCOLS,\n    short dst_ld,\n    short reduction_dim,\n    short tgp_size,\n    short alignment = 1,\n    short n_reads = (BCOLS * BROWS) / (tgp_size),\n    short TCOLS = BCOLS / n_reads,\n    short TROWS = tgp_size / TCOLS>\nstruct BlockLoader {\n  static constant constexpr const short n_rows = (BROWS + TROWS - 1) / TROWS;\n  static constant constexpr const short vec_size = n_reads;\n  const int src_ld;\n  const int tile_stride;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  struct alignas(alignment * sizeof(T)) ReadVector {\n    uint8_t v[sizeof(T) * vec_size];\n  };\n  METAL_FUNC BlockLoader(\n      const device T* src_,\n      const int src_ld_,\n      threadgroup T* dst_,\n      ushort simd_group_id [[simdgroup_index_in_threadgroup]],\n      ushort simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(src_ld_),\n        tile_stride(reduction_dim ? BCOLS : BROWS * src_ld),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld + bj) {}\n  template <typename UnaryOp>\n  METAL_FUNC void apply_inplace_op(thread const UnaryOp& op) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        dst[i * dst_ld + j] = op.apply(dst[i * dst_ld + j]);\n      }\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < BROWS; i += TROWS) {\n      *((threadgroup ReadVector*)(&dst[i * dst_ld])) =\n          *((const device ReadVector*)(&src[i * src_ld]));\n    }\n  }\n  METAL_FUNC void load_safe(short2 src_tile_dim) const {\n    src_tile_dim = src_tile_dim - short2(bj, bi);\n    if (src_tile_dim.x <= 0 || src_tile_dim.y <= 0) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n      return;\n    }\n    bool tmp_idx[vec_size];\n    T tmp_val[vec_size];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        tmp_idx[j] = (i < src_tile_dim.y) && (j < src_tile_dim.x);\n      }\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        tmp_val[j] = src[(tmp_idx[j] ? i * src_ld + j : 0)];\n      }\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        tmp_val[j] = tmp_idx[j] ? tmp_val[j] : T(0);\n      }\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        dst[i * dst_ld + j] = tmp_val[j];\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    src += tile_stride;\n  }\n};\n}\n}\nMETAL_FUNC ulong2 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const int64_t* a_strides,\n    constant const int64_t* b_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n  }\n  return ulong2(loc_a, loc_b);\n}\nMETAL_FUNC ulong3 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const int64_t* a_strides,\n    constant const int64_t* b_strides,\n    constant const int64_t* c_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  ulong loc_c{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n    loc_c += pos_in_dim * c_strides[i];\n  }\n  return ulong3(loc_a, loc_b, loc_c);\n}\nnamespace mlx {\nnamespace steel {\ntemplate <typename OutT, typename InT>\nstruct TransformNone {\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT) {\n    return static_cast<OutT>(x);\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAdd {\n  TransformAdd(const float, const float) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT c) {\n    return static_cast<OutT>(x) + c;\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAxpby {\n  const float alpha;\n  const float beta;\n  TransformAxpby(const float alpha_, const float beta_)\n      : alpha(alpha_), beta(beta_) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  METAL_FUNC OutT apply(InT x, OutT c) const {\n    return static_cast<OutT>(x * alpha + (beta * c));\n  }\n};\ntemplate <typename T>\nstruct AccumHelper {\n  typedef float accum_type;\n};\nstruct BlockSwizzle {\n  static METAL_FUNC int2\n  swizzle(uint3 tid [[threadgroup_position_in_grid]], const int swizzle_log) {\n    const int tid_x = (tid.x) >> swizzle_log;\n    const int tid_y =\n        ((tid.y) << swizzle_log) + ((tid.x) & ((1 << swizzle_log) - 1));\n    return int2(tid_x, tid_y);\n  }\n};\n}\n}\n#pragma METAL internals : enable\nnamespace metal {\ntemplate <typename T>\nstruct is_empty : metal::bool_constant<__is_empty(T)> {};\ntemplate <typename... Ts>\nstruct make_void {\n  typedef void type;\n};\ntemplate <typename... Ts>\nusing void_t = typename make_void<Ts...>::type;\ntemplate <class T>\nstruct is_static : metal::bool_constant<is_empty<remove_cv_t<T>>::value> {};\ntemplate <typename T>\nstruct pointer_element {};\ntemplate <typename T>\nstruct pointer_element<thread T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<device T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<constant T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<threadgroup T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nusing pointer_element_t = typename pointer_element<remove_cv_t<T>>::type;\n}\n#pragma METAL internals : disable\n\n#pragma METAL internals : enable\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, T v>\nstruct integral_constant {\n  static constexpr constant T value = v;\n  using value_type = T;\n  using type = integral_constant;\n  METAL_FUNC constexpr operator value_type() const noexcept {\n    return value;\n  }\n};\ntemplate <bool B>\nusing bool_constant = integral_constant<bool, B>;\nusing true_type = bool_constant<true>;\nusing false_type = bool_constant<false>;\ntemplate <class T>\nstruct is_integral : bool_constant<metal::is_integral<T>::value> {};\ntemplate <class T, T v>\nstruct is_integral<integral_constant<T, v>>\n    : bool_constant<metal::is_integral<T>::value> {};\ntemplate <typename T>\nconstexpr constant bool is_integral_v = is_integral<T>::value;\ntemplate <int val>\nusing Int = integral_constant<int, val>;\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator+( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv + uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator-( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv - uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator*( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv * uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator/( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv / uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator==( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv == uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator!=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv != uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv < uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv > uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv <= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv >= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator&&( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv && uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator||( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv || uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T>\nMETAL_FUNC constexpr T sum(T x) {\n  return x;\n}\ntemplate <typename T, typename... Us>\nMETAL_FUNC constexpr auto sum(T x, Us... us) {\n  return x + sum(us...);\n}\n}\n}\n#pragma METAL internals : disable\n\nusing namespace metal;\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, int kFragRows_, int kFragCols_>\nstruct BaseMMAFrag {\n  static_assert(\n      kFragRows_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n  static_assert(\n      kFragCols_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n};\ntemplate <typename T>\nstruct BaseMMAFrag<T, 8, 8> {\n  static constant constexpr const int kFragRows = 8;\n  static constant constexpr const int kFragCols = 8;\n  static constant constexpr const int kElemsPerFrag = (kFragRows * kFragCols) / 32;\n  static constant constexpr const int kElemRows = 1;\n  static constant constexpr const int kElemCols = 2;\n  static_assert(\n      kElemRows * kElemCols == kElemsPerFrag,\n      \"MMAFrag shape is not consistent with MMAFrag size\");\n  typedef metal::simdgroup_matrix<T, kFragRows, kFragCols> mat_type;\n  typedef metal::vec<T, kElemsPerFrag> frag_type;\n  METAL_FUNC static constexpr short2 get_coord(ushort simd_lane_id\n                                               [[thread_index_in_simdgroup]]) {\n    const short qid = simd_lane_id / 4;\n    const short fm = (qid & 4) + ((simd_lane_id / 2) % 4);\n    const short fn = (qid & 2) * 2 + (simd_lane_id % 2) * 2;\n    return short2{fn, fm};\n  }\n  template <typename SrcPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  load(thread frag_type& dst, SrcPtrType src, StrX str_x, StrY str_y) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * kElemCols + j] = static_cast<T>(src[i * str_x + j * str_y]);\n      }\n    }\n  }\n  template <\n      typename SrcPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void load_safe(\n      thread frag_type& dst,\n      SrcPtrType src,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[i * kElemCols + j] =\n              static_cast<T>(src[(off_x + i) * str_x + (off_x + j) * str_y]);\n        } else {\n          dst[i * kElemCols + j] = T(0);\n        }\n      }\n    }\n  }\n  template <typename DstPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  store(const thread frag_type& src, DstPtrType dst, StrX str_x, StrY str_y) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * str_x + j * str_y] = static_cast<U>(src[i * kElemCols + j]);\n      }\n    }\n  }\n  template <\n      typename DstPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void store_safe(\n      const thread frag_type& src,\n      DstPtrType dst,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[(off_x + i) * str_x + (off_y + j) * str_y] =\n              static_cast<U>(src[i * kElemCols + j]);\n        }\n      }\n    }\n  }\n  template <\n      typename DstPtrType,\n      typename StrX,\n      typename StrY,\n      typename StartX,\n      typename StopX,\n      typename StartY,\n      typename StopY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void store_slice(\n      const thread frag_type& src,\n      DstPtrType dst,\n      StrX str_x,\n      StrY str_y,\n      StartX start_x,\n      StopX stop_x,\n      StartY start_y,\n      StopY stop_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < stop_x && (off_x + i) >= start_x &&\n            (off_y + j) < stop_y && (off_y + j) >= start_y) {\n          dst[(off_x + i) * str_x + (off_y + j) * str_y] =\n              static_cast<U>(src[i * kElemCols + j]);\n        }\n      }\n    }\n  }\n  METAL_FUNC static constexpr void mma(\n      thread frag_type& D,\n      thread frag_type& A,\n      thread frag_type& B,\n      thread frag_type& C) {\n    mat_type D_mat;\n    mat_type A_mat;\n    mat_type B_mat;\n    mat_type C_mat;\n    reinterpret_cast<thread frag_type&>(A_mat.thread_elements()) = A;\n    reinterpret_cast<thread frag_type&>(B_mat.thread_elements()) = B;\n    reinterpret_cast<thread frag_type&>(C_mat.thread_elements()) = C;\n    mma(D_mat, A_mat, B_mat, C_mat);\n    D = reinterpret_cast<thread frag_type&>(D_mat.thread_elements());\n  }\n  METAL_FUNC static constexpr void mma(\n      thread mat_type& D,\n      thread mat_type& A,\n      thread mat_type& B,\n      thread mat_type& C) {\n    simdgroup_multiply_accumulate(D, A, B, C);\n  }\n};\ntemplate <\n    typename T,\n    int kTileRows_,\n    int kTileCols_,\n    class MMAFrag_ = BaseMMAFrag<T, 8, 8>>\nstruct MMATile {\n  using MMAFrag_t = MMAFrag_;\n  using elem_type = T;\n  static constant constexpr const int kFragRows = MMAFrag_t::kFragRows;\n  static constant constexpr const int kFragCols = MMAFrag_t::kFragCols;\n  static constant constexpr const int kElemsPerFrag = MMAFrag_t::kElemsPerFrag;\n  static constant constexpr const int kTileRows = kTileRows_;\n  static constant constexpr const int kTileCols = kTileCols_;\n  static constant constexpr const int kRows = kTileRows * kFragRows;\n  static constant constexpr const int kCols = kTileCols * kFragCols;\n  static constant constexpr const int kNumFrags = kTileRows * kTileCols;\n  static constant constexpr const int kElemsPerTile = kNumFrags * kElemsPerFrag;\n  typedef typename MMAFrag_t::mat_type mat_type;\n  typedef typename MMAFrag_t::frag_type frag_type;\n  frag_type val_frags[kNumFrags] = {frag_type(0)};\n  METAL_FUNC MMATile() thread {}\n  METAL_FUNC constexpr void clear() {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kNumFrags; ++i) {\n      val_frags[i] = frag_type(0);\n    }\n  }\n  METAL_FUNC constexpr thread frag_type& frag_at(const short i, const short j) {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC constexpr const thread frag_type& frag_at(\n      const short i,\n      const short j) const {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC mat_type mat_at(const short i, const short j) {\n    mat_type val_mat;\n#pragma clang loop unroll(full)\n    for (short ii = 0; ii < kElemsPerFrag; ++ii) {\n      val_mat.thread_elements()[ii] = frag_at(i, j)[ii];\n    }\n    return val_mat;\n  }\n  METAL_FUNC thread elem_type* elems() {\n    return reinterpret_cast<thread elem_type*>(val_frags);\n  }\n  METAL_FUNC const thread elem_type* elems() const {\n    return reinterpret_cast<const thread elem_type*>(val_frags);\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void load(const threadgroup U* src) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(\n                src[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void store(threadgroup U* dst) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(\n                dst[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void load(const device U* src, const int ld) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(src[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void store(device U* dst, const int ld) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(dst[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  load_safe(const device U* src, const int ld, const short2 src_tile_dims) {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load_safe(\n            frag_at(i, j),\n            src,\n            ld,\n            Int<1>{},\n            src_tile_dims.y,\n            src_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  store_safe(device U* dst, const int ld, const short2 dst_tile_dims) const {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store_safe(\n            frag_at(i, j),\n            dst,\n            ld,\n            Int<1>{},\n            dst_tile_dims.y,\n            dst_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void store_slice(\n      device U* dst,\n      const int ld,\n      const short2 start,\n      const short2 stop) const {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store_slice(\n            frag_at(i, j),\n            dst,\n            ld,\n            Int<1>{},\n            start.y,\n            stop.y,\n            start.x,\n            stop.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n};\ntemplate <typename T, typename U, int M, int N, int K>\nMETAL_FUNC void tile_matmad(\n    thread MMATile<T, M, N>& D,\n    thread MMATile<U, M, K>& A,\n    thread MMATile<U, K, N>& B,\n    thread MMATile<T, M, N>& C) {\n#pragma clang loop unroll(full)\n  for (short m = 0; m < M; ++m) {\n#pragma clang loop unroll(full)\n    for (short n = 0; n < N; ++n) {\n      short n_serp = (m % 2) ? (N - 1 - n) : n;\n#pragma clang loop unroll(full)\n      for (short k = 0; k < K; ++k) {\n        MMATile<T, M, N>::MMAFrag_t::mma(\n            D.frag_at(m, n_serp),\n            A.frag_at(m, k),\n            B.frag_at(k, n_serp),\n            C.frag_at(m, n_serp));\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    short lda_tgp,\n    short ldb_tgp,\n    typename AccumType = float,\n    typename Epilogue = TransformNone<U, AccumType>>\nstruct BlockMMA {\n  static constant constexpr const short kFragSize = 8;\n  using MMAFrag_acc_t = BaseMMAFrag<AccumType, kFragSize, kFragSize>;\n  static constant constexpr const short TM_stride = kFragSize * WM;\n  static constant constexpr const short TN_stride = kFragSize * WN;\n  static constant constexpr const short TM = BM / (kFragSize * WM);\n  static constant constexpr const short TN = BN / (kFragSize * WN);\n  static constant constexpr const short A_str_m = transpose_a ? 1 : lda_tgp;\n  static constant constexpr const short A_str_k = transpose_a ? lda_tgp : 1;\n  static constant constexpr const short B_str_k = transpose_b ? 1 : ldb_tgp;\n  static constant constexpr const short B_str_n = transpose_b ? ldb_tgp : 1;\n  static constant constexpr const short tile_stride_a = kFragSize * A_str_k;\n  static constant constexpr const short tile_stride_b = kFragSize * B_str_k;\n  MMATile<AccumType, TM, 1, MMAFrag_acc_t> Atile;\n  MMATile<AccumType, 1, TN, MMAFrag_acc_t> Btile;\n  MMATile<AccumType, TM, TN, MMAFrag_acc_t> Ctile;\n  short sm;\n  short sn;\n  short As_offset;\n  short Bs_offset;\n  METAL_FUNC BlockMMA(\n      ushort simd_group_id [[simdgroup_index_in_threadgroup]],\n      ushort simd_lane_id [[thread_index_in_simdgroup]]) {\n    short tm = kFragSize * (simd_group_id / WN);\n    short tn = kFragSize * (simd_group_id % WN);\n    short2 simd_coord = MMAFrag_acc_t::get_coord(simd_lane_id);\n    sm = simd_coord.y;\n    sn = simd_coord.x;\n    As_offset = (tm + sm) * A_str_m + (sn)*A_str_k;\n    Bs_offset = (sm)*B_str_k + (tn + sn) * B_str_n;\n    sm += tm;\n    sn += tn;\n  }\n  METAL_FUNC void mma(const threadgroup T* As, const threadgroup T* Bs) {\n    As += As_offset;\n    Bs += Bs_offset;\n#pragma clang loop unroll(full)\n    for (short kk = 0; kk < BK; kk += kFragSize) {\n      simdgroup_barrier(mem_flags::mem_none);\n      Atile.template load<T, WM, 1, A_str_m, A_str_k>(As);\n      simdgroup_barrier(mem_flags::mem_none);\n      Btile.template load<T, 1, WN, B_str_k, B_str_n>(Bs);\n      simdgroup_barrier(mem_flags::mem_none);\n      tile_matmad(Ctile, Atile, Btile, Ctile);\n      As += tile_stride_a;\n      Bs += tile_stride_b;\n    }\n  }\n  METAL_FUNC void store_result(device U* D, const int ldd) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    Ctile.template store<U, WM, WN>(D, ldd);\n  }\n  METAL_FUNC void\n  store_result_slice(device U* D, const int ldd, short2 start, short2 stop) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    start -= short2(sn, sm);\n    stop -= short2(sn, sm);\n    if (stop.y <= 0 || stop.x <= 0) {\n      return;\n    }\n    Ctile.template store_slice<U, WM, WN>(D, ldd, start, stop);\n  }\n  METAL_FUNC void\n  store_result_safe(device U* D, const int ldd, short2 dst_tile_dims) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    Ctile.template store_safe<U, WM, WN>(D, ldd, dst_tile_dims);\n  }\n  template <typename UnaryEpilogue>\n  METAL_FUNC void apply_epilogue(thread const UnaryEpilogue& epilogue_op) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = epilogue_op.apply(Ctile.elems()[i]);\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n#pragma clang loop unroll(full)\n        for (short k = 0; k < decltype(Ctile)::kElemsPerFrag; k++) {\n          accum[k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue_safe(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n        U c_elems[kelems] = {0};\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          if ((j * TN_stride + k) < dst_tile_dims.x) {\n            c_elems[k] = C[offset_c + k * fdc];\n          }\n        }\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          accum[k] = epilogue_op.apply(accum[k], c_elems[k]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread const auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          D[offset_d + k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result_safe(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (int i = 0; i < TM; i++) {\n      if (i * TM_stride < dst_tile_dims.y) {\n#pragma clang loop unroll(full)\n        for (int j = 0; j < TN; j++) {\n          thread const auto& accum = Ctile.frag_at(i, j);\n          int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n          int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n          for (short k = 0; k < kelems; k++) {\n            if ((j * TN_stride + k) < dst_tile_dims.x) {\n              D[offset_d + k] =\n                  epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n            }\n          }\n        }\n      }\n    }\n  }\n};\n}\n}\nnamespace mlx {\nnamespace steel {\nstruct GEMMParams {\n  const int M;\n  const int N;\n  const int K;\n  const int lda;\n  const int ldb;\n  const int ldd;\n  const int tiles_n;\n  const int tiles_m;\n  const int64_t batch_stride_a;\n  const int64_t batch_stride_b;\n  const int64_t batch_stride_d;\n  const int swizzle_log;\n  const int gemm_k_iterations_aligned;\n  const int batch_ndim;\n};\nstruct GEMMSpiltKParams {\n  const int M;\n  const int N;\n  const int K;\n  const int lda;\n  const int ldb;\n  const int ldc;\n  const int tiles_n;\n  const int tiles_m;\n  const int split_k_partitions;\n  const int split_k_partition_stride;\n  const int split_k_partition_size;\n  const int gemm_k_iterations_aligned;\n};\nstruct GEMMAddMMParams {\n  const int ldc;\n  const int fdc;\n  const int64_t batch_stride_c;\n  const float alpha;\n  const float beta;\n};\n}\n}\nusing namespace metal;\nnamespace mlx {\nnamespace steel {\ntemplate <bool M_aligned, bool N_aligned, bool K_aligned>\nstruct LoopAlignment {};\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    bool MN_aligned,\n    bool K_aligned,\n    typename AccumType = typename AccumHelper<T>::accum_type,\n    typename Epilogue = TransformNone<U, AccumType>>\nstruct GEMMKernel {\n  static constant constexpr const short tgp_padding_a = 16 / sizeof(T);\n  static constant constexpr const short tgp_padding_b = 16 / sizeof(T);\n  static constant constexpr const short tgp_mem_size_a =\n      transpose_a ? BK * (BM + tgp_padding_a) : BM * (BK + tgp_padding_a);\n  static constant constexpr const short tgp_mem_size_b =\n      transpose_b ? BN * (BK + tgp_padding_b) : BK * (BN + tgp_padding_b);\n  static constant constexpr const short tgp_mem_size = tgp_mem_size_a + tgp_mem_size_b;\n  static constant constexpr const short tgp_size = WM * WN * 32;\n  using loader_a_t = BlockLoader<\n      T,\n      transpose_a ? BK : BM,\n      transpose_a ? BM : BK,\n      transpose_a ? BM + tgp_padding_a : BK + tgp_padding_a,\n      !transpose_a,\n      tgp_size>;\n  using loader_b_t = BlockLoader<\n      T,\n      transpose_b ? BN : BK,\n      transpose_b ? BK : BN,\n      transpose_b ? BK + tgp_padding_b : BN + tgp_padding_b,\n      transpose_b,\n      tgp_size>;\n  using mma_t = BlockMMA<\n      T,\n      U,\n      BM,\n      BN,\n      BK,\n      WM,\n      WN,\n      transpose_a,\n      transpose_b,\n      transpose_a ? BM + tgp_padding_a : BK + tgp_padding_a,\n      transpose_b ? BK + tgp_padding_b : BN + tgp_padding_b,\n      AccumType,\n      Epilogue>;\n  template <bool M_aligned, bool N_aligned, bool K_aligned_>\n  static METAL_FUNC void gemm_loop(\n      threadgroup T* As [[threadgroup(0)]],\n      threadgroup T* Bs [[threadgroup(1)]],\n      const int gemm_k_iterations,\n      thread loader_a_t& loader_a,\n      thread loader_b_t& loader_b,\n      thread mma_t& mma_op,\n      thread const short& tgp_bm,\n      thread const short& tgp_bn,\n      thread const short& lbk,\n      LoopAlignment<M_aligned, N_aligned, K_aligned_> l = {}) {\n    (void)l;\n    short2 tile_dims_A = transpose_a ? short2(tgp_bm, BK) : short2(BK, tgp_bm);\n    short2 tile_dims_B = transpose_b ? short2(BK, tgp_bn) : short2(tgp_bn, BK);\n    for (int k = 0; k < gemm_k_iterations; k++) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      if (M_aligned) {\n        loader_a.load_unsafe();\n      } else {\n        loader_a.load_safe(tile_dims_A);\n      }\n      if (N_aligned) {\n        loader_b.load_unsafe();\n      } else {\n        loader_b.load_safe(tile_dims_B);\n      }\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      mma_op.mma(As, Bs);\n      loader_a.next();\n      loader_b.next();\n    }\n    if (!K_aligned_) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      short2 tile_dims_A_last =\n          transpose_a ? short2(tgp_bm, lbk) : short2(lbk, tgp_bm);\n      short2 tile_dims_B_last =\n          transpose_b ? short2(lbk, tgp_bn) : short2(tgp_bn, lbk);\n      loader_a.load_safe(tile_dims_A_last);\n      loader_b.load_safe(tile_dims_B_last);\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      mma_op.mma(As, Bs);\n    }\n  }\n  static METAL_FUNC void run(\n      const device T* A [[buffer(0)]],\n      const device T* B [[buffer(1)]],\n      device U* D [[buffer(2)]],\n      const constant GEMMParams* params [[buffer(3)]],\n      threadgroup T* As [[threadgroup(0)]],\n      threadgroup T* Bs [[threadgroup(1)]],\n      uint simd_lane_id [[thread_index_in_simdgroup]],\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint3 tid [[threadgroup_position_in_grid]],\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    (void)lid;\n    const int tid_y = ((tid.y) << params->swizzle_log) +\n        ((tid.x) & ((1 << params->swizzle_log) - 1));\n    const int tid_x = (tid.x) >> params->swizzle_log;\n    if (params->tiles_n <= tid_x || params->tiles_m <= tid_y) {\n      return;\n    }\n    threadgroup_barrier(mem_flags::mem_none);\n    const int c_row = tid_y * BM;\n    const int c_col = tid_x * BN;\n    const size_t c_row_long = size_t(c_row);\n    const size_t c_col_long = size_t(c_col);\n    A += transpose_a ? c_row_long : c_row_long * params->lda;\n    B += transpose_b ? c_col_long * params->ldb : c_col_long;\n    D += c_row_long * params->ldd + c_col_long;\n    thread loader_a_t loader_a(A, params->lda, As, simd_group_id, simd_lane_id);\n    thread loader_b_t loader_b(B, params->ldb, Bs, simd_group_id, simd_lane_id);\n    thread mma_t mma_op(simd_group_id, simd_lane_id);\n    int gemm_k_iterations = params->gemm_k_iterations_aligned;\n    if (MN_aligned) {\n      for (int k = 0; k < gemm_k_iterations; k++) {\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        loader_a.load_unsafe();\n        loader_b.load_unsafe();\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        mma_op.mma(As, Bs);\n        loader_a.next();\n        loader_b.next();\n      }\n      threadgroup_barrier(mem_flags::mem_none);\n      if (!K_aligned) {\n        int lbk = params->K - params->gemm_k_iterations_aligned * BK;\n        short2 tile_dims_A = transpose_a ? short2(BM, lbk) : short2(lbk, BM);\n        short2 tile_dims_B = transpose_b ? short2(lbk, BN) : short2(BN, lbk);\n        loader_a.load_safe(tile_dims_A);\n        loader_b.load_safe(tile_dims_B);\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        mma_op.mma(As, Bs);\n      }\n      mma_op.store_result(D, params->ldd);\n      return;\n    }\n    else {\n      short tgp_bm = min(BM, params->M - c_row);\n      short tgp_bn = min(BN, params->N - c_col);\n      short leftover_bk = params->K - params->gemm_k_iterations_aligned * BK;\n      if (tgp_bm == BM && tgp_bn == BN) {\n        gemm_loop<true, true, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result(D, params->ldd);\n        return;\n      } else if (tgp_bn == BN) {\n        gemm_loop<false, true, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n        return;\n      } else if (tgp_bm == BM) {\n        gemm_loop<true, false, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n        return;\n      } else {\n        gemm_loop<false, false, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n        return;\n      }\n    }\n  }\n};\n}\n}\n"
+ "\nstruct Add {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x + y;\n  }\n};\nstruct FloorDivide {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x / y;\n  }\n  template <>\n  float operator()(float x, float y) {\n    return trunc(x / y);\n  }\n  template <>\n  half operator()(half x, half y) {\n    return trunc(x / y);\n  }\n  template <>\n  bfloat16_t operator()(bfloat16_t x, bfloat16_t y) {\n    return trunc(x / y);\n  }\n};\nstruct Divide {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x / y;\n  }\n};\nstruct Remainder {\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T> & !metal::is_signed_v<T>, T>\n  operator()(T x, T y) {\n    return x % y;\n  }\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T> & metal::is_signed_v<T>, T>\n  operator()(T x, T y) {\n    auto r = x % y;\n    if (r != 0 && (r < 0 != y < 0)) {\n      r += y;\n    }\n    return r;\n  }\n  template <typename T>\n  metal::enable_if_t<!metal::is_integral_v<T>, T> operator()(T x, T y) {\n    T r = fmod(x, y);\n    if (r != 0 && (r < 0 != y < 0)) {\n      r += y;\n    }\n    return r;\n  }\n  template <>\n  complex64_t operator()(complex64_t x, complex64_t y) {\n    return x % y;\n  }\n};\nstruct Equal {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x == y;\n  }\n};\nstruct NaNEqual {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x == y || (metal::isnan(x) && metal::isnan(y));\n  }\n  template <>\n  bool operator()(complex64_t x, complex64_t y) {\n    return x == y ||\n        (metal::isnan(x.real) && metal::isnan(y.real) && metal::isnan(x.imag) &&\n         metal::isnan(y.imag)) ||\n        (x.real == y.real && metal::isnan(x.imag) && metal::isnan(y.imag)) ||\n        (metal::isnan(x.real) && metal::isnan(y.real) && x.imag == y.imag);\n  }\n};\nstruct Greater {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x > y;\n  }\n};\nstruct GreaterEqual {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x >= y;\n  }\n};\nstruct Less {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x < y;\n  }\n};\nstruct LessEqual {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x <= y;\n  }\n};\nstruct LogAddExp {\n  template <typename T>\n  T operator()(T x, T y) {\n    if (metal::isnan(x) || metal::isnan(y)) {\n      return metal::numeric_limits<T>::quiet_NaN();\n    }\n    constexpr T inf = metal::numeric_limits<T>::infinity();\n    T maxval = metal::max(x, y);\n    T minval = metal::min(x, y);\n    return (minval == -inf || maxval == inf)\n        ? maxval\n        : (maxval + log1p(metal::exp(minval - maxval)));\n  };\n};\nstruct Maximum {\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T>, T> operator()(T x, T y) {\n    return metal::max(x, y);\n  }\n  template <typename T>\n  metal::enable_if_t<!metal::is_integral_v<T>, T> operator()(T x, T y) {\n    if (metal::isnan(x)) {\n      return x;\n    }\n    return x > y ? x : y;\n  }\n  template <>\n  complex64_t operator()(complex64_t x, complex64_t y) {\n    if (metal::isnan(x.real) || metal::isnan(x.imag)) {\n      return x;\n    }\n    return x > y ? x : y;\n  }\n};\nstruct Minimum {\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T>, T> operator()(T x, T y) {\n    return metal::min(x, y);\n  }\n  template <typename T>\n  metal::enable_if_t<!metal::is_integral_v<T>, T> operator()(T x, T y) {\n    if (metal::isnan(x)) {\n      return x;\n    }\n    return x < y ? x : y;\n  }\n  template <>\n  complex64_t operator()(complex64_t x, complex64_t y) {\n    if (metal::isnan(x.real) || metal::isnan(x.imag)) {\n      return x;\n    }\n    return x < y ? x : y;\n  }\n};\nstruct Multiply {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x * y;\n  }\n};\nstruct NotEqual {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x != y;\n  }\n  template <>\n  bool operator()(complex64_t x, complex64_t y) {\n    return x.real != y.real || x.imag != y.imag;\n  }\n};\nstruct Power {\n  template <typename T>\n  metal::enable_if_t<!metal::is_integral_v<T>, T> operator()(T base, T exp) {\n    return metal::pow(base, exp);\n  }\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T>, T> operator()(T base, T exp) {\n    T res = 1;\n    while (exp) {\n      if (exp & 1) {\n        res *= base;\n      }\n      exp >>= 1;\n      base *= base;\n    }\n    return res;\n  }\n  template <>\n  complex64_t operator()(complex64_t x, complex64_t y) {\n    auto x_theta = metal::atan2(x.imag, x.real);\n    auto x_ln_r = 0.5 * metal::log(x.real * x.real + x.imag * x.imag);\n    auto mag = metal::exp(y.real * x_ln_r - y.imag * x_theta);\n    auto phase = y.imag * x_ln_r + y.real * x_theta;\n    return {mag * metal::cos(phase), mag * metal::sin(phase)};\n  }\n};\nstruct Subtract {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x - y;\n  }\n};\nstruct LogicalAnd {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x && y;\n  };\n};\nstruct LogicalOr {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x || y;\n  };\n};\nstruct BitwiseAnd {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x & y;\n  };\n};\nstruct BitwiseOr {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x | y;\n  };\n};\nstruct BitwiseXor {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x ^ y;\n  };\n};\nstruct LeftShift {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x << y;\n  };\n};\nstruct RightShift {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x >> y;\n  };\n};\nstruct ArcTan2 {\n  template <typename T>\n  T operator()(T y, T x) {\n    return metal::precise::atan2(y, x);\n  }\n};\nstruct DivMod {\n  template <typename T>\n  metal::array<T, 2> operator()(T x, T y) {\n    return {FloorDivide{}(x, y), Remainder{}(x, y)};\n  };\n};\ntemplate <typename U>\nstruct CumSum {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_scan(T val) { return simd_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_scan(T val) { for (int i = 1; i <= 16; i *= 2) { val = operator()(val, simd_shuffle_and_fill_up(val, init, i)); } return val; }\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_exclusive_scan(T val) { return simd_exclusive_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_exclusive_scan(T val) { val = simd_scan(val); return simd_shuffle_and_fill_up(val, init, 1); }\n  static constexpr constant U init = static_cast<U>(0);\n  template <typename T>\n  U operator()(U a, T b) {\n    return a + b;\n  }\n  U simd_scan_impl(U x) {\n    return simd_prefix_inclusive_sum(x);\n  }\n  U simd_exclusive_scan_impl(U x) {\n    return simd_prefix_exclusive_sum(x);\n  }\n};\ntemplate <typename U>\nstruct CumProd {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_scan(T val) { return simd_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_scan(T val) { for (int i = 1; i <= 16; i *= 2) { val = operator()(val, simd_shuffle_and_fill_up(val, init, i)); } return val; }\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_exclusive_scan(T val) { return simd_exclusive_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_exclusive_scan(T val) { val = simd_scan(val); return simd_shuffle_and_fill_up(val, init, 1); }\n  static constexpr constant U init = static_cast<U>(1.0f);\n  template <typename T>\n  U operator()(U a, T b) {\n    return a * b;\n  }\n  U simd_scan_impl(U x) {\n    return simd_prefix_inclusive_product(x);\n  }\n  U simd_exclusive_scan_impl(U x) {\n    return simd_prefix_exclusive_product(x);\n  }\n};\ntemplate <>\nstruct CumProd<bool> {\n  static constexpr constant bool init = true;\n  template <typename T>\n  bool operator()(bool a, T b) {\n    return a & static_cast<bool>(b);\n  }\n  bool simd_scan(bool x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      bool other = simd_shuffle_and_fill_up(x, init, i);\n      x &= other;\n    }\n    return x;\n  }\n  bool simd_exclusive_scan(bool x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename U>\nstruct CumMax {\n  static constexpr constant U init = Limits<U>::min;\n  template <typename T>\n  U operator()(U a, T b) {\n    return (a >= b) ? a : b;\n  }\n  U simd_scan(U x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      U other = simd_shuffle_and_fill_up(x, init, i);\n      x = (x >= other) ? x : other;\n    }\n    return x;\n  }\n  U simd_exclusive_scan(U x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename U>\nstruct CumMin {\n  static constexpr constant U init = Limits<U>::max;\n  template <typename T>\n  U operator()(U a, T b) {\n    return (a <= b) ? a : b;\n  }\n  U simd_scan(U x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      U other = simd_shuffle_and_fill_up(x, init, i);\n      x = (x <= other) ? x : other;\n    }\n    return x;\n  }\n  U simd_exclusive_scan(U x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename U>\nstruct CumLogaddexp {\n  static constexpr constant U init = Limits<U>::min;\n  template <typename T>\n  U operator()(U a, T b) {\n    return LogAddExp{}(a, static_cast<U>(b));\n  }\n  U simd_scan(U x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      U other = simd_shuffle_and_fill_up(x, init, i);\n      x = LogAddExp{}(x, other);\n    }\n    return x;\n  }\n  U simd_exclusive_scan(U x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename T, typename U, int N_READS, bool reverse>\ninline void load_unsafe(U values[N_READS], const device T* input) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      values[N_READS - i - 1] = input[i];\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      values[i] = input[i];\n    }\n  }\n}\ntemplate <typename T, typename U, int N_READS, bool reverse>\ninline void load_safe(\n    U values[N_READS],\n    const device T* input,\n    int start,\n    int total,\n    U init) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      values[N_READS - i - 1] =\n          (start + N_READS - i - 1 < total) ? input[i] : init;\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      values[i] = (start + i < total) ? input[i] : init;\n    }\n  }\n}\ntemplate <typename U, int N_READS, bool reverse>\ninline void write_unsafe(U values[N_READS], device U* out) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      out[i] = values[N_READS - i - 1];\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      out[i] = values[i];\n    }\n  }\n}\ntemplate <typename U, int N_READS, bool reverse>\ninline void write_safe(U values[N_READS], device U* out, int start, int total) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      if (start + N_READS - i - 1 < total) {\n        out[i] = values[N_READS - i - 1];\n      }\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      if (start + i < total) {\n        out[i] = values[i];\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS,\n    bool inclusive,\n    bool reverse>\n[[kernel]] void contiguous_scan(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& axis_size [[buffer(2)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  constexpr int simd_size = 32;\n  Op op;\n  size_t offset = (gid.y + gsize.y * size_t(gid.z)) * axis_size;\n  in += offset;\n  out += offset;\n  uint simd_groups = lsize.x / simd_size;\n  U prefix = Op::init;\n  U values[N_READS];\n  threadgroup U simdgroup_sums[32];\n  for (uint r = 0; r < ceildiv(axis_size, N_READS * lsize.x); r++) {\n    uint offset = r * lsize.x * N_READS + lid.x * N_READS;\n    if (reverse) {\n      if ((offset + N_READS) < axis_size) {\n        load_unsafe<T, U, N_READS, reverse>(\n            values, in + axis_size - offset - N_READS);\n      } else {\n        load_safe<T, U, N_READS, reverse>(\n            values,\n            in + axis_size - offset - N_READS,\n            offset,\n            axis_size,\n            Op::init);\n      }\n    } else {\n      if ((offset + N_READS) < axis_size) {\n        load_unsafe<T, U, N_READS, reverse>(values, in + offset);\n      } else {\n        load_safe<T, U, N_READS, reverse>(\n            values, in + offset, offset, axis_size, Op::init);\n      }\n    }\n    for (int i = 1; i < N_READS; i++) {\n      values[i] = op(values[i], values[i - 1]);\n    }\n    U prev_thread = op.simd_exclusive_scan(values[N_READS - 1]);\n    if (simd_lane_id == simd_size - 1) {\n      simdgroup_sums[simd_group_id] = op(prev_thread, values[N_READS - 1]);\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (simd_group_id == 0) {\n      U prev_simdgroup = op.simd_exclusive_scan(simdgroup_sums[simd_lane_id]);\n      simdgroup_sums[simd_lane_id] = prev_simdgroup;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < N_READS; i++) {\n      values[i] = op(values[i], prefix);\n      values[i] = op(values[i], simdgroup_sums[simd_group_id]);\n      values[i] = op(values[i], prev_thread);\n    }\n    if (reverse) {\n      if (inclusive) {\n        if ((offset + N_READS) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(\n              values, out + axis_size - offset - N_READS);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values, out + axis_size - offset - N_READS, offset, axis_size);\n        }\n      } else {\n        if (lid.x == 0 && offset == 0) {\n          out[axis_size - 1] = Op::init;\n        }\n        if ((offset + N_READS + 1) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(\n              values, out + axis_size - offset - 1 - N_READS);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values,\n              out + axis_size - offset - 1 - N_READS,\n              offset + 1,\n              axis_size);\n        }\n      }\n    } else {\n      if (inclusive) {\n        if ((offset + N_READS) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(values, out + offset);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values, out + offset, offset, axis_size);\n        }\n      } else {\n        if (lid.x == 0 && offset == 0) {\n          out[0] = Op::init;\n        }\n        if ((offset + N_READS + 1) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(values, out + offset + 1);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values, out + offset + 1, offset + 1, axis_size);\n        }\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (simd_group_id == simd_groups - 1 && simd_lane_id == simd_size - 1) {\n      simdgroup_sums[0] = values[N_READS - 1];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    prefix = simdgroup_sums[0];\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS,\n    bool inclusive,\n    bool reverse>\n[[kernel]] void strided_scan(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& axis_size [[buffer(2)]],\n    const constant size_t& stride [[buffer(3)]],\n    const constant size_t& stride_blocks [[buffer(4)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  constexpr int simd_size = 32;\n  constexpr int BM = 32;\n  constexpr int BN = 32;\n  constexpr int BN_pad = 32 + 16 / sizeof(U);\n  constexpr int n_simds = BN / N_READS;\n  constexpr int n_scans = BN / n_simds;\n  Op op;\n  threadgroup U read_buffer[BM * BN_pad];\n  U values[n_scans];\n  U prefix[n_scans];\n  for (int i = 0; i < n_scans; i++) {\n    prefix[i] = Op::init;\n  }\n  size_t full_gid = gid.y + gsize.y * size_t(gid.z);\n  size_t offset = full_gid / stride_blocks * axis_size * stride;\n  size_t global_index_x = full_gid % stride_blocks * BN;\n  uint read_offset_y = (lid.x * N_READS) / BN;\n  uint read_offset_x = (lid.x * N_READS) % BN;\n  uint scan_offset_y = simd_lane_id;\n  uint scan_offset_x = simd_group_id * n_scans;\n  uint stride_limit = stride - global_index_x;\n  in += offset + global_index_x + read_offset_x;\n  out += offset + global_index_x + read_offset_x;\n  threadgroup U* read_into =\n      read_buffer + read_offset_y * BN_pad + read_offset_x;\n  threadgroup U* read_from =\n      read_buffer + scan_offset_y * BN_pad + scan_offset_x;\n  for (uint j = 0; j < axis_size; j += BM) {\n    uint index_y = j + read_offset_y;\n    uint check_index_y = index_y;\n    if (reverse) {\n      index_y = axis_size - 1 - index_y;\n    }\n    if (check_index_y < axis_size && (read_offset_x + N_READS) < stride_limit) {\n      for (int i = 0; i < N_READS; i++) {\n        read_into[i] = in[index_y * stride + i];\n      }\n    } else {\n      for (int i = 0; i < N_READS; i++) {\n        if (check_index_y < axis_size && (read_offset_x + i) < stride_limit) {\n          read_into[i] = in[index_y * stride + i];\n        } else {\n          read_into[i] = Op::init;\n        }\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < n_scans; i++) {\n      values[i] = read_from[i];\n    }\n    simdgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < n_scans; i++) {\n      values[i] = op.simd_scan(values[i]);\n      values[i] = op(values[i], prefix[i]);\n      prefix[i] = simd_shuffle(values[i], simd_size - 1);\n    }\n    for (int i = 0; i < n_scans; i++) {\n      read_from[i] = values[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (!inclusive) {\n      if (check_index_y == 0) {\n        if ((read_offset_x + N_READS) < stride_limit) {\n          for (int i = 0; i < N_READS; i++) {\n            out[index_y * stride + i] = Op::init;\n          }\n        } else {\n          for (int i = 0; i < N_READS; i++) {\n            if ((read_offset_x + i) < stride_limit) {\n              out[index_y * stride + i] = Op::init;\n            }\n          }\n        }\n      }\n      if (reverse) {\n        index_y -= 1;\n        check_index_y += 1;\n      } else {\n        index_y += 1;\n        check_index_y += 1;\n      }\n    }\n    if (check_index_y < axis_size && (read_offset_x + N_READS) < stride_limit) {\n      for (int i = 0; i < N_READS; i++) {\n        out[index_y * stride + i] = read_into[i];\n      }\n    } else {\n      for (int i = 0; i < N_READS; i++) {\n        if (check_index_y < axis_size && (read_offset_x + i) < stride_limit) {\n          out[index_y * stride + i] = read_into[i];\n        }\n      }\n    }\n  }\n}\n"
+ "\ntemplate <\n    typename T,\n    typename IdxT,\n    typename LocT,\n    typename Op,\n    bool UpdC,\n    bool IdxC>\n[[kernel]] void scatter_axis(\n    const device T* upd [[buffer(0)]],\n    const device IdxT* indices [[buffer(1)]],\n    device mlx_atomic<T>* out [[buffer(2)]],\n    const constant int* shape [[buffer(3)]],\n    const constant int64_t* upd_strides [[buffer(4)]],\n    const constant int64_t* idx_strides [[buffer(5)]],\n    const constant size_t& ndim [[buffer(6)]],\n    const constant int& axis [[buffer(7)]],\n    const constant int& out_axis_size [[buffer(8)]],\n    const constant size_t& upd_ax_stride [[buffer(9)]],\n    const constant size_t& idx_ax_stride [[buffer(10)]],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  Op op;\n  LocT elem_idx = index.z * static_cast<LocT>(grid_dim.x);\n  LocT idx_loc = index.y * static_cast<LocT>(idx_ax_stride);\n  if (IdxC) {\n    idx_loc += elem_idx * grid_dim.y + index.x;\n  } else {\n    idx_loc += elem_to_loc<LocT>(elem_idx + index.x, shape, idx_strides, ndim);\n  }\n  auto idx_val = indices[idx_loc];\n  if (is_signed_v<IdxT>) {\n    idx_val = (idx_val < 0) ? idx_val + out_axis_size : idx_val;\n  }\n  LocT upd_idx = index.y * static_cast<LocT>(upd_ax_stride);\n  if (UpdC) {\n    upd_idx += elem_idx * grid_dim.y + index.x;\n  } else {\n    upd_idx += elem_to_loc<LocT>(elem_idx + index.x, shape, upd_strides, ndim);\n  }\n  LocT out_idx = elem_idx * static_cast<LocT>(out_axis_size) +\n      idx_val * grid_dim.x + index.x;\n  op.atomic_update(out, upd[upd_idx], out_idx);\n}\n"
+ "\ntemplate <typename IdxT, int NIDX>\nstruct Indices {\n  const array<const device IdxT*, NIDX> buffers;\n  const constant int* shapes;\n  const constant int64_t* strides;\n  const constant bool* row_contiguous;\n  const int ndim;\n};\ntemplate <typename IdxT>\nMETAL_FUNC size_t offset_neg_idx(IdxT idx, int size) {\n  if (is_unsigned_v<IdxT>) {\n    return idx;\n  } else {\n    return (idx < 0) ? idx + size : idx;\n  }\n}\n\ntemplate <\n    typename T,\n    typename IdxT,\n    typename Op,\n    int NIDX,\n    bool UPD_ROW_CONTIG,\n    int NWORK,\n    typename LocT>\nMETAL_FUNC void scatter_impl(\n    const device T* updates,\n    device mlx_atomic<T>* out,\n    const constant int* upd_shape,\n    const constant int64_t* upd_strides,\n    const constant size_t& upd_ndim,\n    const constant size_t& upd_size,\n    const constant int* out_shape,\n    const constant int64_t* out_strides,\n    const constant size_t& out_ndim,\n    const constant int* axes,\n    const constant size_t& idx_size,\n    const thread Indices<IdxT, NIDX>& indices,\n    uint2 gid [[thread_position_in_grid]]) {\n  Op op;\n  auto ind_idx = gid.y * NWORK;\n  LocT out_offset = 0;\n  if (upd_size > 1) {\n    out_offset = elem_to_loc<LocT>(\n        gid.x, upd_shape + indices.ndim, out_strides, out_ndim);\n  }\n  for (int j = 0; j < NWORK && ind_idx < idx_size; ++j, ind_idx++) {\n    LocT out_idx = out_offset;\n    for (int i = 0; i < NIDX; ++i) {\n      auto idx_loc = indices.row_contiguous[i]\n          ? ind_idx\n          : elem_to_loc<LocT>(\n                ind_idx,\n                &indices.shapes[indices.ndim * i],\n                &indices.strides[indices.ndim * i],\n                indices.ndim);\n      auto ax = axes[i];\n      auto idx_val = offset_neg_idx(indices.buffers[i][idx_loc], out_shape[ax]);\n      out_idx +=\n          static_cast<LocT>(idx_val) * static_cast<LocT>(out_strides[ax]);\n    }\n    auto upd_idx = ind_idx * static_cast<LocT>(upd_size) + gid.x;\n    if constexpr (!UPD_ROW_CONTIG) {\n      upd_idx = elem_to_loc<LocT>(upd_idx, upd_shape, upd_strides, upd_ndim);\n    }\n    op.atomic_update(out, updates[upd_idx], out_idx);\n  }\n}\n"
+ "\ntemplate <typename IdxT, int NIDX>\nstruct Indices {\n  const array<const device IdxT*, NIDX> buffers;\n  const constant int* shapes;\n  const constant int64_t* strides;\n  const constant bool* row_contiguous;\n  const int ndim;\n};\ntemplate <typename IdxT>\nMETAL_FUNC size_t offset_neg_idx(IdxT idx, int size) {\n  if (is_unsigned_v<IdxT>) {\n    return idx;\n  } else {\n    return (idx < 0) ? idx + size : idx;\n  }\n}\n\ntemplate <typename T, typename IdxT, int NIDX, int IDX_NDIM, typename LocT>\nMETAL_FUNC void gather_impl(\n    const device T* src [[buffer(0)]],\n    device T* out [[buffer(1)]],\n    const constant int* src_shape [[buffer(2)]],\n    const constant int64_t* src_strides [[buffer(3)]],\n    const constant size_t& src_ndim [[buffer(4)]],\n    const constant int* slice_sizes [[buffer(5)]],\n    const constant int* axes [[buffer(6)]],\n    const thread Indices<IdxT, NIDX>& indices,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  LocT src_idx = 0;\n  for (int i = 0; i < NIDX; ++i) {\n    LocT idx_loc;\n    if (IDX_NDIM == 0) {\n      idx_loc = 0;\n    } else if (IDX_NDIM == 1) {\n      idx_loc = index.x * static_cast<LocT>(indices.strides[indices.ndim * i]);\n    } else {\n      idx_loc = index.x * static_cast<LocT>(indices.strides[indices.ndim * i]);\n      idx_loc += indices.row_contiguous[i]\n          ? index.y\n          : elem_to_loc<LocT>(\n                index.y,\n                &indices.shapes[indices.ndim * i + 1],\n                &indices.strides[indices.ndim * i + 1],\n                indices.ndim - 1);\n    }\n    auto ax = axes[i];\n    auto idx_val = offset_neg_idx(indices.buffers[i][idx_loc], src_shape[ax]);\n    src_idx += static_cast<LocT>(idx_val) * static_cast<LocT>(src_strides[ax]);\n  }\n  auto src_offset =\n      elem_to_loc<LocT>(index.z, slice_sizes, src_strides, src_ndim);\n  LocT out_idx = index.z;\n  if (IDX_NDIM == 1) {\n    out_idx += static_cast<LocT>(grid_dim.z) * index.x;\n  } else if (IDX_NDIM >= 2) {\n    out_idx += grid_dim.z * (index.x * static_cast<LocT>(grid_dim.y) + index.y);\n  }\n  out[out_idx] = src[src_offset + src_idx];\n}\n"
+ "\ntemplate <typename T, typename IdxT, typename LocT, bool SrcC, bool IdxC>\n[[kernel]] void gather_axis(\n    const device T* src [[buffer(0)]],\n    const device IdxT* indices [[buffer(1)]],\n    device T* out [[buffer(2)]],\n    const constant int* shape [[buffer(3)]],\n    const constant int64_t* src_strides [[buffer(4)]],\n    const constant int64_t* idx_strides [[buffer(5)]],\n    const constant size_t& ndim [[buffer(6)]],\n    const constant int& axis [[buffer(7)]],\n    const constant int& axis_size [[buffer(8)]],\n    const constant size_t& src_ax_stride [[buffer(9)]],\n    const constant size_t& idx_ax_stride [[buffer(10)]],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  LocT elem_idx = index.z * static_cast<LocT>(grid_dim.x);\n  LocT out_idx = elem_idx * grid_dim.y + index.x;\n  LocT idx_loc = index.y * static_cast<LocT>(idx_ax_stride);\n  if (IdxC) {\n    idx_loc += out_idx;\n  } else {\n    idx_loc += elem_to_loc<LocT>(elem_idx + index.x, shape, idx_strides, ndim);\n  }\n  auto idx_val = indices[idx_loc];\n  if (is_signed_v<IdxT>) {\n    idx_val = (idx_val < 0) ? idx_val + axis_size : idx_val;\n  }\n  LocT src_idx = idx_val * static_cast<LocT>(src_ax_stride);\n  if (SrcC) {\n    src_idx += elem_idx * axis_size + index.x;\n  } else {\n    src_idx += elem_to_loc<LocT>(elem_idx + index.x, shape, src_strides, ndim);\n  }\n  out_idx += index.y * static_cast<LocT>(grid_dim.x);\n  out[out_idx] = src[src_idx];\n}\n"
+ "\ntemplate <typename T, typename Op>\n[[kernel]] void ternary_v(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    uint index [[thread_position_in_grid]]) {\n  d[index] = Op()(a[index], b[index], c[index]);\n}\ntemplate <typename T, typename Op>\n[[kernel]] void ternary_v2(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto offset = index.x + grid_dim.x * int64_t(index.y);\n  d[offset] = Op()(a[offset], b[offset], c[offset]);\n}\ntemplate <typename T, typename Op, typename IdxT = int64_t>\n[[kernel]] void ternary_g_nd1(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const int64_t& a_strides,\n    constant const int64_t& b_strides,\n    constant const int64_t& c_strides,\n    uint index [[thread_position_in_grid]]) {\n  auto a_idx = elem_to_loc_1<IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_1<IdxT>(index, b_strides);\n  auto c_idx = elem_to_loc_1<IdxT>(index, c_strides);\n  d[index] = Op()(a[a_idx], b[b_idx], c[c_idx]);\n}\ntemplate <typename T, typename Op, typename IdxT = int64_t>\n[[kernel]] void ternary_g_nd2(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const int64_t a_strides[2],\n    constant const int64_t b_strides[2],\n    constant const int64_t c_strides[2],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_2<IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_2<IdxT>(index, b_strides);\n  auto c_idx = elem_to_loc_2<IdxT>(index, c_strides);\n  IdxT out_idx = index.x + IdxT(grid_dim.x) * index.y;\n  d[out_idx] = Op()(a[a_idx], b[b_idx], c[c_idx]);\n}\ntemplate <typename T, typename Op, typename IdxT = int64_t>\n[[kernel]] void ternary_g_nd3(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const int64_t a_strides[3],\n    constant const int64_t b_strides[3],\n    constant const int64_t c_strides[3],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_3<IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_3<IdxT>(index, b_strides);\n  auto c_idx = elem_to_loc_3<IdxT>(index, c_strides);\n  IdxT out_idx = index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n  d[out_idx] = Op()(a[a_idx], b[b_idx], c[c_idx]);\n}\ntemplate <typename T, typename Op, int N = 1, typename IdxT = int64_t>\n[[kernel]] void ternary_g(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const int* shape,\n    constant const int64_t* a_strides,\n    constant const int64_t* b_strides,\n    constant const int64_t* c_strides,\n    constant const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc_3_nd<IdxT>(\n      {N * index.x, index.y, index.z},\n      shape,\n      a_strides,\n      b_strides,\n      c_strides,\n      ndim);\n  auto xshape = shape[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  IdxT a_xstride = a_strides[ndim - 1];\n  IdxT b_xstride = b_strides[ndim - 1];\n  IdxT c_xstride = c_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    d[out_idx++] = Op()(a[idx.x], b[idx.y], c[idx.z]);\n    idx.x += a_xstride;\n    idx.y += b_xstride;\n    idx.z += c_xstride;\n  }\n}\n"
+ "\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_ss(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[0], b[0]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[0], b[index]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[index], b[0]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[index], b[index]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto offset = index.x + grid_dim.x * int64_t(index.y);\n  auto out = Op()(a[0], b[offset]);\n  c[offset] = out[0];\n  d[offset] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto offset = index.x + grid_dim.x * int64_t(index.y);\n  auto out = Op()(a[offset], b[0]);\n  c[offset] = out[0];\n  d[offset] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto offset = index.x + grid_dim.x * int64_t(index.y);\n  auto out = Op()(a[offset], b[offset]);\n  c[offset] = out[0];\n  d[offset] = out[1];\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = int64_t>\n[[kernel]] void binary_g_nd1(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const int64_t& a_stride,\n    constant const int64_t& b_stride,\n    uint index [[thread_position_in_grid]]) {\n  auto a_idx = elem_to_loc_1<IdxT>(index, a_stride);\n  auto b_idx = elem_to_loc_1<IdxT>(index, b_stride);\n  auto out = Op()(a[a_idx], b[b_idx]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = int64_t>\n[[kernel]] void binary_g_nd2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const int64_t a_strides[2],\n    constant const int64_t b_strides[2],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_2<IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_2<IdxT>(index, b_strides);\n  IdxT out_idx = index.x + IdxT(grid_dim.x) * index.y;\n  auto out = Op()(a[a_idx], b[b_idx]);\n  c[out_idx] = out[0];\n  d[out_idx] = out[1];\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = int64_t>\n[[kernel]] void binary_g_nd3(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const int64_t a_strides[3],\n    constant const int64_t b_strides[3],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_3<IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_3<IdxT>(index, b_strides);\n  IdxT out_idx = index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n  auto out = Op()(a[a_idx], b[b_idx]);\n  c[out_idx] = out[0];\n  d[out_idx] = out[1];\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N = 1,\n    typename IdxT = int64_t>\n[[kernel]] void binary_g(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const int* shape,\n    constant const int64_t* a_strides,\n    constant const int64_t* b_strides,\n    constant const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc_2_nd<IdxT>(\n      {N * index.x, index.y, index.z}, shape, a_strides, b_strides, ndim);\n  auto xshape = shape[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  IdxT a_xstride = a_strides[ndim - 1];\n  IdxT b_xstride = b_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    auto out = Op()(a[idx.x], b[idx.y]);\n    c[out_idx] = out[0];\n    d[out_idx++] = out[1];\n    idx.x += a_xstride;\n    idx.y += b_xstride;\n  }\n}\n"
+ "\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_ss(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[0], b[0]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[0], b[index]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[index], b[0]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[index], b[index]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  int64_t offset = index.x + grid_dim.x * int64_t(index.y);\n  c[offset] = Op()(a[0], b[offset]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  int64_t offset = index.x + grid_dim.x * int64_t(index.y);\n  c[offset] = Op()(a[offset], b[0]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  int64_t offset = index.x + grid_dim.x * int64_t(index.y);\n  c[offset] = Op()(a[offset], b[offset]);\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = int64_t>\n[[kernel]] void binary_g_nd1(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const int64_t& a_stride,\n    constant const int64_t& b_stride,\n    uint index [[thread_position_in_grid]]) {\n  auto a_idx = elem_to_loc_1<IdxT>(index, a_stride);\n  auto b_idx = elem_to_loc_1<IdxT>(index, b_stride);\n  c[index] = Op()(a[a_idx], b[b_idx]);\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = int64_t>\n[[kernel]] void binary_g_nd2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const int64_t a_strides[2],\n    constant const int64_t b_strides[2],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_2<IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_2<IdxT>(index, b_strides);\n  IdxT out_idx = index.x + IdxT(grid_dim.x) * index.y;\n  c[out_idx] = Op()(a[a_idx], b[b_idx]);\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = int64_t>\n[[kernel]] void binary_g_nd3(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const int64_t a_strides[3],\n    constant const int64_t b_strides[3],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_3<IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_3<IdxT>(index, b_strides);\n  IdxT out_idx = index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n  c[out_idx] = Op()(a[a_idx], b[b_idx]);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N = 1,\n    typename IdxT = int64_t>\n[[kernel]] void binary_g(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const int* shape,\n    constant const int64_t* a_strides,\n    constant const int64_t* b_strides,\n    constant const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc_2_nd<IdxT>(\n      {N * index.x, index.y, index.z}, shape, a_strides, b_strides, ndim);\n  auto xshape = shape[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  IdxT a_xstride = a_strides[ndim - 1];\n  IdxT b_xstride = b_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    c[out_idx++] = Op()(a[idx.x], b[idx.y]);\n    idx.x += a_xstride;\n    idx.y += b_xstride;\n  }\n}\n"
+ "\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void unary_v(\n    device const T* in,\n    device U* out,\n    uint index [[thread_position_in_grid]]) {\n  out[index] = Op()(in[index]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void unary_v2(\n    device const T* in,\n    device U* out,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto offset = index.x + grid_dim.x * int64_t(index.y);\n  out[offset] = Op()(in[offset]);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N = 1,\n    typename IdxT = int64_t>\n[[kernel]] void unary_g(\n    device const T* in,\n    device U* out,\n    constant const int* in_shape,\n    constant const int64_t* in_strides,\n    device const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc<IdxT>(\n      {N * index.x, index.y, index.z}, in_shape, in_strides, ndim);\n  auto xshape = in_shape[ndim - 1];\n  IdxT xstride = in_strides[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    out[out_idx++] = Op()(in[idx]);\n    idx += xstride;\n  }\n}\n"
+ "\ntemplate <typename T, typename U>\n[[kernel]] void copy_s(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint index [[thread_position_in_grid]]) {\n  dst[index] = static_cast<U>(src[0]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_v(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint index [[thread_position_in_grid]]) {\n  dst[index] = static_cast<U>(src[index]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_s2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto offset = index.x + grid_dim.x * int64_t(index.y);\n  dst[offset] = static_cast<U>(src[0]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_v2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto offset = index.x + grid_dim.x * int64_t(index.y);\n  dst[offset] = static_cast<U>(src[offset]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_g_nd1(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t& src_stride [[buffer(3)]],\n    uint index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_1<IdxT>(index, src_stride);\n  dst[index] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_g_nd2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto src_idx = elem_to_loc_2<IdxT>(index, src_strides);\n  IdxT dst_idx = index.x + IdxT(grid_dim.x) * index.y;\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_g_nd3(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto src_idx = elem_to_loc_3<IdxT>(index, src_strides);\n  IdxT dst_idx =\n      index.x + IdxT(grid_dim.x) * (index.y + IdxT(grid_dim.y) * index.z);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, int N = 1, typename IdxT = int64_t>\n[[kernel]] void copy_g(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int* src_shape [[buffer(2)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int& ndim [[buffer(5)]],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto src_idx = elem_to_loc<IdxT>(\n      {N * index.x, index.y, index.z}, src_shape, src_strides, ndim);\n  if (N == 1) {\n    IdxT dst_idx =\n        index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n    dst[dst_idx] = static_cast<U>(src[src_idx]);\n    return;\n  }\n  auto xshape = src_shape[ndim - 1];\n  IdxT dst_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  auto src_xstride = src_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    dst[dst_idx + i] = static_cast<U>(src[src_idx]);\n    src_idx += src_xstride;\n  }\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_nd1(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t& src_stride [[buffer(3)]],\n    constant const int64_t& dst_stride [[buffer(4)]],\n    uint index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_1<IdxT>(index, src_stride);\n  auto dst_idx = elem_to_loc_1<IdxT>(index, dst_stride);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_nd2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    uint2 index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_2<IdxT>(index, src_strides);\n  auto dst_idx = elem_to_loc_2<IdxT>(index, dst_strides);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_nd3(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    uint3 index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_3<IdxT>(index, src_strides);\n  auto dst_idx = elem_to_loc_3<IdxT>(index, dst_strides);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, int N = 1, typename IdxT = int64_t>\n[[kernel]] void copy_gg(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int* src_shape [[buffer(2)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    constant const int& ndim [[buffer(5)]],\n    uint3 index [[thread_position_in_grid]]) {\n  auto idx = elem_to_loc_2_nd<IdxT>(\n      {N * index.x, index.y, index.z},\n      src_shape,\n      src_strides,\n      dst_strides,\n      ndim);\n  if (N == 1) {\n    dst[idx.y] = static_cast<U>(src[idx.x]);\n    return;\n  }\n  IdxT src_xstride = src_strides[ndim - 1];\n  IdxT dst_xstride = dst_strides[ndim - 1];\n  auto xshape = src_shape[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    dst[idx.y] = static_cast<U>(src[idx.x]);\n    idx.x += src_xstride;\n    idx.y += dst_xstride;\n  }\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_dynamic_nd1(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t& src_stride [[buffer(3)]],\n    constant const int64_t& dst_stride [[buffer(4)]],\n    constant const int64_t& src_offset [[buffer(6)]],\n    constant const int64_t& dst_offset [[buffer(7)]],\n    uint index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_1<IdxT>(index, src_stride);\n  auto dst_idx = elem_to_loc_1<IdxT>(index, dst_stride);\n  dst[dst_idx + dst_offset] = src[src_idx + src_offset];\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_dynamic_nd2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    constant const int64_t& src_offset [[buffer(6)]],\n    constant const int64_t& dst_offset [[buffer(7)]],\n    uint2 index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_2<IdxT>(index, src_strides);\n  auto dst_idx = elem_to_loc_2<IdxT>(index, dst_strides);\n  dst[dst_idx + dst_offset] = src[src_idx + src_offset];\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_dynamic_nd3(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    constant const int64_t& src_offset [[buffer(6)]],\n    constant const int64_t& dst_offset [[buffer(7)]],\n    uint3 index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_3<IdxT>(index, src_strides);\n  auto dst_idx = elem_to_loc_3<IdxT>(index, dst_strides);\n  dst[dst_idx + dst_offset] = src[src_idx + src_offset];\n}\ntemplate <typename T, typename U, int N = 1, typename IdxT = int64_t>\n[[kernel]] void copy_gg_dynamic(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int* src_shape [[buffer(2)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    constant const int& ndim [[buffer(5)]],\n    constant const int64_t& src_offset [[buffer(6)]],\n    constant const int64_t& dst_offset [[buffer(7)]],\n    uint3 index [[thread_position_in_grid]]) {\n  src += src_offset;\n  dst += dst_offset;\n  auto idx = elem_to_loc_2_nd<IdxT>(\n      {N * index.x, index.y, index.z},\n      src_shape,\n      src_strides,\n      dst_strides,\n      ndim);\n  if (N == 1) {\n    dst[idx.y] = src[idx.x];\n    return;\n  }\n  IdxT src_xstride = src_strides[ndim - 1];\n  IdxT dst_xstride = dst_strides[ndim - 1];\n  auto xshape = src_shape[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    dst[idx.y] = src[idx.x];\n    idx.x += src_xstride;\n    idx.y += dst_xstride;\n  }\n}\n"
+ "\nusing namespace metal;\ntemplate <typename T>\nMETAL_FUNC void thread_swap(thread T& a, thread T& b) {\n  T w = a;\n  a = b;\n  b = w;\n}\ntemplate <typename T>\nstruct LessThan {\n  static constexpr constant T init = Limits<T>::max;\n  METAL_FUNC bool operator()(T a, T b) {\n    return a < b;\n  }\n};\ntemplate <\n    typename ValT,\n    typename IdxT,\n    bool ARG_SORT,\n    short N_PER_THREAD,\n    typename CompareOp>\nstruct ThreadSort {\n  static METAL_FUNC void sort(\n      thread ValT (&vals)[N_PER_THREAD],\n      thread IdxT (&idxs)[N_PER_THREAD]) {\n    CompareOp op;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < N_PER_THREAD; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = i & 1; j < N_PER_THREAD - 1; j += 2) {\n        if (op(vals[j + 1], vals[j])) {\n          thread_swap(vals[j + 1], vals[j]);\n          thread_swap(idxs[j + 1], idxs[j]);\n        }\n      }\n    }\n  }\n};\ntemplate <\n    typename ValT,\n    typename IdxT,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp>\nstruct BlockMergeSort {\n  using thread_sort_t =\n      ThreadSort<ValT, IdxT, ARG_SORT, N_PER_THREAD, CompareOp>;\n  static METAL_FUNC int merge_partition(\n      const threadgroup ValT* As,\n      const threadgroup ValT* Bs,\n      short A_sz,\n      short B_sz,\n      short sort_md) {\n    CompareOp op;\n    short A_st = max(0, sort_md - B_sz);\n    short A_ed = min(sort_md, A_sz);\n    while (A_st < A_ed) {\n      short md = A_st + (A_ed - A_st) / 2;\n      auto a = As[md];\n      auto b = Bs[sort_md - 1 - md];\n      if (op(b, a)) {\n        A_ed = md;\n      } else {\n        A_st = md + 1;\n      }\n    }\n    return A_ed;\n  }\n  static METAL_FUNC void merge_step(\n      const threadgroup ValT* As,\n      const threadgroup ValT* Bs,\n      const threadgroup IdxT* As_idx,\n      const threadgroup IdxT* Bs_idx,\n      short A_sz,\n      short B_sz,\n      thread ValT (&vals)[N_PER_THREAD],\n      thread IdxT (&idxs)[N_PER_THREAD]) {\n    CompareOp op;\n    short a_idx = 0;\n    short b_idx = 0;\n    for (int i = 0; i < N_PER_THREAD; ++i) {\n      auto a = As[a_idx];\n      auto b = Bs[b_idx];\n      bool pred = (b_idx < B_sz) && (a_idx >= A_sz || op(b, a));\n      vals[i] = pred ? b : a;\n      idxs[i] = pred ? Bs_idx[b_idx] : As_idx[a_idx];\n      b_idx += short(pred);\n      a_idx += short(!pred);\n    }\n  }\n  static METAL_FUNC void sort(\n      threadgroup ValT* tgp_vals [[threadgroup(0)]],\n      threadgroup IdxT* tgp_idxs [[threadgroup(1)]],\n      int size_sorted_axis,\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    int idx = lid.x * N_PER_THREAD;\n    thread ValT thread_vals[N_PER_THREAD];\n    thread IdxT thread_idxs[N_PER_THREAD];\n    for (int i = 0; i < N_PER_THREAD; ++i) {\n      thread_vals[i] = tgp_vals[idx + i];\n      if (ARG_SORT) {\n        thread_idxs[i] = tgp_idxs[idx + i];\n      }\n    }\n    if (idx < size_sorted_axis) {\n      thread_sort_t::sort(thread_vals, thread_idxs);\n    }\n    for (int merge_threads = 2; merge_threads <= BLOCK_THREADS;\n         merge_threads *= 2) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      for (int i = 0; i < N_PER_THREAD; ++i) {\n        tgp_vals[idx + i] = thread_vals[i];\n        if (ARG_SORT) {\n          tgp_idxs[idx + i] = thread_idxs[i];\n        }\n      }\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      int merge_group = lid.x / merge_threads;\n      int merge_lane = lid.x % merge_threads;\n      int sort_sz = N_PER_THREAD * merge_threads;\n      int sort_st = N_PER_THREAD * merge_threads * merge_group;\n      int A_st = sort_st;\n      int A_ed = sort_st + sort_sz / 2;\n      int B_st = sort_st + sort_sz / 2;\n      int B_ed = sort_st + sort_sz;\n      const threadgroup ValT* As = tgp_vals + A_st;\n      const threadgroup ValT* Bs = tgp_vals + B_st;\n      int A_sz = A_ed - A_st;\n      int B_sz = B_ed - B_st;\n      int sort_md = N_PER_THREAD * merge_lane;\n      int partition = merge_partition(As, Bs, A_sz, B_sz, sort_md);\n      As += partition;\n      Bs += sort_md - partition;\n      A_sz -= partition;\n      B_sz -= sort_md - partition;\n      const threadgroup IdxT* As_idx =\n          ARG_SORT ? tgp_idxs + A_st + partition : nullptr;\n      const threadgroup IdxT* Bs_idx =\n          ARG_SORT ? tgp_idxs + B_st + sort_md - partition : nullptr;\n      merge_step(As, Bs, As_idx, Bs_idx, A_sz, B_sz, thread_vals, thread_idxs);\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < N_PER_THREAD; ++i) {\n      tgp_vals[idx + i] = thread_vals[i];\n      if (ARG_SORT) {\n        tgp_idxs[idx + i] = thread_idxs[i];\n      }\n    }\n  }\n};\ntemplate <\n    typename T,\n    typename U,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp = LessThan<T>>\nstruct KernelMergeSort {\n  using ValT = T;\n  using IdxT = uint;\n  using block_merge_sort_t = BlockMergeSort<\n      ValT,\n      IdxT,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD,\n      CompareOp>;\n  static constant constexpr const short N_PER_BLOCK = BLOCK_THREADS * N_PER_THREAD;\n  static METAL_FUNC void block_sort(\n      const device T* inp,\n      device U* out,\n      const constant int& size_sorted_axis,\n      const constant int& in_stride_sorted_axis,\n      const constant int& out_stride_sorted_axis,\n      const constant int& in_stride_segment_axis,\n      const constant int& out_stride_segment_axis,\n      threadgroup ValT* tgp_vals,\n      threadgroup IdxT* tgp_idxs,\n      uint3 tid [[threadgroup_position_in_grid]],\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    inp += tid.y * in_stride_segment_axis;\n    out += tid.y * out_stride_segment_axis;\n    for (short i = lid.x; i < N_PER_BLOCK; i += BLOCK_THREADS) {\n      tgp_vals[i] = i < size_sorted_axis ? inp[i * in_stride_sorted_axis]\n                                         : ValT(CompareOp::init);\n      if (ARG_SORT) {\n        tgp_idxs[i] = i;\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    block_merge_sort_t::sort(tgp_vals, tgp_idxs, size_sorted_axis, lid);\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = lid.x; i < size_sorted_axis; i += BLOCK_THREADS) {\n      if (ARG_SORT) {\n        out[i * out_stride_sorted_axis] = tgp_idxs[i];\n      } else {\n        out[i * out_stride_sorted_axis] = tgp_vals[i];\n      }\n    }\n  }\n};\ntemplate <\n    typename T,\n    typename U,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void block_sort(\n    const device T* inp [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant int& size_sorted_axis [[buffer(2)]],\n    const constant int& in_stride_sorted_axis [[buffer(3)]],\n    const constant int& out_stride_sorted_axis [[buffer(4)]],\n    const constant int& in_stride_segment_axis [[buffer(5)]],\n    const constant int& out_stride_segment_axis [[buffer(6)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel =\n      KernelMergeSort<T, U, ARG_SORT, BLOCK_THREADS, N_PER_THREAD>;\n  using ValT = typename sort_kernel::ValT;\n  using IdxT = typename sort_kernel::IdxT;\n  if (ARG_SORT) {\n    threadgroup ValT tgp_vals[sort_kernel::N_PER_BLOCK];\n    threadgroup IdxT tgp_idxs[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        in_stride_segment_axis,\n        out_stride_segment_axis,\n        tgp_vals,\n        tgp_idxs,\n        tid,\n        lid);\n  } else {\n    threadgroup ValT tgp_vals[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        in_stride_segment_axis,\n        out_stride_segment_axis,\n        tgp_vals,\n        nullptr,\n        tid,\n        lid);\n  }\n}\nconstant constexpr const int zero_helper = 0;\ntemplate <\n    typename T,\n    typename U,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void block_sort_nc(\n    const device T* inp [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant int& size_sorted_axis [[buffer(2)]],\n    const constant int& in_stride_sorted_axis [[buffer(3)]],\n    const constant int& out_stride_sorted_axis [[buffer(4)]],\n    const constant int& nc_dim [[buffer(5)]],\n    const constant int* nc_shape [[buffer(6)]],\n    const constant int64_t* in_nc_strides [[buffer(7)]],\n    const constant int64_t* out_nc_strides [[buffer(8)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel =\n      KernelMergeSort<T, U, ARG_SORT, BLOCK_THREADS, N_PER_THREAD>;\n  using ValT = typename sort_kernel::ValT;\n  using IdxT = typename sort_kernel::IdxT;\n  auto in_block_idx = elem_to_loc(tid.y, nc_shape, in_nc_strides, nc_dim);\n  auto out_block_idx = elem_to_loc(tid.y, nc_shape, out_nc_strides, nc_dim);\n  inp += in_block_idx;\n  out += out_block_idx;\n  if (ARG_SORT) {\n    threadgroup ValT tgp_vals[sort_kernel::N_PER_BLOCK];\n    threadgroup IdxT tgp_idxs[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        zero_helper,\n        zero_helper,\n        tgp_vals,\n        tgp_idxs,\n        tid,\n        lid);\n  } else {\n    threadgroup ValT tgp_vals[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        zero_helper,\n        zero_helper,\n        tgp_vals,\n        nullptr,\n        tid,\n        lid);\n  }\n}\ntemplate <\n    typename ValT,\n    typename IdxT,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp = LessThan<ValT>>\nstruct KernelMultiBlockMergeSort {\n  using block_merge_sort_t = BlockMergeSort<\n      ValT,\n      IdxT,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD,\n      CompareOp>;\n  static constant constexpr const short N_PER_BLOCK = BLOCK_THREADS * N_PER_THREAD;\n  static METAL_FUNC void block_sort(\n      const device ValT* inp,\n      device ValT* out_vals,\n      device IdxT* out_idxs,\n      const constant int& size_sorted_axis,\n      const constant int& stride_sorted_axis,\n      threadgroup ValT* tgp_vals,\n      threadgroup IdxT* tgp_idxs,\n      uint3 tid [[threadgroup_position_in_grid]],\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    int base_idx = tid.x * N_PER_BLOCK;\n    for (short i = lid.x; i < N_PER_BLOCK; i += BLOCK_THREADS) {\n      int idx = base_idx + i;\n      tgp_vals[i] = idx < size_sorted_axis ? inp[idx * stride_sorted_axis]\n                                           : ValT(CompareOp::init);\n      tgp_idxs[i] = idx;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    block_merge_sort_t::sort(tgp_vals, tgp_idxs, size_sorted_axis, lid);\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = lid.x; i < N_PER_BLOCK; i += BLOCK_THREADS) {\n      int idx = base_idx + i;\n      if (idx < size_sorted_axis) {\n        out_vals[idx] = tgp_vals[i];\n        out_idxs[idx] = tgp_idxs[i];\n      }\n    }\n  }\n  static METAL_FUNC int merge_partition(\n      const device ValT* As,\n      const device ValT* Bs,\n      int A_sz,\n      int B_sz,\n      int sort_md) {\n    CompareOp op;\n    int A_st = max(0, sort_md - B_sz);\n    int A_ed = min(sort_md, A_sz);\n    while (A_st < A_ed) {\n      int md = A_st + (A_ed - A_st) / 2;\n      auto a = As[md];\n      auto b = Bs[sort_md - 1 - md];\n      if (op(b, a)) {\n        A_ed = md;\n      } else {\n        A_st = md + 1;\n      }\n    }\n    return A_ed;\n  }\n};\ntemplate <\n    typename ValT,\n    typename IdxT,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void mb_block_sort(\n    const device ValT* inp [[buffer(0)]],\n    device ValT* out_vals [[buffer(1)]],\n    device IdxT* out_idxs [[buffer(2)]],\n    const constant int& size_sorted_axis [[buffer(3)]],\n    const constant int& stride_sorted_axis [[buffer(4)]],\n    const constant int& nc_dim [[buffer(5)]],\n    const constant int* nc_shape [[buffer(6)]],\n    const constant int64_t* nc_strides [[buffer(7)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel = KernelMultiBlockMergeSort<\n      ValT,\n      IdxT,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD>;\n  auto block_idx = elem_to_loc(tid.y, nc_shape, nc_strides, nc_dim);\n  inp += block_idx;\n  out_vals += tid.y * size_sorted_axis;\n  out_idxs += tid.y * size_sorted_axis;\n  threadgroup ValT tgp_vals[sort_kernel::N_PER_BLOCK];\n  threadgroup IdxT tgp_idxs[sort_kernel::N_PER_BLOCK];\n  sort_kernel::block_sort(\n      inp,\n      out_vals,\n      out_idxs,\n      size_sorted_axis,\n      stride_sorted_axis,\n      tgp_vals,\n      tgp_idxs,\n      tid,\n      lid);\n}\ntemplate <\n    typename ValT,\n    typename IdxT,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel]] void mb_block_partition(\n    device IdxT* block_partitions [[buffer(0)]],\n    const device ValT* dev_vals [[buffer(1)]],\n    const device IdxT* dev_idxs [[buffer(2)]],\n    const constant int& size_sorted_axis [[buffer(3)]],\n    const constant int& merge_tiles [[buffer(4)]],\n    const constant int& n_blocks [[buffer(5)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 tgp_dims [[threads_per_threadgroup]]) {\n  using sort_kernel = KernelMultiBlockMergeSort<\n      ValT,\n      IdxT,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD>;\n  block_partitions += tid.y * tgp_dims.x;\n  dev_vals += tid.y * size_sorted_axis;\n  dev_idxs += tid.y * size_sorted_axis;\n  for (int i = lid.x; i <= n_blocks; i += tgp_dims.x) {\n    int merge_group = i / merge_tiles;\n    int merge_lane = i % merge_tiles;\n    int sort_sz = sort_kernel::N_PER_BLOCK * merge_tiles;\n    int sort_st = sort_kernel::N_PER_BLOCK * merge_tiles * merge_group;\n    int A_st = min(size_sorted_axis, sort_st);\n    int A_ed = min(size_sorted_axis, sort_st + sort_sz / 2);\n    int B_st = A_ed;\n    int B_ed = min(size_sorted_axis, B_st + sort_sz / 2);\n    int partition_at = min(B_ed - A_st, sort_kernel::N_PER_BLOCK * merge_lane);\n    int partition = sort_kernel::merge_partition(\n        dev_vals + A_st,\n        dev_vals + B_st,\n        A_ed - A_st,\n        B_ed - B_st,\n        partition_at);\n    block_partitions[i] = A_st + partition;\n  }\n}\ntemplate <\n    typename ValT,\n    typename IdxT,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp = LessThan<ValT>>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void\nmb_block_merge(\n    const device IdxT* block_partitions [[buffer(0)]],\n    const device ValT* dev_vals_in [[buffer(1)]],\n    const device IdxT* dev_idxs_in [[buffer(2)]],\n    device ValT* dev_vals_out [[buffer(3)]],\n    device IdxT* dev_idxs_out [[buffer(4)]],\n    const constant int& size_sorted_axis [[buffer(5)]],\n    const constant int& merge_tiles [[buffer(6)]],\n    const constant int& num_tiles [[buffer(7)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel = KernelMultiBlockMergeSort<\n      ValT,\n      IdxT,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD,\n      CompareOp>;\n  using block_sort_t = typename sort_kernel::block_merge_sort_t;\n  block_partitions += tid.y * (num_tiles + 1);\n  dev_vals_in += tid.y * size_sorted_axis;\n  dev_idxs_in += tid.y * size_sorted_axis;\n  dev_vals_out += tid.y * size_sorted_axis;\n  dev_idxs_out += tid.y * size_sorted_axis;\n  int block_idx = tid.x;\n  int merge_group = block_idx / merge_tiles;\n  int sort_st = sort_kernel::N_PER_BLOCK * merge_tiles * merge_group;\n  int sort_sz = sort_kernel::N_PER_BLOCK * merge_tiles;\n  int sort_md = sort_kernel::N_PER_BLOCK * block_idx - sort_st;\n  int A_st = block_partitions[block_idx + 0];\n  int A_ed = block_partitions[block_idx + 1];\n  int B_st = min(size_sorted_axis, 2 * sort_st + sort_sz / 2 + sort_md - A_st);\n  int B_ed = min(\n      size_sorted_axis,\n      2 * sort_st + sort_sz / 2 + sort_md + sort_kernel::N_PER_BLOCK - A_ed);\n  if ((block_idx % merge_tiles) == merge_tiles - 1) {\n    A_ed = min(size_sorted_axis, sort_st + sort_sz / 2);\n    B_ed = min(size_sorted_axis, sort_st + sort_sz);\n  }\n  int A_sz = A_ed - A_st;\n  int B_sz = B_ed - B_st;\n  thread ValT thread_vals[N_PER_THREAD];\n  thread IdxT thread_idxs[N_PER_THREAD];\n  for (int i = 0; i < N_PER_THREAD; i++) {\n    int idx = BLOCK_THREADS * i + lid.x;\n    if (idx < (A_sz + B_sz)) {\n      thread_vals[i] = (idx < A_sz) ? dev_vals_in[A_st + idx]\n                                    : dev_vals_in[B_st + idx - A_sz];\n      thread_idxs[i] = (idx < A_sz) ? dev_idxs_in[A_st + idx]\n                                    : dev_idxs_in[B_st + idx - A_sz];\n    } else {\n      thread_vals[i] = CompareOp::init;\n      thread_idxs[i] = 0;\n    }\n  }\n  threadgroup ValT tgp_vals[sort_kernel::N_PER_BLOCK];\n  threadgroup IdxT tgp_idxs[sort_kernel::N_PER_BLOCK];\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  for (int i = 0; i < N_PER_THREAD; i++) {\n    int idx = BLOCK_THREADS * i + lid.x;\n    tgp_vals[idx] = thread_vals[i];\n    tgp_idxs[idx] = thread_idxs[i];\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  int sort_md_local = min(A_sz + B_sz, N_PER_THREAD * int(lid.x));\n  int A_st_local = block_sort_t::merge_partition(\n      tgp_vals, tgp_vals + A_sz, A_sz, B_sz, sort_md_local);\n  int A_ed_local = A_sz;\n  int B_st_local = sort_md_local - A_st_local;\n  int B_ed_local = B_sz;\n  int A_sz_local = A_ed_local - A_st_local;\n  int B_sz_local = B_ed_local - B_st_local;\n  block_sort_t::merge_step(\n      tgp_vals + A_st_local,\n      tgp_vals + A_ed_local + B_st_local,\n      tgp_idxs + A_st_local,\n      tgp_idxs + A_ed_local + B_st_local,\n      A_sz_local,\n      B_sz_local,\n      thread_vals,\n      thread_idxs);\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  for (int i = 0; i < N_PER_THREAD; ++i) {\n    int idx = lid.x * N_PER_THREAD;\n    tgp_vals[idx + i] = thread_vals[i];\n    tgp_idxs[idx + i] = thread_idxs[i];\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  int base_idx = tid.x * sort_kernel::N_PER_BLOCK;\n  for (int i = lid.x; i < sort_kernel::N_PER_BLOCK; i += BLOCK_THREADS) {\n    int idx = base_idx + i;\n    if (idx < size_sorted_axis) {\n      dev_vals_out[idx] = tgp_vals[i];\n      dev_idxs_out[idx] = tgp_idxs[i];\n    }\n  }\n}\n"
+ "\nusing namespace mlx::steel;\nconstant bool has_batch [[function_constant(10)]];\nconstant bool use_out_source [[function_constant(100)]];\nconstant bool do_axpby [[function_constant(110)]];\nconstant bool align_M [[function_constant(200)]];\nconstant bool align_N [[function_constant(201)]];\nconstant bool align_K [[function_constant(202)]];\ntemplate <\n    typename T,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    typename AccumType = float>\n[[kernel, max_total_threads_per_threadgroup(WM* WN * 32)]] void gemm(\n    const device T* A [[buffer(0)]],\n    const device T* B [[buffer(1)]],\n    const device T* C [[buffer(2), function_constant(use_out_source)]],\n    device T* D [[buffer(3)]],\n    const constant GEMMParams* params [[buffer(4)]],\n    const constant GEMMAddMMParams* addmm_params [[buffer(5), function_constant(use_out_source)]],\n    const constant int* batch_shape [[buffer(6)]],\n    const constant int64_t* batch_strides [[buffer(7)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  (void)lid;\n  using gemm_kernel = GEMMKernel<\n      T,\n      T,\n      BM,\n      BN,\n      BK,\n      WM,\n      WN,\n      transpose_a,\n      transpose_b,\n      true,\n      true,\n      AccumType>;\n  using loader_a_t = typename gemm_kernel::loader_a_t;\n  using loader_b_t = typename gemm_kernel::loader_b_t;\n  using mma_t = typename gemm_kernel::mma_t;\n  const int tid_y = ((tid.y) << params->swizzle_log) +\n      ((tid.x) & ((1 << params->swizzle_log) - 1));\n  const int tid_x = (tid.x) >> params->swizzle_log;\n  if (params->tiles_n <= tid_x || params->tiles_m <= tid_y) {\n    return;\n  }\n  if (has_batch) {\n    const constant auto* A_bstrides = batch_strides;\n    const constant auto* B_bstrides = batch_strides + params->batch_ndim;\n    ulong2 batch_offsets = elem_to_loc_broadcast(\n        tid.z, batch_shape, A_bstrides, B_bstrides, params->batch_ndim);\n    A += batch_offsets.x;\n    B += batch_offsets.y;\n    if (use_out_source) {\n      const constant auto* C_bstrides = B_bstrides + params->batch_ndim;\n      C += elem_to_loc(tid.z, batch_shape, C_bstrides, params->batch_ndim);\n    }\n  } else {\n    A += params->batch_stride_a * tid.z;\n    B += params->batch_stride_b * tid.z;\n    if (use_out_source) {\n      C += addmm_params->batch_stride_c * tid.z;\n    }\n  }\n  D += params->batch_stride_d * tid.z;\n  threadgroup T As[gemm_kernel::tgp_mem_size_a];\n  threadgroup T Bs[gemm_kernel::tgp_mem_size_b];\n  threadgroup_barrier(mem_flags::mem_none);\n  const int c_row = tid_y * BM;\n  const int c_col = tid_x * BN;\n  const size_t c_row_long = size_t(c_row);\n  const size_t c_col_long = size_t(c_col);\n  A += transpose_a ? c_row_long : c_row_long * params->lda;\n  B += transpose_b ? c_col_long * params->ldb : c_col_long;\n  D += c_row_long * params->ldd + c_col_long;\n  if (use_out_source) {\n    C += c_row_long * addmm_params->ldc + c_col_long * addmm_params->fdc;\n  }\n  thread mma_t mma_op(simd_group_id, simd_lane_id);\n  thread loader_a_t loader_a(A, params->lda, As, simd_group_id, simd_lane_id);\n  thread loader_b_t loader_b(B, params->ldb, Bs, simd_group_id, simd_lane_id);\n  const short tgp_bm = align_M ? BM : short(min(BM, params->M - c_row));\n  const short tgp_bn = align_N ? BN : short(min(BN, params->N - c_col));\n  int gemm_k_iterations = params->gemm_k_iterations_aligned;\n  if (!align_K) {\n    const int k_last = params->gemm_k_iterations_aligned * BK;\n    const int k_remain = params->K - k_last;\n    const size_t k_jump_a =\n        transpose_a ? params->lda * size_t(k_last) : size_t(k_last);\n    const size_t k_jump_b =\n        transpose_b ? size_t(k_last) : params->ldb * size_t(k_last);\n    loader_a.src += k_jump_a;\n    loader_b.src += k_jump_b;\n    const short2 tile_dims_A =\n        transpose_a ? short2(tgp_bm, k_remain) : short2(k_remain, tgp_bm);\n    const short2 tile_dims_B =\n        transpose_b ? short2(k_remain, tgp_bn) : short2(tgp_bn, k_remain);\n    loader_a.load_safe(tile_dims_A);\n    loader_b.load_safe(tile_dims_B);\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    mma_op.mma(As, Bs);\n    loader_a.src -= k_jump_a;\n    loader_b.src -= k_jump_b;\n  }\n  const TransformAdd<AccumType, AccumType> epilogue_op_add(\n      addmm_params->alpha, addmm_params->beta);\n  const TransformAxpby<AccumType, AccumType> epilogue_op_axpby(\n      addmm_params->alpha, addmm_params->beta);\n  if (align_M && align_N) {\n    for (int k = 0; k < gemm_k_iterations; k++) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      loader_a.load_unsafe();\n      loader_b.load_unsafe();\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      mma_op.mma(As, Bs);\n      loader_a.next();\n      loader_b.next();\n    }\n    threadgroup_barrier(mem_flags::mem_none);\n    if (use_out_source) {\n      if (do_axpby) {\n        mma_op.apply_epilogue(\n            C, addmm_params->ldc, addmm_params->fdc, epilogue_op_axpby);\n      } else {\n        mma_op.apply_epilogue(\n            C, addmm_params->ldc, addmm_params->fdc, epilogue_op_add);\n      }\n    }\n    return mma_op.store_result(D, params->ldd);\n  }\n  else {\n    const int leftover_bk = 0;\n    if ((align_M || tgp_bm == BM) && (align_N || tgp_bn == BN)) {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<true, true, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue(\n              C, addmm_params->ldc, addmm_params->fdc, epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue(\n              C, addmm_params->ldc, addmm_params->fdc, epilogue_op_add);\n        }\n      }\n      return mma_op.store_result(D, params->ldd);\n    } else if (align_N || tgp_bn == BN) {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<false, true, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_add);\n        }\n      }\n      return mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n    } else if (align_M || tgp_bm == BM) {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<true, false, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_add);\n        }\n      }\n      return mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n    } else {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<false, false, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_add);\n        }\n      }\n      return mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n    }\n  }\n}\n"
+ " Indices of dimension "
+ " Received invalid axis "
+ " empty or the total slice size must be 0."
+ " is not inexact."
+ " were provided which results in "
+ ") exceeded."
+ ".metallib. "
+ "/AppleInternal/Library/BuildRoots/4~B4EtugDg65G0U275SrbRtToRvH5PIIeMbzBFcsc/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS26.0.Internal.sdk/usr/local/include/usd/pxr/base/tf/refPtr.h"
+ "/AppleInternal/Library/BuildRoots/4~B4EtugDg65G0U275SrbRtToRvH5PIIeMbzBFcsc/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS26.0.Internal.sdk/usr/local/include/usd/pxr/usd/sdf/declareHandles.h"
+ "/AppleInternal/Library/BuildRoots/4~B4EtugDg65G0U275SrbRtToRvH5PIIeMbzBFcsc/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS26.0.Internal.sdk/usr/local/include/usd/pxr/usd/usd/object.h"
+ "> with error "
+ "AddMM"
+ "AddMM has no CPU implementation."
+ "BroadcastAxes"
+ "BroadcastAxes has no CPU implementation."
+ "CorePhotogrammetry.framework"
+ "ExpandDims"
+ "ExpandDims has no CPU implementation."
+ "Failed to load the default metallib. "
+ "Failed to load the metallib "
+ "Failed to load the metallib from <"
+ "Flatten"
+ "Flatten has no CPU implementation."
+ "GatherAxis"
+ "GatherAxis has no CPU implementation."
+ "Imag"
+ "Imag has no CPU implementation."
+ "LearnedMVS.mlmodelc"
+ "LearnedMVSModel"
+ "Logaddexp"
+ "MLX_MAX_MB_PER_BUFFER"
+ "MLX_METAL_FAST_SYNCH"
+ "Real"
+ "Real has no CPU implementation."
+ "ScatterAxis"
+ "ScatterAxis has no CPU implementation."
+ "Squeeze"
+ "Squeeze has no CPU implementation."
+ "Sum<"
+ "Unflatten"
+ "Unflatten has no CPU implementation."
+ "We attempted to load it from <"
+ "[Arange::eval_gpu] Does not support type."
+ "[BroadcastAxes] VMAP NYI"
+ "[Broadcast] Unable to infer broadcast shape"
+ "[Copy::eval_gpu] Dynamic output offset requires GeneralGeneral copy"
+ "[Event::stream] Cannot access stream on invalid event."
+ "[Fence::wait] Timed out"
+ "[Sort::eval_gpu] Stride too large."
+ "[Unflatten] Can only infer one dimension."
+ "[Unflatten] Cannot unflatten axis "
+ "[addmm] Got 0 dimension input. Inputs must have at least one dimension."
+ "[addmm] Got matrices with incorrectly broadcasted shapes: "
+ "[addmm] Last dimension of first input with shape "
+ "[addmm] Only real floating point types are supported but "
+ "[broadcast_arrays] Received invalid axes to ignore."
+ "[broadcast_shapes] Shapes "
+ "[concatenate] "
+ "[conv] Spatial dimensions of input after padding"
+ "[finfo] dtype "
+ "[gather] If the input is empty, either the indices must be"
+ "[gather_axis] Cannot calculate JVP with respect to indices."
+ "[logcumsumexp] Axis "
+ "[metal::malloc] Attempting to allocate "
+ "[metal::malloc] Resource limit ("
+ "[new_stream] Cannot make gpu stream without gpu backend."
+ "[put_along_axis]"
+ "[repeat] "
+ "[scatter_add_axis]"
+ "[scatter_axis] Cannot calculate JVP with respect to indices."
+ "[scatter_axis] Cannot calculate VJP with respect to indices."
+ "[slice_update] Invalid number of indices or strides for "
+ "[stack] All arrays must have the same shape"
+ "[stack] No arrays provided for stacking"
+ "[unflatten] Invalid axes "
+ "[unflatten] Shape to unflatten to cannot be empty."
+ "_axbpy"
+ "_axpby1"
+ "_do_flip_"
+ "_dynamic"
+ "_ker_h_"
+ "_ker_w_"
+ "_str_h_"
+ "_str_w_"
+ "_tgp_h_"
+ "_tgp_w_"
+ "assetVersion"
+ "com.apple.CorePhotogrammetry.LearnedMVS"
+ "com.apple.MobileAsset.CorePhotogrammetry.MLModel.LearnedMVS"
+ "copy_gg_dynamic"
+ "copy_gg_dynamic_nd1"
+ "copy_gg_dynamic_nd2"
+ "copy_gg_dynamic_nd3"
+ "depthwise_conv_2d_"
+ "device_name"
+ "double"
+ "fence_update"
+ "fence_wait"
+ "float64"
+ "float64 is not supported on the GPU"
+ "g1large"
+ "g1large_"
+ "gather_axis"
+ "gather_axis{0}{1}_{2}"
+ "gemm_splitk"
+ "gemm_splitk_accum"
+ "gemm_splitk_accum_axpby"
+ "gg1large_"
+ "hasPrefix:"
+ "hw.memsize"
+ "implicit_gemm_conv_2d"
+ "implicit_gemm_conv_2d_general"
+ "initForAssetType:withAssetSpecifier:"
+ "initForClientName:selectingAsset:error:"
+ "input_coherent"
+ "int64_t"
+ "interestInContentSync:withInterestPolicy:"
+ "iogpu.rsrc_limit"
+ "lockContentSync:withTimeout:lockedAssetSelector:newerInProgress:error:"
+ "lockContentSync:withUsagePolicy:withTimeout:lockedAssetSelector:newerInProgress:error:"
+ "logaddexp"
+ "resource_limit"
+ "s2_"
+ "scatter_axis"
+ "scatter_axis{0}{1}_{2}_{3}"
+ "setUserInitiated:"
- "\n\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT = int64_t,\n    int N_READS = REDUCE_N_READS>\n[[kernel]] void all_reduce(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& in_size [[buffer(2)]],\n    const constant size_t& row_size [[buffer(3)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  threadgroup U shared_vals[simd_size];\n  U total = Op::init;\n  IdxT start_idx = gid.y * IdxT(row_size);\n  IdxT actual_row =\n      (start_idx + row_size <= in_size) ? row_size : in_size - start_idx;\n  IdxT blocks = actual_row / (lsize.x * N_READS);\n  int extra = actual_row - blocks * (lsize.x * N_READS);\n  extra -= lid.x * N_READS;\n  start_idx += lid.x * N_READS;\n  in += start_idx;\n  if (extra >= N_READS) {\n    blocks++;\n    extra = 0;\n  }\n  for (IdxT b = 0; b < blocks; b++) {\n    for (int i = 0; i < N_READS; i++) {\n      total = op(static_cast<U>(in[i]), total);\n    }\n    in += lsize.x * N_READS;\n  }\n  if (extra > 0) {\n    for (int i = 0; i < extra; i++) {\n      total = op(static_cast<U>(in[i]), total);\n    }\n  }\n  total = op.simd_reduce(total);\n  if (simd_per_group > 1) {\n    if (simd_lane_id == 0) {\n      shared_vals[simd_group_id] = total;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    total = lid.x < simd_per_group ? shared_vals[lid.x] : op.init;\n    total = op.simd_reduce(total);\n  }\n  if (lid.x == 0) {\n    out[gid.y] = total;\n  }\n}\ntemplate <typename T, typename U, typename Op, typename IdxT, int NDIMS>\n[[kernel]] void col_reduce_small(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]]) {\n  constexpr int n_reads = 4;\n  Op op;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  U totals[n_reads];\n  for (int i = 0; i < n_reads; i++) {\n    totals[i] = Op::init;\n  }\n  IdxT column = IdxT(gid.x) * lsize.x * n_reads + lid.x * n_reads;\n  if (column >= reduction_stride) {\n    return;\n  }\n  bool safe = column + n_reads <= reduction_stride;\n  IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n  IdxT in_idx = elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + column;\n  IdxT total_rows = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(lid.y, reduce_shape, reduce_strides);\n  for (IdxT r = lid.y; r < total_rows; r += lsize.y) {\n    row = in + loop.location();\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(static_cast<U>(row[i]), totals[i]);\n      }\n    } else {\n      U vals[n_reads];\n      for (int i = 0; i < n_reads; i++) {\n        vals[i] =\n            (column + i < reduction_stride) ? static_cast<U>(row[i]) : op.init;\n      }\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(vals[i], totals[i]);\n      }\n    }\n    loop.next(lsize.y, reduce_shape, reduce_strides);\n  }\n  if (lsize.y > 1) {\n    threadgroup U shared_vals[32 * 8 * n_reads];\n    for (int i = 0; i < n_reads; i++) {\n      shared_vals[lid.y * lsize.x * n_reads + lid.x * n_reads + i] = totals[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (lid.y == 0) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = shared_vals[lid.x * n_reads + i];\n      }\n      for (uint j = 1; j < lsize.y; j++) {\n        for (int i = 0; i < n_reads; i++) {\n          totals[i] =\n              op(shared_vals[j * lsize.x * n_reads + lid.x * n_reads + i],\n                 totals[i]);\n        }\n      }\n    }\n  }\n  if (lid.y == 0) {\n    out += out_idx * IdxT(reduction_stride) + column;\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        out[i] = totals[i];\n      }\n    } else {\n      for (int i = 0; column + i < reduction_stride; i++) {\n        out[i] = totals[i];\n      }\n    }\n  }\n}\ntemplate <typename T, typename U, typename Op, typename IdxT, int NDIMS>\n[[kernel]] void col_reduce_longcolumn(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    const constant size_t& out_size [[buffer(11)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]]) {\n  Op op;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  IdxT out_idx = gid.x + gsize.x * IdxT(gid.y);\n  IdxT in_idx = elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + lid.x;\n  U total = Op::init;\n  IdxT total_rows = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(gid.z * lsize.y + lid.y, reduce_shape, reduce_strides);\n  for (IdxT r = gid.z * lsize.y + lid.y; r < total_rows;\n       r += lsize.y * gsize.z) {\n    row = in + loop.location();\n    total = op(static_cast<U>(*row), total);\n    loop.next(lsize.y * gsize.z, reduce_shape, reduce_strides);\n  }\n  threadgroup U shared_vals[32 * 32];\n  shared_vals[lid.y * lsize.x + lid.x] = total;\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  if (lid.y == 0) {\n    for (uint i = 1; i < lsize.y; i++) {\n      total = op(total, shared_vals[i * lsize.x + lid.x]);\n    }\n    out[gid.z * IdxT(out_size) + out_idx * IdxT(reduction_stride) + lid.x] =\n        total;\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int BM,\n    int BN>\n[[kernel]] void col_reduce_looped(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  constexpr int n_simdgroups = 8;\n  constexpr short tgp_size = n_simdgroups * simd_size;\n  constexpr short n_reads = (BM * BN) / tgp_size;\n  constexpr short n_read_blocks = BN / n_reads;\n  threadgroup U shared_vals[BN * BM];\n  U totals[n_reads];\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  for (int i = 0; i < n_reads; i++) {\n    totals[i] = Op::init;\n  }\n  short lid = simd_group_id * simd_size + simd_lane_id;\n  short2 offset((lid % n_read_blocks) * n_reads, lid / n_read_blocks);\n  IdxT column = BN * gid.x + offset.x;\n  bool safe = column + n_reads <= reduction_stride;\n  IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n  IdxT in_idx = elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + column;\n  IdxT total = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(offset.y, reduce_shape, reduce_strides);\n  for (IdxT r = offset.y; r < total; r += BM) {\n    row = in + loop.location();\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(static_cast<U>(row[i]), totals[i]);\n      }\n    } else {\n      U vals[n_reads];\n      for (int i = 0; i < n_reads; i++) {\n        vals[i] =\n            (column + i < reduction_stride) ? static_cast<U>(row[i]) : op.init;\n      }\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(vals[i], totals[i]);\n      }\n    }\n    loop.next(BM, reduce_shape, reduce_strides);\n  }\n  if (BM == 32) {\n    constexpr int n_outputs = BN / n_simdgroups;\n    static_assert(\n        BM != 32 || n_outputs == n_reads,\n        \"The tile should be selected such that n_outputs == n_reads\");\n    for (int i = 0; i < n_reads; i++) {\n      shared_vals[offset.y * BN + offset.x + i] = totals[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    short2 out_offset(simd_group_id * n_outputs, simd_lane_id);\n    for (int i = 0; i < n_outputs; i++) {\n      totals[i] =\n          op.simd_reduce(shared_vals[out_offset.y * BN + out_offset.x + i]);\n    }\n    if (simd_lane_id == 0) {\n      IdxT out_column = BN * gid.x + out_offset.x;\n      out += out_idx * IdxT(reduction_stride) + out_column;\n      if (out_column + n_outputs <= reduction_stride) {\n        for (int i = 0; i < n_outputs; i++) {\n          out[i] = totals[i];\n        }\n      } else {\n        for (int i = 0; out_column + i < reduction_stride; i++) {\n          out[i] = totals[i];\n        }\n      }\n    }\n  }\n  else {\n    short x_block = offset.x / n_reads;\n    for (int i = 0; i < n_reads; i++) {\n      shared_vals[x_block * BM * n_reads + i * BM + offset.y] = totals[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (offset.y == 0) {\n      for (int i = 0; i < n_reads; i++) {\n        for (int j = 1; j < BM; j++) {\n          totals[i] =\n              op(shared_vals[x_block * BM * n_reads + i * BM + j], totals[i]);\n        }\n      }\n    }\n    if (offset.y == 0) {\n      out += out_idx * IdxT(reduction_stride) + column;\n      if (safe) {\n        for (int i = 0; i < n_reads; i++) {\n          out[i] = totals[i];\n        }\n      } else {\n        for (int i = 0; column + i < reduction_stride; i++) {\n          out[i] = totals[i];\n        }\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int BM,\n    int BN>\n[[kernel]] void col_reduce_2pass(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    const constant size_t& out_size [[buffer(11)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  constexpr int n_simdgroups = 8;\n  constexpr short tgp_size = n_simdgroups * simd_size;\n  constexpr short n_reads = (BM * BN) / tgp_size;\n  constexpr short n_read_blocks = BN / n_reads;\n  constexpr int n_outputs = BN / n_simdgroups;\n  constexpr short outer_blocks = 32;\n  static_assert(BM == 32, \"BM should be equal to 32\");\n  threadgroup U shared_vals[BN * BM];\n  U totals[n_reads];\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  for (int i = 0; i < n_reads; i++) {\n    totals[i] = Op::init;\n  }\n  short lid = simd_group_id * simd_size + simd_lane_id;\n  short2 offset((lid % n_read_blocks) * n_reads, lid / n_read_blocks);\n  IdxT column = BN * gid.x + offset.x;\n  bool safe = column + n_reads <= reduction_stride;\n  IdxT full_idx = gid.y + gsize.y * IdxT(gid.z);\n  IdxT block_idx = full_idx / IdxT(out_size);\n  IdxT out_idx = full_idx % IdxT(out_size);\n  IdxT in_idx = elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + column;\n  IdxT total = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(offset.y + block_idx * BM, reduce_shape, reduce_strides);\n  for (IdxT r = offset.y + block_idx * BM; r < total; r += outer_blocks * BM) {\n    row = in + loop.location();\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(static_cast<U>(row[i]), totals[i]);\n      }\n    } else {\n      U vals[n_reads];\n      for (int i = 0; i < n_reads; i++) {\n        vals[i] =\n            (column + i < reduction_stride) ? static_cast<U>(row[i]) : op.init;\n      }\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(vals[i], totals[i]);\n      }\n    }\n    loop.next(outer_blocks * BM, reduce_shape, reduce_strides);\n  }\n  for (int i = 0; i < n_reads; i++) {\n    shared_vals[offset.y * BN + offset.x + i] = totals[i];\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  short2 out_offset(simd_group_id * n_outputs, simd_lane_id);\n  for (int i = 0; i < n_outputs; i++) {\n    totals[i] =\n        op.simd_reduce(shared_vals[out_offset.y * BN + out_offset.x + i]);\n  }\n  if (simd_lane_id == 0) {\n    IdxT out_column = BN * gid.x + out_offset.x;\n    out += full_idx * IdxT(reduction_stride) + out_column;\n    if (out_column + n_outputs <= reduction_stride) {\n      for (int i = 0; i < n_outputs; i++) {\n        out[i] = totals[i];\n      }\n    } else {\n      for (int i = 0; out_column + i < reduction_stride; i++) {\n        out[i] = totals[i];\n      }\n    }\n  }\n}\ntemplate <typename T, typename Op>\n[[kernel]] void init_reduce(\n    device T* out [[buffer(0)]],\n    uint tid [[thread_position_in_grid]]) {\n  out[tid] = Op::init;\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void per_thread_row_reduce(\n    thread U totals[N_WRITES],\n    const device T* inputs[N_WRITES],\n    int blocks,\n    int extra,\n    uint lsize_x,\n    uint lid_x) {\n  Op op;\n  for (int i = 0; i < N_WRITES; i++) {\n    totals[i] = Op::init;\n  }\n  for (int i = 0; i < blocks; i++) {\n    for (int j = 0; j < N_WRITES; j++) {\n      for (int i = 0; i < N_READS; i++) {\n        totals[j] = op(static_cast<U>(inputs[j][i]), totals[j]);\n      }\n      inputs[j] += lsize_x * N_READS;\n    }\n  }\n  int index = lid_x * N_READS;\n  if (index + N_READS <= extra) {\n    for (int j = 0; j < N_WRITES; j++) {\n      for (int i = 0; i < N_READS; i++) {\n        totals[j] = op(static_cast<U>(inputs[j][i]), totals[j]);\n      }\n    }\n  } else {\n    for (int j = 0; j < N_WRITES; j++) {\n      for (int i = 0; index + i < extra; i++) {\n        totals[j] = op(static_cast<U>(inputs[j][i]), totals[j]);\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void per_thread_row_reduce(\n    thread U totals[N_WRITES],\n    const device T* in,\n    const constant size_t& reduction_size,\n    int blocks,\n    int extra,\n    uint lsize_x,\n    uint lid_x) {\n  const device T* inputs[N_WRITES];\n  inputs[0] = in + lid_x * N_READS;\n  for (int i = 1; i < N_READS; i++) {\n    inputs[i] = inputs[i - 1] + reduction_size;\n  }\n  per_thread_row_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, inputs, blocks, extra, lsize_x, lid_x);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void per_thread_row_reduce(\n    thread U totals[N_WRITES],\n    const device T* in,\n    const size_t row_idx,\n    int blocks,\n    int extra,\n    const constant int* shape,\n    const constant size_t* strides,\n    const constant int& ndim,\n    uint lsize_x,\n    uint lid_x) {\n  const device T* inputs[N_WRITES];\n  in += lid_x * N_READS;\n  for (int i = 0; i < N_READS; i++) {\n    inputs[i] = in + elem_to_loc(row_idx + i, shape, strides, ndim);\n  }\n  per_thread_row_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, inputs, blocks, extra, lsize_x, lid_x);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void threadgroup_reduce(\n    thread U totals[N_WRITES],\n    threadgroup U* shared_vals,\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  for (int i = 0; i < N_WRITES; i++) {\n    totals[i] = op.simd_reduce(totals[i]);\n  }\n  if (simd_per_group > 1) {\n    if (simd_lane_id == 0) {\n      for (int i = 0; i < N_WRITES; i++) {\n        shared_vals[simd_group_id * N_WRITES + i] = totals[i];\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    U values[N_WRITES];\n    for (int i = 0; i < N_WRITES; i++) {\n      values[i] = (lid.x < simd_per_group) ? shared_vals[lid.x * N_WRITES + i]\n                                           : op.init;\n    }\n    for (int i = 0; i < N_WRITES; i++) {\n      totals[i] = op.simd_reduce(values[i]);\n    }\n  }\n}\ntemplate <typename T, typename U, typename Op, int N_READS = REDUCE_N_READS>\nMETAL_FUNC void\nthread_reduce(thread U& total, const device T* row, int blocks, int extra) {\n  Op op;\n  for (int i = 0; i < blocks; i++) {\n    U vals[N_READS];\n    for (int j = 0; j < N_READS; j++) {\n      vals[j] = row[j];\n    }\n    for (int j = 0; j < N_READS; j++) {\n      total = op(vals[j], total);\n    }\n    row += N_READS;\n  }\n  for (int i = 0; i < extra; i++) {\n    total = op(*row++, total);\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int N_READS = REDUCE_N_READS>\n[[kernel]] void row_reduce_small(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& row_size [[buffer(2)]],\n    const constant size_t& non_row_reductions [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 tid [[thread_position_in_grid]],\n    uint3 tsize [[threads_per_grid]]) {\n  Op op;\n  U total_val = Op::init;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  int blocks = IdxT(row_size) / N_READS;\n  int extra = IdxT(row_size) % N_READS;\n  if ((non_row_reductions < 32 && row_size <= 8) || non_row_reductions <= 8) {\n    IdxT out_idx = tid.x + tsize.y * IdxT(tid.y);\n    in += elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n    for (uint r = 0; r < non_row_reductions; r++) {\n      row = in + loop.location();\n      thread_reduce<T, U, Op, N_READS>(total_val, row, blocks, extra);\n      loop.next(reduce_shape, reduce_strides);\n    }\n    out[out_idx] = total_val;\n  } else {\n    IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n    in += elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n    loop.next(simd_lane_id, reduce_shape, reduce_strides);\n    for (uint r = simd_lane_id; r < non_row_reductions; r += simd_size) {\n      row = in + loop.location();\n      thread_reduce<T, U, Op, N_READS>(total_val, row, blocks, extra);\n      loop.next(simd_size, reduce_shape, reduce_strides);\n    }\n    total_val = op.simd_reduce(total_val);\n    if (simd_lane_id == 0) {\n      out[out_idx] = total_val;\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT = size_t,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\n[[kernel]] void row_reduce_simple(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& out_size [[buffer(3)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  threadgroup U shared_vals[simd_size * N_WRITES];\n  U totals[N_WRITES];\n  IdxT out_idx = N_WRITES * (gid.y + gsize.y * IdxT(gid.z));\n  if (out_idx + N_WRITES > out_size) {\n    out_idx = out_size - N_WRITES;\n  }\n  in += out_idx * IdxT(reduction_size);\n  out += out_idx;\n  int blocks = IdxT(reduction_size) / (lsize.x * N_READS);\n  int extra = reduction_size - blocks * (lsize.x * N_READS);\n  per_thread_row_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, in, reduction_size, blocks, extra, lsize.x, lid.x);\n  threadgroup_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, shared_vals, lid, simd_lane_id, simd_per_group, simd_group_id);\n  if (lid.x == 0) {\n    for (int i = 0; i < N_WRITES; i++) {\n      out[i] = totals[i];\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int N_READS = REDUCE_N_READS>\n[[kernel]] void row_reduce_looped(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& row_size [[buffer(2)]],\n    const constant size_t& non_row_reductions [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  threadgroup U shared_vals[simd_size];\n  U total = Op::init;\n  IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n  in += elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim) +\n      lid.x * N_READS;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  int blocks = IdxT(row_size) / (lsize.x * N_READS);\n  int extra = row_size - blocks * (lsize.x * N_READS);\n  for (IdxT i = 0; i < non_row_reductions; i++) {\n    row = in + loop.location();\n    U row_total;\n    per_thread_row_reduce<T, U, Op, N_READS, 1>(\n        &row_total, &row, blocks, extra, lsize.x, lid.x);\n    total = op(total, row_total);\n    loop.next(reduce_shape, reduce_strides);\n  }\n  threadgroup_reduce<T, U, Op, N_READS, 1>(\n      &total, shared_vals, lid, simd_lane_id, simd_per_group, simd_group_id);\n  if (lid.x == 0) {\n    out[out_idx] = total;\n  }\n}\n"
- "\n#if (__METAL_VERSION__ >= 310)\nusing namespace metal;\ntypedef bfloat bfloat16_t;\ninline uint16_t bfloat16_to_uint16(const bfloat16_t x) {\n  return as_type<uint16_t>(x);\n}\ninline bfloat16_t uint16_to_bfloat16(const uint16_t x) {\n  return as_type<bfloat16_t>(x);\n}\n\n #else\nusing namespace metal;\nconstexpr METAL_FUNC uint16_t float_to_bfloat_bits(float x) {\n  if ((as_type<uint32_t>(x) & ~_fp_encoding_traits<float>::sign_mask) >\n      _fp_encoding_traits<float>::inf_mask) {\n    return uint16_t(as_type<uint32_t>(0x7FC0));\n  }\n  uint32_t float_bits = as_type<uint32_t>(x);\n  float_bits += ((float_bits >> 16) & 1) + as_type<uint32_t>(0x7FFF);\n  return float_bits >> 16;\n}\nconstexpr METAL_FUNC float bfloat_bits_to_float(uint16_t x) {\n  return as_type<float>((uint32_t)x << 16);\n}\nstruct _MLX_BFloat16;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_to_bfloat =\n    !is_same_v<T, _MLX_BFloat16> && is_convertible_v<T, float>;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_from_bfloat =\n    !is_same_v<T, _MLX_BFloat16> && is_convertible_v<float, T>;\nstruct _MLX_BFloat16 {\n  uint16_t bits_;\n  _MLX_BFloat16() thread = default;\n  _MLX_BFloat16() threadgroup = default;\n  _MLX_BFloat16() device = default;\n  _MLX_BFloat16() constant = default;\n  struct bits_to_bfloat_struct {};\n  static constexpr METAL_FUNC bits_to_bfloat_struct bits_to_bfloat() {\n    return bits_to_bfloat_struct();\n  }\n  constexpr METAL_FUNC _MLX_BFloat16(uint16_t bits, bits_to_bfloat_struct)\n      : bits_(bits) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) thread\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) threadgroup\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) device\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) constant\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const thread {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const threadgroup {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const device {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const constant {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n};\nconstexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 x) {\n  return -static_cast<float>(x);\n}\nconstexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC float operator+(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC float operator+(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC float operator+(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC float operator+(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); };;\nconstexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC float operator-(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC float operator-(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC float operator-(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC float operator-(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); };;\nconstexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC float operator*(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC float operator*(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC float operator*(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC float operator*(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); };;\nconstexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC float operator/(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC float operator/(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC float operator/(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC float operator/(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator+=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator+=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator+=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator-=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator-=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator-=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator*=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator*=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator*=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator/=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator/=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator/=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator+=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator+=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator+=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator-=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator-=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator-=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator*=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator*=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator*=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator/=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator/=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator/=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator+=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator+=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator+=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator-=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator-=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator-=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator*=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator*=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator*=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator/=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator/=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator/=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator+=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator+=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator+=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator-=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator-=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator-=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator*=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator*=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator*=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator/=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator/=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator/=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator+=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator+=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator+=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator-=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator-=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator-=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator*=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator*=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator*=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator/=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator/=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator/=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator+=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator+=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator+=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator-=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator-=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator-=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator*=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator*=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator*=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator/=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator/=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator/=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator+=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator+=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator+=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator-=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator-=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator-=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator*=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator*=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator*=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator/=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator/=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator/=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator+=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator+=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator+=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator-=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator-=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator-=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator*=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator*=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator*=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator/=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator/=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator/=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;\ntypedef struct _MLX_BFloat16 bfloat16_t;\n#pragma METAL internals : enable\nnamespace metal {\ntemplate <>\nstruct _numeric_limits_impl<bfloat16_t> : _fp_numeric_limits_impl_base {\n  static constexpr constant int digits = 8;\n  static constexpr constant int digits10 = 2;\n  static constexpr constant int max_digits10 = 4;\n  static constexpr constant int radix = 2;\n  static constexpr constant int min_exponent = -125;\n  static constexpr constant int min_exponent10 = -37;\n  static constexpr constant int max_exponent = 128;\n  static constexpr constant int max_exponent10 = 38;\n  static constexpr bfloat16_t min() {\n    return _MLX_BFloat16(0x0080, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t lowest() {\n    return _MLX_BFloat16(0xFF7F, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t max() {\n    return _MLX_BFloat16(0x7F7F, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t epsilon() {\n    return _MLX_BFloat16(0x3C00, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t round_error() {\n    return _MLX_BFloat16(0x3F00, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t infinity() {\n    return _MLX_BFloat16(0x7F80, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t quiet_NaN() {\n    return _MLX_BFloat16(0x7FC0, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t signaling_NaN() {\n    return _MLX_BFloat16(0x7F80, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t denorm_min() {\n    return _MLX_BFloat16(0x0001, _MLX_BFloat16::bits_to_bfloat());\n  }\n};\nMETAL_FUNC bool isnan(_MLX_BFloat16 x) {\n  return x != x;\n}\n}\n#pragma METAL internals : disable\ninline uint16_t bfloat16_to_uint16(const bfloat16_t x) {\n  return x.bits_;\n}\ninline bfloat16_t uint16_to_bfloat16(const uint16_t x) {\n  return _MLX_BFloat16(x, _MLX_BFloat16::bits_to_bfloat());\n}\n\n #endif\n\nnamespace metal {\nMETAL_FUNC bfloat16_t abs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t acos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acos(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t acosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acosh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t asin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asin(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t asinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asinh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t atan(bfloat16_t y_over_x) { return static_cast<bfloat16_t>( __metal_atan(static_cast<float>(y_over_x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t atan2(bfloat16_t y, bfloat16_t x) { return static_cast<bfloat16_t>( __metal_atan2(static_cast<float>(y), static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t atanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_atanh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t ceil(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_ceil(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t cos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cos(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t cosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cosh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t cospi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cospi(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t divide(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_divide(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t exp(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t exp10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp10(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t exp2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp2(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fabs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fdim(bfloat16_t x, bfloat16_t y) { float t = static_cast<float>(x - y); return static_cast<bfloat16_t>(select(t, float(0), t < float(0) || x == y)); } METAL_FUNC bfloat16_t floor(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_floor(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fma(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fma( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z))); } METAL_FUNC bfloat16_t fmax(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmax3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmedian3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmod(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmod(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fract(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fract(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t frexp(bfloat16_t x, thread int& exp) { return static_cast<bfloat16_t>(__metal_frexp(static_cast<float>(x), &exp)); } METAL_FUNC bfloat16_t ldexp(bfloat16_t x, int k) { return static_cast<bfloat16_t>(__metal_ldexp(static_cast<float>(x), k, __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t log(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t log10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log10(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t log2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log2(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t max(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t max3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t median3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t min(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t min3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t nextafter(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_nextafter(static_cast<float>(x), static_cast<float>(y))); } METAL_FUNC bfloat16_t pow(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_pow(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t powr(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_powr(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t rint(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rint(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t round(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_round(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t rsqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rsqrt(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sin(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sinpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinpi(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sqrt(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t tan(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tan(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t tanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t tanpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanpi(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t trunc(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_trunc(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); };\nnamespace fast {\nMETAL_FUNC bfloat16_t abs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t acos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acos(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t acosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acosh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t asin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asin(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t asinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asinh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t atan(bfloat16_t y_over_x) { return static_cast<bfloat16_t>( __metal_atan(static_cast<float>(y_over_x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t atan2(bfloat16_t y, bfloat16_t x) { return static_cast<bfloat16_t>( __metal_atan2(static_cast<float>(y), static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t atanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_atanh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t ceil(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_ceil(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t cos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cos(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t cosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cosh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t cospi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cospi(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t divide(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_divide(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t exp(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t exp10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp10(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t exp2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp2(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fabs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fdim(bfloat16_t x, bfloat16_t y) { float t = static_cast<float>(x - y); return static_cast<bfloat16_t>(select(t, float(0), t < float(0) || x == y)); } METAL_FUNC bfloat16_t floor(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_floor(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fma(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fma( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z))); } METAL_FUNC bfloat16_t fmax(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmax3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmedian3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmod(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmod(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fract(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fract(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t frexp(bfloat16_t x, thread int& exp) { return static_cast<bfloat16_t>(__metal_frexp(static_cast<float>(x), &exp)); } METAL_FUNC bfloat16_t ldexp(bfloat16_t x, int k) { return static_cast<bfloat16_t>(__metal_ldexp(static_cast<float>(x), k, __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t log(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t log10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log10(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t log2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log2(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t max(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t max3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t median3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t min(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t min3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t nextafter(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_nextafter(static_cast<float>(x), static_cast<float>(y))); } METAL_FUNC bfloat16_t pow(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_pow(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t powr(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_powr(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t rint(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rint(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t round(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_round(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t rsqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rsqrt(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sin(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sinpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinpi(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sqrt(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t tan(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tan(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t tanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t tanpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanpi(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t trunc(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_trunc(static_cast<float>(x), __METAL_FAST_MATH__)); };\n}\nnamespace precise {\nMETAL_FUNC bfloat16_t abs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t acos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acos(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t acosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acosh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t asin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asin(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t asinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asinh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t atan(bfloat16_t y_over_x) { return static_cast<bfloat16_t>( __metal_atan(static_cast<float>(y_over_x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t atan2(bfloat16_t y, bfloat16_t x) { return static_cast<bfloat16_t>( __metal_atan2(static_cast<float>(y), static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t atanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_atanh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t ceil(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_ceil(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t cos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cos(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t cosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cosh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t cospi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cospi(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t divide(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_divide(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t exp(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t exp10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp10(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t exp2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp2(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fabs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fdim(bfloat16_t x, bfloat16_t y) { float t = static_cast<float>(x - y); return static_cast<bfloat16_t>(select(t, float(0), t < float(0) || x == y)); } METAL_FUNC bfloat16_t floor(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_floor(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fma(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fma( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z))); } METAL_FUNC bfloat16_t fmax(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmax3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmedian3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmin(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmin3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmod(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmod(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fract(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fract(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t frexp(bfloat16_t x, thread int& exp) { return static_cast<bfloat16_t>(__metal_frexp(static_cast<float>(x), &exp)); } METAL_FUNC bfloat16_t ldexp(bfloat16_t x, int k) { return static_cast<bfloat16_t>(__metal_ldexp(static_cast<float>(x), k, __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t log(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t log10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log10(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t log2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log2(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t max(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t max3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t median3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t min(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t min3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t nextafter(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_nextafter(static_cast<float>(x), static_cast<float>(y))); } METAL_FUNC bfloat16_t pow(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_pow(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t powr(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_powr(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t rint(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rint(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t round(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_round(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t rsqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rsqrt(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sin(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sinpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinpi(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sqrt(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t tan(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tan(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t tanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t tanpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanpi(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t trunc(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_trunc(static_cast<float>(x), __METAL_PRECISE_MATH__)); };\n}\n}\nnamespace metal {\nMETAL_FUNC bfloat16_t simd_broadcast(bfloat16_t data, ushort broadcast_lane_id) { return uint16_to_bfloat16( __metal_simd_broadcast(bfloat16_to_uint16(data), broadcast_lane_id)); } METAL_FUNC bfloat16_t simd_shuffle(bfloat16_t data, ushort simd_lane_id) { return uint16_to_bfloat16( __metal_simd_shuffle(bfloat16_to_uint16(data), simd_lane_id)); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_down( bfloat16_t data, bfloat16_t filling_data, ushort delta, ushort modulo) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_down( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, modulo)); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_down( bfloat16_t data, bfloat16_t filling_data, ushort delta) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_down( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, __metal_get_simdgroup_size(ushort()))); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_up( bfloat16_t data, bfloat16_t filling_data, ushort delta, ushort modulo) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_up( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, modulo)); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_up( bfloat16_t data, bfloat16_t filling_data, ushort delta) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_up( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, __metal_get_simdgroup_size(ushort()))); } METAL_FUNC bfloat16_t simd_shuffle_down(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_down(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_rotate_down(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_rotate_down(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_rotate_up(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_rotate_up(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_up(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_up(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_xor(bfloat16_t data, ushort mask) { return uint16_to_bfloat16( __metal_simd_shuffle_xor(bfloat16_to_uint16(data), mask)); };\nMETAL_FUNC bfloat16_t simd_max(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_max(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_min(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_min(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_exclusive_product(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_exclusive_product(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_exclusive_sum(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_exclusive_sum(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_inclusive_product(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_inclusive_product(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_inclusive_sum(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_inclusive_sum(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_product(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_product(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_sum(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_sum(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_xor(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_xor(static_cast<float>(data))); };\n}\nusing namespace metal;\nstruct complex64_t;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_to_complex64 =\n    !is_same_v<T, complex64_t> && is_convertible_v<T, float>;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_from_complex64 =\n    !is_same_v<T, complex64_t> &&\n    (is_convertible_v<float, T> || is_convertible_v<bfloat16_t, T>);\nstruct complex64_t {\n  float real;\n  float imag;\n  constexpr complex64_t(float real, float imag) : real(real), imag(imag) {};\n  constexpr complex64_t() : real(0), imag(0) {};\n  constexpr complex64_t() threadgroup : real(0), imag(0) {};\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) thread : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) threadgroup : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) device : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) constant : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const thread {\n    return static_cast<T>(real);\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const threadgroup {\n    return static_cast<T>(real);\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const device {\n    return static_cast<T>(real);\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const constant {\n    return static_cast<T>(real);\n  }\n};\nconstexpr complex64_t operator-(complex64_t x) {\n  return {-x.real, -x.imag};\n}\nconstexpr bool operator>=(complex64_t a, complex64_t b) {\n  return (a.real > b.real) || (a.real == b.real && a.imag >= b.imag);\n}\nconstexpr bool operator>(complex64_t a, complex64_t b) {\n  return (a.real > b.real) || (a.real == b.real && a.imag > b.imag);\n}\nconstexpr bool operator<=(complex64_t a, complex64_t b) {\n  return operator>=(b, a);\n}\nconstexpr bool operator<(complex64_t a, complex64_t b) {\n  return operator>(b, a);\n}\nconstexpr bool operator==(complex64_t a, complex64_t b) {\n  return a.real == b.real && a.imag == b.imag;\n}\nconstexpr complex64_t operator+(complex64_t a, complex64_t b) {\n  return {a.real + b.real, a.imag + b.imag};\n}\nconstexpr complex64_t operator-(complex64_t a, complex64_t b) {\n  return {a.real - b.real, a.imag - b.imag};\n}\nconstexpr complex64_t operator*(complex64_t a, complex64_t b) {\n  return {a.real * b.real - a.imag * b.imag, a.real * b.imag + a.imag * b.real};\n}\nconstexpr complex64_t operator/(complex64_t a, complex64_t b) {\n  auto denom = b.real * b.real + b.imag * b.imag;\n  auto x = a.real * b.real + a.imag * b.imag;\n  auto y = a.imag * b.real - a.real * b.imag;\n  return {x / denom, y / denom};\n}\nconstexpr complex64_t operator%(complex64_t a, complex64_t b) {\n  auto real = a.real - (b.real * static_cast<int64_t>(a.real / b.real));\n  auto imag = a.imag - (b.imag * static_cast<int64_t>(a.imag / b.imag));\n  if (real != 0 && (real < 0 != b.real < 0)) {\n    real += b.real;\n  }\n  if (imag != 0 && (imag < 0 != b.imag < 0)) {\n    imag += b.imag;\n  }\n  return {real, imag};\n}\nstatic constant constexpr int MAX_REDUCE_SPECIALIZED_DIMS = 4;\nstatic constant constexpr int REDUCE_N_READS = 4;\nstatic constant constexpr int REDUCE_N_WRITES = 4;\nstatic constant constexpr int SOFTMAX_N_READS = 4;\nstatic constant constexpr int RMS_N_READS = 4;\nstatic constant constexpr int RMS_LOOPED_LIMIT = 4096;\n\ntypedef half float16_t;\ntemplate <typename U>\nstruct Limits {\n  static const constant U max = metal::numeric_limits<U>::max();\n  static const constant U min = metal::numeric_limits<U>::min();\n  static const constant U finite_max = metal::numeric_limits<U>::max();\n  static const constant U finite_min = metal::numeric_limits<U>::min();\n};\ntemplate <> struct Limits<uint8_t> { static constexpr constant uint8_t max = metal::numeric_limits<uint8_t>::max(); static constexpr constant uint8_t min = metal::numeric_limits<uint8_t>::min(); static constexpr constant uint8_t finite_max = metal::numeric_limits<uint8_t>::max(); static constexpr constant uint8_t finite_min = metal::numeric_limits<uint8_t>::min(); };;\ntemplate <> struct Limits<uint16_t> { static constexpr constant uint16_t max = metal::numeric_limits<uint16_t>::max(); static constexpr constant uint16_t min = metal::numeric_limits<uint16_t>::min(); static constexpr constant uint16_t finite_max = metal::numeric_limits<uint16_t>::max(); static constexpr constant uint16_t finite_min = metal::numeric_limits<uint16_t>::min(); };;\ntemplate <> struct Limits<uint32_t> { static constexpr constant uint32_t max = metal::numeric_limits<uint32_t>::max(); static constexpr constant uint32_t min = metal::numeric_limits<uint32_t>::min(); static constexpr constant uint32_t finite_max = metal::numeric_limits<uint32_t>::max(); static constexpr constant uint32_t finite_min = metal::numeric_limits<uint32_t>::min(); };;\ntemplate <> struct Limits<uint64_t> { static constexpr constant uint64_t max = metal::numeric_limits<uint64_t>::max(); static constexpr constant uint64_t min = metal::numeric_limits<uint64_t>::min(); static constexpr constant uint64_t finite_max = metal::numeric_limits<uint64_t>::max(); static constexpr constant uint64_t finite_min = metal::numeric_limits<uint64_t>::min(); };;\ntemplate <> struct Limits<int8_t> { static constexpr constant int8_t max = metal::numeric_limits<int8_t>::max(); static constexpr constant int8_t min = metal::numeric_limits<int8_t>::min(); static constexpr constant int8_t finite_max = metal::numeric_limits<int8_t>::max(); static constexpr constant int8_t finite_min = metal::numeric_limits<int8_t>::min(); };;\ntemplate <> struct Limits<int16_t> { static constexpr constant int16_t max = metal::numeric_limits<int16_t>::max(); static constexpr constant int16_t min = metal::numeric_limits<int16_t>::min(); static constexpr constant int16_t finite_max = metal::numeric_limits<int16_t>::max(); static constexpr constant int16_t finite_min = metal::numeric_limits<int16_t>::min(); };;\ntemplate <> struct Limits<int32_t> { static constexpr constant int32_t max = metal::numeric_limits<int32_t>::max(); static constexpr constant int32_t min = metal::numeric_limits<int32_t>::min(); static constexpr constant int32_t finite_max = metal::numeric_limits<int32_t>::max(); static constexpr constant int32_t finite_min = metal::numeric_limits<int32_t>::min(); };;\ntemplate <> struct Limits<int64_t> { static constexpr constant int64_t max = metal::numeric_limits<int64_t>::max(); static constexpr constant int64_t min = metal::numeric_limits<int64_t>::min(); static constexpr constant int64_t finite_max = metal::numeric_limits<int64_t>::max(); static constexpr constant int64_t finite_min = metal::numeric_limits<int64_t>::min(); };;\ntemplate <> struct Limits<half> { static constexpr constant half max = metal::numeric_limits<half>::infinity(); static constexpr constant half min = -metal::numeric_limits<half>::infinity(); static constexpr constant half finite_max = metal::numeric_limits<half>::max(); static constexpr constant half finite_min = -metal::numeric_limits<half>::max(); };;\ntemplate <> struct Limits<float> { static constexpr constant float max = metal::numeric_limits<float>::infinity(); static constexpr constant float min = -metal::numeric_limits<float>::infinity(); static constexpr constant float finite_max = metal::numeric_limits<float>::max(); static constexpr constant float finite_min = -metal::numeric_limits<float>::max(); };;\ntemplate <> struct Limits<bfloat16_t> { static constexpr constant bfloat16_t max = metal::numeric_limits<bfloat16_t>::infinity(); static constexpr constant bfloat16_t min = -metal::numeric_limits<bfloat16_t>::infinity(); static constexpr constant bfloat16_t finite_max = metal::numeric_limits<bfloat16_t>::max(); static constexpr constant bfloat16_t finite_min = -metal::numeric_limits<bfloat16_t>::max(); };;\ntemplate <>\nstruct Limits<bool> {\n  static constexpr constant bool max = true;\n  static constexpr constant bool min = false;\n};\ntemplate <>\nstruct Limits<complex64_t> {\n  static constexpr constant complex64_t max = complex64_t(\n      metal::numeric_limits<float>::infinity(),\n      metal::numeric_limits<float>::infinity());\n  static constexpr constant complex64_t min = complex64_t(\n      -metal::numeric_limits<float>::infinity(),\n      -metal::numeric_limits<float>::infinity());\n};\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc(\n    uint elem,\n    constant const int* shape,\n    constant const StrideT* strides,\n    int ndim) {\n  IdxT loc = 0;\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    loc += (elem % shape[i]) * IdxT(strides[i]);\n    elem /= shape[i];\n  }\n  return loc;\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc(\n    StrideT elem,\n    constant const int* shape,\n    constant const StrideT* strides,\n    int ndim) {\n  IdxT loc = 0;\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    loc += (elem % shape[i]) * IdxT(strides[i]);\n    elem /= shape[i];\n  }\n  return loc;\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc(\n    uint3 elem,\n    constant const int* shape,\n    constant const StrideT* strides,\n    int ndim) {\n  IdxT loc =\n      elem.x * IdxT(strides[ndim - 1]) + elem.y * IdxT(strides[ndim - 2]);\n  for (int d = ndim - 3; d >= 0; --d) {\n    loc += (elem.z % shape[d]) * IdxT(strides[d]);\n    elem.z /= shape[d];\n  }\n  return loc;\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc_1(uint elem, constant const StrideT& stride) {\n  return elem * IdxT(stride);\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc_2(uint2 elem, constant const StrideT strides[2]) {\n  return elem.x * IdxT(strides[1]) + elem.y * IdxT(strides[0]);\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc_3(uint3 elem, constant const StrideT strides[3]) {\n  return elem.x * IdxT(strides[2]) + elem.y * IdxT(strides[1]) +\n      elem.z * IdxT(strides[0]);\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC vec<IdxT, 2> elem_to_loc_2_nd(\n    uint3 elem,\n    constant const int* shape,\n    constant const StrideT* a_strides,\n    constant const StrideT* b_strides,\n    int ndim) {\n  vec<IdxT, 2> loc = {\n      IdxT(\n          elem.x * IdxT(a_strides[ndim - 1]) +\n          IdxT(elem.y) * IdxT(a_strides[ndim - 2])),\n      IdxT(\n          elem.x * IdxT(b_strides[ndim - 1]) +\n          elem.y * IdxT(b_strides[ndim - 2]))};\n  for (int d = ndim - 3; d >= 0; --d) {\n    uint l = elem.z % shape[d];\n    loc.x += l * IdxT(a_strides[d]);\n    loc.y += l * IdxT(b_strides[d]);\n    elem.z /= shape[d];\n  }\n  return loc;\n}\ntemplate <typename IdxT = size_t>\nMETAL_FUNC vec<IdxT, 3> elem_to_loc_3_nd(\n    uint3 elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const size_t* c_strides,\n    int ndim) {\n  vec<IdxT, 3> loc = {\n      elem.x * IdxT(a_strides[ndim - 1]) + elem.y * IdxT(a_strides[ndim - 2]),\n      elem.x * IdxT(b_strides[ndim - 1]) + elem.y * IdxT(b_strides[ndim - 2]),\n      elem.x * IdxT(c_strides[ndim - 1]) + elem.y * IdxT(c_strides[ndim - 2])};\n  for (int d = ndim - 3; d >= 0; --d) {\n    uint l = elem.z % shape[d];\n    loc.x += l * IdxT(a_strides[d]);\n    loc.y += l * IdxT(b_strides[d]);\n    loc.z += l * IdxT(c_strides[d]);\n    elem.z /= shape[d];\n  }\n  return loc;\n}\ntemplate <int DIM, typename OffsetT = size_t, bool General = true>\nstruct LoopedElemToLoc {\n  int dim;\n  LoopedElemToLoc<DIM - 1, OffsetT, General> inner_looper;\n  OffsetT offset{0};\n  int index{0};\n  LoopedElemToLoc(int dim) : dim(dim), inner_looper(dim - 1) {}\n  void next(const constant int* shape, const constant size_t* strides) {\n    if (dim == 0) {\n      return;\n    }\n    index++;\n    offset += OffsetT(strides[dim - 1]);\n    if (index >= shape[dim - 1]) {\n      index = 0;\n      inner_looper.next(shape, strides);\n      offset = inner_looper.offset;\n    }\n  }\n  void next(int n, const constant int* shape, const constant size_t* strides) {\n    if (dim == 0) {\n      return;\n    }\n    index += n;\n    offset += n * OffsetT(strides[dim - 1]);\n    if (index >= shape[dim - 1]) {\n      int extra = index - shape[dim - 1];\n      if (extra >= shape[dim - 1]) {\n        inner_looper.next(1 + extra / shape[dim - 1], shape, strides);\n        extra = extra % shape[dim - 1];\n      } else {\n        inner_looper.next(shape, strides);\n      }\n      index = 0;\n      offset = inner_looper.offset;\n      if (extra > 0) {\n        next(extra, shape, strides);\n      }\n    }\n  }\n  OffsetT location() {\n    return offset;\n  }\n};\ntemplate <typename OffsetT>\nstruct LoopedElemToLoc<1, OffsetT, true> {\n  int dim;\n  OffsetT offset{0};\n  uint index{0};\n  LoopedElemToLoc(int dim) : dim(dim) {}\n  void next(const constant int* shape, const constant size_t* strides) {\n    index++;\n    if (dim > 1) {\n      offset = elem_to_loc<size_t, OffsetT>(index, shape, strides, dim);\n    } else {\n      offset += OffsetT(strides[0]);\n    }\n  }\n  void next(int n, const constant int* shape, const constant size_t* strides) {\n    index += n;\n    if (dim > 1) {\n      offset = elem_to_loc<size_t, OffsetT>(index, shape, strides, dim);\n    } else {\n      offset = index * OffsetT(strides[0]);\n    }\n  }\n  OffsetT location() {\n    return offset;\n  }\n};\ntemplate <typename OffsetT>\nstruct LoopedElemToLoc<1, OffsetT, false> {\n  OffsetT offset{0};\n  LoopedElemToLoc(int) {}\n  void next(const constant int*, const constant size_t* strides) {\n    offset += OffsetT(strides[0]);\n  }\n  void next(int n, const constant int*, const constant size_t* strides) {\n    offset += n * OffsetT(strides[0]);\n  }\n  OffsetT location() {\n    return offset;\n  }\n};\ntemplate <typename T, typename U>\ninline T ceildiv(T N, U M) {\n  return (N + M - 1) / M;\n}\ninline float log1p(float x) {\n  float xp1 = 1.0f + x;\n  if (xp1 == Limits<float>::max) {\n    return Limits<float>::max;\n  }\n  if (xp1 == 1.0f) {\n    return x;\n  }\n  return x * (metal::log(xp1) / (xp1 - 1.0f));\n}\ninline bfloat16_t log1p(bfloat16_t x) {\n  float xp1 = 1.0f + static_cast<float>(x);\n  if (xp1 == Limits<float>::max) {\n    return Limits<bfloat16_t>::max;\n  }\n  if (xp1 == 1.0f) {\n    return x;\n  }\n  return bfloat16_t(x * (metal::log(xp1) / (xp1 - 1.0f)));\n}\ninline uint64_t simd_shuffle_down(uint64_t data, uint16_t delta) {\n  return as_type<uint64_t>(\n      metal::simd_shuffle_down(as_type<uint2>(data), delta));\n}\ninline int64_t simd_shuffle_down(int64_t data, uint16_t delta) {\n  return as_type<int64_t>(\n      metal::simd_shuffle_down(as_type<uint2>(data), delta));\n}\ninline bool simd_shuffle_down(bool data, uint16_t delta) {\n  return simd_shuffle_down(static_cast<uint32_t>(data), delta);\n}\ninline complex64_t simd_shuffle_down(complex64_t data, uint16_t delta) {\n  return complex64_t(\n      simd_shuffle_down(data.real, delta), simd_shuffle_down(data.imag, delta));\n}\ninline uint64_t simd_shuffle_up(uint64_t data, uint16_t delta) {\n  return as_type<uint64_t>(metal::simd_shuffle_up(as_type<uint2>(data), delta));\n}\ninline int64_t simd_shuffle_up(int64_t data, uint16_t delta) {\n  return as_type<int64_t>(metal::simd_shuffle_up(as_type<uint2>(data), delta));\n}\ninline bool simd_shuffle_up(bool data, uint16_t delta) {\n  return simd_shuffle_up(static_cast<uint32_t>(data), delta);\n}\ninline complex64_t simd_shuffle_up(complex64_t data, uint16_t delta) {\n  return complex64_t(\n      simd_shuffle_up(data.real, delta), simd_shuffle_up(data.imag, delta));\n}\ninline uint64_t\nsimd_shuffle_and_fill_up(uint64_t data, uint64_t filling, uint16_t delta) {\n  return as_type<uint64_t>(metal::simd_shuffle_and_fill_up(\n      as_type<uint2>(data), as_type<uint2>(filling), delta));\n}\ninline int64_t\nsimd_shuffle_and_fill_up(int64_t data, int64_t filling, uint16_t delta) {\n  return as_type<int64_t>(metal::simd_shuffle_and_fill_up(\n      as_type<uint2>(data), as_type<uint2>(filling), delta));\n}\ninline bool simd_shuffle_and_fill_up(bool data, bool filling, uint16_t delta) {\n  return simd_shuffle_and_fill_up(\n      static_cast<uint32_t>(data), static_cast<uint32_t>(filling), delta);\n}\ninline complex64_t simd_shuffle_and_fill_up(\n    complex64_t data,\n    complex64_t filling,\n    uint16_t delta) {\n  return complex64_t(\n      simd_shuffle_and_fill_up(data.real, filling.real, delta),\n      simd_shuffle_and_fill_up(data.imag, filling.imag, delta));\n}\ninline uint64_t simd_shuffle(uint64_t data, uint16_t lane) {\n  return as_type<uint64_t>(metal::simd_shuffle(as_type<uint2>(data), lane));\n}\ninline int64_t simd_shuffle(int64_t data, uint16_t lane) {\n  return as_type<int64_t>(metal::simd_shuffle(as_type<uint2>(data), lane));\n}\ninline bool simd_shuffle(bool data, uint16_t lane) {\n  return simd_shuffle(static_cast<uint32_t>(data), lane);\n}\ninline complex64_t simd_shuffle(complex64_t data, uint16_t lane) {\n  return complex64_t(\n      simd_shuffle(data.real, lane), simd_shuffle(data.imag, lane));\n}\n"
- "\nMETAL_FUNC ulong2 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n  }\n  return ulong2(loc_a, loc_b);\n}\nMETAL_FUNC ulong3 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const size_t* c_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  ulong loc_c{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n    loc_c += pos_in_dim * c_strides[i];\n  }\n  return ulong3(loc_a, loc_b, loc_c);\n}\n\ntemplate <int NDIM>\nstruct MLXConvParams {\n  const int N;\n  const int C;\n  const int O;\n  const int iS[NDIM];\n  const int wS[NDIM];\n  const int oS[NDIM];\n  const int str[NDIM];\n  const int pad[NDIM];\n  const int kdil[NDIM];\n  const int idil[NDIM];\n  const size_t in_strides[NDIM + 2];\n  const size_t wt_strides[NDIM + 2];\n  const size_t out_strides[NDIM + 2];\n  const int groups;\n  const bool flip;\n};\nnamespace mlx {\nnamespace steel {\nstruct ImplicitGemmConv2DParams {\n  const int M;\n  const int N;\n  const int K;\n  const int gemm_k_iterations;\n  const int inp_jump_w;\n  const int inp_jump_h;\n  const int inp_jump_c;\n  const int tiles_n;\n  const int tiles_m;\n  const int swizzle_log;\n};\nstruct Conv2DGeneralJumpParams {\n  const int f_wgt_jump_h;\n  const int f_wgt_jump_w;\n  const int f_out_jump_h;\n  const int f_out_jump_w;\n  const int adj_out_h;\n  const int adj_out_w;\n  const int adj_out_hw;\n  const int adj_implicit_m;\n};\nstruct Conv2DGeneralBaseInfo {\n  int weight_base;\n  int weight_size;\n};\n}\n}\nnamespace mlx {\nnamespace steel {\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderLargeFilter {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant ImplicitGemmConv2DParams* gemm_params;\n  short weight_h;\n  short weight_w;\n  const device T* src[n_rows];\n  int read_n[n_rows];\n  int read_ih[n_rows];\n  int read_iw[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderLargeFilter(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        gemm_params(gemm_params_),\n        weight_h(0),\n        weight_w(0) {\n    int out_n_pixels = params->oS[0] * params->oS[1];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / out_n_pixels;\n      int hw = offset_nhw % out_n_pixels;\n      int oh = hw / params->oS[1];\n      int ow = hw % params->oS[1];\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n      if (params->flip) {\n        ih += (params->wS[0] - 1) * params->kdil[0];\n        iw += (params->wS[1] - 1) * params->kdil[1];\n      }\n      src[i] = src_ + n * params->in_strides[0] + ih * params->in_strides[1] +\n          iw * params->in_strides[2] + bj;\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      int n = read_n[i];\n      int ih = read_ih[i] + weight_h * params->kdil[0];\n      int iw = read_iw[i] + weight_w * params->kdil[1];\n      if ((n < params->N) && (ih >= 0 && ih < params->iS[0]) &&\n          (iw >= 0 && iw < params->iS[1])) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = src[i][j];\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    if (++weight_w < params->wS[1]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_w;\n      }\n      return;\n    }\n    weight_w = 0;\n    if (++weight_h < params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_h;\n      }\n      return;\n    }\n    weight_h = 0;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; i++) {\n      src[i] += gemm_params->inp_jump_c;\n    }\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderSmallFilter {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  using mask_t = short;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant ImplicitGemmConv2DParams* gemm_params;\n  short weight_h;\n  short weight_w;\n  const device T* src[n_rows];\n  mask_t mask_h[n_rows];\n  mask_t mask_w[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderSmallFilter(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        gemm_params(gemm_params_),\n        weight_h(0),\n        weight_w(0) {\n    int out_n_pixels = params->oS[0] * params->oS[1];\n    int read_n[n_rows];\n    int read_ih[n_rows];\n    int read_iw[n_rows];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / out_n_pixels;\n      int hw = offset_nhw % out_n_pixels;\n      int oh = hw / params->oS[1];\n      int ow = hw % params->oS[1];\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n      if (params->flip) {\n        ih += (params->wS[0] - 1) * params->kdil[0];\n        iw += (params->wS[1] - 1) * params->kdil[1];\n      }\n      src[i] = src_ + n * params->in_strides[0] + ih * params->in_strides[1] +\n          iw * params->in_strides[2] + bj;\n    }\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      mask_h[i] = 0;\n      mask_w[i] = 0;\n    }\n    for (short kh = 0; kh < params->wS[0]; kh++) {\n      short flip_h = params->flip ? params->wS[0] - kh - 1 : kh;\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; ++i) {\n        int n = read_n[i];\n        int ih = read_ih[i] + flip_h * params->kdil[0];\n        bool in_bounds = n < params->N && ih >= 0 && ih < params->iS[0];\n        mask_h[i] |= (in_bounds << kh);\n      }\n    }\n    for (short kw = 0; kw < params->wS[1]; kw++) {\n      short flip_w = params->flip ? params->wS[1] - kw - 1 : kw;\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; ++i) {\n        int iw = read_iw[i] + flip_w * params->kdil[1];\n        bool in_bounds = iw >= 0 && iw < params->iS[1];\n        mask_w[i] |= (in_bounds << kw);\n      }\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n    mask_t h_mask = mask_t(1) << weight_h;\n    mask_t w_mask = mask_t(1) << weight_w;\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      if ((mask_h[i] & h_mask) && (mask_w[i] & w_mask)) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = src[i][j];\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    if (++weight_w < params->wS[1]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_w;\n      }\n      return;\n    }\n    weight_w = 0;\n    if (++weight_h < params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_h;\n      }\n      return;\n    }\n    weight_h = 0;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; i++) {\n      src[i] += gemm_params->inp_jump_c;\n    }\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DWeightBlockLoader {\n  static constant constexpr const short BROWS = BN;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size =\n      (BN == 8) ? 1 : (tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4);\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const int src_ld;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  const constant MLXConvParams<2>* params;\n  int weight_hw;\n  const int read_n;\n  const bool do_read;\n  METAL_FUNC Conv2DWeightBlockLoader(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(params_->wt_strides[0]),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld + bj),\n        params(params_),\n        weight_hw(0),\n        read_n(offsets.y + bi),\n        do_read(read_n + n_rows * TROWS <= gemm_params_->N) {}\n  METAL_FUNC void load_unsafe() const {\n    if (BN != 8 || do_read) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BN; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = src[i * src_ld + j];\n        }\n      }\n    } else {\n      for (short i = 0; i < BN; i += TROWS) {\n        if ((read_n + i) < params->O) {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = src[i * src_ld + j];\n          }\n        } else {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    if (++weight_hw < (params->wS[1] * params->wS[0])) {\n      src += params->wt_strides[2];\n      return;\n    }\n    weight_hw = 0;\n    src += BK - (params->wS[1] * params->wS[0] - 1) * params->wt_strides[2];\n  }\n};\n}\n}\nnamespace mlx {\nnamespace steel {\ntemplate <short n_channels_>\nstruct ChannelHelper {\n  static constant constexpr const short n_channels = n_channels_;\n  static constant constexpr const short vec_size = n_channels_ <= 4 ? 4 : 8;\n  static constant constexpr const short excess = vec_size - n_channels_;\n};\ntemplate <>\nstruct ChannelHelper<1> {\n  static constant constexpr const short n_channels = 1;\n  static constant constexpr const short vec_size = 1;\n  static constant constexpr const short excess = 0;\n};\ntemplate <>\nstruct ChannelHelper<2> {\n  static constant constexpr const short n_channels = 2;\n  static constant constexpr const short vec_size = 2;\n  static constant constexpr const short excess = 0;\n};\ntemplate <>\nstruct ChannelHelper<3> {\n  static constant constexpr const short n_channels = 3;\n  static constant constexpr const short vec_size = 4;\n  static constant constexpr const short excess = 1;\n};\ntemplate <>\nstruct ChannelHelper<4> {\n  static constant constexpr const short n_channels = 4;\n  static constant constexpr const short vec_size = 4;\n  static constant constexpr const short excess = 0;\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short n_channels,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderSmallChannels {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = ChannelHelper<n_channels>::vec_size;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant ImplicitGemmConv2DParams* gemm_params;\n  short weight_hw;\n  const device T* src[n_rows];\n  int read_n[n_rows];\n  int read_ih[n_rows];\n  int read_iw[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderSmallChannels(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        gemm_params(gemm_params_),\n        weight_hw(thread_idx % TCOLS) {\n    int out_n_pixels = params->oS[0] * params->oS[1];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / out_n_pixels;\n      int hw = offset_nhw % out_n_pixels;\n      int oh = hw / params->oS[1];\n      int ow = hw % params->oS[1];\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      src[i] = src_ + n * params->in_strides[0] + ih * params->in_strides[1] +\n          iw * params->in_strides[2];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n    if (weight_hw >= params->wS[1] * params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n      return;\n    }\n    int wh = (weight_hw / params->wS[1]);\n    int ww = (weight_hw % params->wS[1]);\n    int flip_h = params->flip ? params->wS[0] - wh - 1 : wh;\n    int flip_w = params->flip ? params->wS[1] - ww - 1 : ww;\n    int weight_h = flip_h * params->kdil[0];\n    int weight_w = flip_w * params->kdil[1];\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      int n = read_n[i];\n      int ih = read_ih[i] + weight_h;\n      int iw = read_iw[i] + weight_w;\n      if ((n < params->N) && (ih >= 0 && ih < params->iS[0]) &&\n          (iw >= 0 && iw < params->iS[1])) {\n        const device T* curr_src = src[i] + weight_h * params->in_strides[1] +\n            weight_w * params->in_strides[2];\n#pragma clang loop unroll(full)\n        for (short j = 0; j < n_channels; ++j) {\n          dst[is * dst_ld + j] = curr_src[j];\n        }\n#pragma clang loop unroll(full)\n        for (short j = n_channels; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    weight_hw += TCOLS;\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short n_channels,\n    short tgp_padding = 0>\nstruct Conv2DWeightBlockLoaderSmallChannels {\n  static constant constexpr const short BROWS = BN;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = ChannelHelper<n_channels>::vec_size;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const int src_ld;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  const constant MLXConvParams<2>* params;\n  int weight_hw;\n  const int read_n;\n  const bool do_read;\n  METAL_FUNC Conv2DWeightBlockLoaderSmallChannels(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(params_->wt_strides[0]),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld),\n        params(params_),\n        weight_hw(thread_idx % TCOLS),\n        read_n(offsets.y + bi),\n        do_read(read_n + BN <= gemm_params_->N) {}\n  METAL_FUNC void load_unsafe() const {\n    if (bi >= BROWS || bj >= BCOLS)\n      return;\n    if (read_n >= params->O || weight_hw >= params->wS[1] * params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n      return;\n    }\n    const device T* curr_src = src + weight_hw * params->wt_strides[2];\n    if (BN != 8 || do_read) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < n_channels; j++) {\n          dst[i * dst_ld + j] = curr_src[i * src_ld + j];\n        }\n#pragma clang loop unroll(full)\n        for (short j = n_channels; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n    } else {\n      for (short i = 0; i < BROWS; i += TROWS) {\n        if (((read_n + i) < params->O)) {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < n_channels; j++) {\n            dst[i * dst_ld + j] = curr_src[i * src_ld + j];\n          }\n#pragma clang loop unroll(full)\n          for (short j = n_channels; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        } else {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    weight_hw += TCOLS;\n  }\n};\n}\n}\n\nnamespace mlx {\nnamespace steel {\ntemplate <typename OutT, typename InT>\nstruct TransformNone {\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT) {\n    return static_cast<OutT>(x);\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAdd {\n  TransformAdd(const float, const float) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT c) {\n    return static_cast<OutT>(x) + c;\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAxpby {\n  const float alpha;\n  const float beta;\n  TransformAxpby(const float alpha_, const float beta_)\n      : alpha(alpha_), beta(beta_) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  METAL_FUNC OutT apply(InT x, OutT c) const {\n    return static_cast<OutT>(x * alpha + (beta * c));\n  }\n};\ntemplate <typename T>\nstruct AccumHelper {\n  typedef float accum_type;\n};\nstruct BlockSwizzle {\n  static METAL_FUNC int2\n  swizzle(uint3 tid [[threadgroup_position_in_grid]], const int swizzle_log) {\n    const int tid_x = (tid.x) >> swizzle_log;\n    const int tid_y =\n        ((tid.y) << swizzle_log) + ((tid.x) & ((1 << swizzle_log) - 1));\n    return int2(tid_x, tid_y);\n  }\n};\n}\n}\n#pragma METAL internals : enable\nnamespace metal {\ntemplate <typename T>\nstruct is_empty : metal::bool_constant<__is_empty(T)> {};\ntemplate <typename... Ts>\nstruct make_void {\n  typedef void type;\n};\ntemplate <typename... Ts>\nusing void_t = typename make_void<Ts...>::type;\ntemplate <class T>\nstruct is_static : metal::bool_constant<is_empty<remove_cv_t<T>>::value> {};\ntemplate <typename T>\nstruct pointer_element {};\ntemplate <typename T>\nstruct pointer_element<thread T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<device T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<constant T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<threadgroup T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nusing pointer_element_t = typename pointer_element<remove_cv_t<T>>::type;\n}\n#pragma METAL internals : disable\n\n#pragma METAL internals : enable\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, T v>\nstruct integral_constant {\n  static constexpr constant T value = v;\n  using value_type = T;\n  using type = integral_constant;\n  METAL_FUNC constexpr operator value_type() const noexcept {\n    return value;\n  }\n};\ntemplate <bool B>\nusing bool_constant = integral_constant<bool, B>;\nusing true_type = bool_constant<true>;\nusing false_type = bool_constant<false>;\ntemplate <class T>\nstruct is_integral : bool_constant<metal::is_integral<T>::value> {};\ntemplate <class T, T v>\nstruct is_integral<integral_constant<T, v>>\n    : bool_constant<metal::is_integral<T>::value> {};\ntemplate <typename T>\nconstexpr constant bool is_integral_v = is_integral<T>::value;\ntemplate <int val>\nusing Int = integral_constant<int, val>;\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator+( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv + uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator-( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv - uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator*( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv * uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator/( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv / uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator==( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv == uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator!=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv != uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv < uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv > uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv <= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv >= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator&&( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv && uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator||( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv || uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T>\nMETAL_FUNC constexpr T sum(T x) {\n  return x;\n}\ntemplate <typename T, typename... Us>\nMETAL_FUNC constexpr auto sum(T x, Us... us) {\n  return x + sum(us...);\n}\n}\n}\n#pragma METAL internals : disable\n\nusing namespace metal;\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, int kFragRows_, int kFragCols_>\nstruct BaseMMAFrag {\n  static_assert(\n      kFragRows_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n  static_assert(\n      kFragCols_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n};\ntemplate <typename T>\nstruct BaseMMAFrag<T, 8, 8> {\n  static constant constexpr const int kFragRows = 8;\n  static constant constexpr const int kFragCols = 8;\n  static constant constexpr const int kElemsPerFrag = (kFragRows * kFragCols) / 32;\n  static constant constexpr const int kElemRows = 1;\n  static constant constexpr const int kElemCols = 2;\n  static_assert(\n      kElemRows * kElemCols == kElemsPerFrag,\n      \"MMAFrag shape is not consistent with MMAFrag size\");\n  typedef metal::simdgroup_matrix<T, kFragRows, kFragCols> mat_type;\n  typedef metal::vec<T, kElemsPerFrag> frag_type;\n  METAL_FUNC static constexpr short2 get_coord(ushort simd_lane_id\n                                               [[thread_index_in_simdgroup]]) {\n    const short qid = simd_lane_id / 4;\n    const short fm = (qid & 4) + ((simd_lane_id / 2) % 4);\n    const short fn = (qid & 2) * 2 + (simd_lane_id % 2) * 2;\n    return short2{fn, fm};\n  }\n  template <typename SrcPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  load(thread frag_type& dst, SrcPtrType src, StrX str_x, StrY str_y) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * kElemCols + j] = static_cast<T>(src[i * str_x + j * str_y]);\n      }\n    }\n  }\n  template <\n      typename SrcPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void load_safe(\n      thread frag_type& dst,\n      SrcPtrType src,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[i * kElemCols + j] =\n              static_cast<T>(src[(off_x + i) * str_x + (off_x + j) * str_y]);\n        } else {\n          dst[i * kElemCols + j] = T(0);\n        }\n      }\n    }\n  }\n  template <typename DstPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  store(const thread frag_type& src, DstPtrType dst, StrX str_x, StrY str_y) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * str_x + j * str_y] = static_cast<U>(src[i * kElemCols + j]);\n      }\n    }\n  }\n  template <\n      typename DstPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void store_safe(\n      const thread frag_type& src,\n      DstPtrType dst,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[(off_x + i) * str_x + (off_y + j) * str_y] =\n              static_cast<U>(src[i * kElemCols + j]);\n        }\n      }\n    }\n  }\n  METAL_FUNC static constexpr void mma(\n      thread frag_type& D,\n      thread frag_type& A,\n      thread frag_type& B,\n      thread frag_type& C) {\n    mat_type D_mat;\n    mat_type A_mat;\n    mat_type B_mat;\n    mat_type C_mat;\n    reinterpret_cast<thread frag_type&>(A_mat.thread_elements()) = A;\n    reinterpret_cast<thread frag_type&>(B_mat.thread_elements()) = B;\n    reinterpret_cast<thread frag_type&>(C_mat.thread_elements()) = C;\n    mma(D_mat, A_mat, B_mat, C_mat);\n    D = reinterpret_cast<thread frag_type&>(D_mat.thread_elements());\n  }\n  METAL_FUNC static constexpr void mma(\n      thread mat_type& D,\n      thread mat_type& A,\n      thread mat_type& B,\n      thread mat_type& C) {\n    simdgroup_multiply_accumulate(D, A, B, C);\n  }\n};\ntemplate <\n    typename T,\n    int kTileRows_,\n    int kTileCols_,\n    class MMAFrag_ = BaseMMAFrag<T, 8, 8>>\nstruct MMATile {\n  using MMAFrag_t = MMAFrag_;\n  using elem_type = T;\n  static constant constexpr const int kFragRows = MMAFrag_t::kFragRows;\n  static constant constexpr const int kFragCols = MMAFrag_t::kFragCols;\n  static constant constexpr const int kElemsPerFrag = MMAFrag_t::kElemsPerFrag;\n  static constant constexpr const int kTileRows = kTileRows_;\n  static constant constexpr const int kTileCols = kTileCols_;\n  static constant constexpr const int kRows = kTileRows * kFragRows;\n  static constant constexpr const int kCols = kTileCols * kFragCols;\n  static constant constexpr const int kNumFrags = kTileRows * kTileCols;\n  static constant constexpr const int kElemsPerTile = kNumFrags * kElemsPerFrag;\n  typedef typename MMAFrag_t::mat_type mat_type;\n  typedef typename MMAFrag_t::frag_type frag_type;\n  frag_type val_frags[kNumFrags] = {frag_type(0)};\n  METAL_FUNC MMATile() thread {}\n  METAL_FUNC constexpr void clear() {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kNumFrags; ++i) {\n      val_frags[i] = frag_type(0);\n    }\n  }\n  METAL_FUNC constexpr thread frag_type& frag_at(const short i, const short j) {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC constexpr const thread frag_type& frag_at(\n      const short i,\n      const short j) const {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC mat_type mat_at(const short i, const short j) {\n    mat_type val_mat;\n#pragma clang loop unroll(full)\n    for (short ii = 0; ii < kElemsPerFrag; ++ii) {\n      val_mat.thread_elements()[ii] = frag_at(i, j)[ii];\n    }\n    return val_mat;\n  }\n  METAL_FUNC thread elem_type* elems() {\n    return reinterpret_cast<thread elem_type*>(val_frags);\n  }\n  METAL_FUNC const thread elem_type* elems() const {\n    return reinterpret_cast<const thread elem_type*>(val_frags);\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void load(const threadgroup U* src) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(\n                src[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void store(threadgroup U* dst) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(\n                dst[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void load(const device U* src, const int ld) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(src[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void store(device U* dst, const int ld) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(dst[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  load_safe(const device U* src, const int ld, const short2 src_tile_dims) {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load_safe(\n            frag_at(i, j),\n            src,\n            ld,\n            Int<1>{},\n            src_tile_dims.y,\n            src_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  store_safe(device U* dst, const int ld, const short2 dst_tile_dims) const {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store_safe(\n            frag_at(i, j),\n            dst,\n            ld,\n            Int<1>{},\n            dst_tile_dims.y,\n            dst_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n};\ntemplate <typename T, typename U, int M, int N, int K>\nMETAL_FUNC void tile_matmad(\n    thread MMATile<T, M, N>& D,\n    thread MMATile<U, M, K>& A,\n    thread MMATile<U, K, N>& B,\n    thread MMATile<T, M, N>& C) {\n#pragma clang loop unroll(full)\n  for (short m = 0; m < M; ++m) {\n#pragma clang loop unroll(full)\n    for (short n = 0; n < N; ++n) {\n      short n_serp = (m % 2) ? (N - 1 - n) : n;\n#pragma clang loop unroll(full)\n      for (short k = 0; k < K; ++k) {\n        MMATile<T, M, N>::MMAFrag_t::mma(\n            D.frag_at(m, n_serp),\n            A.frag_at(m, k),\n            B.frag_at(k, n_serp),\n            C.frag_at(m, n_serp));\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    short lda_tgp,\n    short ldb_tgp,\n    typename AccumType = float,\n    typename Epilogue = TransformNone<U, AccumType>>\nstruct BlockMMA {\n  static constant constexpr const short kFragSize = 8;\n  using MMAFrag_acc_t = BaseMMAFrag<AccumType, kFragSize, kFragSize>;\n  static constant constexpr const short TM_stride = kFragSize * WM;\n  static constant constexpr const short TN_stride = kFragSize * WN;\n  static constant constexpr const short TM = BM / (kFragSize * WM);\n  static constant constexpr const short TN = BN / (kFragSize * WN);\n  static constant constexpr const short A_str_m = transpose_a ? 1 : lda_tgp;\n  static constant constexpr const short A_str_k = transpose_a ? lda_tgp : 1;\n  static constant constexpr const short B_str_k = transpose_b ? 1 : ldb_tgp;\n  static constant constexpr const short B_str_n = transpose_b ? ldb_tgp : 1;\n  static constant constexpr const short tile_stride_a = kFragSize * A_str_k;\n  static constant constexpr const short tile_stride_b = kFragSize * B_str_k;\n  MMATile<AccumType, TM, 1, MMAFrag_acc_t> Atile;\n  MMATile<AccumType, 1, TN, MMAFrag_acc_t> Btile;\n  MMATile<AccumType, TM, TN, MMAFrag_acc_t> Ctile;\n  short sm;\n  short sn;\n  short As_offset;\n  short Bs_offset;\n  METAL_FUNC BlockMMA(\n      ushort simd_group_id [[simdgroup_index_in_threadgroup]],\n      ushort simd_lane_id [[thread_index_in_simdgroup]]) {\n    short tm = kFragSize * (simd_group_id / WN);\n    short tn = kFragSize * (simd_group_id % WN);\n    short2 simd_coord = MMAFrag_acc_t::get_coord(simd_lane_id);\n    sm = simd_coord.y;\n    sn = simd_coord.x;\n    As_offset = (tm + sm) * A_str_m + (sn)*A_str_k;\n    Bs_offset = (sm)*B_str_k + (tn + sn) * B_str_n;\n    sm += tm;\n    sn += tn;\n  }\n  METAL_FUNC void mma(const threadgroup T* As, const threadgroup T* Bs) {\n    As += As_offset;\n    Bs += Bs_offset;\n#pragma clang loop unroll(full)\n    for (short kk = 0; kk < BK; kk += kFragSize) {\n      simdgroup_barrier(mem_flags::mem_none);\n      Atile.template load<T, WM, 1, A_str_m, A_str_k>(As);\n      simdgroup_barrier(mem_flags::mem_none);\n      Btile.template load<T, 1, WN, B_str_k, B_str_n>(Bs);\n      simdgroup_barrier(mem_flags::mem_none);\n      tile_matmad(Ctile, Atile, Btile, Ctile);\n      As += tile_stride_a;\n      Bs += tile_stride_b;\n    }\n  }\n  METAL_FUNC void store_result(device U* D, const int ldd) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    Ctile.template store<U, WM, WN>(D, ldd);\n  }\n  METAL_FUNC void\n  store_result_safe(device U* D, const int ldd, short2 dst_tile_dims) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    Ctile.template store_safe<U, WM, WN>(D, ldd, dst_tile_dims);\n  }\n  template <typename UnaryEpilogue>\n  METAL_FUNC void apply_epilogue(thread const UnaryEpilogue& epilogue_op) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = epilogue_op.apply(Ctile.elems()[i]);\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n#pragma clang loop unroll(full)\n        for (short k = 0; k < decltype(Ctile)::kElemsPerFrag; k++) {\n          accum[k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue_safe(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n        U c_elems[kelems] = {0};\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          if ((j * TN_stride + k) < dst_tile_dims.x) {\n            c_elems[k] = C[offset_c + k * fdc];\n          }\n        }\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          accum[k] = epilogue_op.apply(accum[k], c_elems[k]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread const auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          D[offset_d + k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result_safe(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (int i = 0; i < TM; i++) {\n      if (i * TM_stride < dst_tile_dims.y) {\n#pragma clang loop unroll(full)\n        for (int j = 0; j < TN; j++) {\n          thread const auto& accum = Ctile.frag_at(i, j);\n          int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n          int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n          for (short k = 0; k < kelems; k++) {\n            if ((j * TN_stride + k) < dst_tile_dims.x) {\n              D[offset_d + k] =\n                  epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n            }\n          }\n        }\n      }\n    }\n  }\n};\n}\n}\n\nusing namespace metal;\nusing namespace mlx::steel;\n"
- "\n[[kernel]] void gather{0}_{3}_{6}_{7}(\n    const device {1}* src [[buffer(0)]],\n    device {1}* out [[buffer(1)]],\n    const constant int* src_shape [[buffer(2)]],\n    const constant size_t* src_strides [[buffer(3)]],\n    const constant size_t& src_ndim [[buffer(4)]],\n    const constant int* slice_sizes [[buffer(5)]],\n    const constant int* axes [[buffer(6)]],\n    const constant int* idx_shapes [[buffer(7)]],\n    const constant size_t* idx_strides [[buffer(8)]],\n    const constant bool* idx_contigs [[buffer(9)]],\n    const constant int& idx_ndim [[buffer(10)]],\n    {4}\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {{\n  Indices<{2}, {3}> idxs{{\n    {{ {5} }}, idx_shapes, idx_strides, idx_contigs, idx_ndim}};\n\n  return gather_impl<{1}, {2}, {3}, {6}, {7}>(\n      src,\n      out,\n      src_shape,\n      src_strides,\n      src_ndim,\n      slice_sizes,\n      axes,\n      idxs,\n      index,\n      grid_dim);\n}}\n"
- "\n[[kernel]] void scatter{0}_{4}_updc_{7}_nwork{8}_{9}(\n    const device {1}* updates [[buffer(1)]],\n    device mlx_atomic<{1}>* out [[buffer(2)]],\n    const constant int* upd_shape [[buffer(3)]],\n    const constant size_t* upd_strides [[buffer(4)]],\n    const constant size_t& upd_ndim [[buffer(5)]],\n    const constant size_t& upd_size [[buffer(6)]],\n    const constant int* out_shape [[buffer(7)]],\n    const constant size_t* out_strides [[buffer(8)]],\n    const constant size_t& out_ndim [[buffer(9)]],\n    const constant int* axes [[buffer(10)]],\n    const constant int* idx_shapes [[buffer(11)]],\n    const constant size_t* idx_strides [[buffer(12)]],\n    const constant bool* idx_contigs [[buffer(13)]],\n    const constant int& idx_ndim [[buffer(14)]],\n    const constant size_t& idx_size [[buffer(15)]],\n    {5}\n    uint2 gid [[thread_position_in_grid]]) {{\n  Indices<{2}, {4}> idxs{{ {{ {6} }}, idx_shapes, idx_strides, idx_contigs, idx_ndim}};\n\n  return scatter_impl<{1}, {2}, {3}, {4}, {7}, {8}, {9}>(\n      updates,\n      out,\n      upd_shape,\n      upd_strides,\n      upd_ndim,\n      upd_size,\n      out_shape,\n      out_strides,\n      out_ndim,\n      axes,\n      idx_size,\n      idxs,\n      gid);\n}}\n"
- "\nfloat erf(float a) {\n  float r, s, t, u;\n  t = metal::abs(a);\n  s = a * a;\n  if (t > 0.927734375f) {\n    r = metal::fma(\n        -1.72853470e-5f, t, 3.83197126e-4f);\n    u = metal::fma(\n        -3.88396438e-3f, t, 2.42546219e-2f);\n    r = metal::fma(r, s, u);\n    r = metal::fma(r, t, -1.06777877e-1f);\n    r = metal::fma(r, t, -6.34846687e-1f);\n    r = metal::fma(r, t, -1.28717512e-1f);\n    r = metal::fma(r, t, -t);\n    r = 1.0f - metal::exp(r);\n    r = metal::copysign(r, a);\n  } else {\n    r = -5.96761703e-4f;\n    r = metal::fma(r, s, 4.99119423e-3f);\n    r = metal::fma(r, s, -2.67681349e-2f);\n    r = metal::fma(r, s, 1.12819925e-1f);\n    r = metal::fma(r, s, -3.76125336e-1f);\n    r = metal::fma(r, s, 1.28379166e-1f);\n    r = metal::fma(r, a, a);\n  }\n  return r;\n}\nfloat erfinv(float a) {\n  auto t = metal::fma(a, 0.0f - a, 1.0f);\n  t = metal::log(t);\n  float p;\n  if (metal::abs(t) > 6.125f) {\n    p = 3.03697567e-10f;\n    p = metal::fma(p, t, 2.93243101e-8f);\n    p = metal::fma(p, t, 1.22150334e-6f);\n    p = metal::fma(p, t, 2.84108955e-5f);\n    p = metal::fma(p, t, 3.93552968e-4f);\n    p = metal::fma(p, t, 3.02698812e-3f);\n    p = metal::fma(p, t, 4.83185798e-3f);\n    p = metal::fma(p, t, -2.64646143e-1f);\n    p = metal::fma(p, t, 8.40016484e-1f);\n  } else {\n    p = 5.43877832e-9f;\n    p = metal::fma(p, t, 1.43285448e-7f);\n    p = metal::fma(p, t, 1.22774793e-6f);\n    p = metal::fma(p, t, 1.12963626e-7f);\n    p = metal::fma(p, t, -5.61530760e-5f);\n    p = metal::fma(p, t, -1.47697632e-4f);\n    p = metal::fma(p, t, 2.31468678e-3f);\n    p = metal::fma(p, t, 1.15392581e-2f);\n    p = metal::fma(p, t, -2.32015476e-1f);\n    p = metal::fma(p, t, 8.86226892e-1f);\n  }\n  return a * p;\n}\nfloat expm1f_scaled_unchecked(float a, float b) {\n  float f, j, r, s, t, u, v, x, y;\n  int i;\n  j = fma(1.442695f, a, 12582912.f);\n  j = j - 12582912.0f;\n  i = (int)j;\n  f = fma(j, -6.93145752e-1f, a);\n  s = f * f;\n  if (a == 0.0f)\n    s = a;\n  r = 1.97350979e-4f;\n  r = fma(r, f, 1.39309070e-3f);\n  r = fma(r, f, 8.33343994e-3f);\n  r = fma(r, f, 4.16668020e-2f);\n  r = fma(r, f, 1.66666716e-1f);\n  r = fma(r, f, 4.99999970e-1f);\n  u = (j == 1) ? (f + 0.5f) : f;\n  v = fma(r, s, u);\n  s = 0.5f * b;\n  t = ldexp(s, i);\n  y = t - s;\n  x = (t - y) - s;\n  r = fma(v, t, x) + y;\n  r = r + r;\n  if (j == 0)\n    r = v;\n  if (j == 1)\n    r = v + v;\n  return r;\n}\nfloat expm1f(float a) {\n  float r;\n  r = expm1f_scaled_unchecked(a, 1.0f);\n  if (abs(a - 1.0f) > 88.0f) {\n    r = pow(2, a);\n    r = fma(r, r, -1.0f);\n  }\n  return r;\n}\n\nnamespace {\nconstant float inf = metal::numeric_limits<float>::infinity();\n}\nstruct Abs {\n  template <typename T>\n  T operator()(T x) {\n    return metal::abs(x);\n  };\n  template <>\n  uint8_t operator()(uint8_t x) {\n    return x;\n  };\n  template <>\n  uint16_t operator()(uint16_t x) {\n    return x;\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x;\n  };\n  template <>\n  uint64_t operator()(uint64_t x) {\n    return x;\n  };\n  template <>\n  bool operator()(bool x) {\n    return x;\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {metal::precise::sqrt(x.real * x.real + x.imag * x.imag), 0};\n  };\n};\nstruct ArcCos {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::acos(x);\n  };\n};\nstruct ArcCosh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::acosh(x);\n  };\n};\nstruct ArcSin {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::asin(x);\n  };\n};\nstruct ArcSinh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::asinh(x);\n  };\n};\nstruct ArcTan {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::atan(x);\n  };\n};\nstruct ArcTanh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::atanh(x);\n  };\n};\nstruct Ceil {\n  template <typename T>\n  T operator()(T x) {\n    return metal::ceil(x);\n  };\n  template <>\n  int8_t operator()(int8_t x) {\n    return x;\n  };\n  template <>\n  int16_t operator()(int16_t x) {\n    return x;\n  };\n  template <>\n  int32_t operator()(int32_t x) {\n    return x;\n  };\n  template <>\n  int64_t operator()(int64_t x) {\n    return x;\n  };\n  template <>\n  uint8_t operator()(uint8_t x) {\n    return x;\n  };\n  template <>\n  uint16_t operator()(uint16_t x) {\n    return x;\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x;\n  };\n  template <>\n  uint64_t operator()(uint64_t x) {\n    return x;\n  };\n  template <>\n  bool operator()(bool x) {\n    return x;\n  };\n};\nstruct Cos {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::cos(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::cos(x.real) * metal::precise::cosh(x.imag),\n        -metal::precise::sin(x.real) * metal::precise::sinh(x.imag)};\n  };\n};\nstruct Cosh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::cosh(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::cosh(x.real) * metal::precise::cos(x.imag),\n        metal::precise::sinh(x.real) * metal::precise::sin(x.imag)};\n  };\n};\nstruct Conjugate {\n  complex64_t operator()(complex64_t x) {\n    return complex64_t{x.real, -x.imag};\n  }\n};\nstruct Erf {\n  template <typename T>\n  T operator()(T x) {\n    return static_cast<T>(erf(static_cast<float>(x)));\n  };\n};\nstruct ErfInv {\n  template <typename T>\n  T operator()(T x) {\n    return static_cast<T>(erfinv(static_cast<float>(x)));\n  };\n};\nstruct Exp {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::exp(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    auto m = metal::precise::exp(x.real);\n    return {m * metal::precise::cos(x.imag), m * metal::precise::sin(x.imag)};\n  }\n};\nstruct Expm1 {\n  template <typename T>\n  T operator()(T x) {\n    return static_cast<T>(expm1f(static_cast<float>(x)));\n  };\n};\nstruct Floor {\n  template <typename T>\n  T operator()(T x) {\n    return metal::floor(x);\n  };\n  template <>\n  int8_t operator()(int8_t x) {\n    return x;\n  };\n  template <>\n  int16_t operator()(int16_t x) {\n    return x;\n  };\n  template <>\n  int32_t operator()(int32_t x) {\n    return x;\n  };\n  template <>\n  int64_t operator()(int64_t x) {\n    return x;\n  };\n  template <>\n  uint8_t operator()(uint8_t x) {\n    return x;\n  };\n  template <>\n  uint16_t operator()(uint16_t x) {\n    return x;\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x;\n  };\n  template <>\n  uint64_t operator()(uint64_t x) {\n    return x;\n  };\n  template <>\n  bool operator()(bool x) {\n    return x;\n  };\n};\nstruct Imag {\n  template <typename T>\n  T operator()(T x) {\n    return x.imag;\n  };\n};\nstruct Log {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::log(x);\n  };\n};\nstruct Log2 {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::log2(x);\n  };\n};\nstruct Log10 {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::log10(x);\n  };\n};\nstruct Log1p {\n  template <typename T>\n  T operator()(T x) {\n    return log1p(x);\n  };\n};\nstruct LogicalNot {\n  template <typename T>\n  T operator()(T x) {\n    return !x;\n  };\n};\nstruct Negative {\n  template <typename T>\n  T operator()(T x) {\n    return -x;\n  };\n};\nstruct Real {\n  template <typename T>\n  T operator()(T x) {\n    return x.real;\n  };\n};\nstruct Round {\n  template <typename T>\n  T operator()(T x) {\n    return metal::rint(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {metal::rint(x.real), metal::rint(x.imag)};\n  };\n};\nstruct Sigmoid {\n  template <typename T>\n  T operator()(T x) {\n    auto y = 1 / (1 + metal::exp(-metal::abs(x)));\n    return (x < 0) ? 1 - y : y;\n  }\n};\nstruct Sign {\n  template <typename T>\n  T operator()(T x) {\n    return (x > T(0)) - (x < T(0));\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x != 0;\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    if (x == complex64_t(0)) {\n      return x;\n    }\n    return x /\n        (complex64_t)metal::precise::sqrt(x.real * x.real + x.imag * x.imag);\n  };\n};\nstruct Sin {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::sin(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::sin(x.real) * metal::precise::cosh(x.imag),\n        metal::precise::cos(x.real) * metal::precise::sinh(x.imag)};\n  };\n};\nstruct Sinh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::sinh(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::sinh(x.real) * metal::precise::cos(x.imag),\n        metal::precise::cosh(x.real) * metal::precise::sin(x.imag)};\n  };\n};\nstruct Square {\n  template <typename T>\n  T operator()(T x) {\n    return x * x;\n  };\n};\nstruct Sqrt {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::sqrt(x);\n  };\n};\nstruct Rsqrt {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::rsqrt(x);\n  };\n};\nstruct Tan {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::tan(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    float tan_a = metal::precise::tan(x.real);\n    float tanh_b = metal::precise::tanh(x.imag);\n    float t1 = tan_a * tanh_b;\n    float denom = 1. + t1 * t1;\n    return {(tan_a - tanh_b * t1) / denom, (tanh_b + tan_a * t1) / denom};\n  };\n};\nstruct Tanh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::tanh(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    float tanh_a = metal::precise::tanh(x.real);\n    float tan_b = metal::precise::tan(x.imag);\n    float t1 = tanh_a * tan_b;\n    float denom = 1. + t1 * t1;\n    return {(tanh_a + tan_b * t1) / denom, (tan_b - tanh_a * t1) / denom};\n  };\n};\n"
- "\nnamespace mlx {\nnamespace steel {\ntemplate <\n    typename T,\n    short BROWS,\n    short BCOLS,\n    short dst_ld,\n    short reduction_dim,\n    short tgp_size,\n    short alignment = 1,\n    short n_reads = (BCOLS * BROWS) / (tgp_size),\n    short TCOLS = BCOLS / n_reads,\n    short TROWS = tgp_size / TCOLS>\nstruct BlockLoader {\n  static constant constexpr const short n_rows = (BROWS + TROWS - 1) / TROWS;\n  static constant constexpr const short vec_size = n_reads;\n  const int src_ld;\n  const int tile_stride;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  struct alignas(alignment * sizeof(T)) ReadVector {\n    uint8_t v[sizeof(T) * vec_size];\n  };\n  METAL_FUNC BlockLoader(\n      const device T* src_,\n      const int src_ld_,\n      threadgroup T* dst_,\n      ushort simd_group_id [[simdgroup_index_in_threadgroup]],\n      ushort simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(src_ld_),\n        tile_stride(reduction_dim ? BCOLS : BROWS * src_ld),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld + bj) {}\n  template <typename UnaryOp>\n  METAL_FUNC void apply_inplace_op(thread const UnaryOp& op) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        dst[i * dst_ld + j] = op.apply(dst[i * dst_ld + j]);\n      }\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < BROWS; i += TROWS) {\n      *((threadgroup ReadVector*)(&dst[i * dst_ld])) =\n          *((const device ReadVector*)(&src[i * src_ld]));\n    }\n  }\n  METAL_FUNC void load_safe(short2 src_tile_dim) const {\n    src_tile_dim = src_tile_dim - short2(bj, bi);\n    if (src_tile_dim.x <= 0 || src_tile_dim.y <= 0) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n      return;\n    }\n    bool tmp_idx[vec_size];\n    T tmp_val[vec_size];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        tmp_idx[j] = (i < src_tile_dim.y) && (j < src_tile_dim.x);\n      }\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        tmp_val[j] = src[(tmp_idx[j] ? i * src_ld + j : 0)];\n      }\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        tmp_val[j] = tmp_idx[j] ? tmp_val[j] : T(0);\n      }\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        dst[i * dst_ld + j] = tmp_val[j];\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    src += tile_stride;\n  }\n};\n}\n}\nMETAL_FUNC ulong2 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n  }\n  return ulong2(loc_a, loc_b);\n}\nMETAL_FUNC ulong3 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const size_t* c_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  ulong loc_c{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n    loc_c += pos_in_dim * c_strides[i];\n  }\n  return ulong3(loc_a, loc_b, loc_c);\n}\nnamespace mlx {\nnamespace steel {\ntemplate <typename OutT, typename InT>\nstruct TransformNone {\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT) {\n    return static_cast<OutT>(x);\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAdd {\n  TransformAdd(const float, const float) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT c) {\n    return static_cast<OutT>(x) + c;\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAxpby {\n  const float alpha;\n  const float beta;\n  TransformAxpby(const float alpha_, const float beta_)\n      : alpha(alpha_), beta(beta_) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  METAL_FUNC OutT apply(InT x, OutT c) const {\n    return static_cast<OutT>(x * alpha + (beta * c));\n  }\n};\ntemplate <typename T>\nstruct AccumHelper {\n  typedef float accum_type;\n};\nstruct BlockSwizzle {\n  static METAL_FUNC int2\n  swizzle(uint3 tid [[threadgroup_position_in_grid]], const int swizzle_log) {\n    const int tid_x = (tid.x) >> swizzle_log;\n    const int tid_y =\n        ((tid.y) << swizzle_log) + ((tid.x) & ((1 << swizzle_log) - 1));\n    return int2(tid_x, tid_y);\n  }\n};\n}\n}\n#pragma METAL internals : enable\nnamespace metal {\ntemplate <typename T>\nstruct is_empty : metal::bool_constant<__is_empty(T)> {};\ntemplate <typename... Ts>\nstruct make_void {\n  typedef void type;\n};\ntemplate <typename... Ts>\nusing void_t = typename make_void<Ts...>::type;\ntemplate <class T>\nstruct is_static : metal::bool_constant<is_empty<remove_cv_t<T>>::value> {};\ntemplate <typename T>\nstruct pointer_element {};\ntemplate <typename T>\nstruct pointer_element<thread T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<device T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<constant T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<threadgroup T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nusing pointer_element_t = typename pointer_element<remove_cv_t<T>>::type;\n}\n#pragma METAL internals : disable\n\n#pragma METAL internals : enable\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, T v>\nstruct integral_constant {\n  static constexpr constant T value = v;\n  using value_type = T;\n  using type = integral_constant;\n  METAL_FUNC constexpr operator value_type() const noexcept {\n    return value;\n  }\n};\ntemplate <bool B>\nusing bool_constant = integral_constant<bool, B>;\nusing true_type = bool_constant<true>;\nusing false_type = bool_constant<false>;\ntemplate <class T>\nstruct is_integral : bool_constant<metal::is_integral<T>::value> {};\ntemplate <class T, T v>\nstruct is_integral<integral_constant<T, v>>\n    : bool_constant<metal::is_integral<T>::value> {};\ntemplate <typename T>\nconstexpr constant bool is_integral_v = is_integral<T>::value;\ntemplate <int val>\nusing Int = integral_constant<int, val>;\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator+( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv + uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator-( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv - uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator*( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv * uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator/( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv / uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator==( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv == uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator!=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv != uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv < uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv > uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv <= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv >= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator&&( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv && uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator||( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv || uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T>\nMETAL_FUNC constexpr T sum(T x) {\n  return x;\n}\ntemplate <typename T, typename... Us>\nMETAL_FUNC constexpr auto sum(T x, Us... us) {\n  return x + sum(us...);\n}\n}\n}\n#pragma METAL internals : disable\n\nusing namespace metal;\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, int kFragRows_, int kFragCols_>\nstruct BaseMMAFrag {\n  static_assert(\n      kFragRows_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n  static_assert(\n      kFragCols_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n};\ntemplate <typename T>\nstruct BaseMMAFrag<T, 8, 8> {\n  static constant constexpr const int kFragRows = 8;\n  static constant constexpr const int kFragCols = 8;\n  static constant constexpr const int kElemsPerFrag = (kFragRows * kFragCols) / 32;\n  static constant constexpr const int kElemRows = 1;\n  static constant constexpr const int kElemCols = 2;\n  static_assert(\n      kElemRows * kElemCols == kElemsPerFrag,\n      \"MMAFrag shape is not consistent with MMAFrag size\");\n  typedef metal::simdgroup_matrix<T, kFragRows, kFragCols> mat_type;\n  typedef metal::vec<T, kElemsPerFrag> frag_type;\n  METAL_FUNC static constexpr short2 get_coord(ushort simd_lane_id\n                                               [[thread_index_in_simdgroup]]) {\n    const short qid = simd_lane_id / 4;\n    const short fm = (qid & 4) + ((simd_lane_id / 2) % 4);\n    const short fn = (qid & 2) * 2 + (simd_lane_id % 2) * 2;\n    return short2{fn, fm};\n  }\n  template <typename SrcPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  load(thread frag_type& dst, SrcPtrType src, StrX str_x, StrY str_y) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * kElemCols + j] = static_cast<T>(src[i * str_x + j * str_y]);\n      }\n    }\n  }\n  template <\n      typename SrcPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void load_safe(\n      thread frag_type& dst,\n      SrcPtrType src,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[i * kElemCols + j] =\n              static_cast<T>(src[(off_x + i) * str_x + (off_x + j) * str_y]);\n        } else {\n          dst[i * kElemCols + j] = T(0);\n        }\n      }\n    }\n  }\n  template <typename DstPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  store(const thread frag_type& src, DstPtrType dst, StrX str_x, StrY str_y) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * str_x + j * str_y] = static_cast<U>(src[i * kElemCols + j]);\n      }\n    }\n  }\n  template <\n      typename DstPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void store_safe(\n      const thread frag_type& src,\n      DstPtrType dst,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[(off_x + i) * str_x + (off_y + j) * str_y] =\n              static_cast<U>(src[i * kElemCols + j]);\n        }\n      }\n    }\n  }\n  METAL_FUNC static constexpr void mma(\n      thread frag_type& D,\n      thread frag_type& A,\n      thread frag_type& B,\n      thread frag_type& C) {\n    mat_type D_mat;\n    mat_type A_mat;\n    mat_type B_mat;\n    mat_type C_mat;\n    reinterpret_cast<thread frag_type&>(A_mat.thread_elements()) = A;\n    reinterpret_cast<thread frag_type&>(B_mat.thread_elements()) = B;\n    reinterpret_cast<thread frag_type&>(C_mat.thread_elements()) = C;\n    mma(D_mat, A_mat, B_mat, C_mat);\n    D = reinterpret_cast<thread frag_type&>(D_mat.thread_elements());\n  }\n  METAL_FUNC static constexpr void mma(\n      thread mat_type& D,\n      thread mat_type& A,\n      thread mat_type& B,\n      thread mat_type& C) {\n    simdgroup_multiply_accumulate(D, A, B, C);\n  }\n};\ntemplate <\n    typename T,\n    int kTileRows_,\n    int kTileCols_,\n    class MMAFrag_ = BaseMMAFrag<T, 8, 8>>\nstruct MMATile {\n  using MMAFrag_t = MMAFrag_;\n  using elem_type = T;\n  static constant constexpr const int kFragRows = MMAFrag_t::kFragRows;\n  static constant constexpr const int kFragCols = MMAFrag_t::kFragCols;\n  static constant constexpr const int kElemsPerFrag = MMAFrag_t::kElemsPerFrag;\n  static constant constexpr const int kTileRows = kTileRows_;\n  static constant constexpr const int kTileCols = kTileCols_;\n  static constant constexpr const int kRows = kTileRows * kFragRows;\n  static constant constexpr const int kCols = kTileCols * kFragCols;\n  static constant constexpr const int kNumFrags = kTileRows * kTileCols;\n  static constant constexpr const int kElemsPerTile = kNumFrags * kElemsPerFrag;\n  typedef typename MMAFrag_t::mat_type mat_type;\n  typedef typename MMAFrag_t::frag_type frag_type;\n  frag_type val_frags[kNumFrags] = {frag_type(0)};\n  METAL_FUNC MMATile() thread {}\n  METAL_FUNC constexpr void clear() {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kNumFrags; ++i) {\n      val_frags[i] = frag_type(0);\n    }\n  }\n  METAL_FUNC constexpr thread frag_type& frag_at(const short i, const short j) {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC constexpr const thread frag_type& frag_at(\n      const short i,\n      const short j) const {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC mat_type mat_at(const short i, const short j) {\n    mat_type val_mat;\n#pragma clang loop unroll(full)\n    for (short ii = 0; ii < kElemsPerFrag; ++ii) {\n      val_mat.thread_elements()[ii] = frag_at(i, j)[ii];\n    }\n    return val_mat;\n  }\n  METAL_FUNC thread elem_type* elems() {\n    return reinterpret_cast<thread elem_type*>(val_frags);\n  }\n  METAL_FUNC const thread elem_type* elems() const {\n    return reinterpret_cast<const thread elem_type*>(val_frags);\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void load(const threadgroup U* src) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(\n                src[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void store(threadgroup U* dst) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(\n                dst[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void load(const device U* src, const int ld) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(src[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void store(device U* dst, const int ld) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(dst[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  load_safe(const device U* src, const int ld, const short2 src_tile_dims) {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load_safe(\n            frag_at(i, j),\n            src,\n            ld,\n            Int<1>{},\n            src_tile_dims.y,\n            src_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  store_safe(device U* dst, const int ld, const short2 dst_tile_dims) const {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store_safe(\n            frag_at(i, j),\n            dst,\n            ld,\n            Int<1>{},\n            dst_tile_dims.y,\n            dst_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n};\ntemplate <typename T, typename U, int M, int N, int K>\nMETAL_FUNC void tile_matmad(\n    thread MMATile<T, M, N>& D,\n    thread MMATile<U, M, K>& A,\n    thread MMATile<U, K, N>& B,\n    thread MMATile<T, M, N>& C) {\n#pragma clang loop unroll(full)\n  for (short m = 0; m < M; ++m) {\n#pragma clang loop unroll(full)\n    for (short n = 0; n < N; ++n) {\n      short n_serp = (m % 2) ? (N - 1 - n) : n;\n#pragma clang loop unroll(full)\n      for (short k = 0; k < K; ++k) {\n        MMATile<T, M, N>::MMAFrag_t::mma(\n            D.frag_at(m, n_serp),\n            A.frag_at(m, k),\n            B.frag_at(k, n_serp),\n            C.frag_at(m, n_serp));\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    short lda_tgp,\n    short ldb_tgp,\n    typename AccumType = float,\n    typename Epilogue = TransformNone<U, AccumType>>\nstruct BlockMMA {\n  static constant constexpr const short kFragSize = 8;\n  using MMAFrag_acc_t = BaseMMAFrag<AccumType, kFragSize, kFragSize>;\n  static constant constexpr const short TM_stride = kFragSize * WM;\n  static constant constexpr const short TN_stride = kFragSize * WN;\n  static constant constexpr const short TM = BM / (kFragSize * WM);\n  static constant constexpr const short TN = BN / (kFragSize * WN);\n  static constant constexpr const short A_str_m = transpose_a ? 1 : lda_tgp;\n  static constant constexpr const short A_str_k = transpose_a ? lda_tgp : 1;\n  static constant constexpr const short B_str_k = transpose_b ? 1 : ldb_tgp;\n  static constant constexpr const short B_str_n = transpose_b ? ldb_tgp : 1;\n  static constant constexpr const short tile_stride_a = kFragSize * A_str_k;\n  static constant constexpr const short tile_stride_b = kFragSize * B_str_k;\n  MMATile<AccumType, TM, 1, MMAFrag_acc_t> Atile;\n  MMATile<AccumType, 1, TN, MMAFrag_acc_t> Btile;\n  MMATile<AccumType, TM, TN, MMAFrag_acc_t> Ctile;\n  short sm;\n  short sn;\n  short As_offset;\n  short Bs_offset;\n  METAL_FUNC BlockMMA(\n      ushort simd_group_id [[simdgroup_index_in_threadgroup]],\n      ushort simd_lane_id [[thread_index_in_simdgroup]]) {\n    short tm = kFragSize * (simd_group_id / WN);\n    short tn = kFragSize * (simd_group_id % WN);\n    short2 simd_coord = MMAFrag_acc_t::get_coord(simd_lane_id);\n    sm = simd_coord.y;\n    sn = simd_coord.x;\n    As_offset = (tm + sm) * A_str_m + (sn)*A_str_k;\n    Bs_offset = (sm)*B_str_k + (tn + sn) * B_str_n;\n    sm += tm;\n    sn += tn;\n  }\n  METAL_FUNC void mma(const threadgroup T* As, const threadgroup T* Bs) {\n    As += As_offset;\n    Bs += Bs_offset;\n#pragma clang loop unroll(full)\n    for (short kk = 0; kk < BK; kk += kFragSize) {\n      simdgroup_barrier(mem_flags::mem_none);\n      Atile.template load<T, WM, 1, A_str_m, A_str_k>(As);\n      simdgroup_barrier(mem_flags::mem_none);\n      Btile.template load<T, 1, WN, B_str_k, B_str_n>(Bs);\n      simdgroup_barrier(mem_flags::mem_none);\n      tile_matmad(Ctile, Atile, Btile, Ctile);\n      As += tile_stride_a;\n      Bs += tile_stride_b;\n    }\n  }\n  METAL_FUNC void store_result(device U* D, const int ldd) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    Ctile.template store<U, WM, WN>(D, ldd);\n  }\n  METAL_FUNC void\n  store_result_safe(device U* D, const int ldd, short2 dst_tile_dims) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    Ctile.template store_safe<U, WM, WN>(D, ldd, dst_tile_dims);\n  }\n  template <typename UnaryEpilogue>\n  METAL_FUNC void apply_epilogue(thread const UnaryEpilogue& epilogue_op) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = epilogue_op.apply(Ctile.elems()[i]);\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n#pragma clang loop unroll(full)\n        for (short k = 0; k < decltype(Ctile)::kElemsPerFrag; k++) {\n          accum[k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue_safe(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n        U c_elems[kelems] = {0};\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          if ((j * TN_stride + k) < dst_tile_dims.x) {\n            c_elems[k] = C[offset_c + k * fdc];\n          }\n        }\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          accum[k] = epilogue_op.apply(accum[k], c_elems[k]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread const auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          D[offset_d + k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result_safe(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (int i = 0; i < TM; i++) {\n      if (i * TM_stride < dst_tile_dims.y) {\n#pragma clang loop unroll(full)\n        for (int j = 0; j < TN; j++) {\n          thread const auto& accum = Ctile.frag_at(i, j);\n          int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n          int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n          for (short k = 0; k < kelems; k++) {\n            if ((j * TN_stride + k) < dst_tile_dims.x) {\n              D[offset_d + k] =\n                  epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n            }\n          }\n        }\n      }\n    }\n  }\n};\n}\n}\nnamespace mlx {\nnamespace steel {\nstruct GEMMParams {\n  const int M;\n  const int N;\n  const int K;\n  const int lda;\n  const int ldb;\n  const int ldd;\n  const int tiles_n;\n  const int tiles_m;\n  const size_t batch_stride_a;\n  const size_t batch_stride_b;\n  const size_t batch_stride_d;\n  const int swizzle_log;\n  const int gemm_k_iterations_aligned;\n  const int batch_ndim;\n};\nstruct GEMMSpiltKParams {\n  const int M;\n  const int N;\n  const int K;\n  const int lda;\n  const int ldb;\n  const int ldc;\n  const int tiles_n;\n  const int tiles_m;\n  const int split_k_partitions;\n  const int split_k_partition_stride;\n  const int split_k_partition_size;\n  const int gemm_k_iterations_aligned;\n};\nstruct GEMMAddMMParams {\n  const int ldc;\n  const int fdc;\n  const size_t batch_stride_c;\n  const float alpha;\n  const float beta;\n};\n}\n}\nusing namespace metal;\nnamespace mlx {\nnamespace steel {\ntemplate <bool M_aligned, bool N_aligned, bool K_aligned>\nstruct LoopAlignment {};\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    bool MN_aligned,\n    bool K_aligned,\n    typename AccumType = typename AccumHelper<T>::accum_type,\n    typename Epilogue = TransformNone<U, AccumType>>\nstruct GEMMKernel {\n  static constant constexpr const short tgp_padding_a = 16 / sizeof(T);\n  static constant constexpr const short tgp_padding_b = 16 / sizeof(T);\n  static constant constexpr const short tgp_mem_size_a =\n      transpose_a ? BK * (BM + tgp_padding_a) : BM * (BK + tgp_padding_a);\n  static constant constexpr const short tgp_mem_size_b =\n      transpose_b ? BN * (BK + tgp_padding_b) : BK * (BN + tgp_padding_b);\n  static constant constexpr const short tgp_mem_size = tgp_mem_size_a + tgp_mem_size_b;\n  static constant constexpr const short tgp_size = WM * WN * 32;\n  using loader_a_t = BlockLoader<\n      T,\n      transpose_a ? BK : BM,\n      transpose_a ? BM : BK,\n      transpose_a ? BM + tgp_padding_a : BK + tgp_padding_a,\n      !transpose_a,\n      tgp_size>;\n  using loader_b_t = BlockLoader<\n      T,\n      transpose_b ? BN : BK,\n      transpose_b ? BK : BN,\n      transpose_b ? BK + tgp_padding_b : BN + tgp_padding_b,\n      transpose_b,\n      tgp_size>;\n  using mma_t = BlockMMA<\n      T,\n      U,\n      BM,\n      BN,\n      BK,\n      WM,\n      WN,\n      transpose_a,\n      transpose_b,\n      transpose_a ? BM + tgp_padding_a : BK + tgp_padding_a,\n      transpose_b ? BK + tgp_padding_b : BN + tgp_padding_b,\n      AccumType,\n      Epilogue>;\n  template <bool M_aligned, bool N_aligned, bool K_aligned_>\n  static METAL_FUNC void gemm_loop(\n      threadgroup T* As [[threadgroup(0)]],\n      threadgroup T* Bs [[threadgroup(1)]],\n      const int gemm_k_iterations,\n      thread loader_a_t& loader_a,\n      thread loader_b_t& loader_b,\n      thread mma_t& mma_op,\n      thread const short& tgp_bm,\n      thread const short& tgp_bn,\n      thread const short& lbk,\n      LoopAlignment<M_aligned, N_aligned, K_aligned_> l = {}) {\n    (void)l;\n    short2 tile_dims_A = transpose_a ? short2(tgp_bm, BK) : short2(BK, tgp_bm);\n    short2 tile_dims_B = transpose_b ? short2(BK, tgp_bn) : short2(tgp_bn, BK);\n    for (int k = 0; k < gemm_k_iterations; k++) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      if (M_aligned) {\n        loader_a.load_unsafe();\n      } else {\n        loader_a.load_safe(tile_dims_A);\n      }\n      if (N_aligned) {\n        loader_b.load_unsafe();\n      } else {\n        loader_b.load_safe(tile_dims_B);\n      }\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      mma_op.mma(As, Bs);\n      loader_a.next();\n      loader_b.next();\n    }\n    if (!K_aligned_) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      short2 tile_dims_A_last =\n          transpose_a ? short2(tgp_bm, lbk) : short2(lbk, tgp_bm);\n      short2 tile_dims_B_last =\n          transpose_b ? short2(lbk, tgp_bn) : short2(tgp_bn, lbk);\n      loader_a.load_safe(tile_dims_A_last);\n      loader_b.load_safe(tile_dims_B_last);\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      mma_op.mma(As, Bs);\n    }\n  }\n  static METAL_FUNC void run(\n      const device T* A [[buffer(0)]],\n      const device T* B [[buffer(1)]],\n      device U* D [[buffer(2)]],\n      const constant GEMMParams* params [[buffer(3)]],\n      threadgroup T* As [[threadgroup(0)]],\n      threadgroup T* Bs [[threadgroup(1)]],\n      uint simd_lane_id [[thread_index_in_simdgroup]],\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint3 tid [[threadgroup_position_in_grid]],\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    (void)lid;\n    const int tid_y = ((tid.y) << params->swizzle_log) +\n        ((tid.x) & ((1 << params->swizzle_log) - 1));\n    const int tid_x = (tid.x) >> params->swizzle_log;\n    if (params->tiles_n <= tid_x || params->tiles_m <= tid_y) {\n      return;\n    }\n    threadgroup_barrier(mem_flags::mem_none);\n    const int c_row = tid_y * BM;\n    const int c_col = tid_x * BN;\n    const size_t c_row_long = size_t(c_row);\n    const size_t c_col_long = size_t(c_col);\n    A += transpose_a ? c_row_long : c_row_long * params->lda;\n    B += transpose_b ? c_col_long * params->ldb : c_col_long;\n    D += c_row_long * params->ldd + c_col_long;\n    thread loader_a_t loader_a(A, params->lda, As, simd_group_id, simd_lane_id);\n    thread loader_b_t loader_b(B, params->ldb, Bs, simd_group_id, simd_lane_id);\n    thread mma_t mma_op(simd_group_id, simd_lane_id);\n    int gemm_k_iterations = params->gemm_k_iterations_aligned;\n    if (MN_aligned) {\n      for (int k = 0; k < gemm_k_iterations; k++) {\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        loader_a.load_unsafe();\n        loader_b.load_unsafe();\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        mma_op.mma(As, Bs);\n        loader_a.next();\n        loader_b.next();\n      }\n      threadgroup_barrier(mem_flags::mem_none);\n      if (!K_aligned) {\n        int lbk = params->K - params->gemm_k_iterations_aligned * BK;\n        short2 tile_dims_A = transpose_a ? short2(BM, lbk) : short2(lbk, BM);\n        short2 tile_dims_B = transpose_b ? short2(lbk, BN) : short2(BN, lbk);\n        loader_a.load_safe(tile_dims_A);\n        loader_b.load_safe(tile_dims_B);\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        mma_op.mma(As, Bs);\n      }\n      mma_op.store_result(D, params->ldd);\n      return;\n    }\n    else {\n      short tgp_bm = min(BM, params->M - c_row);\n      short tgp_bn = min(BN, params->N - c_col);\n      short leftover_bk = params->K - params->gemm_k_iterations_aligned * BK;\n      if (tgp_bm == BM && tgp_bn == BN) {\n        gemm_loop<true, true, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result(D, params->ldd);\n        return;\n      } else if (tgp_bn == BN) {\n        gemm_loop<false, true, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n        return;\n      } else if (tgp_bm == BM) {\n        gemm_loop<true, false, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n        return;\n      } else {\n        gemm_loop<false, false, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n        return;\n      }\n    }\n  }\n};\n}\n}\n"
- "\ntemplate <typename IdxT, int NIDX>\nstruct Indices {\n  const array<const device IdxT*, NIDX> buffers;\n  const constant int* shapes;\n  const constant size_t* strides;\n  const constant bool* row_contiguous;\n  const int ndim;\n};\ntemplate <typename IdxT>\nMETAL_FUNC size_t offset_neg_idx(IdxT idx, int size) {\n  if (is_unsigned_v<IdxT>) {\n    return idx;\n  } else {\n    return (idx < 0) ? idx + size : idx;\n  }\n}\n\ntemplate <\n    typename T,\n    typename IdxT,\n    typename Op,\n    int NIDX,\n    bool UPD_ROW_CONTIG,\n    int NWORK,\n    typename LocT>\nMETAL_FUNC void scatter_impl(\n    const device T* updates,\n    device mlx_atomic<T>* out,\n    const constant int* upd_shape,\n    const constant size_t* upd_strides,\n    const constant size_t& upd_ndim,\n    const constant size_t& upd_size,\n    const constant int* out_shape,\n    const constant size_t* out_strides,\n    const constant size_t& out_ndim,\n    const constant int* axes,\n    const constant size_t& idx_size,\n    const thread Indices<IdxT, NIDX>& indices,\n    uint2 gid [[thread_position_in_grid]]) {\n  Op op;\n  auto ind_idx = gid.y * NWORK;\n  LocT out_offset = 0;\n  if (upd_size > 1) {\n    out_offset = elem_to_loc<size_t, LocT>(\n        gid.x, upd_shape + indices.ndim, out_strides, out_ndim);\n  }\n  for (int j = 0; j < NWORK && ind_idx < idx_size; ++j, ind_idx++) {\n    LocT out_idx = out_offset;\n    for (int i = 0; i < NIDX; ++i) {\n      auto idx_loc = indices.row_contiguous[i]\n          ? ind_idx\n          : elem_to_loc<size_t, LocT>(\n                ind_idx,\n                &indices.shapes[indices.ndim * i],\n                &indices.strides[indices.ndim * i],\n                indices.ndim);\n      auto ax = axes[i];\n      auto idx_val = offset_neg_idx(indices.buffers[i][idx_loc], out_shape[ax]);\n      out_idx +=\n          static_cast<LocT>(idx_val) * static_cast<LocT>(out_strides[ax]);\n    }\n    auto upd_idx = ind_idx * static_cast<LocT>(upd_size) + gid.x;\n    if constexpr (!UPD_ROW_CONTIG) {\n      upd_idx =\n          elem_to_loc<size_t, LocT>(upd_idx, upd_shape, upd_strides, upd_ndim);\n    }\n    op.atomic_update(out, updates[upd_idx], out_idx);\n  }\n}\n"
- "\ntemplate <typename IdxT, int NIDX>\nstruct Indices {\n  const array<const device IdxT*, NIDX> buffers;\n  const constant int* shapes;\n  const constant size_t* strides;\n  const constant bool* row_contiguous;\n  const int ndim;\n};\ntemplate <typename IdxT>\nMETAL_FUNC size_t offset_neg_idx(IdxT idx, int size) {\n  if (is_unsigned_v<IdxT>) {\n    return idx;\n  } else {\n    return (idx < 0) ? idx + size : idx;\n  }\n}\n\ntemplate <typename T, typename IdxT, int NIDX, int IDX_NDIM, typename LocT>\nMETAL_FUNC void gather_impl(\n    const device T* src [[buffer(0)]],\n    device T* out [[buffer(1)]],\n    const constant int* src_shape [[buffer(2)]],\n    const constant size_t* src_strides [[buffer(3)]],\n    const constant size_t& src_ndim [[buffer(4)]],\n    const constant int* slice_sizes [[buffer(5)]],\n    const constant int* axes [[buffer(6)]],\n    const thread Indices<IdxT, NIDX>& indices,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  LocT src_idx = 0;\n  for (int i = 0; i < NIDX; ++i) {\n    LocT idx_loc;\n    if (IDX_NDIM == 0) {\n      idx_loc = 0;\n    } else if (IDX_NDIM == 1) {\n      idx_loc = index.x * static_cast<LocT>(indices.strides[indices.ndim * i]);\n    } else {\n      idx_loc = index.x * static_cast<LocT>(indices.strides[indices.ndim * i]);\n      idx_loc += indices.row_contiguous[i]\n          ? index.y\n          : elem_to_loc<size_t, LocT>(\n                index.y,\n                &indices.shapes[indices.ndim * i + 1],\n                &indices.strides[indices.ndim * i + 1],\n                indices.ndim - 1);\n    }\n    auto ax = axes[i];\n    auto idx_val = offset_neg_idx(indices.buffers[i][idx_loc], src_shape[ax]);\n    src_idx += static_cast<LocT>(idx_val) * static_cast<LocT>(src_strides[ax]);\n  }\n  auto src_offset =\n      elem_to_loc<size_t, LocT>(index.z, slice_sizes, src_strides, src_ndim);\n  LocT out_idx = index.z;\n  if (IDX_NDIM == 1) {\n    out_idx += static_cast<LocT>(grid_dim.z) * index.x;\n  } else if (IDX_NDIM >= 2) {\n    out_idx += grid_dim.z * (index.x * static_cast<LocT>(grid_dim.y) + index.y);\n  }\n  out[out_idx] = src[src_offset + src_idx];\n}\n"
- "\ntemplate <typename T, typename Op>\n[[kernel]] void ternary_v(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    uint index [[thread_position_in_grid]]) {\n  d[index] = Op()(a[index], b[index], c[index]);\n}\ntemplate <typename T, typename Op>\n[[kernel]] void ternary_v2(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  d[offset] = Op()(a[offset], b[offset], c[offset]);\n}\ntemplate <typename T, typename Op>\n[[kernel]] void ternary_g_nd1(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const size_t& a_strides,\n    constant const size_t& b_strides,\n    constant const size_t& c_strides,\n    uint index [[thread_position_in_grid]]) {\n  auto a_idx = elem_to_loc_1<size_t, uint>(index, a_strides);\n  auto b_idx = elem_to_loc_1<size_t, uint>(index, b_strides);\n  auto c_idx = elem_to_loc_1<size_t, uint>(index, c_strides);\n  d[index] = Op()(a[a_idx], b[b_idx], c[c_idx]);\n}\ntemplate <typename T, typename Op, typename IdxT = size_t>\n[[kernel]] void ternary_g_nd2(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const size_t a_strides[2],\n    constant const size_t b_strides[2],\n    constant const size_t c_strides[2],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_2<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_2<size_t, IdxT>(index, b_strides);\n  auto c_idx = elem_to_loc_2<size_t, IdxT>(index, c_strides);\n  IdxT out_idx = index.x + IdxT(grid_dim.x) * index.y;\n  d[out_idx] = Op()(a[a_idx], b[b_idx], c[c_idx]);\n}\ntemplate <typename T, typename Op, typename IdxT = size_t>\n[[kernel]] void ternary_g_nd3(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const size_t a_strides[3],\n    constant const size_t b_strides[3],\n    constant const size_t c_strides[3],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_3<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_3<size_t, IdxT>(index, b_strides);\n  auto c_idx = elem_to_loc_3<size_t, IdxT>(index, c_strides);\n  IdxT out_idx = index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n  d[out_idx] = Op()(a[a_idx], b[b_idx], c[c_idx]);\n}\ntemplate <typename T, typename Op, int N = 1, typename IdxT = size_t>\n[[kernel]] void ternary_g(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const size_t* c_strides,\n    constant const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc_3_nd<IdxT>(\n      {N * index.x, index.y, index.z},\n      shape,\n      a_strides,\n      b_strides,\n      c_strides,\n      ndim);\n  auto xshape = shape[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  IdxT a_xstride = a_strides[ndim - 1];\n  IdxT b_xstride = b_strides[ndim - 1];\n  IdxT c_xstride = c_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    d[out_idx++] = Op()(a[idx.x], b[idx.y], c[idx.z]);\n    idx.x += a_xstride;\n    idx.y += b_xstride;\n    idx.z += c_xstride;\n  }\n}\n"
- "\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_ss(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[0], b[0]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[0], b[index]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[index], b[0]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[index], b[index]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  auto out = Op()(a[0], b[offset]);\n  c[offset] = out[0];\n  d[offset] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  auto out = Op()(a[offset], b[0]);\n  c[offset] = out[0];\n  d[offset] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  auto out = Op()(a[offset], b[offset]);\n  c[offset] = out[0];\n  d[offset] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_g_nd1(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const size_t& a_stride,\n    constant const size_t& b_stride,\n    uint index [[thread_position_in_grid]]) {\n  auto a_idx = elem_to_loc_1<size_t, uint>(index, a_stride);\n  auto b_idx = elem_to_loc_1<size_t, uint>(index, b_stride);\n  auto out = Op()(a[a_idx], b[b_idx]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = size_t>\n[[kernel]] void binary_g_nd2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const size_t a_strides[2],\n    constant const size_t b_strides[2],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_2<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_2<size_t, IdxT>(index, b_strides);\n  IdxT out_idx = index.x + IdxT(grid_dim.x) * index.y;\n  auto out = Op()(a[a_idx], b[b_idx]);\n  c[out_idx] = out[0];\n  d[out_idx] = out[1];\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = size_t>\n[[kernel]] void binary_g_nd3(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const size_t a_strides[3],\n    constant const size_t b_strides[3],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_3<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_3<size_t, IdxT>(index, b_strides);\n  IdxT out_idx = index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n  auto out = Op()(a[a_idx], b[b_idx]);\n  c[out_idx] = out[0];\n  d[out_idx] = out[1];\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N = 1,\n    typename IdxT = size_t>\n[[kernel]] void binary_g(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc_2_nd<size_t, IdxT>(\n      {N * index.x, index.y, index.z}, shape, a_strides, b_strides, ndim);\n  auto xshape = shape[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  IdxT a_xstride = a_strides[ndim - 1];\n  IdxT b_xstride = b_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    auto out = Op()(a[idx.x], b[idx.y]);\n    c[out_idx] = out[0];\n    d[out_idx++] = out[1];\n    idx.x += a_xstride;\n    idx.y += b_xstride;\n  }\n}\n"
- "\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_ss(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[0], b[0]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[0], b[index]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[index], b[0]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[index], b[index]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  c[offset] = Op()(a[0], b[offset]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  c[offset] = Op()(a[offset], b[0]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  c[offset] = Op()(a[offset], b[offset]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_g_nd1(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const size_t& a_stride,\n    constant const size_t& b_stride,\n    uint index [[thread_position_in_grid]]) {\n  auto a_idx = elem_to_loc_1<size_t, uint>(index, a_stride);\n  auto b_idx = elem_to_loc_1<size_t, uint>(index, b_stride);\n  c[index] = Op()(a[a_idx], b[b_idx]);\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = size_t>\n[[kernel]] void binary_g_nd2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const size_t a_strides[2],\n    constant const size_t b_strides[2],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_2<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_2<size_t, IdxT>(index, b_strides);\n  IdxT out_idx = index.x + IdxT(grid_dim.x) * index.y;\n  c[out_idx] = Op()(a[a_idx], b[b_idx]);\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = size_t>\n[[kernel]] void binary_g_nd3(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const size_t a_strides[3],\n    constant const size_t b_strides[3],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_3<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_3<size_t, IdxT>(index, b_strides);\n  IdxT out_idx = index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n  c[out_idx] = Op()(a[a_idx], b[b_idx]);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N = 1,\n    typename IdxT = size_t>\n[[kernel]] void binary_g(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc_2_nd<size_t, IdxT>(\n      {N * index.x, index.y, index.z}, shape, a_strides, b_strides, ndim);\n  auto xshape = shape[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  IdxT a_xstride = a_strides[ndim - 1];\n  IdxT b_xstride = b_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    c[out_idx++] = Op()(a[idx.x], b[idx.y]);\n    idx.x += a_xstride;\n    idx.y += b_xstride;\n  }\n}\n"
- "\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void unary_v(\n    device const T* in,\n    device U* out,\n    uint index [[thread_position_in_grid]]) {\n  out[index] = Op()(in[index]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void unary_v2(\n    device const T* in,\n    device U* out,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  out[offset] = Op()(in[offset]);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N = 1,\n    typename IdxT = size_t>\n[[kernel]] void unary_g(\n    device const T* in,\n    device U* out,\n    constant const int* in_shape,\n    constant const size_t* in_strides,\n    device const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc<size_t, IdxT>(\n      {N * index.x, index.y, index.z}, in_shape, in_strides, ndim);\n  auto xshape = in_shape[ndim - 1];\n  IdxT xstride = in_strides[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    out[out_idx++] = Op()(in[idx]);\n    idx += xstride;\n  }\n}\n"
- "\ntemplate <typename T, typename U>\n[[kernel]] void copy_s(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint index [[thread_position_in_grid]]) {\n  dst[index] = static_cast<U>(src[0]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_v(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint index [[thread_position_in_grid]]) {\n  dst[index] = static_cast<U>(src[index]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_s2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  dst[offset] = static_cast<U>(src[0]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_v2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  dst[offset] = static_cast<U>(src[offset]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_g_nd1(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t& src_stride [[buffer(3)]],\n    uint index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_1<int64_t, int>(index, src_stride);\n  dst[index] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_g_nd2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto src_idx = elem_to_loc_2<int64_t, IdxT>(index, src_strides);\n  IdxT dst_idx = index.x + IdxT(grid_dim.x) * index.y;\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_g_nd3(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto src_idx = elem_to_loc_3<int64_t, IdxT>(index, src_strides);\n  IdxT dst_idx =\n      index.x + IdxT(grid_dim.x) * (index.y + IdxT(grid_dim.y) * index.z);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, int N = 1, typename IdxT = int64_t>\n[[kernel]] void copy_g(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int* src_shape [[buffer(2)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int& ndim [[buffer(5)]],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto src_idx = elem_to_loc<int64_t, IdxT>(\n      {N * index.x, index.y, index.z}, src_shape, src_strides, ndim);\n  if (N == 1) {\n    IdxT dst_idx =\n        index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n    dst[dst_idx] = static_cast<U>(src[src_idx]);\n    return;\n  }\n  auto xshape = src_shape[ndim - 1];\n  IdxT dst_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  auto src_xstride = src_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    dst[dst_idx + i] = static_cast<U>(src[src_idx]);\n    src_idx += src_xstride;\n  }\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_gg_nd1(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t& src_stride [[buffer(3)]],\n    constant const int64_t& dst_stride [[buffer(4)]],\n    uint index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_1<int64_t, int>(index, src_stride);\n  auto dst_idx = elem_to_loc_1<int64_t, int>(index, dst_stride);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_nd2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    uint2 index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_2<int64_t, IdxT>(index, src_strides);\n  auto dst_idx = elem_to_loc_2<int64_t, IdxT>(index, dst_strides);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_nd3(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    uint3 index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_3<int64_t, IdxT>(index, src_strides);\n  auto dst_idx = elem_to_loc_3<int64_t, IdxT>(index, dst_strides);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, int N = 1, typename IdxT = int64_t>\n[[kernel]] void copy_gg(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int* src_shape [[buffer(2)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    constant const int& ndim [[buffer(5)]],\n    uint3 index [[thread_position_in_grid]]) {\n  auto idx = elem_to_loc_2_nd<int64_t, IdxT>(\n      {N * index.x, index.y, index.z},\n      src_shape,\n      src_strides,\n      dst_strides,\n      ndim);\n  if (N == 1) {\n    dst[idx.y] = static_cast<U>(src[idx.x]);\n    return;\n  }\n  IdxT src_xstride = src_strides[ndim - 1];\n  IdxT dst_xstride = dst_strides[ndim - 1];\n  auto xshape = src_shape[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    dst[idx.y] = static_cast<U>(src[idx.x]);\n    idx.x += src_xstride;\n    idx.y += dst_xstride;\n  }\n}\n"
- "\ntemplate <typename U>\nstruct CumSum {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_scan(T val) { return simd_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_scan(T val) { for (int i = 1; i <= 16; i *= 2) { val = operator()(val, simd_shuffle_and_fill_up(val, init, i)); } return val; }\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_exclusive_scan(T val) { return simd_exclusive_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_exclusive_scan(T val) { val = simd_scan(val); return simd_shuffle_and_fill_up(val, init, 1); }\n  static constexpr constant U init = static_cast<U>(0);\n  template <typename T>\n  U operator()(U a, T b) {\n    return a + b;\n  }\n  U simd_scan_impl(U x) {\n    return simd_prefix_inclusive_sum(x);\n  }\n  U simd_exclusive_scan_impl(U x) {\n    return simd_prefix_exclusive_sum(x);\n  }\n};\ntemplate <typename U>\nstruct CumProd {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_scan(T val) { return simd_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_scan(T val) { for (int i = 1; i <= 16; i *= 2) { val = operator()(val, simd_shuffle_and_fill_up(val, init, i)); } return val; }\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_exclusive_scan(T val) { return simd_exclusive_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_exclusive_scan(T val) { val = simd_scan(val); return simd_shuffle_and_fill_up(val, init, 1); }\n  static constexpr constant U init = static_cast<U>(1.0f);\n  template <typename T>\n  U operator()(U a, T b) {\n    return a * b;\n  }\n  U simd_scan_impl(U x) {\n    return simd_prefix_inclusive_product(x);\n  }\n  U simd_exclusive_scan_impl(U x) {\n    return simd_prefix_exclusive_product(x);\n  }\n};\ntemplate <>\nstruct CumProd<bool> {\n  static constexpr constant bool init = true;\n  template <typename T>\n  bool operator()(bool a, T b) {\n    return a & static_cast<bool>(b);\n  }\n  bool simd_scan(bool x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      bool other = simd_shuffle_and_fill_up(x, init, i);\n      x &= other;\n    }\n    return x;\n  }\n  bool simd_exclusive_scan(bool x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename U>\nstruct CumMax {\n  static constexpr constant U init = Limits<U>::min;\n  template <typename T>\n  U operator()(U a, T b) {\n    return (a >= b) ? a : b;\n  }\n  U simd_scan(U x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      U other = simd_shuffle_and_fill_up(x, init, i);\n      x = (x >= other) ? x : other;\n    }\n    return x;\n  }\n  U simd_exclusive_scan(U x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename U>\nstruct CumMin {\n  static constexpr constant U init = Limits<U>::max;\n  template <typename T>\n  U operator()(U a, T b) {\n    return (a <= b) ? a : b;\n  }\n  U simd_scan(U x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      U other = simd_shuffle_and_fill_up(x, init, i);\n      x = (x <= other) ? x : other;\n    }\n    return x;\n  }\n  U simd_exclusive_scan(U x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename T, typename U, int N_READS, bool reverse>\ninline void load_unsafe(U values[N_READS], const device T* input) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      values[N_READS - i - 1] = input[i];\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      values[i] = input[i];\n    }\n  }\n}\ntemplate <typename T, typename U, int N_READS, bool reverse>\ninline void load_safe(\n    U values[N_READS],\n    const device T* input,\n    int start,\n    int total,\n    U init) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      values[N_READS - i - 1] =\n          (start + N_READS - i - 1 < total) ? input[i] : init;\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      values[i] = (start + i < total) ? input[i] : init;\n    }\n  }\n}\ntemplate <typename U, int N_READS, bool reverse>\ninline void write_unsafe(U values[N_READS], device U* out) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      out[i] = values[N_READS - i - 1];\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      out[i] = values[i];\n    }\n  }\n}\ntemplate <typename U, int N_READS, bool reverse>\ninline void write_safe(U values[N_READS], device U* out, int start, int total) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      if (start + N_READS - i - 1 < total) {\n        out[i] = values[N_READS - i - 1];\n      }\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      if (start + i < total) {\n        out[i] = values[i];\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS,\n    bool inclusive,\n    bool reverse>\n[[kernel]] void contiguous_scan(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& axis_size [[buffer(2)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  constexpr int simd_size = 32;\n  Op op;\n  size_t offset = (gid.y + gsize.y * size_t(gid.z)) * axis_size;\n  in += offset;\n  out += offset;\n  uint simd_groups = lsize.x / simd_size;\n  U prefix = Op::init;\n  U values[N_READS];\n  threadgroup U simdgroup_sums[32];\n  for (uint r = 0; r < ceildiv(axis_size, N_READS * lsize.x); r++) {\n    uint offset = r * lsize.x * N_READS + lid.x * N_READS;\n    if (reverse) {\n      if ((offset + N_READS) < axis_size) {\n        load_unsafe<T, U, N_READS, reverse>(\n            values, in + axis_size - offset - N_READS);\n      } else {\n        load_safe<T, U, N_READS, reverse>(\n            values,\n            in + axis_size - offset - N_READS,\n            offset,\n            axis_size,\n            Op::init);\n      }\n    } else {\n      if ((offset + N_READS) < axis_size) {\n        load_unsafe<T, U, N_READS, reverse>(values, in + offset);\n      } else {\n        load_safe<T, U, N_READS, reverse>(\n            values, in + offset, offset, axis_size, Op::init);\n      }\n    }\n    for (int i = 1; i < N_READS; i++) {\n      values[i] = op(values[i], values[i - 1]);\n    }\n    U prev_thread = op.simd_exclusive_scan(values[N_READS - 1]);\n    if (simd_lane_id == simd_size - 1) {\n      simdgroup_sums[simd_group_id] = op(prev_thread, values[N_READS - 1]);\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (simd_group_id == 0) {\n      U prev_simdgroup = op.simd_exclusive_scan(simdgroup_sums[simd_lane_id]);\n      simdgroup_sums[simd_lane_id] = prev_simdgroup;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < N_READS; i++) {\n      values[i] = op(values[i], prefix);\n      values[i] = op(values[i], simdgroup_sums[simd_group_id]);\n      values[i] = op(values[i], prev_thread);\n    }\n    if (reverse) {\n      if (inclusive) {\n        if ((offset + N_READS) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(\n              values, out + axis_size - offset - N_READS);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values, out + axis_size - offset - N_READS, offset, axis_size);\n        }\n      } else {\n        if (lid.x == 0 && offset == 0) {\n          out[axis_size - 1] = Op::init;\n        }\n        if ((offset + N_READS + 1) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(\n              values, out + axis_size - offset - 1 - N_READS);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values,\n              out + axis_size - offset - 1 - N_READS,\n              offset + 1,\n              axis_size);\n        }\n      }\n    } else {\n      if (inclusive) {\n        if ((offset + N_READS) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(values, out + offset);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values, out + offset, offset, axis_size);\n        }\n      } else {\n        if (lid.x == 0 && offset == 0) {\n          out[0] = Op::init;\n        }\n        if ((offset + N_READS + 1) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(values, out + offset + 1);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values, out + offset + 1, offset + 1, axis_size);\n        }\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (simd_group_id == simd_groups - 1 && simd_lane_id == simd_size - 1) {\n      simdgroup_sums[0] = values[N_READS - 1];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    prefix = simdgroup_sums[0];\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS,\n    bool inclusive,\n    bool reverse>\n[[kernel]] void strided_scan(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& axis_size [[buffer(2)]],\n    const constant size_t& stride [[buffer(3)]],\n    const constant size_t& stride_blocks [[buffer(4)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  constexpr int simd_size = 32;\n  constexpr int BM = 32;\n  constexpr int BN = 32;\n  constexpr int BN_pad = 32 + 16 / sizeof(U);\n  constexpr int n_simds = BN / N_READS;\n  constexpr int n_scans = BN / n_simds;\n  Op op;\n  threadgroup U read_buffer[BM * BN_pad];\n  U values[n_scans];\n  U prefix[n_scans];\n  for (int i = 0; i < n_scans; i++) {\n    prefix[i] = Op::init;\n  }\n  size_t full_gid = gid.y + gsize.y * size_t(gid.z);\n  size_t offset = full_gid / stride_blocks * axis_size * stride;\n  size_t global_index_x = full_gid % stride_blocks * BN;\n  uint read_offset_y = (lid.x * N_READS) / BN;\n  uint read_offset_x = (lid.x * N_READS) % BN;\n  uint scan_offset_y = simd_lane_id;\n  uint scan_offset_x = simd_group_id * n_scans;\n  uint stride_limit = stride - global_index_x;\n  in += offset + global_index_x + read_offset_x;\n  out += offset + global_index_x + read_offset_x;\n  threadgroup U* read_into =\n      read_buffer + read_offset_y * BN_pad + read_offset_x;\n  threadgroup U* read_from =\n      read_buffer + scan_offset_y * BN_pad + scan_offset_x;\n  for (uint j = 0; j < axis_size; j += BM) {\n    uint index_y = j + read_offset_y;\n    uint check_index_y = index_y;\n    if (reverse) {\n      index_y = axis_size - 1 - index_y;\n    }\n    if (check_index_y < axis_size && (read_offset_x + N_READS) < stride_limit) {\n      for (int i = 0; i < N_READS; i++) {\n        read_into[i] = in[index_y * stride + i];\n      }\n    } else {\n      for (int i = 0; i < N_READS; i++) {\n        if (check_index_y < axis_size && (read_offset_x + i) < stride_limit) {\n          read_into[i] = in[index_y * stride + i];\n        } else {\n          read_into[i] = Op::init;\n        }\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < n_scans; i++) {\n      values[i] = read_from[i];\n    }\n    simdgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < n_scans; i++) {\n      values[i] = op.simd_scan(values[i]);\n      values[i] = op(values[i], prefix[i]);\n      prefix[i] = simd_shuffle(values[i], simd_size - 1);\n    }\n    for (int i = 0; i < n_scans; i++) {\n      read_from[i] = values[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (!inclusive) {\n      if (check_index_y == 0) {\n        if ((read_offset_x + N_READS) < stride_limit) {\n          for (int i = 0; i < N_READS; i++) {\n            out[index_y * stride + i] = Op::init;\n          }\n        } else {\n          for (int i = 0; i < N_READS; i++) {\n            if ((read_offset_x + i) < stride_limit) {\n              out[index_y * stride + i] = Op::init;\n            }\n          }\n        }\n      }\n      if (reverse) {\n        index_y -= 1;\n        check_index_y += 1;\n      } else {\n        index_y += 1;\n        check_index_y += 1;\n      }\n    }\n    if (check_index_y < axis_size && (read_offset_x + N_READS) < stride_limit) {\n      for (int i = 0; i < N_READS; i++) {\n        out[index_y * stride + i] = read_into[i];\n      }\n    } else {\n      for (int i = 0; i < N_READS; i++) {\n        if (check_index_y < axis_size && (read_offset_x + i) < stride_limit) {\n          out[index_y * stride + i] = read_into[i];\n        }\n      }\n    }\n  }\n}\n"
- "\ntemplate [[host_name(\"{0}\")]] [[kernel]] void arange<{1}>(\n    constant const {1}& start,\n    constant const {1}& step,\n    device {1}* out,\n    uint index [[thread_position_in_grid]]);\n"
- "\ntemplate [[host_name(\"{name}\")]]\n[[kernel]] void gemm<{itype}, {bm}, {bn}, {bk}, {wm}, {wn}, {trans_a}, {trans_b}, float>(\n    const device {itype} *A [[buffer(0)]],\n    const device {itype} *B [[buffer(1)]],\n    const device {itype} *C [[buffer(2), function_constant(use_out_source)]],\n    device {itype} *D [[buffer(3)]],\n    const constant GEMMParams* params [[buffer(4)]],\n    const constant GEMMAddMMParams* addmm_params [[buffer(5), function_constant(use_out_source)]],\n    const constant int* batch_shape [[buffer(6)]],\n    const constant size_t* batch_strides [[buffer(7)]],\n    const constant uint32_t* lhs_indices [[buffer(10), function_constant(do_gather)]],\n    const constant uint32_t* rhs_indices [[buffer(11), function_constant(do_gather)]],\n    const constant uint32_t* C_indices [[buffer(12), function_constant(gather_bias)]],\n    const constant int* operand_shape [[buffer(13), function_constant(do_gather)]],\n    const constant size_t* operand_strides [[buffer(14), function_constant(do_gather)]],\n    const constant packed_int3& operand_batch_ndim [[buffer(15), function_constant(do_gather)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]);\n"
- "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\n    implicit_gemm_conv_2d_general<{itype}, {bm}, {bn}, {bk}, {wm}, {wn}>(\n        const device {itype}* A [[buffer(0)]],\n        const device {itype}* B [[buffer(1)]],\n        device {itype}* C [[buffer(2)]],\n        const constant MLXConvParams<2>* params [[buffer(3)]],\n        const constant ImplicitGemmConv2DParams* gemm_params [[buffer(4)]],\n        const constant Conv2DGeneralJumpParams* jump_params [[buffer(5)]],\n        const constant Conv2DGeneralBaseInfo* base_h [[buffer(6)]],\n        const constant Conv2DGeneralBaseInfo* base_w [[buffer(7)]],\n        uint3 tid [[threadgroup_position_in_grid]],\n        uint3 lid [[thread_position_in_threadgroup]],\n        uint simd_gid [[simdgroup_index_in_threadgroup]],\n        uint simd_lid [[thread_index_in_simdgroup]]);\n"
- "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\ngemm_splitk<\n    {itype},\n    {otype},\n    {bm},\n    {bn},\n    {bk},\n    {wm},\n    {wn},\n    {trans_a},\n    {trans_b},\n    {mn_aligned},\n    {k_aligned}>(\n    const device {itype}* A [[buffer(0)]],\n    const device {itype}* B [[buffer(1)]],\n    device {otype}* C [[buffer(2)]],\n    const constant GEMMSpiltKParams* params [[buffer(3)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]);\n"
- "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\ngemm_splitk_accum<{atype}, {otype}>(\n    const device {atype}* C_split [[buffer(0)]],\n    device {otype}* D [[buffer(1)]],\n    const constant int& k_partitions [[buffer(2)]],\n    const constant int& partition_stride [[buffer(3)]],\n    const constant int& ldd [[buffer(4)]],\n    uint2 gid [[thread_position_in_grid]]);\n"
- "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\ngemm_splitk_accum_axpby<{atype}, {otype}>(\n    const device {atype}* C_split [[buffer(0)]],\n    device {otype}* D [[buffer(1)]],\n    const constant int& k_partitions [[buffer(2)]],\n    const constant int& partition_stride [[buffer(3)]],\n    const constant int& ldd [[buffer(4)]],\n    const device {otype}* C [[buffer(5)]],\n    const constant int& ldc [[buffer(6)]],\n    const constant int& fdc [[buffer(7)]],\n    const constant float& alpha [[buffer(8)]],\n    const constant float& beta [[buffer(9)]],\n    uint2 gid [[thread_position_in_grid]]);\n"
- "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\nimplicit_gemm_conv_2d<{itype}, {bm}, {bn}, {bk}, {wm}, {wn}, {n_channels}, {small_filter}>(\n    const device {itype}* A [[buffer(0)]],\n    const device {itype}* B [[buffer(1)]],\n    device {itype}* C [[buffer(2)]],\n    const constant MLXConvParams<2>* params [[buffer(3)]],\n    const constant ImplicitGemmConv2DParams* gemm_params [[buffer(4)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_gid [[simdgroup_index_in_threadgroup]],\n    uint simd_lid [[thread_index_in_simdgroup]]);\n"
- "\nusing namespace metal;\ntemplate <typename T>\nMETAL_FUNC void thread_swap(thread T& a, thread T& b) {\n  T w = a;\n  a = b;\n  b = w;\n}\ntemplate <typename T>\nstruct LessThan {\n  static constexpr constant T init = Limits<T>::max;\n  METAL_FUNC bool operator()(T a, T b) {\n    return a < b;\n  }\n};\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short N_PER_THREAD,\n    typename CompareOp>\nstruct ThreadSort {\n  static METAL_FUNC void sort(\n      thread val_t (&vals)[N_PER_THREAD],\n      thread idx_t (&idxs)[N_PER_THREAD]) {\n    CompareOp op;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < N_PER_THREAD; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = i & 1; j < N_PER_THREAD - 1; j += 2) {\n        if (op(vals[j + 1], vals[j])) {\n          thread_swap(vals[j + 1], vals[j]);\n          thread_swap(idxs[j + 1], idxs[j]);\n        }\n      }\n    }\n  }\n};\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp>\nstruct BlockMergeSort {\n  using thread_sort_t =\n      ThreadSort<val_t, idx_t, ARG_SORT, N_PER_THREAD, CompareOp>;\n  static METAL_FUNC int merge_partition(\n      const threadgroup val_t* As,\n      const threadgroup val_t* Bs,\n      short A_sz,\n      short B_sz,\n      short sort_md) {\n    CompareOp op;\n    short A_st = max(0, sort_md - B_sz);\n    short A_ed = min(sort_md, A_sz);\n    while (A_st < A_ed) {\n      short md = A_st + (A_ed - A_st) / 2;\n      auto a = As[md];\n      auto b = Bs[sort_md - 1 - md];\n      if (op(b, a)) {\n        A_ed = md;\n      } else {\n        A_st = md + 1;\n      }\n    }\n    return A_ed;\n  }\n  static METAL_FUNC void merge_step(\n      const threadgroup val_t* As,\n      const threadgroup val_t* Bs,\n      const threadgroup idx_t* As_idx,\n      const threadgroup idx_t* Bs_idx,\n      short A_sz,\n      short B_sz,\n      thread val_t (&vals)[N_PER_THREAD],\n      thread idx_t (&idxs)[N_PER_THREAD]) {\n    CompareOp op;\n    short a_idx = 0;\n    short b_idx = 0;\n    for (int i = 0; i < N_PER_THREAD; ++i) {\n      auto a = As[a_idx];\n      auto b = Bs[b_idx];\n      bool pred = (b_idx < B_sz) && (a_idx >= A_sz || op(b, a));\n      vals[i] = pred ? b : a;\n      idxs[i] = pred ? Bs_idx[b_idx] : As_idx[a_idx];\n      b_idx += short(pred);\n      a_idx += short(!pred);\n    }\n  }\n  static METAL_FUNC void sort(\n      threadgroup val_t* tgp_vals [[threadgroup(0)]],\n      threadgroup idx_t* tgp_idxs [[threadgroup(1)]],\n      int size_sorted_axis,\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    int idx = lid.x * N_PER_THREAD;\n    thread val_t thread_vals[N_PER_THREAD];\n    thread idx_t thread_idxs[N_PER_THREAD];\n    for (int i = 0; i < N_PER_THREAD; ++i) {\n      thread_vals[i] = tgp_vals[idx + i];\n      if (ARG_SORT) {\n        thread_idxs[i] = tgp_idxs[idx + i];\n      }\n    }\n    if (idx < size_sorted_axis) {\n      thread_sort_t::sort(thread_vals, thread_idxs);\n    }\n    for (int merge_threads = 2; merge_threads <= BLOCK_THREADS;\n         merge_threads *= 2) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      for (int i = 0; i < N_PER_THREAD; ++i) {\n        tgp_vals[idx + i] = thread_vals[i];\n        if (ARG_SORT) {\n          tgp_idxs[idx + i] = thread_idxs[i];\n        }\n      }\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      int merge_group = lid.x / merge_threads;\n      int merge_lane = lid.x % merge_threads;\n      int sort_sz = N_PER_THREAD * merge_threads;\n      int sort_st = N_PER_THREAD * merge_threads * merge_group;\n      int A_st = sort_st;\n      int A_ed = sort_st + sort_sz / 2;\n      int B_st = sort_st + sort_sz / 2;\n      int B_ed = sort_st + sort_sz;\n      const threadgroup val_t* As = tgp_vals + A_st;\n      const threadgroup val_t* Bs = tgp_vals + B_st;\n      int A_sz = A_ed - A_st;\n      int B_sz = B_ed - B_st;\n      int sort_md = N_PER_THREAD * merge_lane;\n      int partition = merge_partition(As, Bs, A_sz, B_sz, sort_md);\n      As += partition;\n      Bs += sort_md - partition;\n      A_sz -= partition;\n      B_sz -= sort_md - partition;\n      const threadgroup idx_t* As_idx =\n          ARG_SORT ? tgp_idxs + A_st + partition : nullptr;\n      const threadgroup idx_t* Bs_idx =\n          ARG_SORT ? tgp_idxs + B_st + sort_md - partition : nullptr;\n      merge_step(As, Bs, As_idx, Bs_idx, A_sz, B_sz, thread_vals, thread_idxs);\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < N_PER_THREAD; ++i) {\n      tgp_vals[idx + i] = thread_vals[i];\n      if (ARG_SORT) {\n        tgp_idxs[idx + i] = thread_idxs[i];\n      }\n    }\n  }\n};\ntemplate <\n    typename T,\n    typename U,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp = LessThan<T>>\nstruct KernelMergeSort {\n  using val_t = T;\n  using idx_t = uint;\n  using block_merge_sort_t = BlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD,\n      CompareOp>;\n  static constant constexpr const short N_PER_BLOCK = BLOCK_THREADS * N_PER_THREAD;\n  static METAL_FUNC void block_sort(\n      const device T* inp,\n      device U* out,\n      const constant int& size_sorted_axis,\n      const constant int& in_stride_sorted_axis,\n      const constant int& out_stride_sorted_axis,\n      const constant int& in_stride_segment_axis,\n      const constant int& out_stride_segment_axis,\n      threadgroup val_t* tgp_vals,\n      threadgroup idx_t* tgp_idxs,\n      uint3 tid [[threadgroup_position_in_grid]],\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    inp += tid.y * in_stride_segment_axis;\n    out += tid.y * out_stride_segment_axis;\n    for (short i = lid.x; i < N_PER_BLOCK; i += BLOCK_THREADS) {\n      tgp_vals[i] = i < size_sorted_axis ? inp[i * in_stride_sorted_axis]\n                                         : val_t(CompareOp::init);\n      if (ARG_SORT) {\n        tgp_idxs[i] = i;\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    block_merge_sort_t::sort(tgp_vals, tgp_idxs, size_sorted_axis, lid);\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = lid.x; i < size_sorted_axis; i += BLOCK_THREADS) {\n      if (ARG_SORT) {\n        out[i * out_stride_sorted_axis] = tgp_idxs[i];\n      } else {\n        out[i * out_stride_sorted_axis] = tgp_vals[i];\n      }\n    }\n  }\n};\ntemplate <\n    typename T,\n    typename U,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void block_sort(\n    const device T* inp [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant int& size_sorted_axis [[buffer(2)]],\n    const constant int& in_stride_sorted_axis [[buffer(3)]],\n    const constant int& out_stride_sorted_axis [[buffer(4)]],\n    const constant int& in_stride_segment_axis [[buffer(5)]],\n    const constant int& out_stride_segment_axis [[buffer(6)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel =\n      KernelMergeSort<T, U, ARG_SORT, BLOCK_THREADS, N_PER_THREAD>;\n  using val_t = typename sort_kernel::val_t;\n  using idx_t = typename sort_kernel::idx_t;\n  if (ARG_SORT) {\n    threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n    threadgroup idx_t tgp_idxs[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        in_stride_segment_axis,\n        out_stride_segment_axis,\n        tgp_vals,\n        tgp_idxs,\n        tid,\n        lid);\n  } else {\n    threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        in_stride_segment_axis,\n        out_stride_segment_axis,\n        tgp_vals,\n        nullptr,\n        tid,\n        lid);\n  }\n}\nconstant constexpr const int zero_helper = 0;\ntemplate <\n    typename T,\n    typename U,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void block_sort_nc(\n    const device T* inp [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant int& size_sorted_axis [[buffer(2)]],\n    const constant int& in_stride_sorted_axis [[buffer(3)]],\n    const constant int& out_stride_sorted_axis [[buffer(4)]],\n    const constant int& nc_dim [[buffer(5)]],\n    const constant int* nc_shape [[buffer(6)]],\n    const constant size_t* in_nc_strides [[buffer(7)]],\n    const constant size_t* out_nc_strides [[buffer(8)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel =\n      KernelMergeSort<T, U, ARG_SORT, BLOCK_THREADS, N_PER_THREAD>;\n  using val_t = typename sort_kernel::val_t;\n  using idx_t = typename sort_kernel::idx_t;\n  auto in_block_idx = elem_to_loc(tid.y, nc_shape, in_nc_strides, nc_dim);\n  auto out_block_idx = elem_to_loc(tid.y, nc_shape, out_nc_strides, nc_dim);\n  inp += in_block_idx;\n  out += out_block_idx;\n  if (ARG_SORT) {\n    threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n    threadgroup idx_t tgp_idxs[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        zero_helper,\n        zero_helper,\n        tgp_vals,\n        tgp_idxs,\n        tid,\n        lid);\n  } else {\n    threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        zero_helper,\n        zero_helper,\n        tgp_vals,\n        nullptr,\n        tid,\n        lid);\n  }\n}\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp = LessThan<val_t>>\nstruct KernelMultiBlockMergeSort {\n  using block_merge_sort_t = BlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD,\n      CompareOp>;\n  static constant constexpr const short N_PER_BLOCK = BLOCK_THREADS * N_PER_THREAD;\n  static METAL_FUNC void block_sort(\n      const device val_t* inp,\n      device val_t* out_vals,\n      device idx_t* out_idxs,\n      const constant int& size_sorted_axis,\n      const constant int& stride_sorted_axis,\n      threadgroup val_t* tgp_vals,\n      threadgroup idx_t* tgp_idxs,\n      uint3 tid [[threadgroup_position_in_grid]],\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    int base_idx = tid.x * N_PER_BLOCK;\n    for (short i = lid.x; i < N_PER_BLOCK; i += BLOCK_THREADS) {\n      int idx = base_idx + i;\n      tgp_vals[i] = idx < size_sorted_axis ? inp[idx * stride_sorted_axis]\n                                           : val_t(CompareOp::init);\n      tgp_idxs[i] = idx;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    block_merge_sort_t::sort(tgp_vals, tgp_idxs, size_sorted_axis, lid);\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = lid.x; i < N_PER_BLOCK; i += BLOCK_THREADS) {\n      int idx = base_idx + i;\n      if (idx < size_sorted_axis) {\n        out_vals[idx] = tgp_vals[i];\n        out_idxs[idx] = tgp_idxs[i];\n      }\n    }\n  }\n  static METAL_FUNC int merge_partition(\n      const device val_t* As,\n      const device val_t* Bs,\n      int A_sz,\n      int B_sz,\n      int sort_md) {\n    CompareOp op;\n    int A_st = max(0, sort_md - B_sz);\n    int A_ed = min(sort_md, A_sz);\n    while (A_st < A_ed) {\n      int md = A_st + (A_ed - A_st) / 2;\n      auto a = As[md];\n      auto b = Bs[sort_md - 1 - md];\n      if (op(b, a)) {\n        A_ed = md;\n      } else {\n        A_st = md + 1;\n      }\n    }\n    return A_ed;\n  }\n};\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void mb_block_sort(\n    const device val_t* inp [[buffer(0)]],\n    device val_t* out_vals [[buffer(1)]],\n    device idx_t* out_idxs [[buffer(2)]],\n    const constant int& size_sorted_axis [[buffer(3)]],\n    const constant int& stride_sorted_axis [[buffer(4)]],\n    const constant int& nc_dim [[buffer(5)]],\n    const constant int* nc_shape [[buffer(6)]],\n    const constant size_t* nc_strides [[buffer(7)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel = KernelMultiBlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD>;\n  auto block_idx = elem_to_loc(tid.y, nc_shape, nc_strides, nc_dim);\n  inp += block_idx;\n  out_vals += tid.y * size_sorted_axis;\n  out_idxs += tid.y * size_sorted_axis;\n  threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n  threadgroup idx_t tgp_idxs[sort_kernel::N_PER_BLOCK];\n  sort_kernel::block_sort(\n      inp,\n      out_vals,\n      out_idxs,\n      size_sorted_axis,\n      stride_sorted_axis,\n      tgp_vals,\n      tgp_idxs,\n      tid,\n      lid);\n}\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel]] void mb_block_partition(\n    device idx_t* block_partitions [[buffer(0)]],\n    const device val_t* dev_vals [[buffer(1)]],\n    const device idx_t* dev_idxs [[buffer(2)]],\n    const constant int& size_sorted_axis [[buffer(3)]],\n    const constant int& merge_tiles [[buffer(4)]],\n    const constant int& n_blocks [[buffer(5)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 tgp_dims [[threads_per_threadgroup]]) {\n  using sort_kernel = KernelMultiBlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD>;\n  block_partitions += tid.y * tgp_dims.x;\n  dev_vals += tid.y * size_sorted_axis;\n  dev_idxs += tid.y * size_sorted_axis;\n  for (int i = lid.x; i <= n_blocks; i += tgp_dims.x) {\n    int merge_group = i / merge_tiles;\n    int merge_lane = i % merge_tiles;\n    int sort_sz = sort_kernel::N_PER_BLOCK * merge_tiles;\n    int sort_st = sort_kernel::N_PER_BLOCK * merge_tiles * merge_group;\n    int A_st = min(size_sorted_axis, sort_st);\n    int A_ed = min(size_sorted_axis, sort_st + sort_sz / 2);\n    int B_st = A_ed;\n    int B_ed = min(size_sorted_axis, B_st + sort_sz / 2);\n    int partition_at = min(B_ed - A_st, sort_kernel::N_PER_BLOCK * merge_lane);\n    int partition = sort_kernel::merge_partition(\n        dev_vals + A_st,\n        dev_vals + B_st,\n        A_ed - A_st,\n        B_ed - B_st,\n        partition_at);\n    block_partitions[i] = A_st + partition;\n  }\n}\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp = LessThan<val_t>>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void\nmb_block_merge(\n    const device idx_t* block_partitions [[buffer(0)]],\n    const device val_t* dev_vals_in [[buffer(1)]],\n    const device idx_t* dev_idxs_in [[buffer(2)]],\n    device val_t* dev_vals_out [[buffer(3)]],\n    device idx_t* dev_idxs_out [[buffer(4)]],\n    const constant int& size_sorted_axis [[buffer(5)]],\n    const constant int& merge_tiles [[buffer(6)]],\n    const constant int& num_tiles [[buffer(7)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel = KernelMultiBlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD,\n      CompareOp>;\n  using block_sort_t = typename sort_kernel::block_merge_sort_t;\n  block_partitions += tid.y * (num_tiles + 1);\n  dev_vals_in += tid.y * size_sorted_axis;\n  dev_idxs_in += tid.y * size_sorted_axis;\n  dev_vals_out += tid.y * size_sorted_axis;\n  dev_idxs_out += tid.y * size_sorted_axis;\n  int block_idx = tid.x;\n  int merge_group = block_idx / merge_tiles;\n  int sort_st = sort_kernel::N_PER_BLOCK * merge_tiles * merge_group;\n  int sort_sz = sort_kernel::N_PER_BLOCK * merge_tiles;\n  int sort_md = sort_kernel::N_PER_BLOCK * block_idx - sort_st;\n  int A_st = block_partitions[block_idx + 0];\n  int A_ed = block_partitions[block_idx + 1];\n  int B_st = min(size_sorted_axis, 2 * sort_st + sort_sz / 2 + sort_md - A_st);\n  int B_ed = min(\n      size_sorted_axis,\n      2 * sort_st + sort_sz / 2 + sort_md + sort_kernel::N_PER_BLOCK - A_ed);\n  if ((block_idx % merge_tiles) == merge_tiles - 1) {\n    A_ed = min(size_sorted_axis, sort_st + sort_sz / 2);\n    B_ed = min(size_sorted_axis, sort_st + sort_sz);\n  }\n  int A_sz = A_ed - A_st;\n  int B_sz = B_ed - B_st;\n  thread val_t thread_vals[N_PER_THREAD];\n  thread idx_t thread_idxs[N_PER_THREAD];\n  for (int i = 0; i < N_PER_THREAD; i++) {\n    int idx = BLOCK_THREADS * i + lid.x;\n    if (idx < (A_sz + B_sz)) {\n      thread_vals[i] = (idx < A_sz) ? dev_vals_in[A_st + idx]\n                                    : dev_vals_in[B_st + idx - A_sz];\n      thread_idxs[i] = (idx < A_sz) ? dev_idxs_in[A_st + idx]\n                                    : dev_idxs_in[B_st + idx - A_sz];\n    } else {\n      thread_vals[i] = CompareOp::init;\n      thread_idxs[i] = 0;\n    }\n  }\n  threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n  threadgroup idx_t tgp_idxs[sort_kernel::N_PER_BLOCK];\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  for (int i = 0; i < N_PER_THREAD; i++) {\n    int idx = BLOCK_THREADS * i + lid.x;\n    tgp_vals[idx] = thread_vals[i];\n    tgp_idxs[idx] = thread_idxs[i];\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  int sort_md_local = min(A_sz + B_sz, N_PER_THREAD * int(lid.x));\n  int A_st_local = block_sort_t::merge_partition(\n      tgp_vals, tgp_vals + A_sz, A_sz, B_sz, sort_md_local);\n  int A_ed_local = A_sz;\n  int B_st_local = sort_md_local - A_st_local;\n  int B_ed_local = B_sz;\n  int A_sz_local = A_ed_local - A_st_local;\n  int B_sz_local = B_ed_local - B_st_local;\n  block_sort_t::merge_step(\n      tgp_vals + A_st_local,\n      tgp_vals + A_ed_local + B_st_local,\n      tgp_idxs + A_st_local,\n      tgp_idxs + A_ed_local + B_st_local,\n      A_sz_local,\n      B_sz_local,\n      thread_vals,\n      thread_idxs);\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  for (int i = 0; i < N_PER_THREAD; ++i) {\n    int idx = lid.x * N_PER_THREAD;\n    tgp_vals[idx + i] = thread_vals[i];\n    tgp_idxs[idx + i] = thread_idxs[i];\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  int base_idx = tid.x * sort_kernel::N_PER_BLOCK;\n  for (int i = lid.x; i < sort_kernel::N_PER_BLOCK; i += BLOCK_THREADS) {\n    int idx = base_idx + i;\n    if (idx < size_sorted_axis) {\n      dev_vals_out[idx] = tgp_vals[i];\n      dev_idxs_out[idx] = tgp_idxs[i];\n    }\n  }\n}\n"
- "\nusing namespace mlx::steel;\nconstant bool has_batch [[function_constant(10)]];\nconstant bool use_out_source [[function_constant(100)]];\nconstant bool do_axpby [[function_constant(110)]];\nconstant bool align_M [[function_constant(200)]];\nconstant bool align_N [[function_constant(201)]];\nconstant bool align_K [[function_constant(202)]];\nconstant bool do_gather [[function_constant(300)]];\nconstant bool gather_bias = do_gather && use_out_source;\ntemplate <\n    typename T,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    typename AccumType = float>\n[[kernel, max_total_threads_per_threadgroup(WM* WN * 32)]] void gemm(\n    const device T* A [[buffer(0)]],\n    const device T* B [[buffer(1)]],\n    const device T* C [[buffer(2), function_constant(use_out_source)]],\n    device T* D [[buffer(3)]],\n    const constant GEMMParams* params [[buffer(4)]],\n    const constant GEMMAddMMParams* addmm_params [[buffer(5), function_constant(use_out_source)]],\n    const constant int* batch_shape [[buffer(6)]],\n    const constant size_t* batch_strides [[buffer(7)]],\n    const constant uint32_t* lhs_indices [[buffer(10), function_constant(do_gather)]],\n    const constant uint32_t* rhs_indices [[buffer(11), function_constant(do_gather)]],\n    const constant uint32_t* C_indices [[buffer(12), function_constant(gather_bias)]],\n    const constant int* operand_shape [[buffer(13), function_constant(do_gather)]],\n    const constant size_t* operand_strides [[buffer(14), function_constant(do_gather)]],\n    const constant packed_int3& operand_batch_ndim [[buffer(15), function_constant(do_gather)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  (void)lid;\n  using gemm_kernel = GEMMKernel<\n      T,\n      T,\n      BM,\n      BN,\n      BK,\n      WM,\n      WN,\n      transpose_a,\n      transpose_b,\n      true,\n      true,\n      AccumType>;\n  using loader_a_t = typename gemm_kernel::loader_a_t;\n  using loader_b_t = typename gemm_kernel::loader_b_t;\n  using mma_t = typename gemm_kernel::mma_t;\n  const int tid_y = ((tid.y) << params->swizzle_log) +\n      ((tid.x) & ((1 << params->swizzle_log) - 1));\n  const int tid_x = (tid.x) >> params->swizzle_log;\n  if (params->tiles_n <= tid_x || params->tiles_m <= tid_y) {\n    return;\n  }\n  if (do_gather) {\n    uint32_t indx_A, indx_B, indx_C;\n    if (has_batch) {\n      const constant size_t* indx_A_bstrides = batch_strides;\n      const constant size_t* indx_B_bstrides =\n          batch_strides + params->batch_ndim;\n      ulong2 indx_offsets = elem_to_loc_broadcast(\n          tid.z,\n          batch_shape,\n          indx_A_bstrides,\n          indx_B_bstrides,\n          params->batch_ndim);\n      indx_A = lhs_indices[indx_offsets.x];\n      indx_B = rhs_indices[indx_offsets.y];\n      if (use_out_source) {\n        const constant size_t* indx_C_bstrides =\n            indx_B_bstrides + params->batch_ndim;\n        auto indx_offset_C = elem_to_loc(\n            tid.z, batch_shape, indx_C_bstrides, params->batch_ndim);\n        indx_C = C_indices[indx_offset_C];\n      }\n    } else {\n      indx_A = lhs_indices[params->batch_stride_a * tid.z];\n      indx_B = rhs_indices[params->batch_stride_b * tid.z];\n      if (use_out_source) {\n        indx_C = C_indices[addmm_params->batch_stride_c * tid.z];\n      }\n    }\n    int batch_ndim_A = operand_batch_ndim.x;\n    const constant int* batch_shape_A = operand_shape;\n    const constant size_t* batch_strides_A = operand_strides;\n    A += elem_to_loc(indx_A, batch_shape_A, batch_strides_A, batch_ndim_A);\n    int batch_ndim_B = operand_batch_ndim.y;\n    const constant int* batch_shape_B = batch_shape_A + batch_ndim_A;\n    const constant size_t* batch_strides_B = batch_strides_A + batch_ndim_A;\n    B += elem_to_loc(indx_B, batch_shape_B, batch_strides_B, batch_ndim_B);\n    if (use_out_source) {\n      int batch_ndim_C = operand_batch_ndim.z;\n      const constant int* batch_shape_C = batch_shape_B + batch_ndim_B;\n      const constant size_t* batch_strides_C = batch_strides_B + batch_ndim_B;\n      C += elem_to_loc(indx_C, batch_shape_C, batch_strides_C, batch_ndim_C);\n    }\n  }\n  else {\n    if (has_batch) {\n      const constant size_t* A_bstrides = batch_strides;\n      const constant size_t* B_bstrides = batch_strides + params->batch_ndim;\n      ulong2 batch_offsets = elem_to_loc_broadcast(\n          tid.z, batch_shape, A_bstrides, B_bstrides, params->batch_ndim);\n      A += batch_offsets.x;\n      B += batch_offsets.y;\n      if (use_out_source) {\n        const constant size_t* C_bstrides = B_bstrides + params->batch_ndim;\n        C += elem_to_loc(tid.z, batch_shape, C_bstrides, params->batch_ndim);\n      }\n    } else {\n      A += params->batch_stride_a * tid.z;\n      B += params->batch_stride_b * tid.z;\n      if (use_out_source) {\n        C += addmm_params->batch_stride_c * tid.z;\n      }\n    }\n  }\n  D += params->batch_stride_d * tid.z;\n  threadgroup T As[gemm_kernel::tgp_mem_size_a];\n  threadgroup T Bs[gemm_kernel::tgp_mem_size_b];\n  threadgroup_barrier(mem_flags::mem_none);\n  const int c_row = tid_y * BM;\n  const int c_col = tid_x * BN;\n  const size_t c_row_long = size_t(c_row);\n  const size_t c_col_long = size_t(c_col);\n  A += transpose_a ? c_row_long : c_row_long * params->lda;\n  B += transpose_b ? c_col_long * params->ldb : c_col_long;\n  D += c_row_long * params->ldd + c_col_long;\n  if (use_out_source) {\n    C += c_row_long * addmm_params->ldc + c_col_long * addmm_params->fdc;\n  }\n  thread mma_t mma_op(simd_group_id, simd_lane_id);\n  thread loader_a_t loader_a(A, params->lda, As, simd_group_id, simd_lane_id);\n  thread loader_b_t loader_b(B, params->ldb, Bs, simd_group_id, simd_lane_id);\n  const short tgp_bm = align_M ? BM : short(min(BM, params->M - c_row));\n  const short tgp_bn = align_N ? BN : short(min(BN, params->N - c_col));\n  int gemm_k_iterations = params->gemm_k_iterations_aligned;\n  if (!align_K) {\n    const int k_last = params->gemm_k_iterations_aligned * BK;\n    const int k_remain = params->K - k_last;\n    const size_t k_jump_a =\n        transpose_a ? params->lda * size_t(k_last) : size_t(k_last);\n    const size_t k_jump_b =\n        transpose_b ? size_t(k_last) : params->ldb * size_t(k_last);\n    loader_a.src += k_jump_a;\n    loader_b.src += k_jump_b;\n    const short2 tile_dims_A =\n        transpose_a ? short2(tgp_bm, k_remain) : short2(k_remain, tgp_bm);\n    const short2 tile_dims_B =\n        transpose_b ? short2(k_remain, tgp_bn) : short2(tgp_bn, k_remain);\n    loader_a.load_safe(tile_dims_A);\n    loader_b.load_safe(tile_dims_B);\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    mma_op.mma(As, Bs);\n    loader_a.src -= k_jump_a;\n    loader_b.src -= k_jump_b;\n  }\n  const TransformAdd<AccumType, AccumType> epilogue_op_add(\n      addmm_params->alpha, addmm_params->beta);\n  const TransformAxpby<AccumType, AccumType> epilogue_op_axpby(\n      addmm_params->alpha, addmm_params->beta);\n  if (align_M && align_N) {\n    for (int k = 0; k < gemm_k_iterations; k++) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      loader_a.load_unsafe();\n      loader_b.load_unsafe();\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      mma_op.mma(As, Bs);\n      loader_a.next();\n      loader_b.next();\n    }\n    threadgroup_barrier(mem_flags::mem_none);\n    if (use_out_source) {\n      if (do_axpby) {\n        mma_op.apply_epilogue(\n            C, addmm_params->ldc, addmm_params->fdc, epilogue_op_axpby);\n      } else {\n        mma_op.apply_epilogue(\n            C, addmm_params->ldc, addmm_params->fdc, epilogue_op_add);\n      }\n    }\n    return mma_op.store_result(D, params->ldd);\n  }\n  else {\n    const int leftover_bk = 0;\n    if ((align_M || tgp_bm == BM) && (align_N || tgp_bn == BN)) {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<true, true, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue(\n              C, addmm_params->ldc, addmm_params->fdc, epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue(\n              C, addmm_params->ldc, addmm_params->fdc, epilogue_op_add);\n        }\n      }\n      return mma_op.store_result(D, params->ldd);\n    } else if (align_N || tgp_bn == BN) {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<false, true, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_add);\n        }\n      }\n      return mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n    } else if (align_M || tgp_bm == BM) {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<true, false, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_add);\n        }\n      }\n      return mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n    } else {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<false, false, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_add);\n        }\n      }\n      return mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n    }\n  }\n}\n"
- " or <"
- ") passed to concatenate"
- "/AppleInternal/Library/BuildRoots/4~B2_augDnvQMcoWH4ScGO_3fEMBBkBEldoGm-DSk/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS26.0.Internal.sdk/usr/local/include/usd/pxr/base/tf/refPtr.h"
- "/AppleInternal/Library/BuildRoots/4~B2_augDnvQMcoWH4ScGO_3fEMBBkBEldoGm-DSk/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS26.0.Internal.sdk/usr/local/include/usd/pxr/usd/sdf/declareHandles.h"
- "/AppleInternal/Library/BuildRoots/4~B2_augDnvQMcoWH4ScGO_3fEMBBkBEldoGm-DSk/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS26.0.Internal.sdk/usr/local/include/usd/pxr/usd/usd/object.h"
- ">."
- "All arrays must have the same shape"
- "Attempting to allocate "
- "Failed to load device library from <"
- "Metal GPU is not available."
- "No arrays provided for stacking"
- "Number of dimensions must be positive."
- "Shapes "
- "[Arange::eval_gpu] Does not support complex64"
- "[concatenate] Invalid axis ("
- "[conv] Spatial dimensions of input after padding "
- "[malloc_or_wait] Unable to allocate "
- "[squeeze] Invalid axis "
- "_do_gather_"
- "atype"
- "bk"
- "bm"
- "bn"
- "g1"
- "itype"
- "k_aligned"
- "mn_aligned"
- "n_channels"
- "otype"
- "small_filter"
- "trans_a"
- "trans_b"
- "uint"
- "wm"
- "wn"

```
