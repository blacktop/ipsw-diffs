## libBNNS.dylib

> `/System/Library/Frameworks/Accelerate.framework/Frameworks/vecLib.framework/libBNNS.dylib`

```diff

-830.60.4.0.0
-  __TEXT.__text: 0x78671c
-  __TEXT.__auth_stubs: 0xd00
-  __TEXT.__gcc_except_tab: 0x14ed8
-  __TEXT.__const: 0xc500
-  __TEXT.__cstring: 0x22462
+830.102.2.0.0
+  __TEXT.__text: 0x86f8bc
+  __TEXT.__auth_stubs: 0x1080
+  __TEXT.__gcc_except_tab: 0x1c854
+  __TEXT.__const: 0xf20f
+  __TEXT.__cstring: 0x267ea
   __TEXT.__oslogstring: 0x1b7
-  __TEXT.__unwind_info: 0x83d0
-  __TEXT.__eh_frame: 0x8090
-  __DATA_CONST.__got: 0xf0
-  __DATA_CONST.__const: 0x2ee8
-  __AUTH_CONST.__const: 0xa778
+  __TEXT.__unwind_info: 0xa2d8
+  __TEXT.__eh_frame: 0x82a0
+  __DATA_CONST.__got: 0xf8
+  __DATA_CONST.__const: 0x3068
+  __AUTH_CONST.__const: 0xca60
   __AUTH_CONST.__cfstring: 0x200
-  __AUTH_CONST.__auth_got: 0x688
+  __AUTH_CONST.__auth_got: 0x848
   __DATA.__data: 0x1c
   __DATA.__bss: 0xc0
   __DATA_DIRTY.__data: 0x4e0

   - /System/Library/PrivateFrameworks/MIL.framework/MIL
   - /usr/lib/libSystem.B.dylib
   - /usr/lib/libc++.1.dylib
-  UUID: A72EC604-B4C6-3D75-81A4-6849510E9F81
-  Functions: 9247
-  Symbols:   514
-  CStrings:  3073
+  UUID: 40EDC6FA-5F12-36EA-8C1C-B2D8F926DF71
+  Functions: 10948
+  Symbols:   596
+  CStrings:  3601
 
Symbols:
+ _BNNSDataLayoutGetRank
+ _BNNSGraphCompileOptionsGetDeduplicationMode
+ _BNNSGraphCompileOptionsGetOutputFD
+ _BNNSGraphCompileOptionsGetPreallocateOutputFile
+ _BNNSGraphCompileOptionsGetWeightsFD
+ _BNNSGraphCompileOptionsGetWeightsPath
+ _BNNSGraphCompileOptionsSetDeduplicationMode
+ _BNNSGraphCompileOptionsSetMessageLogCallback
+ _BNNSGraphCompileOptionsSetMessageLogMask
+ _BNNSGraphCompileOptionsSetOutputFD
+ _BNNSGraphCompileOptionsSetOutputPathWithPermissionsAndProtectionClass
+ _BNNSGraphCompileOptionsSetPreallocateOutputFile
+ _BNNSGraphCompileOptionsSetWeightsFD
+ _BNNSGraphCompileOptionsSetWeightsPathWithPermissions
+ _BNNSGraphContextReplaceAllFunctions
+ _BNNSGraphContextReplaceFunctions
+ _BNNSGraphContextSetMessageLogCallback
+ _BNNSGraphContextSetMessageLogMask
+ _BNNSGraphContextSetWeights
+ _BNNSGraphGetFunctionCount
+ _BNNSGraphGetFunctionNames
+ _BNNSGraphGetPackageVersion
+ __ZN3MIL10IROperator4MakeENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEENS1_10shared_ptrINS1_13unordered_mapIS7_NS8_INS_11IRParameterEEENS1_4hashIS7_EENS1_8equal_toIS7_EENS5_INS1_4pairIKS7_SB_EEEEEEEENS8_INS1_6vectorISB_NS5_ISB_EEEEEENS1_8functionIFNS_16ValidationResultERKNS_11IROperationEEEENSQ_IFNS8_IKNS_7IRValueEEESU_EEE
+ __ZN3MIL10MILContext4MakeEv
+ __ZN3MIL11IRParameter11MakeTypeSetESt16initializer_listIPKNS_11IRValueTypeEE
+ __ZN3MIL11IRParameter4MakeENSt3__110shared_ptrINS1_13unordered_setIPKNS_11IRValueTypeENS1_4hashIS6_EENS1_8equal_toIS6_EENS1_9allocatorIS6_EEEEEE
+ __ZN3MIL16ValidationResultC1ENSt3__110shared_ptrIKNS_8LocationEEENS_16ValidationReasonENS1_12basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEE
+ __ZN3MIL16ValidationResultC1Ev
+ __ZN3MIL16ValidationResultD1Ev
+ __ZN3MIL17IRTensorValueType10MakeScalarERNS_10MILContextENS_10IRDataTypeE
+ __ZN3MIL17IRTensorValueType13MakeWithShapeERNS_10MILContextENS_10IRDataTypeEONSt3__16vectorIPKNS_11IRDimensionENS4_9allocatorIS8_EEEE
+ __ZN3MIL18IRUnknownDimension4MakeERNS_10MILContextEb
+ __ZN3MIL19IRConstantDimension4MakeERNS_10MILContextEy
+ __ZN3MIL7IROpset10RegisterOpEONSt3__110shared_ptrIKNS_10IROperatorEEE
+ __ZN3MIL7IROpsetC2Ev
+ __ZN3MIL7IROpsetD2Ev
+ __ZN4bnns14RegisterOpsetsERN3MIL10MILContextE
+ __ZNK3MIL10IRArgument7GetNameEv
+ __ZNK3MIL10IRProperty13TryAsConstantEv
+ __ZNK3MIL11IROperation16GetParameterTypeERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEEj
+ __ZNK3MIL11IROperation19TryGetParameterTypeERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEEj
+ __ZNK3MIL11IRValueType25TryCastAsMemoryLayoutTypeEv
+ __ZNK3MIL13IRTensorValue11GetDataViewIjEENS_4Util4SpanIKT_Lm18446744073709551615EEEv
+ __ZNK3MIL13IRTensorValue11GetDataViewIsEENS_4Util4SpanIKT_Lm18446744073709551615EEEv
+ __ZNK3MIL13IRTensorValue11GetDataViewItEENS_4Util4SpanIKT_Lm18446744073709551615EEEv
+ __ZNK3MIL13IRTensorValue11GetDataViewIxEENS_4Util4SpanIKT_Lm18446744073709551615EEEv
+ __ZNK3MIL13IRTensorValue11GetDataViewIyEENS_4Util4SpanIKT_Lm18446744073709551615EEEv
+ __ZNK3MIL13IRTensorValue14TryGetDataViewINSt3__112basic_stringIcNS2_11char_traitsIcEENS2_9allocatorIcEEEEEENS2_4pairIbNS_4Util4SpanIKT_Lm18446744073709551615EEEEEv
+ __ZNK3MIL13IRTensorValue14TryGetDataViewIyEENSt3__14pairIbNS_4Util4SpanIKT_Lm18446744073709551615EEEEEv
+ __ZNK3MIL15IRTypedArgument12GetValueTypeEv
+ __ZNK3MIL15IRTypedArgument8GetValueEv
+ __ZNK3MIL18IRConstantProperty19GetInt64ScalarValueEv
+ __ZNK3MIL18IRConstantProperty19GetUInt8ScalarValueEv
+ __ZNK3MIL22IRPixelBufferValueType11IsFixedRankEv
+ __ZNK3MIL22IRPixelBufferValueType18GetPixelFormatTypeEv
+ __ZNK3MIL22IRPixelBufferValueType7GetRankEv
+ __ZNK3MIL22IRPixelBufferValueType8GetShapeEv
+ __ZNK3MIL23IRMemoryLayoutValueType20TryAsPixelBufferTypeEv
+ __ZNK3MIL23IRMemoryLayoutValueType21TryAsTensorBufferTypeEv
+ __ZNK3MIL23IRTensorBufferValueType10GetStridesEv
+ __ZNK3MIL23IRTensorBufferValueType11GetDataTypeEv
+ __ZNK3MIL23IRTensorBufferValueType11IsFixedRankEv
+ __ZNK3MIL23IRTensorBufferValueType20GetInterleaveFactorsEv
+ __ZNK3MIL23IRTensorBufferValueType7GetRankEv
+ __ZNK3MIL23IRTensorBufferValueType8GetShapeEv
+ __ZNK3MIL7IRValue12TryGetScalarItEEPKT_v
+ __ZNK3MIL7IRValue9GetScalarIjEET_v
+ __ZNK3MIL8IRObject14GetLocationPtrEv
+ __ZNKSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE4findEcm
+ __ZNKSt3__119__shared_weak_count13__get_deleterERKSt9type_info
+ __ZNKSt3__16locale9use_facetERNS0_2idE
+ __ZNKSt3__18ios_base6getlocEv
+ __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEED1Ev
+ __ZNSt3__113basic_ostreamIcNS_11char_traitsIcEEE6sentryC1ERS3_
+ __ZNSt3__113basic_ostreamIcNS_11char_traitsIcEEE6sentryD1Ev
+ __ZNSt3__113basic_ostreamIcNS_11char_traitsIcEEElsEm
+ __ZNSt3__15ctypeIcE2idE
+ __ZNSt3__16localeD1Ev
+ __ZNSt3__18ios_base33__set_badbit_and_consider_rethrowEv
+ __ZNSt3__18ios_base5clearEj
+ __ZTIN3MIL7IROpsetE
+ ___toupper
+ _fmod
+ _fstat
+ _ftruncate
+ _nextafterf
+ _open_dprotected_np
- __ZN3MIL6Opsets6Common16CreateMILContextEv
- __ZNK3MIL17IRTensorValueType8IsScalarEv
- __ZNKSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEE3strEv
- __ZNSt3__113basic_ostreamIcNS_11char_traitsIcEEElsEj
- _strcpy
CStrings:
+ "\n  *** INPUTS ***\n"
+ "\n  *** OUTPUTS ***\n"
+ "\n*** FUNCTIONS ***\n"
+ "\n*** SUBGRAPH %zu (fn: %u) ***\n"
+ "\n*** TENSORS ***\n"
+ "\n=== END OF DUMP ===\n"
+ "\n=== USER DATA === \n"
+ "     - %s tensor<%s, ["
+ "    Input Memory : "
+ "    Output Memory : "
+ "    Persistent Memory : "
+ "    Scratch Memory : "
+ "    Workspace : "
+ "  '%s' -> 0x%llx\n"
+ "  - function name table "
+ "  - function table      "
+ "  .layout_info_table_offset = %llu\n"
+ "  persistent memory = %llu bytes\n"
+ "  scratch memory = %llu bytes\n"
+ "  workspace = %llu bytes\n"
+ " (rank mismatch or not first-major)."
+ " = "
+ " = cond(pred="
+ " = copy_interleave(from="
+ " = dequantize(x="
+ " = kbit_dequantize(x="
+ " = list_length()"
+ " = local_response_norm(input="
+ " = quantize(x="
+ " = shape_deduce_concat(values=["
+ " = shape_deduce_reshape(shape = "
+ " = slice_expression(x="
+ " = sliding_windows(x="
+ " = while(loop_vars="
+ " Failed to parse input argument"
+ " as both an input and output. Inputs and outputs must be distict,"
+ " but function signatures don't match"
+ " but required at least "
+ " by equivalence"
+ " by value"
+ " has tensor "
+ " is not permitted"
+ " is too big for reflect mode"
+ " is too big for replicate mode"
+ " multiple times. This is not yet supported"
+ " not in [0,"
+ " outputs, but loop_var only contains "
+ " please an explicit identity op."
+ " referenced multiple times in graph."
+ " scratch_size=%llu"
+ " subgraph=%u"
+ " subgraph=user"
+ " tensors"
+ " to offset "
+ " } = non_maximum_suppression(boxes ="
+ "%s0x%llx[layout_flags=%hhx]"
+ "%s: exec op %zu of %zu\n"
+ "' (reversed name '"
+ "' may only be a tensor, list type or memory layout type."
+ "' not found"
+ "' to body block"
+ "' to condition block"
+ "'%s' -> #%llu\n"
+ "')\n"
+ "'. Must be 'true' or 'false'"
+ "'. Must be one of "
+ "': non-const interleave factors are not supported by mil2bnns"
+ "': unsupported pixel format type"
+ ") < expected_footprint ("
+ ")["
+ ", begin = ["
+ ", bits="
+ ", but expected "
+ ", but found "
+ ", custom_lut="
+ ", for=conv3d_bias)"
+ ", group_size="
+ ", input_interleave="
+ ", iou_threshold="
+ ", lower="
+ ", max_boxes="
+ ", n_selected="
+ ", old_shape = "
+ ", op="
+ ", output_interleave="
+ ", peephole="
+ ", per_class_suppression="
+ ", scale="
+ ", scheme="
+ ", scores="
+ ", size="
+ ", stride="
+ ", transform_matrix="
+ ", upper="
+ ", zero_point="
+ "-th argument"
+ ".flatten2d(axis = "
+ ": Failed to parse `transform_matrix` argument"
+ "; Op Info: "
+ "<operation %llu>"
+ "= affine(x="
+ "= band_part(from="
+ "=conv(conv_trans_manipulate_input(of:"
+ "ASYMMETRIC"
+ "ASYMMETRIC-PRESERVE-ZERO"
+ "An error ocurred while executing relational ops"
+ "Argument iou_threshold is required."
+ "Argument max_boxes is required."
+ "Arithmetic: Cannot find binary kernel"
+ "Arithmetic: Cannot find binary kernel."
+ "Attempted to execute intel computation kernel in unsupported build\n"
+ "BNNS BNNSGraphGetFunctionCount passed graph with unsupported ir_version %x"
+ "BNNS BNNSGraphGetFunctionCount passed invalid graph"
+ "BNNS BNNSGraphGetFunctionNames passed graph with unsupported ir_version %x"
+ "BNNS BNNSGraphGetFunctionNames passed invalid graph"
+ "BNNS BNNSGraphGetInputNames: function named '%s' not found"
+ "BNNS CONVOLUTIONS VERSION2: Error allocating scratch space."
+ "BNNS CONVOLUTIONS VERSION2: Received insufficient scratch size."
+ "BNNS CONVOLUTIONS VERSION2: received null scratch buffer but expected nonzero scratch size."
+ "BNNS Convolution 3D: scratch is too small"
+ "BNNS Convolution Transposed: no scratch is provided"
+ "BNNS Convolution Transposed: scratch is too small"
+ "BNNS Convolutions repackImage : malloc failed"
+ "BNNS Graph Compile: "
+ "BNNS Graph Compile: Allocation failed in construction of conv transpose op"
+ "BNNS Graph Compile: Attempted to parse unsupported op '"
+ "BNNS Graph Compile: failed seek to end of file for writing with error: "
+ "BNNS Graph Compile: failed to open file for writing with error: "
+ "BNNS Graph Compile: failed to preallocate file with error: "
+ "BNNS Graph Compile: failed to preallocate while resizing bnns ir file with error: "
+ "BNNS Graph Compile: failed to write out file with error: "
+ "BNNS Graph Compile: failed write to end of file with error:"
+ "BNNS Graph Compile: mmap of bnns ir file for writing failed with error: "
+ "BNNS Graph Compile: mmap of resize bnns ir file failed with error: "
+ "BNNS Graph Execute: "
+ "BNNS Graph Execute: Unrecognized kernel id "
+ "BNNS Graph Shape Deduction: Unsupported kernel id "
+ "BNNS Graph Shape Deduction: loop variable changes shape across while-loop iterations"
+ "BNNS Graph Shape Deduction: while loop does not support non-exact shape-deduction"
+ "BNNS Graph: Called batch size interface, but could not find function: "
+ "BNNS Graph: Called batch size interface, but graph requires a non-batch dynamic size for argument "
+ "BNNS Graph: Function name must be supplied if multiple functions present"
+ "BNNS Graph: Missing required dynamic size for "
+ "BNNS Graph: Static size specified in sizes["
+ "BNNS Graph: Tried to get workspace size, but could not find function: "
+ "BNNS Graph: args["
+ "BNNS Graph: get_tensor_id_from_name: function '%s' not found"
+ "BNNS Graph: num_inputs: function '%s' not found"
+ "BNNS Graph: num_outputs: function '%s' not found"
+ "BNNS Graph: set_args: function '%s' not found"
+ "BNNS Graph: set_dynamic_shapes: function not found: "
+ "BNNS GraphGetArgPosition: Argument '"
+ "BNNS GraphGetArgPosition: Function '"
+ "BNNS IR describes a graph with persistent memory requirement, BNNSGraphContextExecute() must be used  instead of BNNSGraphExecute()."
+ "BNNS MHA: batch_size > 1 isn't supported"
+ "BNNS Prefix Sum: unsupported data type"
+ "BNNS Quantize: Unsupported data type"
+ "BNNS Random Fill Bernoulli Float: probability (%g) is not within zero and one\n"
+ "BNNS Random Fill Bernoulli Float: probability is not exactly representable as bf16\n"
+ "BNNS Random Fill Bernoulli Float: probability is not exactly representable as fp16\n"
+ "BNNS Resize: unsupported data type"
+ "BNNS Resize: unsupported interpolation method"
+ "BNNS SLICE_EXPR: Unexpected kind for slice_expr.%s[%zu] = 0x%hx\n"
+ "BNNS SLICE_EXPR: bound tensor index is out of range for slice_expr.%s[%zu] = %lld\n"
+ "BNNS SLICE_EXPR: bound tensor is not rank 1 for slice_expr.%s[%zu] = %lld\n"
+ "BNNS SLICE_EXPR: bound tensor out of bounds for slice_expr.%s[%zu] = %lld\n"
+ "BNNS SLICE_EXPR: bound tensor shape dimn out of bounds for slice_expr.%s[%zu] = %lld\n"
+ "BNNS SLICE_EXPR: effective index begin[%zu]=%lld not in range [0, %llu)\n"
+ "BNNS SLICE_EXPR: effective index end[%zu]=%lld not in range (0, %llu]\n"
+ "BNNS SPMPR APPLY: failed to calculate Fully Connected\n"
+ "BNNS Shuffle: Received incorrect scaling factors."
+ "BNNS Shuffle: Received invalid scaling factor."
+ "BNNS Shuffle: input/output layouts should correspond to either BNNSDataLayout3D- LastMajor or FirstMajor."
+ "BNNS TopK: Unsupported best_indices data type for this operation (supported data types are: int32, uint16)\n"
+ "BNNS convolution: Error in shape deduction: dilated kernel size should not be smaller than padded input."
+ "BNNS distribution type not supported"
+ "BNNS fill: input shape must have positive entries"
+ "BNNS fuse_dequant_matvec: encountered nexpected dequantization scheme"
+ "BNNS only supports output type of int32 and uint16 for argmin/argmax"
+ "BNNS resize: input shape data type not supported"
+ "BNNS resize: input shape must have rank 1"
+ "BNNS: BNNSGraphContextReplaceFunctions: tried to link "
+ "BNNS: Failed to parse BNNSOptions, expected dict<string, string>\n"
+ "BNNS: Failed to parse BNNSOptions, unrecognized option '"
+ "BNNS: Failed to parse BNNSOptions.PredefinedOptimization, unsupported value '"
+ "BNNS: Failed to parse BNNSOptions.SingleThread, unsupported value '"
+ "BNNS: Invalid data type for tensor."
+ "BNNS: Provided workspace is not page-aligned."
+ "BNNS: Unsupported behavior: function "
+ "BNNSGraphCompileOptionsGetDeduplicationMode"
+ "BNNSGraphCompileOptionsGetOutputFD"
+ "BNNSGraphCompileOptionsGetPreallocateOutputFile"
+ "BNNSGraphCompileOptionsGetWeightsFD"
+ "BNNSGraphCompileOptionsGetWeightsPath"
+ "BNNSGraphCompileOptionsSetCompileErrorLogCallback"
+ "BNNSGraphCompileOptionsSetDeduplicationMode"
+ "BNNSGraphCompileOptionsSetOutputFD"
+ "BNNSGraphCompileOptionsSetOutputPathWithPermissionsAndProtectionClass"
+ "BNNSGraphCompileOptionsSetPreallocateOutputFile"
+ "BNNSGraphCompileOptionsSetWeightsFD"
+ "BNNSGraphCompileOptionsSetWeightsPathWithPermissions"
+ "BNNSGraphContextEnablePerformanceData: perf_buffer_size (%zu) is too small, requires at least %zu bytes"
+ "BNNSGraphContextPerformanceDataGetExecutionCount called  on context that has not enabled performance data collection"
+ "BNNSGraphContextReplaceFunctions"
+ "BNNSGraphContextSetMessageLogCallback"
+ "BNNSGraphContextSetWeights"
+ "BNNSGraphExecute: Insufficient number of arguments supplied, argument_count="
+ "BNNSGraphExecute: Insufficient workspace supplied. workspace_size="
+ "BNNSGraphGetFunctionNames"
+ "BNNSOptions"
+ "BNNS_Shuffle: scaling factor not provided for batch_to_space."
+ "BNNS_Shuffle: scaling factor not provided for space_to_batch."
+ "BasicNeuralNetworkSubroutines-830.102.2~11"
+ "Batchnorm: Unsupported data type"
+ "Channelnorm: Unsupported data type"
+ "Deduplicated tensor "
+ "Dynamic input shapes were not set before calling "
+ "Einsum does not support more than two inputs"
+ "Einsum does not support the specified data type"
+ "Encountered invalid data type for "
+ "Encountered invalid data type for begin."
+ "Encountered invalid data type for size."
+ "Epsilon must be in {fp32, fp16}"
+ "Expected exactly two attached blocks"
+ "Failed to find loop_var '"
+ "Failed to parse argument '"
+ "Failed to parse beta argument"
+ "Failed to parse input argument"
+ "Failed to parse output argument"
+ "Failed to parse pad argument"
+ "Failed to parse pred argument"
+ "Failed to parse scale argument"
+ "Failed to parse scores argument"
+ "Failed to parse size argument"
+ "Failed to parse values argument"
+ "Failed to parse: "
+ "Fully Connected A1: error running Accelerate1 code path"
+ "Function "
+ "Functions:\n"
+ "Gather op failed"
+ "Gather op falied with out of bound index"
+ "GraphExecute: scratch buffer exceeds workspace."
+ "Input of parse_tensor_to_tensor_buffer must be a tensor"
+ "Input of tensor_buffer_to_tensor must be a tensor buffer"
+ "JS_Conformer"
+ "KERNEL_3D_AVGPOOL"
+ "KERNEL_3D_MAXPOOL"
+ "KERNEL_AFFINE"
+ "KERNEL_BATCHNORM_ST"
+ "KERNEL_CALL"
+ "KERNEL_CHANNELNORM_ST"
+ "KERNEL_CHANNELNORM_V2_ST"
+ "KERNEL_CHANNELWISE_ACTIVATION"
+ "KERNEL_CHANNELWISE_ACTIVATION_DYNAMIC"
+ "KERNEL_CONV_3D"
+ "KERNEL_CONV_3D_AMX1"
+ "KERNEL_CONV_3D_AMX2"
+ "KERNEL_CONV_3D_AMX3"
+ "KERNEL_CONV_3D_NEON"
+ "KERNEL_CONV_TRANS_2D_AMX1"
+ "KERNEL_CONV_TRANS_2D_AMX2"
+ "KERNEL_CONV_TRANS_2D_AMX3"
+ "KERNEL_CONV_TRANS_2D_NEON"
+ "KERNEL_CONV_TRANS_3D_AMX1"
+ "KERNEL_CONV_TRANS_3D_AMX2"
+ "KERNEL_CONV_TRANS_3D_AMX3"
+ "KERNEL_CONV_TRANS_3D_EXPAND_INPUT"
+ "KERNEL_CONV_TRANS_3D_NEON"
+ "KERNEL_COPY_APPEND_SLICE_MGLM_V0"
+ "KERNEL_COPY_APPEND_SLICE_MGLM_V1"
+ "KERNEL_COPY_FLATTEN2D_DYNAMIC"
+ "KERNEL_COPY_INTERLEAVE_DYNAMIC"
+ "KERNEL_COPY_SLICE_EXPR"
+ "KERNEL_COPY_SLIDING_WINDOWS_DYNAMIC"
+ "KERNEL_EINSUM_STRIDED_MATMUL_AMX1_DYNAMIC"
+ "KERNEL_EINSUM_STRIDED_MATMUL_AMX2_DYNAMIC"
+ "KERNEL_EINSUM_STRIDED_MATMUL_AMX3_DYNAMIC"
+ "KERNEL_EINSUM_STRIDED_MATMUL_INTEL64_DYNAMIC"
+ "KERNEL_EINSUM_STRIDED_MATMUL_NEON_DYNAMIC"
+ "KERNEL_INSTANCENORM_ST"
+ "KERNEL_KBIT_DEQUANTIZE_ASYMMETRIC_INT4"
+ "KERNEL_KBIT_DEQUANTIZE_ASYMMETRIC_UINT8_INT8"
+ "KERNEL_KBIT_DEQUANTIZE_SYMMETRIC_INT4"
+ "KERNEL_KBIT_DEQUANTIZE_SYMMETRIC_LUT_INT4"
+ "KERNEL_KBIT_DEQUANTIZE_SYMMETRIC_UINT8_INT8"
+ "KERNEL_KBIT_QUANTIZE_ASYMMETRIC_INT4"
+ "KERNEL_KBIT_QUANTIZE_ASYMMETRIC_PZ_INT4"
+ "KERNEL_KBIT_QUANTIZE_ASYMMETRIC_UINT8_INT8"
+ "KERNEL_KBIT_QUANTIZE_SYMMETRIC_FULL_INT4"
+ "KERNEL_KBIT_QUANTIZE_SYMMETRIC_LUT_INT4"
+ "KERNEL_KBIT_QUANTIZE_SYMMETRIC_RESTRICT_INT4"
+ "KERNEL_KBIT_QUANTIZE_SYMMETRIC_UINT8_INT8"
+ "KERNEL_LAYERNORM_ST"
+ "KERNEL_LAYERNORM_TRAIN_ST"
+ "KERNEL_LAYERNORM_V2"
+ "KERNEL_LAYERNORM_V2_ST"
+ "KERNEL_LAYERNORM_V2_TRAIN"
+ "KERNEL_LAYERNORM_V2_TRAIN_ST"
+ "KERNEL_LOCAL_RESPONSE_NORM"
+ "KERNEL_MATMUL_AMX3_GRP_INT4_V2"
+ "KERNEL_MGLM_MHA_PART0_V1_BF16_FP32_AMX3"
+ "KERNEL_MGLM_MHA_PART1_V1"
+ "KERNEL_MGLM_MHA_PART1_V1_BF16_FP32_1x32x64_AMX3"
+ "KERNEL_MISC_BAND_PART"
+ "KERNEL_MISC_LIST_LENGTH"
+ "KERNEL_NON_MAXIMUM_SUPPRESSION"
+ "KERNEL_SHAPE_DEDUCE_CONCAT"
+ "KERNEL_SHAPE_DEDUCE_RESHAPE"
+ "KERNEL_SUBGRAPH_COND"
+ "KERNEL_SUBGRAPH_WHILE"
+ "Layernorm: Unsupported data type"
+ "Logic"
+ "MGLM"
+ "MGLM_noint4"
+ "MIL program calls function "
+ "NEAREST_NEIGHBOR"
+ "NULL"
+ "Negative strides not supported for slice_expression"
+ "Non-maximum suppression does not support the specified data type"
+ "Only input tensors of rank 3, 4 or 5 are supported."
+ "Only input tensors of rank 4 are supported."
+ "Only int32 data type supported for elem_shape."
+ "Op Execute: unexpected kernel routine"
+ "Output of tensor_buffer_to_tensor must be a tensor"
+ "Pad cannot exceed twice of input rank and must be even"
+ "PredefinedOptimization"
+ "Provided too many shapes to set"
+ "RESIZE NEAREST_NEIGHBOR only supports DEFAULT sampeling_mode"
+ "RESIZE interpolation_mode only supports LINEAR and NEAREST_NEIGHBOR"
+ "RESIZE need shape and resized_dims"
+ "RESIZE resize_dims should be <= 2"
+ "Repeated output tensors are not supported"
+ "SYMMETRIC-CUSTOM"
+ "SYMMETRIC-FULL"
+ "SYMMETRIC-RESTRICTED"
+ "Scatter mode: "
+ "Scatter op failed"
+ "Scatter op falied with out of bound index"
+ "Shape mismatch in indices. Indices must be one-dimensional and satisfy input_size_in_bytes == ceil(output_count * nbits / 8)."
+ "Shape of x is not compatible with value of output_shape"
+ "Subgraph %u: workspace size %zu, lower bound (LOAD) %zu"
+ "This routine should only be used when the subgraph has unbounded dynamic output shapes."
+ "UPSAMPLE_BILINEAR has invalid configuration of align_corners and half_pixel_centers"
+ "Unknown activation function used in channelwise_activation"
+ "Unsupported data type\n"
+ "Unsupported gather type detected"
+ "Unsupported input data type, only {bf16, fp16, fp32} supported"
+ "Unsupported opset for gather op"
+ "Unsupported opset for scatter op"
+ "Unsupported output data type, only {bf16, fp16, fp32} supported"
+ "Unsupported output data type, only {int8, uint8} supported"
+ "Unsupported scheme for kbit_quantize"
+ "Unsupported shape deduction for tensor with count of selected boxes."
+ "WaveRNN"
+ "[ "
+ "] = kbit_quantize(x="
+ "] does not match input MIL size"
+ "] does not match input MIL stride"
+ "] was specified as a sized pointer, but this argument requires a user-specified stride"
+ "] was specified as dynamic but not specified at runtime"
+ "], end = ["
+ "]->layout must be "
+ "]->size["
+ "]->size[0]!=1 for MIL input that is a scalar"
+ "]->stride["
+ "].data_ptr_size ("
+ "]["
+ "__wl_"
+ "_anon_default_beta_for:batchnorm(of:{"
+ "_anon_default_beta_for:instance_norm(of:{"
+ "_anon_default_beta_for:layernorm(of:{"
+ "_anon_default_gamma_for:batchnorm(of:{"
+ "_anon_default_gamma_for:instance_norm(of:{"
+ "_anon_default_gamma_for:layernorm(of:{"
+ "_anon_indices_for_nms_from_boxes={"
+ "_anon_padded(of:"
+ "_anon_reduce_sum_for_nms_score={"
+ "_body_itr"
+ "_cond_itr"
+ "_nonzero_data"
+ "affine"
+ "affine op only supports "
+ "aliasing_opt_pass"
+ "amx2"
+ "amx3"
+ "args["
+ "asymmetric"
+ "asymmetric-integer-zero-point"
+ "asymmetric-preserve-zero"
+ "at unspecified MIL location."
+ "axis specifies a value that is out of range given shape of x"
+ "band_part"
+ "band_part only supports fp32, fp16, bool and bf16 inputs."
+ "batch_dims"
+ "bernoulli random op could be replaced by a constant"
+ "bernoulli random op requires valid probability"
+ "bits"
+ "bnnslinalg"
+ "bnnsx"
+ "bound"
+ "call"
+ "call("
+ "casted("
+ "channelwise_activation alpha and beta must match types"
+ "channelwise_activation only supports fp16, bf16 and fp32 alpha and beta"
+ "channelwise_activation only supports fp16, bf16 and fp32 operands"
+ "channelwise_activation requires input and output types to match"
+ "clip must be in {fp32, fp16}"
+ "combo_mglm_mha_part0_v1"
+ "combo_mglm_mha_part1_v1"
+ "combo_mglm_mha_v1"
+ "concat op only supports FP16, BF16, FP32, BOOL and INT32 operands"
+ "concat with interleave=true requires for all input tensors to match in shapes"
+ "concat(of:"
+ "cond op only supports predicates of type bool"
+ "cond_var"
+ "constexpr_opt_pass"
+ "constexpr_sparse_to_dense"
+ "conv op only supports bf16, fp16 and fp32 operands"
+ "custom_lut"
+ "dequantize"
+ "div"
+ "dyn"
+ "elem_shape does not match expected output dimension for make_list op"
+ "encountered unsupported alpha type"
+ "encountered unsupported input type"
+ "execute_reverse_sequence_op: encountered unsupported indexing type."
+ "expand_dims(of:"
+ "failed to open file error: %s\n"
+ "flatten2d"
+ "flatten2d encountered invalid value for axis"
+ "flatten2d requires input and output types to match"
+ "flatten2d: received invalid axis parameter."
+ "for file descriptor: "
+ "for path: "
+ "func "
+ "fusion_opt_pass"
+ "gather only supports int32, int16, uint16, int8 and uint8 indices"
+ "gather only supports int32, int16, uint16, int8, uint8, fp16, bf16 and fp32 inputs and outputs"
+ "gather_along_indices_with_validate_indices_false"
+ "gather_along_indices_with_validate_indices_true"
+ "gather_nd_with_validate_indices_false"
+ "gather_nd_with_validate_indices_true"
+ "gather_with_validate_indices_false"
+ "gather_with_validate_indices_true"
+ "group_size"
+ "group_size must be positive"
+ "half_pixel_centers"
+ "in_cached_key_values"
+ "in_cached_step"
+ "input rank must match output rank for copy_interleave"
+ "input shape must match output shape for copy_interleave"
+ "input size and padding don't have enough elements to generate output"
+ "input4"
+ "input5"
+ "input_var"
+ "interleaved copy from "
+ "internal gemm: unsupported type combination"
+ "internal.combo_mglm_mha_part1_v1"
+ "internal.combo_mglm_mha_v1"
+ "internal.fill"
+ "internal.flatten2d"
+ "internal.identity_replacing_reverse"
+ "internal.matvec"
+ "internal.mglm_append_slice"
+ "internal.select"
+ "internal.shape"
+ "internal.shape_deduce_concat"
+ "internal.shape_deduce_reshape"
+ "internal.shape_deduce_tile"
+ "internal.slice_expression"
+ "internal::copy"
+ "internal_transposed("
+ "interpolation_mode"
+ "invalid"
+ "invalid output shape for sliding_windows op"
+ "invalid shapes for boxes"
+ "invalid shapes, operation is not broadcastable"
+ "ios17.dequantize"
+ "ios17.quantize"
+ "iou_threshold"
+ "kbit_(de)quantize currently only supports bits=4"
+ "kbit_dequantize"
+ "kbit_dequantize requires input to have same type as input scale"
+ "kbit_dequantize requires input to have same type as input zero_point for 8 bit"
+ "kbit_dequantize requires output to have same type as input scale"
+ "kbit_dequantize requires output to have same type as input zero_point"
+ "kbit_dequantize requires output to have same type as input zero_point for 8 bit"
+ "kbit_quantize"
+ "kbit_quantize requires output scale to have same type as input"
+ "kbit_quantize requires output zero_point to have same type as input"
+ "kbit_quantize requires zero_point output with this scheme"
+ "layer_norm currently does not support mixed-and-matched gamma and beta for training."
+ "linalg_lstsq"
+ "list_length"
+ "list_length op only supports int32 type"
+ "local response norm op for non-contiguous input/output tensors is not yet supported."
+ "local_opt_pass"
+ "local_response_norm"
+ "loop_var"
+ "loop_vars"
+ "lower"
+ "mask"
+ "matvec op requires vector and output with ranks >= 1"
+ "max"
+ "max_boxes"
+ "mglm_append_slice"
+ "mha_scratch(for:"
+ "mil_sparse_bitmap"
+ "min"
+ "mismatching shapes for boxes and scores."
+ "mmap of file failed with error: %s\n"
+ "mod"
+ "n_selected"
+ "non-positive stride is not supported for sliding_windows"
+ "non_maximum_suppression"
+ "non_maximum_suppression requires matching types for boxes, scores and their outputs."
+ "non_maximum_suppression supports fp32, bf16 and fp16 types for boxes and scores."
+ "non_maximum_suppression supports int32 and uint16 types for indexing."
+ "nonzero_data"
+ "normal random op could be replaced by a constant"
+ "normal random op requires valid stddev"
+ "normalization op only supports {FP16, FP32} parameters"
+ "normalization op requires input and output types to match"
+ "not found in reverse map"
+ "offset"
+ "only 1d, 2d and 3d avg pooling are supported"
+ "only 1d, 2d and 3d convolutions are supported"
+ "only 1d, 2d and 3d max pooling are supported"
+ "out of bounds slice_expression: begin["
+ "out of bounds slice_expression: end["
+ "out-of-bounds axis for sliding_windows"
+ "out-of-bounds size, larger than input size at axis, for sliding_windows"
+ "output has different rank to input x"
+ "output shape does not correspond to that of scale"
+ "output shape does not correspond to that of x"
+ "output shape does not match value of output_shape parameter"
+ "output0"
+ "output_boxes"
+ "output_height"
+ "output_scores"
+ "output_width"
+ "pad size for dim "
+ "path"
+ "per_class_suppression"
+ "pre_reverse(of:"
+ "pred"
+ "prob"
+ "qparam"
+ "quantize"
+ "quantized output values has different rank to input x"
+ "quantized output values shape does not correspond to that of x"
+ "quantized output values shapes does not correspond to that of x"
+ "random op only supports fp16, fp32, and bf16 input operands"
+ "random op only supports fp16, fp32, and bf16 output operands"
+ "random_bernoulli"
+ "reflect pad expects pad length is twice as many as input and output rank"
+ "repeat_until_no_change"
+ "replicate pad expects pad length is twice as many as input and output rank"
+ "reshape only supports fp16, bf16, fp32, bool, int16, int8, uint16, uint8 and int32 operands"
+ "reshape(of"
+ "reshape_like only supports fp16,fp32,int32,int16,int8,uint16,uint8,bool,bf16 operands"
+ "resize"
+ "resized_dims"
+ "reverse_sequence op only supports lengths of types: "
+ "rnn/gru/lstm op weights repacking failed"
+ "same_lower"
+ "scalar %.6f\n"
+ "scale has different rank to input x"
+ "scale output has different rank to input x"
+ "scale shape does not correspond to that of x"
+ "scatter only supports fp32, fp16, bf16, int16, uint16, int8, uint8, and int32 types for input and updates, and int32, int16, uint16, int8, uint8 type for indices"
+ "scatter only supports fp32, fp16, int32, int16, uint16, int8, uint8 outputs"
+ "scatter_along_axis_with_validate_indices_false"
+ "scatter_along_axis_with_validate_indices_true"
+ "scatter_nd_with_validate_indices_false"
+ "scatter_nd_with_validate_indices_true"
+ "scatter_with_validate_indices_false"
+ "scatter_with_validate_indices_true"
+ "scheme"
+ "score_threshold"
+ "scores"
+ "shape_deduce_concat"
+ "shape_deduce_reshape"
+ "shape_deduce_tile"
+ "shape_of_"
+ "shape_of_input"
+ "shape_op_replacement_pass"
+ "shaped_like"
+ "slice index out of range: begin[%zu]=%llu end[%zu]=%llu, but shape[%zu] = %llu\n"
+ "slice only supports {int32, int16, int8} begin parameter"
+ "slice only supports {int32, int16, int8} end parameter"
+ "slice only supports {int32, int16, int8} size parameter"
+ "slice only supports {int32, int16, int8} stride parameter"
+ "slice_by_index only support {int32, int16, int8} tensors for begin parameter"
+ "slice_by_index only support {int32, int16, int8} tensors for end parameter"
+ "slice_by_index only support {int32, int16, int8} tensors for stride parameter"
+ "slice_by_index: negative dynamic strides are not supported"
+ "slice_by_size only support {int32, int16, int8} tensors for begin parameter"
+ "slice_by_size only support {int32, int16, int8} tensors for size parameter"
+ "slice_expression"
+ "slice_expression only supports fp16, bf16, fp32 and int32 operands"
+ "slice_expression requires input and output types to match"
+ "slice_slide_pass"
+ "sliced{"
+ "sliding_windows"
+ "softplus_parametric"
+ "spmpr_scratch(for:"
+ "stack_output"
+ "strided matmul op expect inputs and output have the same rank"
+ "strided matmul op requires final dimension to have stride 1"
+ "strided matmul op requires input0 types to be FP16, BF16 or FP32"
+ "strided matmul op requires input1 types to be FP16, BF16 or FP32"
+ "strided matmul op requires output type to be FP16, BF16 or FP32"
+ "strided_matmul(x="
+ "symmetric-full"
+ "symmetric-lut"
+ "symmetric-restricted"
+ "temp_indices"
+ "temp_reduce_max"
+ "tensor %p '%s' %s"
+ "tensor_buffer_to_tensor"
+ "tensor_to_tensor_buffer"
+ "too many interleaved dimensions for copy_interleave"
+ "topk from "
+ "transform_matrix"
+ "uniform random op could be replaced by a constant"
+ "uniform random op requires valid interval"
+ "unsupported type for flatten2d"
+ "upper"
+ "validate_indices"
+ "while_loop"
+ "while_loop does not currently support dynamic shaped loop_vars"
+ "while_loop expected exactly two attached blocks"
+ "while_loop has "
+ "workspace"
+ "workspace(for: cond op_id="
+ "workspace(for: while op_id="
+ "zero_point has different rank to input x"
+ "zero_point output has different rank to input x"
+ "zero_point shape does not correspond to that of x"
+ "zero_point table shape does not correspond to that of x"
+ "{"
+ "{ "
+ "{cond_var for while_loop:"
+ "{input_var for while_loop:"
+ "} param_offset=%llu param_size=%llu"
+ "})"
- "  .workspace_size = %llu bytes\n"
- " - %s tensor<%s, ["
- " input width and padding don't have enough elements to generate output"
- " only 1d and 2d convolutions are supported"
- "%s0x%llx"
- "' is not a tensor"
- "'%s' -> 0x%llx\n"
- "*** INPUTS ***\n"
- "*** OPERATIONS ***\n"
- "*** OUTPUTS ***\n"
- "*** TENSORS ***\n"
- ", begin = "
- ", end = "
- ", lower bound (LOAD) "
- "->"
- ": workspace size "
- "=== END OF DUMP ===\n"
- "=== USER DATA === \n"
- "BNNS Arithmetic Filter Internal: pow not supported for int32"
- "BNNS Arithmetic Filter Internal: truncremainder not supported for int32"
- "BNNS Arithmetic: Cannot find binary kernel\n"
- "BNNS Embedding Apply: Input value %zu is out of range [0, %zu)\n"
- "BNNS Fully Connected A1: error running Accelerate1 code path"
- "BNNS Graph Compile: Attempted to parse unsupported op '%s' (reversed name '%s')\n"
- "BNNS Graph Compile: failed seek to end of bnns ir file for writing with error: %s for path: %s"
- "BNNS Graph Compile: failed to open bnns ir file for writing with error: %s for path: %s"
- "BNNS Graph Compile: failed to write out file with error: %s"
- "BNNS Graph Compile: failed write to end of bnns ir file with error: %s for path: %s"
- "BNNS Graph Compile: mmap of bnns ir file for writing failed with error: %s for path: %s"
- "BNNS Graph Execute: Unrecognized kernel id %u"
- "BNNS Graph Shape Deduction: Unsupported kernel id %x"
- "BNNS Graph: Called batch size interface, but graph requires a non-batch dynamic size for argument %zu"
- "BNNS Graph: Missing required dynamic size for %zu-th argument"
- "BNNS Graph: Static size specified in sizes[%zu][%zu] does not match input MIL size"
- "BNNS Graph: args[%zu]->size[%zu] does not match input MIL size"
- "BNNS Graph: args[%zu]->size[0]!=1 for MIL input that is a scalar"
- "BNNS IR describes a graph with persistent memory requirement, BNNSGraphContextExecute() must be used instead of BNNSGraphExecute()."
- "BNNS Op Execute: unexpected kernel routine"
- "BNNS PAD::REPLICATE: unexpcted input/output shape for pad operation\n"
- "BNNS Prefix sum: unsupported data type"
- "BNNS Shuffle: No support for input/output ranks different than 4."
- "BNNS Shuffle: attempted to apply operation on null input data pointer."
- "BNNS Shuffle: incorrect scaling factor."
- "BNNS Shuffle: input and output sizes not compatible."
- "BNNS Shuffle: input/output layouts should correspond to either BNNSDataLayout4D- LastMajor or FirstMajor."
- "BNNS TopK: Unsupported best_indices data type for this operation (supported data types are: int32)\n"
- "BNNS distribution type not supported\n"
- "BNNS graph batchnorm: Unrecognized data type"
- "BNNS graph channelnorm: Unrecognized data type"
- "BNNS graph layernorm: Unrecognized data type"
- "BNNS only supports output type of int32 for argmin/argmax"
- "BNNS slice_by_size: begin[%zu]=%d not in range [0, %llu)"
- "BNNSGraphContextPerformanceDataGetExecutionCount called on context that has not enabled performance data collection"
- "BNNSGraphExecute: Insufficient number of arguments supplied, argument_count=%zu, but expected %zu."
- "BNNSGraphExecute: Insufficient workspace supplied. workspace_size=%zu but required at least %zu"
- "Cross compilation not currently supported"
- "DefaultShapes"
- "Input Memory : "
- "KERNEL_COPY_APPEND_SLICE_MGLM"
- "MIL program has more than one function, and no specific function specified for compilation"
- "Output Memory : "
- "Persistent Memory : "
- "Scratch Memory : "
- "Shape mismatch in indices. Indices must be one-dimensional and have the same number of elements as the output."
- "Subgraph "
- "TANH_APPROXIMATION"
- "UNKNOWN_KERNEL("
- "Workspace : "
- "_"
- "_anon_padded"
- "_anon_unnamed"
- "args[i]->layout must be %x, but found %x (rank mismatch or not first-major)\n"
- "axes are not contiguous"
- "batch_to_space does not support non-contiguous outputs."
- "batch_to_space encountered an error while using output tensor."
- "batch_to_space encountered an error while using padded tensor"
- "binary_input_slice("
- "concat op only supports FP16, BF16, FP32 and INT32 operands"
- "does not support more than two inputs"
- "gather only supports int32 indices"
- "gather only supports int32, fp16 and fp32 outputs"
- "input height or padding don't have enough rows to generate output"
- "input tensors of ndim other than 4 not supported yet for this shuffling op."
- "input width and padding don't have enough elements to generate output"
- "int32 pow not currently supported"
- "int32 real_div not currently supported"
- "ios17.gather is not yet supported"
- "ios17.scatter is not yet supported"
- "logits"
- "matmul_input1_permute.reshape"
- "matmul_output_permute.reshape"
- "no support for LSTM clip"
- "non_zero op shape deduction resolved to zero in the first axis, which breaks other ops downstream."
- "one_hot op does not currently support data dependent vec_size"
- "only 1d and 2d avg pooling are supported"
- "only 1d and 2d max pooling are supported"
- "optimization_passes"
- "pad op doesn't support mode=\"reflect\""
- "pad op only supports rank <= 4"
- "prelu with different alpha and output types not supported"
- "random op only supports fp16 and fp32 input operands"
- "random op only supports fp16 and fp32 output operands"
- "reshape only supports fp16, bf16, fp32, bool and int32 operands"
- "reshape_like only supports fp16, fp32, bool and int32 operands"
- "reverse_sequence op requires axes type to be INT32"
- "scatter only supports fp32, fp16, bf16 and int32 types for input and updates, and int32 type for indices"
- "scatter only supports fp32, fp16, int32 outputs"
- "slice only supports int32 begin parameter"
- "slice only supports int32 end parameter"
- "slice only supports int32 size parameter"
- "slice only supports int32 stride parameter"
- "slice_by_index only support int32 tensors for begin parameter"
- "slice_by_index only support int32 tensors for end parameter"
- "slice_by_index only support int32 tensors for stride parameter"
- "slice_by_size only support int32 tensors for begin parameter"
- "slice_by_size only support int32 tensors for size parameter"
- "tensor '%s' %s"
- "this shuffling op only supports input tensors of rank between 3 and 5."
- "topk from"
- "} param_offset=%llu param_size=%llu\n"

```
