## MetalPerformanceShadersGraph

> `/System/Library/Frameworks/MetalPerformanceShadersGraph.framework/MetalPerformanceShadersGraph`

```diff

-4.3.0.0.0
-  __TEXT.__text: 0xe9302c
-  __TEXT.__auth_stubs: 0x2420
-  __TEXT.__objc_methlist: 0x5584
-  __TEXT.__const: 0x3af39
-  __TEXT.__gcc_except_tab: 0x33244
-  __TEXT.__cstring: 0xd710d
-  __TEXT.__mpsgraph_init_: 0x68
-  __TEXT.__unwind_info: 0x29c6c
-  __TEXT.__eh_frame: 0x1eb0
-  __TEXT.__objc_classname: 0x18c9
-  __TEXT.__objc_methname: 0x10b97
-  __TEXT.__objc_methtype: 0x41ea
-  __TEXT.__objc_stubs: 0x7cc0
-  __DATA_CONST.__got: 0x270
-  __DATA_CONST.__const: 0x207c0
-  __DATA_CONST.__objc_classlist: 0x678
+4.4.7.0.0
+  __TEXT.__text: 0x10e1aa8
+  __TEXT.__auth_stubs: 0x24c0
+  __TEXT.__objc_methlist: 0x5954
+  __TEXT.__const: 0x45aa0
+  __TEXT.__cstring: 0xea68b
+  __TEXT.__mpsgraph_init_: 0x6c
+  __TEXT.__gcc_except_tab: 0x372d8
+  __TEXT.__unwind_info: 0x27b94
+  __TEXT.__eh_frame: 0x20a0
+  __TEXT.__objc_classname: 0x198b
+  __TEXT.__objc_methname: 0x11a11
+  __TEXT.__objc_methtype: 0x3da1
+  __TEXT.__objc_stubs: 0x86a0
+  __DATA_CONST.__got: 0x2b0
+  __DATA_CONST.__const: 0x240c8
+  __DATA_CONST.__objc_classlist: 0x6a0
   __DATA_CONST.__objc_catlist: 0x0
-  __DATA_CONST.__objc_protolist: 0x20
+  __DATA_CONST.__objc_protolist: 0x30
   __DATA_CONST.__objc_imageinfo: 0x8
-  __DATA_CONST.__objc_const: 0x98e0
-  __DATA_CONST.__objc_selrefs: 0x2b50
-  __DATA_CONST.__objc_arraydata: 0x800
-  __AUTH_CONST.__objc_const: 0x4020
-  __AUTH_CONST.__const: 0x469a8
-  __AUTH_CONST.__cfstring: 0x9ee0
+  __DATA_CONST.__objc_const: 0x9d08
+  __DATA_CONST.__objc_selrefs: 0x2e40
+  __DATA_CONST.__objc_classrefs: 0x8c0
+  __DATA_CONST.__objc_superrefs: 0x168
+  __DATA_CONST.__objc_arraydata: 0x898
+  __AUTH_CONST.__const: 0x4df98
+  __AUTH_CONST.__cfstring: 0xa8c0
+  __AUTH_CONST.__objc_const: 0x42a8
   __AUTH_CONST.__objc_intobj: 0x4c8
-  __AUTH_CONST.__objc_arrayobj: 0xe40
-  __AUTH_CONST.__objc_dictobj: 0xa0
-  __AUTH_CONST.__auth_got: 0x1220
-  __AUTH.__objc_data: 0x4060
-  __AUTH.__data: 0x3590
-  __DATA.__objc_classrefs: 0x870
-  __DATA.__objc_superrefs: 0x150
-  __DATA.__objc_ivar: 0x88c
-  __DATA.__data: 0x59e0
-  __DATA.__thread_vars: 0xc0
-  __DATA.__thread_bss: 0xfc
-  __DATA.__bss: 0x248
-  __DATA.__common: 0x152a
+  __AUTH_CONST.__objc_arrayobj: 0xe58
+  __AUTH_CONST.__objc_dictobj: 0xc8
+  __AUTH_CONST.__auth_got: 0x1270
+  __AUTH.__objc_data: 0x41f0
+  __AUTH.__data: 0x3660
+  __DATA.__objc_ivar: 0x15c
+  __DATA.__data: 0x5f30
+  __DATA.__thread_vars: 0xf0
+  __DATA.__thread_bss: 0x16c
+  __DATA.__common: 0x1612
+  __DATA.__bss: 0x290
   __DATA_DIRTY.__const: 0x78
+  __DATA_DIRTY.__objc_ivar: 0x778
   __DATA_DIRTY.__objc_data: 0x50
   __DATA_DIRTY.__data: 0xa8
-  __DATA_DIRTY.__bss: 0x3068
+  __DATA_DIRTY.__bss: 0x3158
   __DATA_DIRTY.__common: 0x3d8
   - /System/Library/Frameworks/Accelerate.framework/Accelerate
   - /System/Library/Frameworks/CoreFoundation.framework/CoreFoundation

   - /usr/lib/libc++.1.dylib
   - /usr/lib/libncurses.5.4.dylib
   - /usr/lib/libobjc.A.dylib
-  UUID: ECEB8E93-1710-35FF-AB8F-49D2E0C479E3
-  Functions: 57166
-  Symbols:   843
-  CStrings:  31025
+  UUID: 66A7D32D-C1D4-3FB9-A4DC-764C4417AAD0
+  Functions: 58087
+  Symbols:   871
+  CStrings:  33608
 
Symbols:
+ OBJC_IVAR_$_MPSNDArrayDescriptor._preferPackedRows
+ _IOSurfacePropertyKeyAllocSize
+ _IOSurfacePropertyKeyBytesPerElement
+ _OBJC_CLASS_$_IOSurface
+ _OBJC_CLASS_$_MPSGraphExecutableConstantData
+ _OBJC_CLASS_$_MPSGraphExecutableConstantDataRequest
+ _OBJC_CLASS_$_MPSNDArrayAffineInt4Dequantize
+ _OBJC_CLASS_$_MPSNDArrayLUTQuantizationDescriptor
+ _OBJC_CLASS_$_MPSNDArrayMaterializeSparseTensor
+ _OBJC_CLASS_$_MPSNDArrayScaledDotProductAttention
+ _OBJC_CLASS_$_MPSNDArrayStitchedReductionRMSNorm
+ _OBJC_CLASS_$_MPSNDArrayStitchedReductionSoftmax
+ _OBJC_METACLASS_$_MPSGraphExecutableConstantData
+ _OBJC_METACLASS_$_MPSGraphExecutableConstantDataRequest
+ __ZN12MPSKernelDAG12dequantizeOpEP10BaseTensorRKNSt3__16vectorIlNS2_9allocatorIlEEEE11MPSDataTypePKc
+ __ZN12MPSKernelDAG12senaryCoreOpEP10BaseTensorS1_S1_S1_S1_S1_RKNSt3__16vectorIlNS2_9allocatorIlEEEE11MPSDataTypePKc
+ __ZN12MPSKernelDAG13quinaryCoreOpEP10BaseTensorS1_S1_S1_S1_RKNSt3__16vectorIlNS2_9allocatorIlEEEE11MPSDataTypePKc
+ __ZN12MPSKernelDAG14quartaryCoreOpEP10BaseTensorS1_S1_S1_RKNSt3__16vectorIlNS2_9allocatorIlEEEE11MPSDataTypePKc
+ __ZN12MPSKernelDAG5padOpEP10BaseTensorDv4_ib16MPSImageEdgeModeRKNSt3__16vectorIlNS4_9allocatorIlEEEE11MPSDataTypePKc
+ __ZNK3MIL23IRMemoryLayoutValueType20TryAsPixelBufferTypeEv
+ __ZNSt3__111__call_onceERVmPvPFvS2_E
+ __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE6appendEmc
+ __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE6assignEPKcm
+ ___kCFBooleanFalse
+ ___kCFBooleanTrue
+ _dlclose
+ _kANEFModelInput16KAlignmentArrayKey
+ _kANEFModelOutput16KAlignmentArrayKey
+ _malloc_type_posix_memalign
+ _pthread_rwlock_destroy
+ _pthread_rwlock_init
+ _pthread_rwlock_rdlock
+ _pthread_rwlock_unlock
+ _pthread_rwlock_wrlock
+ _system
- __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEC1ERKS5_mmRKS4_
- __ZNSt3__119__shared_mutex_base11lock_sharedEv
- __ZNSt3__119__shared_mutex_base13unlock_sharedEv
- __ZNSt3__119__shared_mutex_base4lockEv
- __ZNSt3__119__shared_mutex_base6unlockEv
- __ZNSt3__119__shared_mutex_baseC1Ev
- _objc_retain_x9
CStrings:
+ "\n        MPSGraphTensor *%s = \n            [graph multiplicationWithPrimaryTensor:%s\n                                   secondaryTensor:%s\n                                              name:%s];\n        "
+ "\n    MPSGraphTensor *%s = \n    [graph gatherWithUpdatesTensor:%s\n                     indicesTensor:%s\n                              axis:%lld\n                   batchDimensions:%d\n                            name:%s];\n    "
+ "\n    MPSGraphTensor *%s = \n    [graph sigmoidWithTensor:%s\n                        name:%s];\n        "
+ "\nINS: "
+ "\nOUTS: "
+ " != 0"
+ " % "
+ " -shared -fPIC "
+ " 1.0 1.0"
+ " ARM_EXCEPTION_STATE64 extends past end of command in "
+ " along control flow edge"
+ " auto"
+ " but only "
+ " count not ARM_EXCEPTION_STATE64_COUNT for flavor number "
+ " decltype(auto)"
+ " deduced from "
+ " disjoint"
+ " for key `value`"
+ " in '.ptrauth_abi_version' directive"
+ " in '.ptrauth_kernel_abi_version' directive"
+ " is not consistent with rank deduced from "
+ " must be 0D tensor of 32-bit unsigned integer or 64-bit unsigned integer values or static-shape defined tensor with shape equal to [1] or unranked tensor of 32-bit unsigned integer or 64-bit unsigned integer values, but got "
+ " must be 0D tensor of 32-bit unsigned integer values or 1D tensor of 32-bit unsigned integer values or unranked tensor of 32-bit unsigned integer values, but got "
+ " must be 0D tensor of 64-bit signed integer values or static-shape defined tensor with shape equal to [1] or unranked tensor of 64-bit signed integer values, but got "
+ " must be 0D tensor of floating point or 32/64-bit signed integer values or static-shape defined tensor with shape equal to [1] or unranked tensor of floating point or 32/64-bit signed integer values, but got "
+ " must be 0D tensor of floating point values or 1D tensor of floating point values or unranked tensor of floating point values, but got "
+ " must be 0D tensor of floating point values or static-shape defined tensor with shape equal to [1] or unranked tensor of floating point values, but got "
+ " must be 0D tensor of mps index type values or 1D tensor of mps index type values or unranked tensor of mps index type values, but got "
+ " must be 0D tensor of mps index type values or static-shape defined tensor with shape equal to [1] or unranked tensor of mps index type values, but got "
+ " must be 0D tensor of mps native type values or static-shape defined tensor with shape equal to [1] or unranked tensor of mps native type values, but got "
+ " must be 0D/1D/2D/3D tensor of floating point or quantized values, but got "
+ " must be 0D/1D/2D/3D tensor of floating point values, but got "
+ " must be 0D/1D/2D/3D tensor of quantized values, but got "
+ " must be MPSX list element type, but got "
+ " must be MPSX list type, but got "
+ " must be static-shape defined tensor with shape equal to [7] or unranked tensor of 32-bit signed integer values, but got "
+ " must be tensor of 1-bit signless integer values, but got "
+ " must be tensor of 16-bit float or 32-bit float values, but got "
+ " must be tensor of 32-bit unsigned integer values, but got "
+ " must be tensor of complex values, but got "
+ " must be tensor of floating point values or tensor of complex values, but got "
+ " must be tensor of int values, but got "
+ " must be tensor of int8 values or memref of int8 values, but got "
+ " must be tensor of mps index type values, but got "
+ " must be tensor of mps native type or complex or quantized values or memref of mps native type or complex or quantized values, but got "
+ " must be tensor of mps native type values or tensor of complex values, but got "
+ " must be tensor of mps native type values or tensor of quantized values, but got "
+ " must be tensor of mps native type values, but got "
+ " must be tensor of palette LUT index values, but got "
+ " must be tensor of quantized values, but got "
+ " must be unranked tensor of 32-bit float values or 1D tensor of 32-bit float values, but got "
+ " must be unranked tensor of 32-bit float values or 2D tensor of 32-bit float values, but got "
+ " must be unranked tensor of 32-bit float values or 3D tensor of 32-bit float values, but got "
+ " must be unranked tensor of 32-bit float values or 4D tensor of 32-bit float values, but got "
+ " must be unranked tensor of 32-bit signed integer values or 1D tensor of 32-bit signed integer values, but got "
+ " must be unranked tensor of 32-bit signed integer values or static-shape defined tensor with shape equal to [5], but got "
+ " must be unranked tensor of 32-bit unsigned integer values or ranked tensor type with rank equal to or greater than 2, but got "
+ " must be unranked tensor of 32/64-bit signed integer values or 1D tensor of 32/64-bit signed integer values, but got "
+ " must be unranked tensor of floating point values or 4D tensor of floating point values, but got "
+ " must be unranked tensor of floating point values or ranked tensor type with rank equal to or greater than 2, but got "
+ " must be unranked tensor of int values or 1D tensor of int values, but got "
+ " must be unranked tensor of mps index type values or 3D tensor of mps index type values, but got "
+ " must be unranked tensor of mps native type or complex values or ranked tensor type with rank equal to or greater than 1, but got "
+ " must be unranked tensor of mps native type values or 4D tensor of mps native type values, but got "
+ " must be unranked tensor of mps native type values or 5D tensor of mps native type values, but got "
+ " must be unranked tensor of mps native type values or ranked tensor type with rank equal to or greater than 5, but got "
+ " must be variadic of 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got "
+ " must be variadic of PDL handle to an `mlir::Attribute`, but got "
+ " must be variadic of PDL handle to an `mlir::Operation *`, but got "
+ " must be variadic of index, but got "
+ " must be variadic of pdl type, but got "
+ " must be variadic of ranked tensor of any type values, but got "
+ " must be variadic of single element or range of PDL handle for an `mlir::Value`, but got "
+ " must be variadic of single element or range of PDL handle to an `mlir::Type`, but got "
+ " must be variadic of tensor of any type values or memref of any type values, but got "
+ " must be variadic of tensor of mps native type values or memref of mps native type values, but got "
+ " must be variadic of tensor of mps native type values or tensor of complex values, but got "
+ " must be variadic of tensor of mps native type values, but got "
+ " must be variadic of unranked tensor of 32/64-bit signed integer values or 1D tensor of 32/64-bit signed integer values, but got "
+ " nneg"
+ " noexcept"
+ " rank "
+ " rank differs from input rank"
+ " rank must be 0 or 1"
+ " requires "
+ " section '"
+ " section offset"
+ " section size"
+ " shape not compatible with input for axis "
+ " shapes not compatible for `axis` dim"
+ " should match dimension size: "
+ " storage available."
+ " token data size"
+ " typename "
+ " was scheduled to run under the inliner, but does not define a symbol table"
+ " which is a ARM_EXCEPTION_STATE64 flavor in "
+ "!_aneRegionCallOpHandler"
+ "!entry->isValid()"
+ "!hasDoubleQuantZeroPoint && \"double quantization zeroPoint not supported\""
+ "!val.use_empty() && \"ANERegionCall output must be followed by MemrefTensor\""
+ "\"S"
+ "%.*g"
+ "%@ : %@"
+ "' but never implemented. This is generally an indication that the dialect extension implementing the interface was never registered."
+ "' does not refer to a registered pass or pass pipeline"
+ "' failed to satisfy constraint: allowed 64-bit signless integer cases: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14"
+ "' failed to satisfy constraint: any type attribute MPSX list element type attribute"
+ "' failed to satisfy constraint: index attribute"
+ "' failed to satisfy constraint: valid CallInlineMode"
+ "'mps.dequantize_lut' op attribute 'axis' failed to satisfy constraint: 32-bit signed integer attribute"
+ "'mps.fast_fourier_transform' op requires attribute 'scaling_mode'"
+ "'mpsx.deinterleave' op attribute 'interleave_factor' failed to satisfy constraint: 32-bit unsigned integer attribute"
+ "'mpsx.deinterleave' op requires attribute 'interleave_factor'"
+ "'mpsx.fp_to_int_clamped' op attribute 'resultElementType' failed to satisfy constraint: any type attribute"
+ "'mpsx.fp_to_int_clamped' op requires attribute 'resultElementType'"
+ "'mpsx.interleave' op attribute 'interleave_factor' failed to satisfy constraint: 32-bit unsigned integer attribute"
+ "'mpsx.interleave' op requires attribute 'interleave_factor'"
+ "'mpsx.make_list' op attribute 'element_type' failed to satisfy constraint: any type attribute MPSX list element type attribute"
+ "'mpsx.make_list' op attribute 'max_size' failed to satisfy constraint: 32-bit unsigned integer attribute"
+ "'mpsx.make_list' op requires attribute 'element_type'"
+ "'mpsx.quantized_matmul' op 'operandSegmentSizes' attribute for specifying operand segments must have 11 elements, but got "
+ "'mpsx.quantized_matmul' op requires attribute 'operandSegmentSizes'"
+ "'mpsx.sparse_dense_matmul' op attribute 'storage_type' failed to satisfy constraint: valid SparseTensorStorage"
+ "'mpsx.sparse_dense_matmul' op attribute 'transpose_lhs' failed to satisfy constraint: bool attribute"
+ "'mpsx.sparse_dense_matmul' op attribute 'transpose_rhs' failed to satisfy constraint: bool attribute"
+ "'mpsx.sparse_dense_matmul' op requires attribute 'storage_type'"
+ "'operandSegmentSizes' attribute for specifying operand segments must have 11 elements, but got "
+ "'vscale_range' maximum must be power-of-two value"
+ "'vscale_range' minimum must be power-of-two value"
+ ") end ("
+ "+avx512f"
+ "+infinity"
+ ", must be in range ["
+ ", platform value is not allowed when the mach header flag MH_SIM_SUPPORT is set"
+ ",(){"
+ "-[MPSGraphCallOp initWithGraph:inputTensors:controlDependencies:outputTypes:symbolName:inliningOption:name:]_block_invoke"
+ "-[MPSGraphCallOp makeMLIROpWithBuilder:symbolTable:inputValues:opInitialization:name:]"
+ "-[MPSGraphExecutable specializedModuleWithDevice:inputShapes:compilationDescriptor:fallingBack:fallbackRuntimeKey:]"
+ "-[MPSGraphExecutableConstantData encodeWithCoder:]"
+ "-[MPSGraphExecutableConstantData initWithCoder:]"
+ "-[MPSGraphExecutableConstantData loadIntoResourceManager:]"
+ "-[MPSGraphExecutableConstantDataRequest encodeWithCoder:]"
+ "-[MPSGraphExecutableConstantDataRequest initWithCoder:]"
+ "-[MPSGraphPackage writeResources:]"
+ "-[MPSGraphStridedSliceUpdateOp makeMLIROpWithBuilder:symbolTable:inputValues:opInitialization:name:]"
+ "-[MPSGraphVariableFromTensorOp makeMLIROpWithBuilder:symbolTable:inputValues:opInitialization:name:]"
+ "-evex512"
+ "-i128:128"
+ "-infinity"
+ "-macosx"
+ "-simulator"
+ "-th init and "
+ "-th loop result have different type: "
+ "-th region iter_arg and "
+ "-th region iter_arg have different type: "
+ "-th yielded value have different type: "
+ "../mlir-mps/third_party/mlir-apple/include/Dialect/MPS/IR/MPSOps.h"
+ "../mlir-mps/third_party/mlir-apple/include/Dialect/MPS/Utils/MPSRawAttributeUtils.h"
+ "../mlir-mps/third_party/mlir-apple/include/Dialect/MPS/Utils/MPSTypeUtils.h"
+ ".ada"
+ ".bridgeos_version_min"
+ ".ptrauth_abi_version"
+ ".ptrauth_kernel_abi_version"
+ ".weak_anti_dep"
+ "// autogenerated main\n\n#import \"modelObjC.h\"\n\nstatic inline size_t sizeofMPSDataTypeLocal(MPSDataType t)\n{\n    return (t & 0xFFFF) >> 3;\n}\n\nMPSGraphTensorData *allocateRandTensorDataWithType(MPSGraphShapedType *type,\n                                                   id<MTLDevice> runDevice,\n                                                   BOOL forceOnes){\n    srand(1);\n    NSInteger numberOfElements = 1;\n    for (int i = 0; i < type.shape.count; i++) {\n        numberOfElements *= type.shape[i].integerValue;\n    }\n    \n    size_t bufferLength = 1;\n    bufferLength *= (((type.shape[type.shape.count - 1].integerValue * sizeofMPSDataTypeLocal(type.dataType) + 63) / 64) * 64);\n    size_t rowBytes = bufferLength;\n    for (int i = 0; i < type.shape.count - 1; i++) {\n        bufferLength *= type.shape[i].integerValue;\n    }\n    \n    id<MTLBuffer> buffer = [runDevice newBufferWithLength:bufferLength options:MTLResourceStorageModeShared];\n    MPSGraphTensorData *tensorData = [[MPSGraphTensorData alloc] initWithMTLBuffer:buffer\n                                                                             shape:type.shape\n                                                                          dataType:type.dataType\n                                                                          rowBytes:rowBytes];\n    \n    MPSNDArray *ndArray = tensorData.mpsndarray;\n                                      \n    void *data = malloc(sizeofMPSDataTypeLocal(type.dataType) * numberOfElements);\n    if(type.dataType == MPSDataTypeFloat32){\n        float *dataIn = (float *)data;\n        for (int i = 0; i < numberOfElements; i++) {\n            dataIn[i] = forceOnes ? 1.0f : ((float)rand()/(float)RAND_MAX);\n        }\n    } else if (type.dataType == MPSDataTypeFloat16){\n        __fp16 *dataIn = (__fp16 *)data;\n        for (int i = 0; i < numberOfElements; i++) {\n            dataIn[i] = forceOnes ? 1.0f : ((float)rand()/(float)RAND_MAX);\n        }\n    } else if(type.dataType == MPSDataTypeComplexFloat32){\n        float *dataIn = (float *)data;\n        for (int i = 0; i < numberOfElements * 2; i++) {\n            dataIn[i] = forceOnes ? 1.0f : ((float)rand()/(float)RAND_MAX);\n        }\n    } else if (type.dataType == MPSDataTypeComplexFloat16){\n        __fp16 *dataIn = (__fp16 *)data;\n        for (int i = 0; i < numberOfElements * 2; i++) {\n            dataIn[i] = forceOnes ? 1.0f : ((float)rand()/(float)RAND_MAX);\n        }\n    } else if (type.dataType == MPSDataTypeInt32){\n        int *dataIn = (int *)data;\n        for (int i = 0; i < numberOfElements; i++) {\n            dataIn[i] = rand();\n        }\n    } else {\n        assert(0);\n    }\n    \n    [ndArray writeBytes:data strideBytes:nil];\n    free(data);\n    \n    return tensorData;\n}\n\nvoid runPerfLoop(id<MTLCommandQueue> commandQueue,\n                 MPSGraphExecutable *graph,\n                 NSArray<MPSGraphTensorData *> * inputArray,\n                 NSArray<MPSGraphTensorData *> * _Nullable fetch,\n                 uint32_t iterations,\n                 uint32_t printEvery)\n{\n    // Build graph descriptor.\n    MPSGraphExecutableExecutionDescriptor *executableExecutionDescriptor = [MPSGraphExecutableExecutionDescriptor new];\n    executableExecutionDescriptor.waitUntilCompleted = YES;\n    // pre-warm up iteration\n    // If the outputs are dynamic, they'll be allocated by the runtime in the first run.\n    fetch = [graph runAsyncWithMTLCommandQueue:commandQueue\n                                   inputsArray:inputArray\n                                  resultsArray:fetch\n                           executionDescriptor:executableExecutionDescriptor];\n    \n    __block double sumOfTimes = 0;\n    CFAbsoluteTime startTime = CFAbsoluteTimeGetCurrent();\n    __block CFAbsoluteTime tic = CFAbsoluteTimeGetCurrent();\n    dispatch_semaphore_t doubleBufferingSemaphore = dispatch_semaphore_create(2);\n    __block uint32_t currentIteration = 0;\n    for (uint32_t i = 1; i <= iterations; i++) {\n        @autoreleasepool {\n            dispatch_semaphore_wait(doubleBufferingSemaphore, DISPATCH_TIME_FOREVER);\n            MPSGraphExecutableExecutionDescriptor *executableExecutionDescriptor = [MPSGraphExecutableExecutionDescriptor new];\n            if (i == iterations)\n                executableExecutionDescriptor.waitUntilCompleted = YES;\n            \n            executableExecutionDescriptor.completionHandler =\n            ^(NSArray<MPSGraphTensorData *> *_Nonnull results, NSError *_Nullable error) {\n                if (error) {\n                    NSLog(@\"%@\\n%@\", error.localizedDescription, error.description);\n                    exit(1);\n                }\n                dispatch_semaphore_signal(doubleBufferingSemaphore);\n                currentIteration++;\n                CFAbsoluteTime toc = CFAbsoluteTimeGetCurrent();\n                double time = 1000 * (toc - tic) / currentIteration;\n                // Exclude the first iteration from the average.\n                sumOfTimes += time;\n                \n                if (currentIteration % printEvery == 0)\n                    NSLog(@\"Time taken by MPSGraph eval in iteration %d = %f ms\\n\", currentIteration, time);\n            };\n            \n            // If the outputs are dynamic, they'll be allocated by the runtime in the first run.\n            fetch = [graph runAsyncWithMTLCommandQueue:commandQueue\n                                           inputsArray:inputArray\n                                          resultsArray:fetch\n                                   executionDescriptor:executableExecutionDescriptor];\n        }\n    }\n    CFAbsoluteTime endTime = CFAbsoluteTimeGetCurrent();\n    double numIter = MAX(iterations, 1);\n    NSLog(@\"Average MPSGraph full execution time = %f ms\\n\", 1000 * (endTime - startTime) / numIter);\n}\n\nBOOL isDynamicShape(MPSGraphShapedType *type){\n    if(!type.shape){\n        return YES;\n    }\n    for (int i = 0; i < type.shape.count; i++) {\n        if(type.shape[i].integerValue == -1){\n            return YES;\n        }\n    }\n    return NO;\n}\n\nint main(int argc, const char * argv[]) {\n    @autoreleasepool {\n        // Replace nil with path to .dat file if model has constants\n        NSString* constantsPath = [[NSBundle mainBundle] pathForResource: @\"modelObjCConstants\" ofType: @\"dat\"];\n        NSData *loadedData = constantsPath ? [NSData dataWithContentsOfFile:constantsPath] : nil;\n        \n        NSMutableArray<MPSGraphShapedType *> *inputTypes = [NSMutableArray new];\n        NSMutableArray<MPSGraphShapedType *> *outputTypes = [NSMutableArray new];\n        \n        MPSGraphExecutable *executable = getMPSGraphExecutable(loadedData, inputTypes, outputTypes, MPSGraphOptimizationLevel1);\n//        printf(\"%s\\n\", [executable debugDescription].UTF8String);\n        \n        id<MTLDevice> device = MTLCreateSystemDefaultDevice();\n        id<MTLCommandQueue> commandQueue = device.newCommandQueue;\n        \n        NSMutableArray<MPSGraphTensorData *> *inputArray = @[].mutableCopy;\n        for(NSUInteger i = 0; i < inputTypes.count; i++){\n            MPSGraphShapedType *type = inputTypes[i];\n            assert(!isDynamicShape(type) && \"Please provide the shape since the model supports dynamic shapes\");\n            [inputArray addObject:allocateRandTensorDataWithType(type, device, NO)];\n        }\n        NSMutableArray<MPSGraphTensorData *> *outputArray = @[].mutableCopy;\n        for(NSUInteger i = 0; i < outputTypes.count; i++){\n            MPSGraphShapedType *type = outputTypes[i];\n            assert(!isDynamicShape(type) && \"Please provide the shape since the model supports dynamic shapes\");\n            [outputArray addObject:allocateRandTensorDataWithType(type, device, NO)];\n        }\n        \n        runPerfLoop(commandQueue, executable, inputArray, outputArray, 100, 20);\n        \n    }\n    return 0;\n}\n        "
+ "/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphConstantData.mm"
+ "/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/Operations/MPSGraphCallOp.mm"
+ "/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/Operations/MPSGraphVariableFromTensorOp.mm"
+ "/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Runtimes/MPSRuntime/Operations/GPUCallOps.mm"
+ "/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Runtimes/MPSRuntime/Operations/GPUQuantizationOps.mm"
+ "/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Runtimes/MPSRuntime/Operations/GPUTransformerOps.mm"
+ "/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Runtimes/MPSRuntime/Operations/RegionOps/ANRegion.mm"
+ "/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Runtimes/MPSRuntime/Operations/RegionOps/CPURegion.mm"
+ "0 && \"folder type doesn't match expected type\""
+ "0.0"
+ "1.1.0"
+ "16@0:8"
+ "18.0.0git"
+ "4.4.0"
+ "4.4.7"
+ "512-bit vector arguments require 'evex512' for AVX512"
+ ": entries("
+ "::friend "
+ "::mlir::mps::CallInlineMode"
+ ":I="
+ "; "
+ "; dbgmarker @ "
+ "<nullptr>"
+ "@\"MPSGraphExecutableConstantData\""
+ "@24@0:8@\"NSCoder\"16"
+ "@32@0:8{ArrayRef<mlir::mps::MPSResourceBlobEntry *>=^^{MPSResourceBlobEntry}Q}16"
+ "@48@0:8Q16{MPSGraphOperatingSystemVersion=qqq}24"
+ "@52@0:8@16@24@32@40B48"
+ "@56@0:8@16@24@32{shared_ptr<ModuleResourcesLoader>=^{ModuleResourcesLoader}^{__shared_weak_count}}40"
+ "@60@0:8@16@24@32@40I48@52"
+ "@60@0:8@16@24@32I40I44I48@52"
+ "@72@0:8@16@24@32@40@48Q56@64"
+ "ANE Overflow Error = %@"
+ "ANE Split supports only supports const positive axis"
+ "ANERegionCallOpHandler"
+ "ANRegion.mm"
+ "ARM64_RELOC_AUTHENTICATED_POINTER"
+ "An ANERegion input must always be produced by TensorToMemref op"
+ "Apple"
+ "ArrayRef"
+ "Attempting to attach an interface to an unregistered operation "
+ "Attribute writable and memory without argmem: write are incompatible!"
+ "Attributes 'minsize and optdebug' are incompatible!"
+ "Attributes 'optdebug and optnone' are incompatible!"
+ "Attributes 'optsize and optdebug' are incompatible!"
+ "Attributes writable and readnone are incompatible!"
+ "Attributes writable and readonly are incompatible!"
+ "CHPE code map"
+ "CHPE entry point ranges"
+ "CHPE metadata"
+ "CHPE redirection metadata"
+ "CSCBSuccess"
+ "Calculating gradients with call operations is not supported."
+ "CallOp"
+ "Callable operation specific optimizer pipelines (in the form of `dialect.op(pipeline)`)"
+ "Callables"
+ "Cannot find the entry function in the module."
+ "Cannot infer channel and depth dimensions for 3D Pooling"
+ "Cannot infer channel and depth dimensions for pooling"
+ "Cannot infer split sizes"
+ "Cannot initialize MC for MetalLib object files."
+ "Cannot locate resource"
+ "Cannot mix controlled and uncontrolled convergence in the same function."
+ "Cannot statically infer split sizes"
+ "Check that tail calls from swifttailcc functions to swifttailcc functions are marked musttail."
+ "Convergence control token can only be used in a convergent call."
+ "Convergence control tokens can only be produced by calls to the convergence control intrinsics."
+ "Convergence region is not well-nested."
+ "Convergence token used by an instruction other than llvm.experimental.convergence.loop in a cycle that does not contain the token's definition."
+ "Convert mps.call operations to func.call which then can be inlined."
+ "Could not create correct resource name"
+ "Could not find an executable for symbol name %@"
+ "Could not serialize a resource"
+ "Couldn't get value for parameter: \"shape\""
+ "Created buffer of length %d"
+ "Cycle heart must dominate all blocks in the cycle."
+ "DISPFlagIsTransparentStepping"
+ "DW_CC_LLVM_M68kRTD"
+ "DW_CC_LLVM_SwiftTail"
+ "DW_LANG_Mojo"
+ "DW_OP_LLVM_user"
+ "DXContainer is not supported yet"
+ "DYNAMIC_HEADER"
+ "Debug Marker points to incorrect instruction?"
+ "DebugProgramValues must not appear before PHI nodes in a block!"
+ "DenseI32ArrayAttr"
+ "Direct calls to amdgpu_cs_chain/amdgpu_cs_chain_preserve functions not allowed. Please use the @llvm.amdgpu.cs.chain intrinsic instead."
+ "ENABLE_OPAQUE_POINTERS_BY_DEFAULT"
+ "Elide printing value of resources if string is too long in chars."
+ "Enable communicating debuginfo positions through iterators, eliminating intrinsics"
+ "Enables the canonicalizations which create operations in MPSX dialect."
+ "Entry intrinsic can occur only in a convergent function."
+ "Entry intrinsic can occur only in the entry block."
+ "Entry intrinsic cannot be preceded by a convergent operation in the same basic block."
+ "Entry or anchor intrinsic cannot have a convergencectrl token operand."
+ "Error importing MLIR module.\n"
+ "Error: NDArray dimension length > INT_MAX"
+ "Error: The executable has already been created with a set of callables, if the callables need to change, please provide them when creating a new MPSGraphExecutable."
+ "Error: The executable has already been created with an entry function name, if the entry function needs to change, please provide it when creating a new MPSGraphExecutable."
+ "Error: executable initialization failed."
+ "Error: the minimum deployment target for visionOS is 1.1.0"
+ "FUNCINDEX"
+ "FUNCTION_LIST"
+ "Failed to allocate 16K aligned memory"
+ "Failed to get the file size."
+ "Failed to lock resource file."
+ "Failed to open resource file."
+ "Failed to read resource file."
+ "GPUCallOps.mm"
+ "Globals cannot contain scalable types"
+ "Got an error: %@"
+ "HDYN"
+ "HSRC"
+ "HSRD"
+ "ILST"
+ "IMPORTED_SYMBOL_LIST"
+ "IR upgrade to version {0} failed"
+ "If a Conv2D/Conv2DDataGradient/DepthwiseConv2D operation has a NCHW data layout and constant weights, convert the weights to HWIO layout."
+ "If a MatMul operation does not have a transpose on the RHS operand, and that operand is a constant, transpose the constant."
+ "If enabled, constants will always be fused irrespectively of whether they might increase memory usage."
+ "If enabled, sdpa with gqa will be canonicalized as a single-dispatch SDPA op."
+ "Index width cannot be larger than pointer width"
+ "Inline function calls"
+ "Inliner"
+ "Inlining callables is not supported"
+ "Inlining is not currently supported"
+ "Intrinsic can only be used from functions with the amdgpu_cs, amdgpu_cs_chain or amdgpu_cs_chain_preserve calling conventions"
+ "Intrinsic can only be used from functions with the amdgpu_cs_chain or amdgpu_cs_chain_preserve calling conventions"
+ "Invalid attribute `inlineMode` in property conversion: "
+ "Invalid attribute `isNegated` in property conversion: "
+ "Invalid attribute `lowerBoundMap` in property conversion: "
+ "Invalid attribute `step` in property conversion: "
+ "Invalid attribute `symbolName` in property conversion: "
+ "Invalid attribute `upperBoundMap` in property conversion: "
+ "Invalid configuration of new-debug-info data found"
+ "Invalid operands count for QuantizedMatmul: %d"
+ "Iter != this->end() && \"StringMap::at failed due to a missing key\""
+ "JSONObjectWithData:options:error:"
+ "LC_BUILD_VERSION and some LC_VERSION_MIN load command also found"
+ "Loop intrinsic cannot be preceded by a convergent operation in the same basic block."
+ "Loop intrinsic must have a convergencectrl token operand."
+ "MH_SIM_SUPPORT files only support LC_BUILD_VERSION and LC_VERSION_MIN_MACOSX load commands"
+ "MLIR Textual PassPipeline Parser"
+ "MLIR18.0.0git"
+ "MODULE_LIST"
+ "MPSCallConversion"
+ "MPSGRAPH_CPU_CODEGEN_CLANG"
+ "MPSGRAPH_CPU_CODEGEN_CLANG EV is set"
+ "MPSGRAPH_DISABLE_PRE_ENCODE_TI"
+ "MPSGRAPH_DISABLE_PRE_ENCODE_TI EV is set."
+ "MPSGRAPH_DUMP_MODULE_FILE_PATH EV is set to %s.\n"
+ "MPSGRAPH_ENABLE_ANE_SHARED_EVENTS"
+ "MPSGRAPH_ENABLE_ANE_SHARED_EVENTS EV is on"
+ "MPSGRAPH_ENABLE_PRE_ENCODE_TI_READ_INPUTS"
+ "MPSGRAPH_ENABLE_PRE_ENCODE_TI_READ_INPUTS EV is set."
+ "MPSGRAPH_ENABLE_RUNTIME_TI_VERIFIERS"
+ "MPSGRAPH_ENABLE_RUNTIME_TI_VERIFIERS EV is set."
+ "MPSGRAPH_SPECIALIZATION_COUNT_MAX"
+ "MPSGRAPH_SPECIALIZATION_COUNT_MAX EV is set to %lu.\n"
+ "MPSGraphCallOp"
+ "MPSGraphCallOp.mm"
+ "MPSGraphComputePackage.mm"
+ "MPSGraphConstantData.mm"
+ "MPSGraphExecutableConstantData"
+ "MPSGraphExecutableConstantDataRequest"
+ "MPSGraphExecutable_Project.h"
+ "MPSGraphMIL.mm"
+ "MPSGraphStridedSliceUpdateOp"
+ "MPSGraphVariableFromTensorOp"
+ "MPSRawAttributeUtils.h"
+ "MPSRefineDynamicShapes"
+ "MPS_GRAPH_ENABLE_ANE_OVERFLOW_AS_FAILURES"
+ "MPS_GRAPH_ENABLE_ANE_OVERFLOW_AS_FAILURES EV is set"
+ "Mach-O 32-bit air"
+ "Mach-O 64-bit AMD GPU"
+ "Mach-O 64-bit Apple GPU"
+ "Mach-O 64-bit Intel GPU"
+ "Mach-O 64-bit NVidia GPU"
+ "Mach-O 64-bit air"
+ "MatMul already has bias"
+ "MatMul has multiple users"
+ "Maximum number of inputs supported by MPS quantization kernels."
+ "Maximum number of iterations when inlining within an SCC"
+ "MemrefType must be >= 1D"
+ "MetalLib"
+ "Mismatch in number of outputs for symbol name %@"
+ "Mismatched storage mode"
+ "Missing callable for symbol name: %@"
+ "ND "
+ "NSCoding"
+ "NSSecureCoding"
+ "Name.ends_with(\"]\") && \"Name doesn't end in the substitution key!\""
+ "Nested callables are not supported and the callable for symbol name %@ contains other callables"
+ "No resources to read."
+ "No resources to write."
+ "Not pointing at correct next marker!"
+ "NumEntries == 0 && \"Node count imbalance!\""
+ "Operands 'pred' and 'pred0' must be equal"
+ "Operands 'y' and 'y0' must be equal"
+ "Optimized No Device Resources Used"
+ "Optimized Resources Used"
+ "Original Resources Used"
+ "PRIVATE_METADATA"
+ "PUBLIC_METADATA"
+ "Parent function doesn't have the same debug-info format"
+ "PreEncode TI: TI Failed: Invalid operation"
+ "PreEncode TI: Verification Failed: Invalid operation"
+ "Provided MPSGraph Package does not support constant data sharing as it was configured for a deployment target which is too old."
+ "QK^T"
+ "Query did not have a ShapedType"
+ "REFLECTION_LIST"
+ "RLST"
+ "R_AARCH64_AUTH_ABS64"
+ "R_ARM_THM_ALU_ABS_G0_NC"
+ "R_ARM_THM_ALU_ABS_G1_NC"
+ "R_ARM_THM_ALU_ABS_G2_NC"
+ "R_ARM_THM_ALU_ABS_G3"
+ "R_LARCH_64_PCREL"
+ "R_LARCH_ADD6"
+ "R_LARCH_ADD_ULEB128"
+ "R_LARCH_ALIGN"
+ "R_LARCH_CFA"
+ "R_LARCH_DELETE"
+ "R_LARCH_PCREL20_S2"
+ "R_LARCH_SUB6"
+ "R_LARCH_SUB_ULEB128"
+ "R_RISCV_SET_ULEB128"
+ "R_RISCV_SUB_ULEB128"
+ "R_WASM_FUNCTION_INDEX_I32"
+ "Refine dynamic shape info to enable more stitching"
+ "Resource Offsets"
+ "Resource Storage Mode"
+ "Runtime Wait count: "
+ "SCRIPT_LIST"
+ "SGPR arguments must have the `inreg` attribute"
+ "SHT_LLVM_LTO"
+ "SLST"
+ "SOURCES"
+ "Softmax should only have a single reduction axes"
+ "Split supports only 1 axis"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::DimOfDestStyleOp]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::FoldComponentNeg<mlir::complex::ImOp, 1>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::FoldComponentNeg<mlir::complex::ReOp, 0>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::FunctionOpInterfaceSignatureConversion]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::MulIMulIConstant]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::RedundantSelectFalse]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::RedundantSelectTrue]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::SelectAndCond]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::SelectAndNotCond]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::SelectNotCond]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::SelectOrCond]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = (anonymous namespace)::SelectOrNotCond]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = ArithBitcast]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = MergeArithBitcast]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = MergeComplexBitcast]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::(anonymous namespace)::ConvertElementwiseBinaryA14Minus<mlir::mps::PowerOp, mlir::anec::ElementwisePower, mlir::anec::Family::A12>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::(anonymous namespace)::ConvertElementwiseBinaryA14Minus<mlir::mps::PowerOp, mlir::anec::ElementwisePower, mlir::anec::Family::A13>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::(anonymous namespace)::ConvertElementwiseBinaryA14Plus<mlir::mps::PowerOp, mlir::anec::ElementwisePower, mlir::anec::Family::A14>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::(anonymous namespace)::ConvertSignBit]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::(anonymous namespace)::ConvertSplit]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::(anonymous namespace)::RegionReturnTypeConversion]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::CallGraph]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::ConvertOpLayoutInterface]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::DistinctAttr]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::DowngraderInterface::Trait<Empty>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::DowngraderInterface]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::MapDynamicShapeOpInterface]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::OpTrait::HasParent<mlir::mpsx::StitchedOp>::Impl<Empty>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::OpTrait::InferTypeOpAdaptor<Empty>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::OpTrait::Stitchable<Empty>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::affine::detail::AffineForOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::affine::detail::AffineLoadOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::affine::detail::AffineStoreOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::affine::detail::AffineVectorLoadOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::affine::detail::AffineVectorStoreOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::anec::(anonymous namespace)::CanonicalizeANEIOWithInterleave<mlir::mpsx::InterleaveOp, mlir::placement::MemrefToTensor>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::anec::(anonymous namespace)::CanonicalizeANEIOWithInterleave<mlir::placement::TensorToMemref, mlir::mpsx::DeinterleaveOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::arith::detail::MaxNumFOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::arith::detail::MaximumFOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::arith::detail::MinNumFOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::arith::detail::MinimumFOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::bufferization::detail::ToMemrefOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::AbsOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::AddOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::AngleOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::Atan2OpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::ConjOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::CosOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::DivOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::ExpOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::Expm1OpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::ImOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::Log1pOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::LogOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::MulOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::NegOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::PowOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::ReOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::RsqrtOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::SignOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::SinOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::SqrtOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::SubOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::TanOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::complex::detail::TanhOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::detail::ModuleOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeAdd]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeConcat]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeConv2DWithConstantOperands]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeIdentity]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeMatMulExpandSqueezeBinary<mlir::mps::AddOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeMatMulExpandSqueezeUnary<mlir::mps::ReluOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeMatMulTransposeConstantRHS]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeMatMulTransposes]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeMultiply]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeNCHWConv2DConstWeightsToHWIO<mlir::mps::Conv2DDataGradientOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeNCHWConv2DConstWeightsToHWIO<mlir::mps::Conv2DOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeNCHWConv2DConstWeightsToHWIO<mlir::mps::DepthwiseConv2DOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeOpsWith0Dims]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizePermute]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeQuantizedMatMulExpandSqueeze]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeQuantizedMatmulTranpose]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeSDPAWithGQA]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeSDPA]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeSparseMatMulTranspose]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeSparseMatMul]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeTileAsBroadcast]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CanonicalizeTranspose]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::CastToFFT]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::FuseRMS<mlir::mps::DivideOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::FuseRMS<mlir::mps::MultiplyOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::LowerDequantizeND]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::LowerInterleave<mlir::mpsx::DeinterleaveOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::LowerInterleave<mlir::mpsx::InterleaveOp>]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::LowerPerAxisDequantize]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::LowerQuantizedMatmul]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::LowerSigmoid]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::MPS_LowerQuantizeWithMPSXQuantDAG]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::CallInlineModeAttr]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::CallOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::DequantizeLUTOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::MaterializeSparseTensorOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::StridedSliceUpdateOpGenericAdaptorBase::Properties]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mpsx::ListType]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::ub::PoisonAttrInterface]"
+ "StringRef llvm::getTypeName() [DesiredTypeName = mlir::ub::detail::PoisonOpGenericAdaptorBase::Properties]"
+ "T@\"MPSGraphExecutableConstantData\",&,V_constantData"
+ "T@\"NSDictionary\",&,V_callables"
+ "T@\"NSDictionary\",&,V_ioSurfaces"
+ "T@\"NSDictionary\",&,V_resourceOffsets"
+ "T@\"NSString\",&,V_entryFunctionName"
+ "T@\"NSString\",?,R,C"
+ "T@\"NSURL\",&,V_mpsgraphPackageURL"
+ "TB,R"
+ "TBAA metadata cannot have 0 operands"
+ "TI failed for `"
+ "The 'convergencectrl' bundle can occur at most once on a call"
+ "The 'convergencectrl' bundle requires exactly one token use."
+ "The default optimizer pipeline used for callables"
+ "The upper and lower limits cannot be the same value"
+ "There's no operation that defines operand 2 of castedOp0"
+ "Trailing DPValues in block"
+ "Trying to extend the constants data with data from a different MPSGraph Package"
+ "Two static convergence token uses in a cycle that does not contain either token's definition."
+ "Unexepected storage-mapping class for ReadOnlyWithRel kind"
+ "Unexpected MIL type"
+ "Unknown encodeMode"
+ "Unrecognized inlining option"
+ "Unsupported region call type"
+ "VARIABLE_LIST"
+ "VGPR arguments must not have the `inreg` attribute"
+ "VLST"
+ "Value for inactive lanes must be a VGPR function argument"
+ "Value for inactive lanes must be a function argument"
+ "Value for inactive lanes must not have the `inreg` attribute"
+ "VariableFromTensorOp"
+ "[coder allowsKeyedCoding]"
+ "[inputs[idx] isKindOfClass:NSNull.class]"
+ "]<"
+ "^([a-z]+)\\.[a-z][0-9]+"
+ "^(e(-[mpi][^-]*)*)((-[^mpi][^-]*)*)$"
+ "^v2\\.([a-z]+)\\.[fi][0-9]+"
+ "^v60@0:8@16{ModuleOp=^{Operation}}24@32@40B48r^v52"
+ "__bf16"
+ "__no_dynamic_broadcast"
+ "_alignBytes can only be 0 or 16384"
+ "_aneRegionCallOpHandler"
+ "_callables"
+ "_callablesDescription"
+ "_constantData"
+ "_disablePreEncodeTI"
+ "_enablePreEncodeTIReadInputs"
+ "_enableRuntimeTIVerifiers"
+ "_entryFunctionName"
+ "_inliningOption"
+ "_ioSurfaces"
+ "_mpsgraphPackageURL"
+ "_outputTypes"
+ "_preEncodeValueTypes.find(value.getAsOpaquePointer()) == _preEncodeValueTypes.end() && \"static type already defined\""
+ "_resourceOffsets"
+ "_specializationCount"
+ "_specializationCountMax"
+ "_swish"
+ "_symbolName"
+ "` but it isn't known in this MLIRContext: the dialect may not be loaded or this operation hasn't been added by the dialect. See also https://mlir.llvm.org/getting_started/Faq/#registered-loaded-dependent-whats-up-with-dialects-management"
+ "` must be a of size 1"
+ "` to inner pipeline"
+ "` with options `"
+ "`) that was promised by dialect '"
+ "`, but folder result is "
+ "`fwd_shape`"
+ "`grad_input`"
+ "`shape(data)`"
+ "`update`"
+ "abs."
+ "addOptimizedNoDeviceResourcesUsed:withSignature:"
+ "addOptimizedResourcesUsed:withSignature:"
+ "addOriginalResourcesUsed:"
+ "addcarryx.u32"
+ "address discriminator isn't a constant integer or expr"
+ "agent"
+ "air32"
+ "air32-apple"
+ "air32_amdgpu"
+ "air32_applegpu"
+ "air32_intelgpu"
+ "air32_nvidiagpu"
+ "air32_v111"
+ "air32_v16"
+ "air32_v18"
+ "air32_v21"
+ "air32_v22"
+ "air32_v23"
+ "air32_v24"
+ "air32_v25"
+ "air32_v26"
+ "air32_v27"
+ "air32_v28"
+ "air64"
+ "air64-apple"
+ "air64-apple-macosx10.11.0"
+ "air64-apple-macosx10.13.0"
+ "air64-apple-macosx11.0.0"
+ "air64-apple-macosx12.0.0"
+ "air64_amdgpu"
+ "air64_applegpu"
+ "air64_intelgpu"
+ "air64_nvidiagpu"
+ "air64_v111"
+ "air64_v16"
+ "air64_v18"
+ "air64_v21"
+ "air64_v22"
+ "air64_v23"
+ "air64_v24"
+ "air64_v25"
+ "air64_v26"
+ "air64_v27"
+ "air64_v28"
+ "air_intersection"
+ "allocationSize"
+ "allowsKeyedCoding"
+ "always-fuse-constants"
+ "amdgcn."
+ "amdgpu_cs_chain"
+ "amdgpu_cs_chain_preserve"
+ "amdgpu_gfx1011"
+ "amdgpu_gfx1012"
+ "amdgpu_gfx1030"
+ "amdgpu_gfx1032"
+ "anec.div"
+ "anec.power"
+ "anec.unrealized_conversion_cast"
+ "arith.maximumf"
+ "arith.maxnumf"
+ "arith.minimumf"
+ "arith.minnumf"
+ "array types must have a base type"
+ "at"
+ "atomic.load.add."
+ "attn_mask"
+ "attribute is not integer type"
+ "automatic"
+ "back"
+ "bad "
+ "bad !vcall_visibility attachment"
+ "bad bug fix version number"
+ "bad file type"
+ "bad magic number"
+ "bad major version number"
+ "bad minor version number"
+ "bad platform type"
+ "bad reserved bytes"
+ "baseAddress"
+ "begin <= end"
+ "boolValue"
+ "bridgeos"
+ "bytesPerElement"
+ "callWithInputTensors:outputTypes:symbolName:inliningOption:name:"
+ "call_inline_mode"
+ "callables"
+ "callablesDescriptor"
+ "cannot apply pattern if the squeeze is applied to the last two dimensions"
+ "cannot apply pattern unless output of cast is complex"
+ "cannot apply squeeze->matmul->add->expand pattern if the expand/squeeze dimension is not 0"
+ "cannot detect a squeeze over a single dimension"
+ "cannot find the function in the MIL program."
+ "cannot get axes"
+ "castedOp1 is not ::mlir::arith::SelectOp type"
+ "castedOp1 is not ::mlir::arith::XOrIOp type"
+ "castedOp1 is not ::mlir::mps::QuantizeOp type"
+ "casting bitwidths do not match"
+ "checkCallablesForModule:"
+ "checking for an interface (`"
+ "clang"
+ "clear"
+ "code"
+ "codegen of the CPU region is not done yet"
+ "codegen_file_path"
+ "codegen_func"
+ "compare:"
+ "complex.bitcast"
+ "condAfterArgTensorData.mpsndarray != nil"
+ "constantData"
+ "constexpr_blockwise_shift_scale"
+ "convergencectrl"
+ "coro.outside.frame"
+ "coro_only_destroy_when_complete"
+ "corrupted bitcode"
+ "could not detect cast -> fft op pattern"
+ "could not find a function for "
+ "could not read resources from MPSGraphPackage"
+ "createBytecodeFromMlirModule:fileHandle:resourceStorageMode:"
+ "createVersionedBytecodeFromMlirModule:packageKey:fileHandle:resourceStorageMode:downgradedModuleCallback:"
+ "current mps dialect version is {0}, can't parse version {1}"
+ "cvtdq2.pd.256"
+ "cvtdq2pd"
+ "cvtsi2ss"
+ "cvtsi642sd"
+ "cvtsi642ss"
+ "cvtss2sd"
+ "darwin.target_variant.triple"
+ "dataUsingEncoding:"
+ "dbg."
+ "decodeObjectOfClass:forKey:"
+ "decodeObjectOfClasses:forKey:"
+ "default-pipeline"
+ "definition subprograms cannot be nested within DICompositeType when enabling ODR"
+ "depth="
+ "dequantizeTensor:scaleTensor:zeroPointTensor:minTensor:dataType:name:"
+ "descriptorWithDictionary:"
+ "deviceRegistry"
+ "dictionaryWithObjects:forKeys:"
+ "different number of inits and region iter_args: "
+ "different number of loop results and region iter_args: "
+ "different number of region iter_args and yielded values: "
+ "dilationRate must be > 0 for all dimensions"
+ "discriminator isn't a constant integer"
+ "distinct"
+ "distinct["
+ "div.sd"
+ "div.ss"
+ "does not match pattern"
+ "dot.i8x16.i7x16."
+ "double quant scale ndarray should have been allocated"
+ "dstOp && \"ANE inputs should always be TensorToMemRef\""
+ "dynamic"
+ "enable-canonicalization-to-mpsx-dialect"
+ "enable-matmul-transpose-constant-rhs"
+ "enable-nchw-conv2d-const-weights-to-hwio"
+ "enable-sdpa-with-gqa"
+ "enable-swifttailcc-musttail-check"
+ "enableGPUQuantOps"
+ "encodeCSCBOp"
+ "encodeEndIf"
+ "encodeEndWhile"
+ "encodeObject:forKey:"
+ "encodeStartElse"
+ "encodeStartIf:offset:comparison:referenceValue:"
+ "encodeStartWhile:offset:comparison:referenceValue:"
+ "encodeWithCoder:"
+ "encountered extra closing ')' creating unbalanced parentheses while parsing pipeline"
+ "encountered unbalanced parentheses while parsing pipeline"
+ "entities 'input' failed to satisfy constraint: 'isn't a native MPS quantization type'"
+ "entities 'input, old' failed to satisfy constraint: 'mpsx dag op for quantize not supported'"
+ "entities 'input, old' failed to satisfy constraint: 'mpsx dag op for quantize supported'"
+ "entities 'ones' failed to satisfy constraint: ''"
+ "entities 'op' failed to satisfy constraint: 'Dequantize has 0D / 1D parameters'"
+ "entry"
+ "entryFunctionName"
+ "epsilon must be a scalar"
+ "expected ',' after parsing pipeline"
+ "expected '<' after distinct ID"
+ "expected '>' to close distinct attribute"
+ "expected '[' after 'distinct'"
+ "expected ']' to close distinct ID"
+ "expected an unsigned 64-bit integer"
+ "expected attribute"
+ "expected dense matrix to be non transposed"
+ "expected distinct ID"
+ "expected integer version in '.ptrauth_abi_version' directive"
+ "expected integer version in '.ptrauth_kernel_abi_version' directive"
+ "expected key entry for inlineMode in DictionaryAttr to set Properties."
+ "expected key entry for lowerBoundMap in DictionaryAttr to set Properties."
+ "expected key entry for operandSegmentSizes in DictionaryAttr to set Properties."
+ "expected key entry for step in DictionaryAttr to set Properties."
+ "expected key entry for symbolName in DictionaryAttr to set Properties."
+ "expected key entry for upperBoundMap in DictionaryAttr to set Properties."
+ "expected offsets to be non-negative, but got "
+ "expected pass pipeline to be wrapped with the anchor operation type, e.g. 'builtin.module(...)'"
+ "expected sizes to be non-negative, but got "
+ "expected string or keyword containing one of the following enum values for attribute 'kind' [addf, addi, assign, maximumf, maxs, maxu, minimumf, mins, minu, mulf, muli, ori, andi, maxnumf, minnumf]"
+ "expected type `"
+ "experimental-debuginfo-iterators"
+ "experimental.vector."
+ "export trie"
+ "extendWithRequest:"
+ "failed to add `"
+ "failed to downgrade: requested target version is {0}, but ND Params in only supported from version {1}"
+ "failed to downgrade: requested target version is {0}, but the op was first defined in version {1}"
+ "failed to parse MPS_CallInlineModeAttr parameter 'value' which is to be a `::mlir::mps::CallInlineMode`"
+ "failed to verify that all of {input, epsilon} have same element type"
+ "failed to verify that condition is scalar or has matching shape"
+ "failed: Transpose of LHS and RHS when matrix multiplying query and key should be false and true respectively"
+ "failed: Transpose of LHS and RHS when matrix multiplying softmax and value should be false and false respectively"
+ "failed: all the operands are expected to return ShapedTypes"
+ "failed: cannot apply pattern for tile operations along a dynamic dimension of the input"
+ "failed: constant axis ({0}) is not within range for the input tensor rank ({1})"
+ "failed: expected a number of channels equal to 1 for pixel format {0}, but found {1}"
+ "failed: expected a number of channels equal to 2 for pixel format {0}, but found {1}"
+ "failed: expected a number of channels equal to 4 for pixel format {0}, but found {1}"
+ "failed: expected key to have at least two dimensions but got {0}"
+ "failed: expected query to have at least two dimensions but got {0}"
+ "failed: expected same element type between the sparse and the dense matrix, but got {0} and {1}"
+ "failed: expected scale to be scalar but it is unranked"
+ "failed: expected value to have at least two dimensions but got {0}"
+ "failed: grouping for the value tensor does not match the one available on the key tensor"
+ "failed: groups / block / batch quantization not supported on ANEC"
+ "failed: index_tensor0 and sparse_values shape mismatch, {0} and {1}"
+ "failed: input must be a ranked shaped type"
+ "failed: key and value must have matching outer dimension but have {0} and {1}"
+ "failed: key heads dimensions do not represent a valid group for the head dimensions available in the query tensor"
+ "failed: mask with type {0} is not broadcastable with the product of query and key transpose of type {1}"
+ "failed: multipliers must be constant to rewrite Tile as a broadcast op"
+ "failed: multipliers need to be applied to a dimension size 1"
+ "failed: pattern can be applied only if the broadcasting dimension is the head dimension"
+ "failed: pattern can match only a single broadcasting dimension"
+ "failed: product of query and key transpose of type {0} cannot be multiplied with value of type {1}"
+ "failed: quantized op requires more inputs than what is supported by the MPS kernel."
+ "failed: query and key must have matching inner dimension but have {0} and {1}"
+ "failed: query and value must have matching inner dimension but have {0} and {1}"
+ "failed: query with type {0} cannot be multiplied with the key (transposed) of type {1}"
+ "failed: reshape does not concatenate groups"
+ "failed: root pattern did not match"
+ "failed: scale should be scalar"
+ "failed: scale should have rank 0 or 1"
+ "failed: softmax axis should be -1 or equal to inputRank - 1"
+ "failed: softmax must be applied to a single axis"
+ "failed: sparse tensor shape is {0}x{1} while index_tensor1 shape is {2}"
+ "failed: the input type must be more specialized than the result type"
+ "failed: the value for scale must be a constant"
+ "failed: value heads dimensions do not represent a valid group for the head dimensions available in the query tensor"
+ "failure: expected at least two operands to represent COO, CSR or CSC sparse formats."
+ "fillcolor"
+ "filled"
+ "findLatestPackageKey"
+ "folder failure for: "
+ "funcindex"
+ "function AIR major version required"
+ "function AIR minor version required"
+ "function Metal major version required"
+ "function Metal minor version required"
+ "function hash required"
+ "function module offset required"
+ "function name required"
+ "function private metadata offset required"
+ "function public metadata offset required"
+ "function type required"
+ "getAttributesFromDescriptors:context:device:"
+ "getCallablesDescription"
+ "getEntryFuncOp"
+ "getEntryFuncOpForModule:"
+ "getKernelAxes"
+ "getLeftDQuantScaleIndexWithLeftAffineQuantizationDescriptor:rightQuantizationDescriptor:"
+ "getLeftMinValIndexWithLeftAffineQuantizationDescriptor:rightQuantizationDescriptor:"
+ "getLeftScaleIndexWithLeftAffineQuantizationDescriptor:rightQuantizationDescriptor:"
+ "getLeftZeroPointIndexWithLeftAffineQuantizationDescriptor:rightQuantizationDescriptor:"
+ "getMutableData"
+ "getNDArrayCount"
+ "getNewRuntimeForDevice:module:inputShapes:compilationDescriptor:fallingBack:fallbackRuntimeKey:"
+ "getOptimizedNoDeviceResourcesUsedLibrary"
+ "getOptimizedResourcesUsedLibrary"
+ "getOriginalResourcesUsed"
+ "getResourceFileName"
+ "getResourceOffsetsLibrary"
+ "getResourceStorageMode"
+ "getResourcesTotalSize"
+ "getRightDQuantScaleIndexWithLeftAffineQuantizationDescriptor:rightQuantizationDescriptor:"
+ "getRightMinValIndexWithLeftAffineQuantizationDescriptor:rightQuantizationDescriptor:"
+ "getRightScaleIndexWithLeftAffineQuantizationDescriptor:rightQuantizationDescriptor:"
+ "getRightZeroPointIndexWithLeftAffineQuantizationDescriptor:rightQuantizationDescriptor:"
+ "getShapesFromTypes_block_invoke"
+ "gfx1150"
+ "gfx1151"
+ "gfx1200"
+ "gfx1201"
+ "global doesn't have an initializer"
+ "global doesn't have type '{ i8*, i32, i64, i64 }'"
+ "global isn't a struct"
+ "global isn't in section \"llvm.ptrauth\""
+ "graalcc"
+ "iOS17.4.0"
+ "ignoring non-zero fill value in "
+ "imported symbol name required"
+ "initForMPSGraphPackageAtURL:device:multipleInputTypes:compilationDescriptor:includeConstantDataForNewSpecializations:"
+ "initForRequest:"
+ "initWithCoder:"
+ "initWithData:encoding:"
+ "initWithDataType:hasZeroPoint:hasMinValue:hasDoubleQuantScale:hasDoubleQuantMinVal:"
+ "initWithDataType:vectorAxes:"
+ "initWithDevice:axis:"
+ "initWithDevice:axis:epsilon:"
+ "initWithDevice:quantizationDescriptor:sourceCount:"
+ "initWithGraph:inputTensors:controlDependencies:outputTypes:symbolName:inliningOption:name:"
+ "initWithGraph:inputTensors:controlDependencies:startMask:endMask:shrinkAxisMask:name:"
+ "initWithGraph:tensor:name:"
+ "initWithProperties:"
+ "initializeForExecution"
+ "inline"
+ "inlineMode"
+ "input and "
+ "input and zeroPoint types must match"
+ "inputShapes"
+ "inputShapesCount"
+ "inputs[idx] == nullptr"
+ "invalid data segment index: "
+ "invalid function alias"
+ "invalid function name"
+ "invalid imported symbol name"
+ "invalid llvm.ptrauth global: "
+ "invalid llvm.used.conditional member"
+ "invalid ptrauth ABI version number"
+ "invalid ptrauth kernel ABI version number"
+ "invalid retained nodes, expected DILocalVariable, DILabel or DIImportedEntity"
+ "invalid specialization script path"
+ "invalid unspecialized MetalLib path"
+ "invalid variable alias"
+ "invalid variable name"
+ "ioSurfaces"
+ "isEqualForModuleTo:"
+ "isMutable() && \"cannot access mutable reference to non-mutable data\""
+ "isNegated"
+ "kandn.w"
+ "key"
+ "key isn't a constant integer"
+ "keyTransposeTensor"
+ "knot.w"
+ "kor.w"
+ "kortestc.w"
+ "kxor.w"
+ "lazyInitWithModuleURL:executableDescriptor:callablesDescription:moduleResourcesLoader:"
+ "le"
+ "llvm.aarch64.sme.ldr.zt"
+ "llvm.aarch64.sme.str.zt"
+ "llvm.aarch64.sve.bfmlslb"
+ "llvm.aarch64.sve.bfmlslb.lane"
+ "llvm.aarch64.sve.bfmlslt"
+ "llvm.aarch64.sve.bfmlslt.lane"
+ "llvm.aarch64.sve.extq.lane"
+ "llvm.aarch64.sve.ld1q.gather.index"
+ "llvm.aarch64.sve.ld1q.gather.scalar.offset"
+ "llvm.aarch64.sve.ld1q.gather.vector.offset"
+ "llvm.aarch64.sve.ld1udq"
+ "llvm.aarch64.sve.ld1uwq"
+ "llvm.aarch64.sve.ld2q.sret"
+ "llvm.aarch64.sve.ld3q.sret"
+ "llvm.aarch64.sve.ld4q.sret"
+ "llvm.aarch64.sve.pmov.to.pred.lane"
+ "llvm.aarch64.sve.pmov.to.pred.lane.zero"
+ "llvm.aarch64.sve.pmov.to.vector.lane.merging"
+ "llvm.aarch64.sve.pmov.to.vector.lane.zeroing"
+ "llvm.aarch64.sve.st1q.scatter.index"
+ "llvm.aarch64.sve.st1q.scatter.scalar.offset"
+ "llvm.aarch64.sve.st1q.scatter.vector.offset"
+ "llvm.aarch64.sve.st1udq"
+ "llvm.aarch64.sve.st1uwq"
+ "llvm.aarch64.sve.st2q"
+ "llvm.aarch64.sve.st3q"
+ "llvm.aarch64.sve.st4q"
+ "llvm.aarch64.sve.tblq"
+ "llvm.aarch64.sve.tbxq"
+ "llvm.aarch64.sve.uzpq1"
+ "llvm.aarch64.sve.uzpq2"
+ "llvm.aarch64.sve.zipq1"
+ "llvm.aarch64.sve.zipq2"
+ "llvm.amdgcn.cs.chain"
+ "llvm.amdgcn.exp2"
+ "llvm.amdgcn.log"
+ "llvm.amdgcn.make.buffer.rsrc"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.add"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.and"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.cmpswap"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.dec"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.fadd"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.fmax"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.fmin"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.inc"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.or"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.smax"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.smin"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.sub"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.swap"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.umax"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.umin"
+ "llvm.amdgcn.raw.ptr.buffer.atomic.xor"
+ "llvm.amdgcn.raw.ptr.buffer.load"
+ "llvm.amdgcn.raw.ptr.buffer.load.format"
+ "llvm.amdgcn.raw.ptr.buffer.load.lds"
+ "llvm.amdgcn.raw.ptr.buffer.store"
+ "llvm.amdgcn.raw.ptr.buffer.store.format"
+ "llvm.amdgcn.raw.ptr.tbuffer.load"
+ "llvm.amdgcn.raw.ptr.tbuffer.store"
+ "llvm.amdgcn.s.bitreplicate"
+ "llvm.amdgcn.s.nop"
+ "llvm.amdgcn.s.quadmask"
+ "llvm.amdgcn.s.ttracedata"
+ "llvm.amdgcn.s.ttracedata.imm"
+ "llvm.amdgcn.s.wqm"
+ "llvm.amdgcn.set.inactive.chain.arg"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.add"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.and"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.cmpswap"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.dec"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.fadd"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.fmax"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.fmin"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.inc"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.or"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.smax"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.smin"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.sub"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.swap"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.umax"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.umin"
+ "llvm.amdgcn.struct.ptr.buffer.atomic.xor"
+ "llvm.amdgcn.struct.ptr.buffer.load"
+ "llvm.amdgcn.struct.ptr.buffer.load.format"
+ "llvm.amdgcn.struct.ptr.buffer.load.lds"
+ "llvm.amdgcn.struct.ptr.buffer.store"
+ "llvm.amdgcn.struct.ptr.buffer.store.format"
+ "llvm.amdgcn.struct.ptr.tbuffer.load"
+ "llvm.amdgcn.struct.ptr.tbuffer.store"
+ "llvm.amdgcn.wave.reduce.umax"
+ "llvm.amdgcn.wave.reduce.umin"
+ "llvm.amdgcn.wmma.bf16.16x16x16.bf16.tied"
+ "llvm.amdgcn.wmma.f16.16x16x16.f16.tied"
+ "llvm.coro.end.results"
+ "llvm.exp10"
+ "llvm.experimental.constrained.ldexp"
+ "llvm.experimental.convergence.anchor"
+ "llvm.experimental.convergence.entry"
+ "llvm.experimental.convergence.loop"
+ "llvm.experimental.cttz.elts"
+ "llvm.experimental.vp.reverse"
+ "llvm.frexp"
+ "llvm.get.fpenv"
+ "llvm.get.fpmode"
+ "llvm.instrprof.mcdc.condbitmap.update"
+ "llvm.instrprof.mcdc.parameters"
+ "llvm.instrprof.mcdc.tvbitmap.update"
+ "llvm.ldexp"
+ "llvm.loongarch.lasx.vext2xv.d.b"
+ "llvm.loongarch.lasx.vext2xv.d.h"
+ "llvm.loongarch.lasx.vext2xv.d.w"
+ "llvm.loongarch.lasx.vext2xv.du.bu"
+ "llvm.loongarch.lasx.vext2xv.du.hu"
+ "llvm.loongarch.lasx.vext2xv.du.wu"
+ "llvm.loongarch.lasx.vext2xv.h.b"
+ "llvm.loongarch.lasx.vext2xv.hu.bu"
+ "llvm.loongarch.lasx.vext2xv.w.b"
+ "llvm.loongarch.lasx.vext2xv.w.h"
+ "llvm.loongarch.lasx.vext2xv.wu.bu"
+ "llvm.loongarch.lasx.vext2xv.wu.hu"
+ "llvm.loongarch.lasx.xbnz.b"
+ "llvm.loongarch.lasx.xbnz.d"
+ "llvm.loongarch.lasx.xbnz.h"
+ "llvm.loongarch.lasx.xbnz.v"
+ "llvm.loongarch.lasx.xbnz.w"
+ "llvm.loongarch.lasx.xbz.b"
+ "llvm.loongarch.lasx.xbz.d"
+ "llvm.loongarch.lasx.xbz.h"
+ "llvm.loongarch.lasx.xbz.v"
+ "llvm.loongarch.lasx.xbz.w"
+ "llvm.loongarch.lasx.xvabsd.b"
+ "llvm.loongarch.lasx.xvabsd.bu"
+ "llvm.loongarch.lasx.xvabsd.d"
+ "llvm.loongarch.lasx.xvabsd.du"
+ "llvm.loongarch.lasx.xvabsd.h"
+ "llvm.loongarch.lasx.xvabsd.hu"
+ "llvm.loongarch.lasx.xvabsd.w"
+ "llvm.loongarch.lasx.xvabsd.wu"
+ "llvm.loongarch.lasx.xvadd.b"
+ "llvm.loongarch.lasx.xvadd.d"
+ "llvm.loongarch.lasx.xvadd.h"
+ "llvm.loongarch.lasx.xvadd.q"
+ "llvm.loongarch.lasx.xvadd.w"
+ "llvm.loongarch.lasx.xvadda.b"
+ "llvm.loongarch.lasx.xvadda.d"
+ "llvm.loongarch.lasx.xvadda.h"
+ "llvm.loongarch.lasx.xvadda.w"
+ "llvm.loongarch.lasx.xvaddi.bu"
+ "llvm.loongarch.lasx.xvaddi.du"
+ "llvm.loongarch.lasx.xvaddi.hu"
+ "llvm.loongarch.lasx.xvaddi.wu"
+ "llvm.loongarch.lasx.xvaddwev.d.w"
+ "llvm.loongarch.lasx.xvaddwev.d.wu"
+ "llvm.loongarch.lasx.xvaddwev.d.wu.w"
+ "llvm.loongarch.lasx.xvaddwev.h.b"
+ "llvm.loongarch.lasx.xvaddwev.h.bu"
+ "llvm.loongarch.lasx.xvaddwev.h.bu.b"
+ "llvm.loongarch.lasx.xvaddwev.q.d"
+ "llvm.loongarch.lasx.xvaddwev.q.du"
+ "llvm.loongarch.lasx.xvaddwev.q.du.d"
+ "llvm.loongarch.lasx.xvaddwev.w.h"
+ "llvm.loongarch.lasx.xvaddwev.w.hu"
+ "llvm.loongarch.lasx.xvaddwev.w.hu.h"
+ "llvm.loongarch.lasx.xvaddwod.d.w"
+ "llvm.loongarch.lasx.xvaddwod.d.wu"
+ "llvm.loongarch.lasx.xvaddwod.d.wu.w"
+ "llvm.loongarch.lasx.xvaddwod.h.b"
+ "llvm.loongarch.lasx.xvaddwod.h.bu"
+ "llvm.loongarch.lasx.xvaddwod.h.bu.b"
+ "llvm.loongarch.lasx.xvaddwod.q.d"
+ "llvm.loongarch.lasx.xvaddwod.q.du"
+ "llvm.loongarch.lasx.xvaddwod.q.du.d"
+ "llvm.loongarch.lasx.xvaddwod.w.h"
+ "llvm.loongarch.lasx.xvaddwod.w.hu"
+ "llvm.loongarch.lasx.xvaddwod.w.hu.h"
+ "llvm.loongarch.lasx.xvand.v"
+ "llvm.loongarch.lasx.xvandi.b"
+ "llvm.loongarch.lasx.xvandn.v"
+ "llvm.loongarch.lasx.xvavg.b"
+ "llvm.loongarch.lasx.xvavg.bu"
+ "llvm.loongarch.lasx.xvavg.d"
+ "llvm.loongarch.lasx.xvavg.du"
+ "llvm.loongarch.lasx.xvavg.h"
+ "llvm.loongarch.lasx.xvavg.hu"
+ "llvm.loongarch.lasx.xvavg.w"
+ "llvm.loongarch.lasx.xvavg.wu"
+ "llvm.loongarch.lasx.xvavgr.b"
+ "llvm.loongarch.lasx.xvavgr.bu"
+ "llvm.loongarch.lasx.xvavgr.d"
+ "llvm.loongarch.lasx.xvavgr.du"
+ "llvm.loongarch.lasx.xvavgr.h"
+ "llvm.loongarch.lasx.xvavgr.hu"
+ "llvm.loongarch.lasx.xvavgr.w"
+ "llvm.loongarch.lasx.xvavgr.wu"
+ "llvm.loongarch.lasx.xvbitclr.b"
+ "llvm.loongarch.lasx.xvbitclr.d"
+ "llvm.loongarch.lasx.xvbitclr.h"
+ "llvm.loongarch.lasx.xvbitclr.w"
+ "llvm.loongarch.lasx.xvbitclri.b"
+ "llvm.loongarch.lasx.xvbitclri.d"
+ "llvm.loongarch.lasx.xvbitclri.h"
+ "llvm.loongarch.lasx.xvbitclri.w"
+ "llvm.loongarch.lasx.xvbitrev.b"
+ "llvm.loongarch.lasx.xvbitrev.d"
+ "llvm.loongarch.lasx.xvbitrev.h"
+ "llvm.loongarch.lasx.xvbitrev.w"
+ "llvm.loongarch.lasx.xvbitrevi.b"
+ "llvm.loongarch.lasx.xvbitrevi.d"
+ "llvm.loongarch.lasx.xvbitrevi.h"
+ "llvm.loongarch.lasx.xvbitrevi.w"
+ "llvm.loongarch.lasx.xvbitsel.v"
+ "llvm.loongarch.lasx.xvbitseli.b"
+ "llvm.loongarch.lasx.xvbitset.b"
+ "llvm.loongarch.lasx.xvbitset.d"
+ "llvm.loongarch.lasx.xvbitset.h"
+ "llvm.loongarch.lasx.xvbitset.w"
+ "llvm.loongarch.lasx.xvbitseti.b"
+ "llvm.loongarch.lasx.xvbitseti.d"
+ "llvm.loongarch.lasx.xvbitseti.h"
+ "llvm.loongarch.lasx.xvbitseti.w"
+ "llvm.loongarch.lasx.xvbsll.v"
+ "llvm.loongarch.lasx.xvbsrl.v"
+ "llvm.loongarch.lasx.xvclo.b"
+ "llvm.loongarch.lasx.xvclo.d"
+ "llvm.loongarch.lasx.xvclo.h"
+ "llvm.loongarch.lasx.xvclo.w"
+ "llvm.loongarch.lasx.xvclz.b"
+ "llvm.loongarch.lasx.xvclz.d"
+ "llvm.loongarch.lasx.xvclz.h"
+ "llvm.loongarch.lasx.xvclz.w"
+ "llvm.loongarch.lasx.xvdiv.b"
+ "llvm.loongarch.lasx.xvdiv.bu"
+ "llvm.loongarch.lasx.xvdiv.d"
+ "llvm.loongarch.lasx.xvdiv.du"
+ "llvm.loongarch.lasx.xvdiv.h"
+ "llvm.loongarch.lasx.xvdiv.hu"
+ "llvm.loongarch.lasx.xvdiv.w"
+ "llvm.loongarch.lasx.xvdiv.wu"
+ "llvm.loongarch.lasx.xvexth.d.w"
+ "llvm.loongarch.lasx.xvexth.du.wu"
+ "llvm.loongarch.lasx.xvexth.h.b"
+ "llvm.loongarch.lasx.xvexth.hu.bu"
+ "llvm.loongarch.lasx.xvexth.q.d"
+ "llvm.loongarch.lasx.xvexth.qu.du"
+ "llvm.loongarch.lasx.xvexth.w.h"
+ "llvm.loongarch.lasx.xvexth.wu.hu"
+ "llvm.loongarch.lasx.xvextl.q.d"
+ "llvm.loongarch.lasx.xvextl.qu.du"
+ "llvm.loongarch.lasx.xvextrins.b"
+ "llvm.loongarch.lasx.xvextrins.d"
+ "llvm.loongarch.lasx.xvextrins.h"
+ "llvm.loongarch.lasx.xvextrins.w"
+ "llvm.loongarch.lasx.xvfadd.d"
+ "llvm.loongarch.lasx.xvfadd.s"
+ "llvm.loongarch.lasx.xvfclass.d"
+ "llvm.loongarch.lasx.xvfclass.s"
+ "llvm.loongarch.lasx.xvfcmp.caf.d"
+ "llvm.loongarch.lasx.xvfcmp.caf.s"
+ "llvm.loongarch.lasx.xvfcmp.ceq.d"
+ "llvm.loongarch.lasx.xvfcmp.ceq.s"
+ "llvm.loongarch.lasx.xvfcmp.cle.d"
+ "llvm.loongarch.lasx.xvfcmp.cle.s"
+ "llvm.loongarch.lasx.xvfcmp.clt.d"
+ "llvm.loongarch.lasx.xvfcmp.clt.s"
+ "llvm.loongarch.lasx.xvfcmp.cne.d"
+ "llvm.loongarch.lasx.xvfcmp.cne.s"
+ "llvm.loongarch.lasx.xvfcmp.cor.d"
+ "llvm.loongarch.lasx.xvfcmp.cor.s"
+ "llvm.loongarch.lasx.xvfcmp.cueq.d"
+ "llvm.loongarch.lasx.xvfcmp.cueq.s"
+ "llvm.loongarch.lasx.xvfcmp.cule.d"
+ "llvm.loongarch.lasx.xvfcmp.cule.s"
+ "llvm.loongarch.lasx.xvfcmp.cult.d"
+ "llvm.loongarch.lasx.xvfcmp.cult.s"
+ "llvm.loongarch.lasx.xvfcmp.cun.d"
+ "llvm.loongarch.lasx.xvfcmp.cun.s"
+ "llvm.loongarch.lasx.xvfcmp.cune.d"
+ "llvm.loongarch.lasx.xvfcmp.cune.s"
+ "llvm.loongarch.lasx.xvfcmp.saf.d"
+ "llvm.loongarch.lasx.xvfcmp.saf.s"
+ "llvm.loongarch.lasx.xvfcmp.seq.d"
+ "llvm.loongarch.lasx.xvfcmp.seq.s"
+ "llvm.loongarch.lasx.xvfcmp.sle.d"
+ "llvm.loongarch.lasx.xvfcmp.sle.s"
+ "llvm.loongarch.lasx.xvfcmp.slt.d"
+ "llvm.loongarch.lasx.xvfcmp.slt.s"
+ "llvm.loongarch.lasx.xvfcmp.sne.d"
+ "llvm.loongarch.lasx.xvfcmp.sne.s"
+ "llvm.loongarch.lasx.xvfcmp.sor.d"
+ "llvm.loongarch.lasx.xvfcmp.sor.s"
+ "llvm.loongarch.lasx.xvfcmp.sueq.d"
+ "llvm.loongarch.lasx.xvfcmp.sueq.s"
+ "llvm.loongarch.lasx.xvfcmp.sule.d"
+ "llvm.loongarch.lasx.xvfcmp.sule.s"
+ "llvm.loongarch.lasx.xvfcmp.sult.d"
+ "llvm.loongarch.lasx.xvfcmp.sult.s"
+ "llvm.loongarch.lasx.xvfcmp.sun.d"
+ "llvm.loongarch.lasx.xvfcmp.sun.s"
+ "llvm.loongarch.lasx.xvfcmp.sune.d"
+ "llvm.loongarch.lasx.xvfcmp.sune.s"
+ "llvm.loongarch.lasx.xvfcvt.h.s"
+ "llvm.loongarch.lasx.xvfcvt.s.d"
+ "llvm.loongarch.lasx.xvfcvth.d.s"
+ "llvm.loongarch.lasx.xvfcvth.s.h"
+ "llvm.loongarch.lasx.xvfcvtl.d.s"
+ "llvm.loongarch.lasx.xvfcvtl.s.h"
+ "llvm.loongarch.lasx.xvfdiv.d"
+ "llvm.loongarch.lasx.xvfdiv.s"
+ "llvm.loongarch.lasx.xvffint.d.l"
+ "llvm.loongarch.lasx.xvffint.d.lu"
+ "llvm.loongarch.lasx.xvffint.s.l"
+ "llvm.loongarch.lasx.xvffint.s.w"
+ "llvm.loongarch.lasx.xvffint.s.wu"
+ "llvm.loongarch.lasx.xvffinth.d.w"
+ "llvm.loongarch.lasx.xvffintl.d.w"
+ "llvm.loongarch.lasx.xvflogb.d"
+ "llvm.loongarch.lasx.xvflogb.s"
+ "llvm.loongarch.lasx.xvfmadd.d"
+ "llvm.loongarch.lasx.xvfmadd.s"
+ "llvm.loongarch.lasx.xvfmax.d"
+ "llvm.loongarch.lasx.xvfmax.s"
+ "llvm.loongarch.lasx.xvfmaxa.d"
+ "llvm.loongarch.lasx.xvfmaxa.s"
+ "llvm.loongarch.lasx.xvfmin.d"
+ "llvm.loongarch.lasx.xvfmin.s"
+ "llvm.loongarch.lasx.xvfmina.d"
+ "llvm.loongarch.lasx.xvfmina.s"
+ "llvm.loongarch.lasx.xvfmsub.d"
+ "llvm.loongarch.lasx.xvfmsub.s"
+ "llvm.loongarch.lasx.xvfmul.d"
+ "llvm.loongarch.lasx.xvfmul.s"
+ "llvm.loongarch.lasx.xvfnmadd.d"
+ "llvm.loongarch.lasx.xvfnmadd.s"
+ "llvm.loongarch.lasx.xvfnmsub.d"
+ "llvm.loongarch.lasx.xvfnmsub.s"
+ "llvm.loongarch.lasx.xvfrecip.d"
+ "llvm.loongarch.lasx.xvfrecip.s"
+ "llvm.loongarch.lasx.xvfrint.d"
+ "llvm.loongarch.lasx.xvfrint.s"
+ "llvm.loongarch.lasx.xvfrintrm.d"
+ "llvm.loongarch.lasx.xvfrintrm.s"
+ "llvm.loongarch.lasx.xvfrintrne.d"
+ "llvm.loongarch.lasx.xvfrintrne.s"
+ "llvm.loongarch.lasx.xvfrintrp.d"
+ "llvm.loongarch.lasx.xvfrintrp.s"
+ "llvm.loongarch.lasx.xvfrintrz.d"
+ "llvm.loongarch.lasx.xvfrintrz.s"
+ "llvm.loongarch.lasx.xvfrsqrt.d"
+ "llvm.loongarch.lasx.xvfrsqrt.s"
+ "llvm.loongarch.lasx.xvfrstp.b"
+ "llvm.loongarch.lasx.xvfrstp.h"
+ "llvm.loongarch.lasx.xvfrstpi.b"
+ "llvm.loongarch.lasx.xvfrstpi.h"
+ "llvm.loongarch.lasx.xvfsqrt.d"
+ "llvm.loongarch.lasx.xvfsqrt.s"
+ "llvm.loongarch.lasx.xvfsub.d"
+ "llvm.loongarch.lasx.xvfsub.s"
+ "llvm.loongarch.lasx.xvftint.l.d"
+ "llvm.loongarch.lasx.xvftint.lu.d"
+ "llvm.loongarch.lasx.xvftint.w.d"
+ "llvm.loongarch.lasx.xvftint.w.s"
+ "llvm.loongarch.lasx.xvftint.wu.s"
+ "llvm.loongarch.lasx.xvftinth.l.s"
+ "llvm.loongarch.lasx.xvftintl.l.s"
+ "llvm.loongarch.lasx.xvftintrm.l.d"
+ "llvm.loongarch.lasx.xvftintrm.w.d"
+ "llvm.loongarch.lasx.xvftintrm.w.s"
+ "llvm.loongarch.lasx.xvftintrmh.l.s"
+ "llvm.loongarch.lasx.xvftintrml.l.s"
+ "llvm.loongarch.lasx.xvftintrne.l.d"
+ "llvm.loongarch.lasx.xvftintrne.w.d"
+ "llvm.loongarch.lasx.xvftintrne.w.s"
+ "llvm.loongarch.lasx.xvftintrneh.l.s"
+ "llvm.loongarch.lasx.xvftintrnel.l.s"
+ "llvm.loongarch.lasx.xvftintrp.l.d"
+ "llvm.loongarch.lasx.xvftintrp.w.d"
+ "llvm.loongarch.lasx.xvftintrp.w.s"
+ "llvm.loongarch.lasx.xvftintrph.l.s"
+ "llvm.loongarch.lasx.xvftintrpl.l.s"
+ "llvm.loongarch.lasx.xvftintrz.l.d"
+ "llvm.loongarch.lasx.xvftintrz.lu.d"
+ "llvm.loongarch.lasx.xvftintrz.w.d"
+ "llvm.loongarch.lasx.xvftintrz.w.s"
+ "llvm.loongarch.lasx.xvftintrz.wu.s"
+ "llvm.loongarch.lasx.xvftintrzh.l.s"
+ "llvm.loongarch.lasx.xvftintrzl.l.s"
+ "llvm.loongarch.lasx.xvhaddw.d.w"
+ "llvm.loongarch.lasx.xvhaddw.du.wu"
+ "llvm.loongarch.lasx.xvhaddw.h.b"
+ "llvm.loongarch.lasx.xvhaddw.hu.bu"
+ "llvm.loongarch.lasx.xvhaddw.q.d"
+ "llvm.loongarch.lasx.xvhaddw.qu.du"
+ "llvm.loongarch.lasx.xvhaddw.w.h"
+ "llvm.loongarch.lasx.xvhaddw.wu.hu"
+ "llvm.loongarch.lasx.xvhsubw.d.w"
+ "llvm.loongarch.lasx.xvhsubw.du.wu"
+ "llvm.loongarch.lasx.xvhsubw.h.b"
+ "llvm.loongarch.lasx.xvhsubw.hu.bu"
+ "llvm.loongarch.lasx.xvhsubw.q.d"
+ "llvm.loongarch.lasx.xvhsubw.qu.du"
+ "llvm.loongarch.lasx.xvhsubw.w.h"
+ "llvm.loongarch.lasx.xvhsubw.wu.hu"
+ "llvm.loongarch.lasx.xvilvh.b"
+ "llvm.loongarch.lasx.xvilvh.d"
+ "llvm.loongarch.lasx.xvilvh.h"
+ "llvm.loongarch.lasx.xvilvh.w"
+ "llvm.loongarch.lasx.xvilvl.b"
+ "llvm.loongarch.lasx.xvilvl.d"
+ "llvm.loongarch.lasx.xvilvl.h"
+ "llvm.loongarch.lasx.xvilvl.w"
+ "llvm.loongarch.lasx.xvinsgr2vr.d"
+ "llvm.loongarch.lasx.xvinsgr2vr.w"
+ "llvm.loongarch.lasx.xvinsve0.d"
+ "llvm.loongarch.lasx.xvinsve0.w"
+ "llvm.loongarch.lasx.xvld"
+ "llvm.loongarch.lasx.xvldi"
+ "llvm.loongarch.lasx.xvldrepl.b"
+ "llvm.loongarch.lasx.xvldrepl.d"
+ "llvm.loongarch.lasx.xvldrepl.h"
+ "llvm.loongarch.lasx.xvldrepl.w"
+ "llvm.loongarch.lasx.xvldx"
+ "llvm.loongarch.lasx.xvmadd.b"
+ "llvm.loongarch.lasx.xvmadd.d"
+ "llvm.loongarch.lasx.xvmadd.h"
+ "llvm.loongarch.lasx.xvmadd.w"
+ "llvm.loongarch.lasx.xvmaddwev.d.w"
+ "llvm.loongarch.lasx.xvmaddwev.d.wu"
+ "llvm.loongarch.lasx.xvmaddwev.d.wu.w"
+ "llvm.loongarch.lasx.xvmaddwev.h.b"
+ "llvm.loongarch.lasx.xvmaddwev.h.bu"
+ "llvm.loongarch.lasx.xvmaddwev.h.bu.b"
+ "llvm.loongarch.lasx.xvmaddwev.q.d"
+ "llvm.loongarch.lasx.xvmaddwev.q.du"
+ "llvm.loongarch.lasx.xvmaddwev.q.du.d"
+ "llvm.loongarch.lasx.xvmaddwev.w.h"
+ "llvm.loongarch.lasx.xvmaddwev.w.hu"
+ "llvm.loongarch.lasx.xvmaddwev.w.hu.h"
+ "llvm.loongarch.lasx.xvmaddwod.d.w"
+ "llvm.loongarch.lasx.xvmaddwod.d.wu"
+ "llvm.loongarch.lasx.xvmaddwod.d.wu.w"
+ "llvm.loongarch.lasx.xvmaddwod.h.b"
+ "llvm.loongarch.lasx.xvmaddwod.h.bu"
+ "llvm.loongarch.lasx.xvmaddwod.h.bu.b"
+ "llvm.loongarch.lasx.xvmaddwod.q.d"
+ "llvm.loongarch.lasx.xvmaddwod.q.du"
+ "llvm.loongarch.lasx.xvmaddwod.q.du.d"
+ "llvm.loongarch.lasx.xvmaddwod.w.h"
+ "llvm.loongarch.lasx.xvmaddwod.w.hu"
+ "llvm.loongarch.lasx.xvmaddwod.w.hu.h"
+ "llvm.loongarch.lasx.xvmax.b"
+ "llvm.loongarch.lasx.xvmax.bu"
+ "llvm.loongarch.lasx.xvmax.d"
+ "llvm.loongarch.lasx.xvmax.du"
+ "llvm.loongarch.lasx.xvmax.h"
+ "llvm.loongarch.lasx.xvmax.hu"
+ "llvm.loongarch.lasx.xvmax.w"
+ "llvm.loongarch.lasx.xvmax.wu"
+ "llvm.loongarch.lasx.xvmaxi.b"
+ "llvm.loongarch.lasx.xvmaxi.bu"
+ "llvm.loongarch.lasx.xvmaxi.d"
+ "llvm.loongarch.lasx.xvmaxi.du"
+ "llvm.loongarch.lasx.xvmaxi.h"
+ "llvm.loongarch.lasx.xvmaxi.hu"
+ "llvm.loongarch.lasx.xvmaxi.w"
+ "llvm.loongarch.lasx.xvmaxi.wu"
+ "llvm.loongarch.lasx.xvmin.b"
+ "llvm.loongarch.lasx.xvmin.bu"
+ "llvm.loongarch.lasx.xvmin.d"
+ "llvm.loongarch.lasx.xvmin.du"
+ "llvm.loongarch.lasx.xvmin.h"
+ "llvm.loongarch.lasx.xvmin.hu"
+ "llvm.loongarch.lasx.xvmin.w"
+ "llvm.loongarch.lasx.xvmin.wu"
+ "llvm.loongarch.lasx.xvmini.b"
+ "llvm.loongarch.lasx.xvmini.bu"
+ "llvm.loongarch.lasx.xvmini.d"
+ "llvm.loongarch.lasx.xvmini.du"
+ "llvm.loongarch.lasx.xvmini.h"
+ "llvm.loongarch.lasx.xvmini.hu"
+ "llvm.loongarch.lasx.xvmini.w"
+ "llvm.loongarch.lasx.xvmini.wu"
+ "llvm.loongarch.lasx.xvmod.b"
+ "llvm.loongarch.lasx.xvmod.bu"
+ "llvm.loongarch.lasx.xvmod.d"
+ "llvm.loongarch.lasx.xvmod.du"
+ "llvm.loongarch.lasx.xvmod.h"
+ "llvm.loongarch.lasx.xvmod.hu"
+ "llvm.loongarch.lasx.xvmod.w"
+ "llvm.loongarch.lasx.xvmod.wu"
+ "llvm.loongarch.lasx.xvmskgez.b"
+ "llvm.loongarch.lasx.xvmskltz.b"
+ "llvm.loongarch.lasx.xvmskltz.d"
+ "llvm.loongarch.lasx.xvmskltz.h"
+ "llvm.loongarch.lasx.xvmskltz.w"
+ "llvm.loongarch.lasx.xvmsknz.b"
+ "llvm.loongarch.lasx.xvmsub.b"
+ "llvm.loongarch.lasx.xvmsub.d"
+ "llvm.loongarch.lasx.xvmsub.h"
+ "llvm.loongarch.lasx.xvmsub.w"
+ "llvm.loongarch.lasx.xvmuh.b"
+ "llvm.loongarch.lasx.xvmuh.bu"
+ "llvm.loongarch.lasx.xvmuh.d"
+ "llvm.loongarch.lasx.xvmuh.du"
+ "llvm.loongarch.lasx.xvmuh.h"
+ "llvm.loongarch.lasx.xvmuh.hu"
+ "llvm.loongarch.lasx.xvmuh.w"
+ "llvm.loongarch.lasx.xvmuh.wu"
+ "llvm.loongarch.lasx.xvmul.b"
+ "llvm.loongarch.lasx.xvmul.d"
+ "llvm.loongarch.lasx.xvmul.h"
+ "llvm.loongarch.lasx.xvmul.w"
+ "llvm.loongarch.lasx.xvmulwev.d.w"
+ "llvm.loongarch.lasx.xvmulwev.d.wu"
+ "llvm.loongarch.lasx.xvmulwev.d.wu.w"
+ "llvm.loongarch.lasx.xvmulwev.h.b"
+ "llvm.loongarch.lasx.xvmulwev.h.bu"
+ "llvm.loongarch.lasx.xvmulwev.h.bu.b"
+ "llvm.loongarch.lasx.xvmulwev.q.d"
+ "llvm.loongarch.lasx.xvmulwev.q.du"
+ "llvm.loongarch.lasx.xvmulwev.q.du.d"
+ "llvm.loongarch.lasx.xvmulwev.w.h"
+ "llvm.loongarch.lasx.xvmulwev.w.hu"
+ "llvm.loongarch.lasx.xvmulwev.w.hu.h"
+ "llvm.loongarch.lasx.xvmulwod.d.w"
+ "llvm.loongarch.lasx.xvmulwod.d.wu"
+ "llvm.loongarch.lasx.xvmulwod.d.wu.w"
+ "llvm.loongarch.lasx.xvmulwod.h.b"
+ "llvm.loongarch.lasx.xvmulwod.h.bu"
+ "llvm.loongarch.lasx.xvmulwod.h.bu.b"
+ "llvm.loongarch.lasx.xvmulwod.q.d"
+ "llvm.loongarch.lasx.xvmulwod.q.du"
+ "llvm.loongarch.lasx.xvmulwod.q.du.d"
+ "llvm.loongarch.lasx.xvmulwod.w.h"
+ "llvm.loongarch.lasx.xvmulwod.w.hu"
+ "llvm.loongarch.lasx.xvmulwod.w.hu.h"
+ "llvm.loongarch.lasx.xvneg.b"
+ "llvm.loongarch.lasx.xvneg.d"
+ "llvm.loongarch.lasx.xvneg.h"
+ "llvm.loongarch.lasx.xvneg.w"
+ "llvm.loongarch.lasx.xvnor.v"
+ "llvm.loongarch.lasx.xvnori.b"
+ "llvm.loongarch.lasx.xvor.v"
+ "llvm.loongarch.lasx.xvori.b"
+ "llvm.loongarch.lasx.xvorn.v"
+ "llvm.loongarch.lasx.xvpackev.b"
+ "llvm.loongarch.lasx.xvpackev.d"
+ "llvm.loongarch.lasx.xvpackev.h"
+ "llvm.loongarch.lasx.xvpackev.w"
+ "llvm.loongarch.lasx.xvpackod.b"
+ "llvm.loongarch.lasx.xvpackod.d"
+ "llvm.loongarch.lasx.xvpackod.h"
+ "llvm.loongarch.lasx.xvpackod.w"
+ "llvm.loongarch.lasx.xvpcnt.b"
+ "llvm.loongarch.lasx.xvpcnt.d"
+ "llvm.loongarch.lasx.xvpcnt.h"
+ "llvm.loongarch.lasx.xvpcnt.w"
+ "llvm.loongarch.lasx.xvperm.w"
+ "llvm.loongarch.lasx.xvpermi.d"
+ "llvm.loongarch.lasx.xvpermi.q"
+ "llvm.loongarch.lasx.xvpermi.w"
+ "llvm.loongarch.lasx.xvpickev.b"
+ "llvm.loongarch.lasx.xvpickev.d"
+ "llvm.loongarch.lasx.xvpickev.h"
+ "llvm.loongarch.lasx.xvpickev.w"
+ "llvm.loongarch.lasx.xvpickod.b"
+ "llvm.loongarch.lasx.xvpickod.d"
+ "llvm.loongarch.lasx.xvpickod.h"
+ "llvm.loongarch.lasx.xvpickod.w"
+ "llvm.loongarch.lasx.xvpickve.d"
+ "llvm.loongarch.lasx.xvpickve.d.f"
+ "llvm.loongarch.lasx.xvpickve.w"
+ "llvm.loongarch.lasx.xvpickve.w.f"
+ "llvm.loongarch.lasx.xvpickve2gr.d"
+ "llvm.loongarch.lasx.xvpickve2gr.du"
+ "llvm.loongarch.lasx.xvpickve2gr.w"
+ "llvm.loongarch.lasx.xvpickve2gr.wu"
+ "llvm.loongarch.lasx.xvrepl128vei.b"
+ "llvm.loongarch.lasx.xvrepl128vei.d"
+ "llvm.loongarch.lasx.xvrepl128vei.h"
+ "llvm.loongarch.lasx.xvrepl128vei.w"
+ "llvm.loongarch.lasx.xvreplgr2vr.b"
+ "llvm.loongarch.lasx.xvreplgr2vr.d"
+ "llvm.loongarch.lasx.xvreplgr2vr.h"
+ "llvm.loongarch.lasx.xvreplgr2vr.w"
+ "llvm.loongarch.lasx.xvrepli.b"
+ "llvm.loongarch.lasx.xvrepli.d"
+ "llvm.loongarch.lasx.xvrepli.h"
+ "llvm.loongarch.lasx.xvrepli.w"
+ "llvm.loongarch.lasx.xvreplve.b"
+ "llvm.loongarch.lasx.xvreplve.d"
+ "llvm.loongarch.lasx.xvreplve.h"
+ "llvm.loongarch.lasx.xvreplve.w"
+ "llvm.loongarch.lasx.xvreplve0.b"
+ "llvm.loongarch.lasx.xvreplve0.d"
+ "llvm.loongarch.lasx.xvreplve0.h"
+ "llvm.loongarch.lasx.xvreplve0.q"
+ "llvm.loongarch.lasx.xvreplve0.w"
+ "llvm.loongarch.lasx.xvrotr.b"
+ "llvm.loongarch.lasx.xvrotr.d"
+ "llvm.loongarch.lasx.xvrotr.h"
+ "llvm.loongarch.lasx.xvrotr.w"
+ "llvm.loongarch.lasx.xvrotri.b"
+ "llvm.loongarch.lasx.xvrotri.d"
+ "llvm.loongarch.lasx.xvrotri.h"
+ "llvm.loongarch.lasx.xvrotri.w"
+ "llvm.loongarch.lasx.xvsadd.b"
+ "llvm.loongarch.lasx.xvsadd.bu"
+ "llvm.loongarch.lasx.xvsadd.d"
+ "llvm.loongarch.lasx.xvsadd.du"
+ "llvm.loongarch.lasx.xvsadd.h"
+ "llvm.loongarch.lasx.xvsadd.hu"
+ "llvm.loongarch.lasx.xvsadd.w"
+ "llvm.loongarch.lasx.xvsadd.wu"
+ "llvm.loongarch.lasx.xvsat.b"
+ "llvm.loongarch.lasx.xvsat.bu"
+ "llvm.loongarch.lasx.xvsat.d"
+ "llvm.loongarch.lasx.xvsat.du"
+ "llvm.loongarch.lasx.xvsat.h"
+ "llvm.loongarch.lasx.xvsat.hu"
+ "llvm.loongarch.lasx.xvsat.w"
+ "llvm.loongarch.lasx.xvsat.wu"
+ "llvm.loongarch.lasx.xvseq.b"
+ "llvm.loongarch.lasx.xvseq.d"
+ "llvm.loongarch.lasx.xvseq.h"
+ "llvm.loongarch.lasx.xvseq.w"
+ "llvm.loongarch.lasx.xvseqi.b"
+ "llvm.loongarch.lasx.xvseqi.d"
+ "llvm.loongarch.lasx.xvseqi.h"
+ "llvm.loongarch.lasx.xvseqi.w"
+ "llvm.loongarch.lasx.xvshuf.b"
+ "llvm.loongarch.lasx.xvshuf.d"
+ "llvm.loongarch.lasx.xvshuf.h"
+ "llvm.loongarch.lasx.xvshuf.w"
+ "llvm.loongarch.lasx.xvshuf4i.b"
+ "llvm.loongarch.lasx.xvshuf4i.d"
+ "llvm.loongarch.lasx.xvshuf4i.h"
+ "llvm.loongarch.lasx.xvshuf4i.w"
+ "llvm.loongarch.lasx.xvsigncov.b"
+ "llvm.loongarch.lasx.xvsigncov.d"
+ "llvm.loongarch.lasx.xvsigncov.h"
+ "llvm.loongarch.lasx.xvsigncov.w"
+ "llvm.loongarch.lasx.xvsle.b"
+ "llvm.loongarch.lasx.xvsle.bu"
+ "llvm.loongarch.lasx.xvsle.d"
+ "llvm.loongarch.lasx.xvsle.du"
+ "llvm.loongarch.lasx.xvsle.h"
+ "llvm.loongarch.lasx.xvsle.hu"
+ "llvm.loongarch.lasx.xvsle.w"
+ "llvm.loongarch.lasx.xvsle.wu"
+ "llvm.loongarch.lasx.xvslei.b"
+ "llvm.loongarch.lasx.xvslei.bu"
+ "llvm.loongarch.lasx.xvslei.d"
+ "llvm.loongarch.lasx.xvslei.du"
+ "llvm.loongarch.lasx.xvslei.h"
+ "llvm.loongarch.lasx.xvslei.hu"
+ "llvm.loongarch.lasx.xvslei.w"
+ "llvm.loongarch.lasx.xvslei.wu"
+ "llvm.loongarch.lasx.xvsll.b"
+ "llvm.loongarch.lasx.xvsll.d"
+ "llvm.loongarch.lasx.xvsll.h"
+ "llvm.loongarch.lasx.xvsll.w"
+ "llvm.loongarch.lasx.xvslli.b"
+ "llvm.loongarch.lasx.xvslli.d"
+ "llvm.loongarch.lasx.xvslli.h"
+ "llvm.loongarch.lasx.xvslli.w"
+ "llvm.loongarch.lasx.xvsllwil.d.w"
+ "llvm.loongarch.lasx.xvsllwil.du.wu"
+ "llvm.loongarch.lasx.xvsllwil.h.b"
+ "llvm.loongarch.lasx.xvsllwil.hu.bu"
+ "llvm.loongarch.lasx.xvsllwil.w.h"
+ "llvm.loongarch.lasx.xvsllwil.wu.hu"
+ "llvm.loongarch.lasx.xvslt.b"
+ "llvm.loongarch.lasx.xvslt.bu"
+ "llvm.loongarch.lasx.xvslt.d"
+ "llvm.loongarch.lasx.xvslt.du"
+ "llvm.loongarch.lasx.xvslt.h"
+ "llvm.loongarch.lasx.xvslt.hu"
+ "llvm.loongarch.lasx.xvslt.w"
+ "llvm.loongarch.lasx.xvslt.wu"
+ "llvm.loongarch.lasx.xvslti.b"
+ "llvm.loongarch.lasx.xvslti.bu"
+ "llvm.loongarch.lasx.xvslti.d"
+ "llvm.loongarch.lasx.xvslti.du"
+ "llvm.loongarch.lasx.xvslti.h"
+ "llvm.loongarch.lasx.xvslti.hu"
+ "llvm.loongarch.lasx.xvslti.w"
+ "llvm.loongarch.lasx.xvslti.wu"
+ "llvm.loongarch.lasx.xvsra.b"
+ "llvm.loongarch.lasx.xvsra.d"
+ "llvm.loongarch.lasx.xvsra.h"
+ "llvm.loongarch.lasx.xvsra.w"
+ "llvm.loongarch.lasx.xvsrai.b"
+ "llvm.loongarch.lasx.xvsrai.d"
+ "llvm.loongarch.lasx.xvsrai.h"
+ "llvm.loongarch.lasx.xvsrai.w"
+ "llvm.loongarch.lasx.xvsran.b.h"
+ "llvm.loongarch.lasx.xvsran.h.w"
+ "llvm.loongarch.lasx.xvsran.w.d"
+ "llvm.loongarch.lasx.xvsrani.b.h"
+ "llvm.loongarch.lasx.xvsrani.d.q"
+ "llvm.loongarch.lasx.xvsrani.h.w"
+ "llvm.loongarch.lasx.xvsrani.w.d"
+ "llvm.loongarch.lasx.xvsrar.b"
+ "llvm.loongarch.lasx.xvsrar.d"
+ "llvm.loongarch.lasx.xvsrar.h"
+ "llvm.loongarch.lasx.xvsrar.w"
+ "llvm.loongarch.lasx.xvsrari.b"
+ "llvm.loongarch.lasx.xvsrari.d"
+ "llvm.loongarch.lasx.xvsrari.h"
+ "llvm.loongarch.lasx.xvsrari.w"
+ "llvm.loongarch.lasx.xvsrarn.b.h"
+ "llvm.loongarch.lasx.xvsrarn.h.w"
+ "llvm.loongarch.lasx.xvsrarn.w.d"
+ "llvm.loongarch.lasx.xvsrarni.b.h"
+ "llvm.loongarch.lasx.xvsrarni.d.q"
+ "llvm.loongarch.lasx.xvsrarni.h.w"
+ "llvm.loongarch.lasx.xvsrarni.w.d"
+ "llvm.loongarch.lasx.xvsrl.b"
+ "llvm.loongarch.lasx.xvsrl.d"
+ "llvm.loongarch.lasx.xvsrl.h"
+ "llvm.loongarch.lasx.xvsrl.w"
+ "llvm.loongarch.lasx.xvsrli.b"
+ "llvm.loongarch.lasx.xvsrli.d"
+ "llvm.loongarch.lasx.xvsrli.h"
+ "llvm.loongarch.lasx.xvsrli.w"
+ "llvm.loongarch.lasx.xvsrln.b.h"
+ "llvm.loongarch.lasx.xvsrln.h.w"
+ "llvm.loongarch.lasx.xvsrln.w.d"
+ "llvm.loongarch.lasx.xvsrlni.b.h"
+ "llvm.loongarch.lasx.xvsrlni.d.q"
+ "llvm.loongarch.lasx.xvsrlni.h.w"
+ "llvm.loongarch.lasx.xvsrlni.w.d"
+ "llvm.loongarch.lasx.xvsrlr.b"
+ "llvm.loongarch.lasx.xvsrlr.d"
+ "llvm.loongarch.lasx.xvsrlr.h"
+ "llvm.loongarch.lasx.xvsrlr.w"
+ "llvm.loongarch.lasx.xvsrlri.b"
+ "llvm.loongarch.lasx.xvsrlri.d"
+ "llvm.loongarch.lasx.xvsrlri.h"
+ "llvm.loongarch.lasx.xvsrlri.w"
+ "llvm.loongarch.lasx.xvsrlrn.b.h"
+ "llvm.loongarch.lasx.xvsrlrn.h.w"
+ "llvm.loongarch.lasx.xvsrlrn.w.d"
+ "llvm.loongarch.lasx.xvsrlrni.b.h"
+ "llvm.loongarch.lasx.xvsrlrni.d.q"
+ "llvm.loongarch.lasx.xvsrlrni.h.w"
+ "llvm.loongarch.lasx.xvsrlrni.w.d"
+ "llvm.loongarch.lasx.xvssran.b.h"
+ "llvm.loongarch.lasx.xvssran.bu.h"
+ "llvm.loongarch.lasx.xvssran.h.w"
+ "llvm.loongarch.lasx.xvssran.hu.w"
+ "llvm.loongarch.lasx.xvssran.w.d"
+ "llvm.loongarch.lasx.xvssran.wu.d"
+ "llvm.loongarch.lasx.xvssrani.b.h"
+ "llvm.loongarch.lasx.xvssrani.bu.h"
+ "llvm.loongarch.lasx.xvssrani.d.q"
+ "llvm.loongarch.lasx.xvssrani.du.q"
+ "llvm.loongarch.lasx.xvssrani.h.w"
+ "llvm.loongarch.lasx.xvssrani.hu.w"
+ "llvm.loongarch.lasx.xvssrani.w.d"
+ "llvm.loongarch.lasx.xvssrani.wu.d"
+ "llvm.loongarch.lasx.xvssrarn.b.h"
+ "llvm.loongarch.lasx.xvssrarn.bu.h"
+ "llvm.loongarch.lasx.xvssrarn.h.w"
+ "llvm.loongarch.lasx.xvssrarn.hu.w"
+ "llvm.loongarch.lasx.xvssrarn.w.d"
+ "llvm.loongarch.lasx.xvssrarn.wu.d"
+ "llvm.loongarch.lasx.xvssrarni.b.h"
+ "llvm.loongarch.lasx.xvssrarni.bu.h"
+ "llvm.loongarch.lasx.xvssrarni.d.q"
+ "llvm.loongarch.lasx.xvssrarni.du.q"
+ "llvm.loongarch.lasx.xvssrarni.h.w"
+ "llvm.loongarch.lasx.xvssrarni.hu.w"
+ "llvm.loongarch.lasx.xvssrarni.w.d"
+ "llvm.loongarch.lasx.xvssrarni.wu.d"
+ "llvm.loongarch.lasx.xvssrln.b.h"
+ "llvm.loongarch.lasx.xvssrln.bu.h"
+ "llvm.loongarch.lasx.xvssrln.h.w"
+ "llvm.loongarch.lasx.xvssrln.hu.w"
+ "llvm.loongarch.lasx.xvssrln.w.d"
+ "llvm.loongarch.lasx.xvssrln.wu.d"
+ "llvm.loongarch.lasx.xvssrlni.b.h"
+ "llvm.loongarch.lasx.xvssrlni.bu.h"
+ "llvm.loongarch.lasx.xvssrlni.d.q"
+ "llvm.loongarch.lasx.xvssrlni.du.q"
+ "llvm.loongarch.lasx.xvssrlni.h.w"
+ "llvm.loongarch.lasx.xvssrlni.hu.w"
+ "llvm.loongarch.lasx.xvssrlni.w.d"
+ "llvm.loongarch.lasx.xvssrlni.wu.d"
+ "llvm.loongarch.lasx.xvssrlrn.b.h"
+ "llvm.loongarch.lasx.xvssrlrn.bu.h"
+ "llvm.loongarch.lasx.xvssrlrn.h.w"
+ "llvm.loongarch.lasx.xvssrlrn.hu.w"
+ "llvm.loongarch.lasx.xvssrlrn.w.d"
+ "llvm.loongarch.lasx.xvssrlrn.wu.d"
+ "llvm.loongarch.lasx.xvssrlrni.b.h"
+ "llvm.loongarch.lasx.xvssrlrni.bu.h"
+ "llvm.loongarch.lasx.xvssrlrni.d.q"
+ "llvm.loongarch.lasx.xvssrlrni.du.q"
+ "llvm.loongarch.lasx.xvssrlrni.h.w"
+ "llvm.loongarch.lasx.xvssrlrni.hu.w"
+ "llvm.loongarch.lasx.xvssrlrni.w.d"
+ "llvm.loongarch.lasx.xvssrlrni.wu.d"
+ "llvm.loongarch.lasx.xvssub.b"
+ "llvm.loongarch.lasx.xvssub.bu"
+ "llvm.loongarch.lasx.xvssub.d"
+ "llvm.loongarch.lasx.xvssub.du"
+ "llvm.loongarch.lasx.xvssub.h"
+ "llvm.loongarch.lasx.xvssub.hu"
+ "llvm.loongarch.lasx.xvssub.w"
+ "llvm.loongarch.lasx.xvssub.wu"
+ "llvm.loongarch.lasx.xvst"
+ "llvm.loongarch.lasx.xvstelm.b"
+ "llvm.loongarch.lasx.xvstelm.d"
+ "llvm.loongarch.lasx.xvstelm.h"
+ "llvm.loongarch.lasx.xvstelm.w"
+ "llvm.loongarch.lasx.xvstx"
+ "llvm.loongarch.lasx.xvsub.b"
+ "llvm.loongarch.lasx.xvsub.d"
+ "llvm.loongarch.lasx.xvsub.h"
+ "llvm.loongarch.lasx.xvsub.q"
+ "llvm.loongarch.lasx.xvsub.w"
+ "llvm.loongarch.lasx.xvsubi.bu"
+ "llvm.loongarch.lasx.xvsubi.du"
+ "llvm.loongarch.lasx.xvsubi.hu"
+ "llvm.loongarch.lasx.xvsubi.wu"
+ "llvm.loongarch.lasx.xvsubwev.d.w"
+ "llvm.loongarch.lasx.xvsubwev.d.wu"
+ "llvm.loongarch.lasx.xvsubwev.h.b"
+ "llvm.loongarch.lasx.xvsubwev.h.bu"
+ "llvm.loongarch.lasx.xvsubwev.q.d"
+ "llvm.loongarch.lasx.xvsubwev.q.du"
+ "llvm.loongarch.lasx.xvsubwev.w.h"
+ "llvm.loongarch.lasx.xvsubwev.w.hu"
+ "llvm.loongarch.lasx.xvsubwod.d.w"
+ "llvm.loongarch.lasx.xvsubwod.d.wu"
+ "llvm.loongarch.lasx.xvsubwod.h.b"
+ "llvm.loongarch.lasx.xvsubwod.h.bu"
+ "llvm.loongarch.lasx.xvsubwod.q.d"
+ "llvm.loongarch.lasx.xvsubwod.q.du"
+ "llvm.loongarch.lasx.xvsubwod.w.h"
+ "llvm.loongarch.lasx.xvsubwod.w.hu"
+ "llvm.loongarch.lasx.xvxor.v"
+ "llvm.loongarch.lasx.xvxori.b"
+ "llvm.loongarch.lsx.bnz.b"
+ "llvm.loongarch.lsx.bnz.d"
+ "llvm.loongarch.lsx.bnz.h"
+ "llvm.loongarch.lsx.bnz.v"
+ "llvm.loongarch.lsx.bnz.w"
+ "llvm.loongarch.lsx.bz.b"
+ "llvm.loongarch.lsx.bz.d"
+ "llvm.loongarch.lsx.bz.h"
+ "llvm.loongarch.lsx.bz.v"
+ "llvm.loongarch.lsx.bz.w"
+ "llvm.loongarch.lsx.vabsd.b"
+ "llvm.loongarch.lsx.vabsd.bu"
+ "llvm.loongarch.lsx.vabsd.d"
+ "llvm.loongarch.lsx.vabsd.du"
+ "llvm.loongarch.lsx.vabsd.h"
+ "llvm.loongarch.lsx.vabsd.hu"
+ "llvm.loongarch.lsx.vabsd.w"
+ "llvm.loongarch.lsx.vabsd.wu"
+ "llvm.loongarch.lsx.vadd.b"
+ "llvm.loongarch.lsx.vadd.d"
+ "llvm.loongarch.lsx.vadd.h"
+ "llvm.loongarch.lsx.vadd.q"
+ "llvm.loongarch.lsx.vadd.w"
+ "llvm.loongarch.lsx.vadda.b"
+ "llvm.loongarch.lsx.vadda.d"
+ "llvm.loongarch.lsx.vadda.h"
+ "llvm.loongarch.lsx.vadda.w"
+ "llvm.loongarch.lsx.vaddi.bu"
+ "llvm.loongarch.lsx.vaddi.du"
+ "llvm.loongarch.lsx.vaddi.hu"
+ "llvm.loongarch.lsx.vaddi.wu"
+ "llvm.loongarch.lsx.vaddwev.d.w"
+ "llvm.loongarch.lsx.vaddwev.d.wu"
+ "llvm.loongarch.lsx.vaddwev.d.wu.w"
+ "llvm.loongarch.lsx.vaddwev.h.b"
+ "llvm.loongarch.lsx.vaddwev.h.bu"
+ "llvm.loongarch.lsx.vaddwev.h.bu.b"
+ "llvm.loongarch.lsx.vaddwev.q.d"
+ "llvm.loongarch.lsx.vaddwev.q.du"
+ "llvm.loongarch.lsx.vaddwev.q.du.d"
+ "llvm.loongarch.lsx.vaddwev.w.h"
+ "llvm.loongarch.lsx.vaddwev.w.hu"
+ "llvm.loongarch.lsx.vaddwev.w.hu.h"
+ "llvm.loongarch.lsx.vaddwod.d.w"
+ "llvm.loongarch.lsx.vaddwod.d.wu"
+ "llvm.loongarch.lsx.vaddwod.d.wu.w"
+ "llvm.loongarch.lsx.vaddwod.h.b"
+ "llvm.loongarch.lsx.vaddwod.h.bu"
+ "llvm.loongarch.lsx.vaddwod.h.bu.b"
+ "llvm.loongarch.lsx.vaddwod.q.d"
+ "llvm.loongarch.lsx.vaddwod.q.du"
+ "llvm.loongarch.lsx.vaddwod.q.du.d"
+ "llvm.loongarch.lsx.vaddwod.w.h"
+ "llvm.loongarch.lsx.vaddwod.w.hu"
+ "llvm.loongarch.lsx.vaddwod.w.hu.h"
+ "llvm.loongarch.lsx.vand.v"
+ "llvm.loongarch.lsx.vandi.b"
+ "llvm.loongarch.lsx.vandn.v"
+ "llvm.loongarch.lsx.vavg.b"
+ "llvm.loongarch.lsx.vavg.bu"
+ "llvm.loongarch.lsx.vavg.d"
+ "llvm.loongarch.lsx.vavg.du"
+ "llvm.loongarch.lsx.vavg.h"
+ "llvm.loongarch.lsx.vavg.hu"
+ "llvm.loongarch.lsx.vavg.w"
+ "llvm.loongarch.lsx.vavg.wu"
+ "llvm.loongarch.lsx.vavgr.b"
+ "llvm.loongarch.lsx.vavgr.bu"
+ "llvm.loongarch.lsx.vavgr.d"
+ "llvm.loongarch.lsx.vavgr.du"
+ "llvm.loongarch.lsx.vavgr.h"
+ "llvm.loongarch.lsx.vavgr.hu"
+ "llvm.loongarch.lsx.vavgr.w"
+ "llvm.loongarch.lsx.vavgr.wu"
+ "llvm.loongarch.lsx.vbitclr.b"
+ "llvm.loongarch.lsx.vbitclr.d"
+ "llvm.loongarch.lsx.vbitclr.h"
+ "llvm.loongarch.lsx.vbitclr.w"
+ "llvm.loongarch.lsx.vbitclri.b"
+ "llvm.loongarch.lsx.vbitclri.d"
+ "llvm.loongarch.lsx.vbitclri.h"
+ "llvm.loongarch.lsx.vbitclri.w"
+ "llvm.loongarch.lsx.vbitrev.b"
+ "llvm.loongarch.lsx.vbitrev.d"
+ "llvm.loongarch.lsx.vbitrev.h"
+ "llvm.loongarch.lsx.vbitrev.w"
+ "llvm.loongarch.lsx.vbitrevi.b"
+ "llvm.loongarch.lsx.vbitrevi.d"
+ "llvm.loongarch.lsx.vbitrevi.h"
+ "llvm.loongarch.lsx.vbitrevi.w"
+ "llvm.loongarch.lsx.vbitsel.v"
+ "llvm.loongarch.lsx.vbitseli.b"
+ "llvm.loongarch.lsx.vbitset.b"
+ "llvm.loongarch.lsx.vbitset.d"
+ "llvm.loongarch.lsx.vbitset.h"
+ "llvm.loongarch.lsx.vbitset.w"
+ "llvm.loongarch.lsx.vbitseti.b"
+ "llvm.loongarch.lsx.vbitseti.d"
+ "llvm.loongarch.lsx.vbitseti.h"
+ "llvm.loongarch.lsx.vbitseti.w"
+ "llvm.loongarch.lsx.vbsll.v"
+ "llvm.loongarch.lsx.vbsrl.v"
+ "llvm.loongarch.lsx.vclo.b"
+ "llvm.loongarch.lsx.vclo.d"
+ "llvm.loongarch.lsx.vclo.h"
+ "llvm.loongarch.lsx.vclo.w"
+ "llvm.loongarch.lsx.vclz.b"
+ "llvm.loongarch.lsx.vclz.d"
+ "llvm.loongarch.lsx.vclz.h"
+ "llvm.loongarch.lsx.vclz.w"
+ "llvm.loongarch.lsx.vdiv.b"
+ "llvm.loongarch.lsx.vdiv.bu"
+ "llvm.loongarch.lsx.vdiv.d"
+ "llvm.loongarch.lsx.vdiv.du"
+ "llvm.loongarch.lsx.vdiv.h"
+ "llvm.loongarch.lsx.vdiv.hu"
+ "llvm.loongarch.lsx.vdiv.w"
+ "llvm.loongarch.lsx.vdiv.wu"
+ "llvm.loongarch.lsx.vexth.d.w"
+ "llvm.loongarch.lsx.vexth.du.wu"
+ "llvm.loongarch.lsx.vexth.h.b"
+ "llvm.loongarch.lsx.vexth.hu.bu"
+ "llvm.loongarch.lsx.vexth.q.d"
+ "llvm.loongarch.lsx.vexth.qu.du"
+ "llvm.loongarch.lsx.vexth.w.h"
+ "llvm.loongarch.lsx.vexth.wu.hu"
+ "llvm.loongarch.lsx.vextl.q.d"
+ "llvm.loongarch.lsx.vextl.qu.du"
+ "llvm.loongarch.lsx.vextrins.b"
+ "llvm.loongarch.lsx.vextrins.d"
+ "llvm.loongarch.lsx.vextrins.h"
+ "llvm.loongarch.lsx.vextrins.w"
+ "llvm.loongarch.lsx.vfadd.d"
+ "llvm.loongarch.lsx.vfadd.s"
+ "llvm.loongarch.lsx.vfclass.d"
+ "llvm.loongarch.lsx.vfclass.s"
+ "llvm.loongarch.lsx.vfcmp.caf.d"
+ "llvm.loongarch.lsx.vfcmp.caf.s"
+ "llvm.loongarch.lsx.vfcmp.ceq.d"
+ "llvm.loongarch.lsx.vfcmp.ceq.s"
+ "llvm.loongarch.lsx.vfcmp.cle.d"
+ "llvm.loongarch.lsx.vfcmp.cle.s"
+ "llvm.loongarch.lsx.vfcmp.clt.d"
+ "llvm.loongarch.lsx.vfcmp.clt.s"
+ "llvm.loongarch.lsx.vfcmp.cne.d"
+ "llvm.loongarch.lsx.vfcmp.cne.s"
+ "llvm.loongarch.lsx.vfcmp.cor.d"
+ "llvm.loongarch.lsx.vfcmp.cor.s"
+ "llvm.loongarch.lsx.vfcmp.cueq.d"
+ "llvm.loongarch.lsx.vfcmp.cueq.s"
+ "llvm.loongarch.lsx.vfcmp.cule.d"
+ "llvm.loongarch.lsx.vfcmp.cule.s"
+ "llvm.loongarch.lsx.vfcmp.cult.d"
+ "llvm.loongarch.lsx.vfcmp.cult.s"
+ "llvm.loongarch.lsx.vfcmp.cun.d"
+ "llvm.loongarch.lsx.vfcmp.cun.s"
+ "llvm.loongarch.lsx.vfcmp.cune.d"
+ "llvm.loongarch.lsx.vfcmp.cune.s"
+ "llvm.loongarch.lsx.vfcmp.saf.d"
+ "llvm.loongarch.lsx.vfcmp.saf.s"
+ "llvm.loongarch.lsx.vfcmp.seq.d"
+ "llvm.loongarch.lsx.vfcmp.seq.s"
+ "llvm.loongarch.lsx.vfcmp.sle.d"
+ "llvm.loongarch.lsx.vfcmp.sle.s"
+ "llvm.loongarch.lsx.vfcmp.slt.d"
+ "llvm.loongarch.lsx.vfcmp.slt.s"
+ "llvm.loongarch.lsx.vfcmp.sne.d"
+ "llvm.loongarch.lsx.vfcmp.sne.s"
+ "llvm.loongarch.lsx.vfcmp.sor.d"
+ "llvm.loongarch.lsx.vfcmp.sor.s"
+ "llvm.loongarch.lsx.vfcmp.sueq.d"
+ "llvm.loongarch.lsx.vfcmp.sueq.s"
+ "llvm.loongarch.lsx.vfcmp.sule.d"
+ "llvm.loongarch.lsx.vfcmp.sule.s"
+ "llvm.loongarch.lsx.vfcmp.sult.d"
+ "llvm.loongarch.lsx.vfcmp.sult.s"
+ "llvm.loongarch.lsx.vfcmp.sun.d"
+ "llvm.loongarch.lsx.vfcmp.sun.s"
+ "llvm.loongarch.lsx.vfcmp.sune.d"
+ "llvm.loongarch.lsx.vfcmp.sune.s"
+ "llvm.loongarch.lsx.vfcvt.h.s"
+ "llvm.loongarch.lsx.vfcvt.s.d"
+ "llvm.loongarch.lsx.vfcvth.d.s"
+ "llvm.loongarch.lsx.vfcvth.s.h"
+ "llvm.loongarch.lsx.vfcvtl.d.s"
+ "llvm.loongarch.lsx.vfcvtl.s.h"
+ "llvm.loongarch.lsx.vfdiv.d"
+ "llvm.loongarch.lsx.vfdiv.s"
+ "llvm.loongarch.lsx.vffint.d.l"
+ "llvm.loongarch.lsx.vffint.d.lu"
+ "llvm.loongarch.lsx.vffint.s.l"
+ "llvm.loongarch.lsx.vffint.s.w"
+ "llvm.loongarch.lsx.vffint.s.wu"
+ "llvm.loongarch.lsx.vffinth.d.w"
+ "llvm.loongarch.lsx.vffintl.d.w"
+ "llvm.loongarch.lsx.vflogb.d"
+ "llvm.loongarch.lsx.vflogb.s"
+ "llvm.loongarch.lsx.vfmadd.d"
+ "llvm.loongarch.lsx.vfmadd.s"
+ "llvm.loongarch.lsx.vfmax.d"
+ "llvm.loongarch.lsx.vfmax.s"
+ "llvm.loongarch.lsx.vfmaxa.d"
+ "llvm.loongarch.lsx.vfmaxa.s"
+ "llvm.loongarch.lsx.vfmin.d"
+ "llvm.loongarch.lsx.vfmin.s"
+ "llvm.loongarch.lsx.vfmina.d"
+ "llvm.loongarch.lsx.vfmina.s"
+ "llvm.loongarch.lsx.vfmsub.d"
+ "llvm.loongarch.lsx.vfmsub.s"
+ "llvm.loongarch.lsx.vfmul.d"
+ "llvm.loongarch.lsx.vfmul.s"
+ "llvm.loongarch.lsx.vfnmadd.d"
+ "llvm.loongarch.lsx.vfnmadd.s"
+ "llvm.loongarch.lsx.vfnmsub.d"
+ "llvm.loongarch.lsx.vfnmsub.s"
+ "llvm.loongarch.lsx.vfrecip.d"
+ "llvm.loongarch.lsx.vfrecip.s"
+ "llvm.loongarch.lsx.vfrint.d"
+ "llvm.loongarch.lsx.vfrint.s"
+ "llvm.loongarch.lsx.vfrintrm.d"
+ "llvm.loongarch.lsx.vfrintrm.s"
+ "llvm.loongarch.lsx.vfrintrne.d"
+ "llvm.loongarch.lsx.vfrintrne.s"
+ "llvm.loongarch.lsx.vfrintrp.d"
+ "llvm.loongarch.lsx.vfrintrp.s"
+ "llvm.loongarch.lsx.vfrintrz.d"
+ "llvm.loongarch.lsx.vfrintrz.s"
+ "llvm.loongarch.lsx.vfrsqrt.d"
+ "llvm.loongarch.lsx.vfrsqrt.s"
+ "llvm.loongarch.lsx.vfrstp.b"
+ "llvm.loongarch.lsx.vfrstp.h"
+ "llvm.loongarch.lsx.vfrstpi.b"
+ "llvm.loongarch.lsx.vfrstpi.h"
+ "llvm.loongarch.lsx.vfsqrt.d"
+ "llvm.loongarch.lsx.vfsqrt.s"
+ "llvm.loongarch.lsx.vfsub.d"
+ "llvm.loongarch.lsx.vfsub.s"
+ "llvm.loongarch.lsx.vftint.l.d"
+ "llvm.loongarch.lsx.vftint.lu.d"
+ "llvm.loongarch.lsx.vftint.w.d"
+ "llvm.loongarch.lsx.vftint.w.s"
+ "llvm.loongarch.lsx.vftint.wu.s"
+ "llvm.loongarch.lsx.vftinth.l.s"
+ "llvm.loongarch.lsx.vftintl.l.s"
+ "llvm.loongarch.lsx.vftintrm.l.d"
+ "llvm.loongarch.lsx.vftintrm.w.d"
+ "llvm.loongarch.lsx.vftintrm.w.s"
+ "llvm.loongarch.lsx.vftintrmh.l.s"
+ "llvm.loongarch.lsx.vftintrml.l.s"
+ "llvm.loongarch.lsx.vftintrne.l.d"
+ "llvm.loongarch.lsx.vftintrne.w.d"
+ "llvm.loongarch.lsx.vftintrne.w.s"
+ "llvm.loongarch.lsx.vftintrneh.l.s"
+ "llvm.loongarch.lsx.vftintrnel.l.s"
+ "llvm.loongarch.lsx.vftintrp.l.d"
+ "llvm.loongarch.lsx.vftintrp.w.d"
+ "llvm.loongarch.lsx.vftintrp.w.s"
+ "llvm.loongarch.lsx.vftintrph.l.s"
+ "llvm.loongarch.lsx.vftintrpl.l.s"
+ "llvm.loongarch.lsx.vftintrz.l.d"
+ "llvm.loongarch.lsx.vftintrz.lu.d"
+ "llvm.loongarch.lsx.vftintrz.w.d"
+ "llvm.loongarch.lsx.vftintrz.w.s"
+ "llvm.loongarch.lsx.vftintrz.wu.s"
+ "llvm.loongarch.lsx.vftintrzh.l.s"
+ "llvm.loongarch.lsx.vftintrzl.l.s"
+ "llvm.loongarch.lsx.vhaddw.d.w"
+ "llvm.loongarch.lsx.vhaddw.du.wu"
+ "llvm.loongarch.lsx.vhaddw.h.b"
+ "llvm.loongarch.lsx.vhaddw.hu.bu"
+ "llvm.loongarch.lsx.vhaddw.q.d"
+ "llvm.loongarch.lsx.vhaddw.qu.du"
+ "llvm.loongarch.lsx.vhaddw.w.h"
+ "llvm.loongarch.lsx.vhaddw.wu.hu"
+ "llvm.loongarch.lsx.vhsubw.d.w"
+ "llvm.loongarch.lsx.vhsubw.du.wu"
+ "llvm.loongarch.lsx.vhsubw.h.b"
+ "llvm.loongarch.lsx.vhsubw.hu.bu"
+ "llvm.loongarch.lsx.vhsubw.q.d"
+ "llvm.loongarch.lsx.vhsubw.qu.du"
+ "llvm.loongarch.lsx.vhsubw.w.h"
+ "llvm.loongarch.lsx.vhsubw.wu.hu"
+ "llvm.loongarch.lsx.vilvh.b"
+ "llvm.loongarch.lsx.vilvh.d"
+ "llvm.loongarch.lsx.vilvh.h"
+ "llvm.loongarch.lsx.vilvh.w"
+ "llvm.loongarch.lsx.vilvl.b"
+ "llvm.loongarch.lsx.vilvl.d"
+ "llvm.loongarch.lsx.vilvl.h"
+ "llvm.loongarch.lsx.vilvl.w"
+ "llvm.loongarch.lsx.vinsgr2vr.b"
+ "llvm.loongarch.lsx.vinsgr2vr.d"
+ "llvm.loongarch.lsx.vinsgr2vr.h"
+ "llvm.loongarch.lsx.vinsgr2vr.w"
+ "llvm.loongarch.lsx.vld"
+ "llvm.loongarch.lsx.vldi"
+ "llvm.loongarch.lsx.vldrepl.b"
+ "llvm.loongarch.lsx.vldrepl.d"
+ "llvm.loongarch.lsx.vldrepl.h"
+ "llvm.loongarch.lsx.vldrepl.w"
+ "llvm.loongarch.lsx.vldx"
+ "llvm.loongarch.lsx.vmadd.b"
+ "llvm.loongarch.lsx.vmadd.d"
+ "llvm.loongarch.lsx.vmadd.h"
+ "llvm.loongarch.lsx.vmadd.w"
+ "llvm.loongarch.lsx.vmaddwev.d.w"
+ "llvm.loongarch.lsx.vmaddwev.d.wu"
+ "llvm.loongarch.lsx.vmaddwev.d.wu.w"
+ "llvm.loongarch.lsx.vmaddwev.h.b"
+ "llvm.loongarch.lsx.vmaddwev.h.bu"
+ "llvm.loongarch.lsx.vmaddwev.h.bu.b"
+ "llvm.loongarch.lsx.vmaddwev.q.d"
+ "llvm.loongarch.lsx.vmaddwev.q.du"
+ "llvm.loongarch.lsx.vmaddwev.q.du.d"
+ "llvm.loongarch.lsx.vmaddwev.w.h"
+ "llvm.loongarch.lsx.vmaddwev.w.hu"
+ "llvm.loongarch.lsx.vmaddwev.w.hu.h"
+ "llvm.loongarch.lsx.vmaddwod.d.w"
+ "llvm.loongarch.lsx.vmaddwod.d.wu"
+ "llvm.loongarch.lsx.vmaddwod.d.wu.w"
+ "llvm.loongarch.lsx.vmaddwod.h.b"
+ "llvm.loongarch.lsx.vmaddwod.h.bu"
+ "llvm.loongarch.lsx.vmaddwod.h.bu.b"
+ "llvm.loongarch.lsx.vmaddwod.q.d"
+ "llvm.loongarch.lsx.vmaddwod.q.du"
+ "llvm.loongarch.lsx.vmaddwod.q.du.d"
+ "llvm.loongarch.lsx.vmaddwod.w.h"
+ "llvm.loongarch.lsx.vmaddwod.w.hu"
+ "llvm.loongarch.lsx.vmaddwod.w.hu.h"
+ "llvm.loongarch.lsx.vmax.b"
+ "llvm.loongarch.lsx.vmax.bu"
+ "llvm.loongarch.lsx.vmax.d"
+ "llvm.loongarch.lsx.vmax.du"
+ "llvm.loongarch.lsx.vmax.h"
+ "llvm.loongarch.lsx.vmax.hu"
+ "llvm.loongarch.lsx.vmax.w"
+ "llvm.loongarch.lsx.vmax.wu"
+ "llvm.loongarch.lsx.vmaxi.b"
+ "llvm.loongarch.lsx.vmaxi.bu"
+ "llvm.loongarch.lsx.vmaxi.d"
+ "llvm.loongarch.lsx.vmaxi.du"
+ "llvm.loongarch.lsx.vmaxi.h"
+ "llvm.loongarch.lsx.vmaxi.hu"
+ "llvm.loongarch.lsx.vmaxi.w"
+ "llvm.loongarch.lsx.vmaxi.wu"
+ "llvm.loongarch.lsx.vmin.b"
+ "llvm.loongarch.lsx.vmin.bu"
+ "llvm.loongarch.lsx.vmin.d"
+ "llvm.loongarch.lsx.vmin.du"
+ "llvm.loongarch.lsx.vmin.h"
+ "llvm.loongarch.lsx.vmin.hu"
+ "llvm.loongarch.lsx.vmin.w"
+ "llvm.loongarch.lsx.vmin.wu"
+ "llvm.loongarch.lsx.vmini.b"
+ "llvm.loongarch.lsx.vmini.bu"
+ "llvm.loongarch.lsx.vmini.d"
+ "llvm.loongarch.lsx.vmini.du"
+ "llvm.loongarch.lsx.vmini.h"
+ "llvm.loongarch.lsx.vmini.hu"
+ "llvm.loongarch.lsx.vmini.w"
+ "llvm.loongarch.lsx.vmini.wu"
+ "llvm.loongarch.lsx.vmod.b"
+ "llvm.loongarch.lsx.vmod.bu"
+ "llvm.loongarch.lsx.vmod.d"
+ "llvm.loongarch.lsx.vmod.du"
+ "llvm.loongarch.lsx.vmod.h"
+ "llvm.loongarch.lsx.vmod.hu"
+ "llvm.loongarch.lsx.vmod.w"
+ "llvm.loongarch.lsx.vmod.wu"
+ "llvm.loongarch.lsx.vmskgez.b"
+ "llvm.loongarch.lsx.vmskltz.b"
+ "llvm.loongarch.lsx.vmskltz.d"
+ "llvm.loongarch.lsx.vmskltz.h"
+ "llvm.loongarch.lsx.vmskltz.w"
+ "llvm.loongarch.lsx.vmsknz.b"
+ "llvm.loongarch.lsx.vmsub.b"
+ "llvm.loongarch.lsx.vmsub.d"
+ "llvm.loongarch.lsx.vmsub.h"
+ "llvm.loongarch.lsx.vmsub.w"
+ "llvm.loongarch.lsx.vmuh.b"
+ "llvm.loongarch.lsx.vmuh.bu"
+ "llvm.loongarch.lsx.vmuh.d"
+ "llvm.loongarch.lsx.vmuh.du"
+ "llvm.loongarch.lsx.vmuh.h"
+ "llvm.loongarch.lsx.vmuh.hu"
+ "llvm.loongarch.lsx.vmuh.w"
+ "llvm.loongarch.lsx.vmuh.wu"
+ "llvm.loongarch.lsx.vmul.b"
+ "llvm.loongarch.lsx.vmul.d"
+ "llvm.loongarch.lsx.vmul.h"
+ "llvm.loongarch.lsx.vmul.w"
+ "llvm.loongarch.lsx.vmulwev.d.w"
+ "llvm.loongarch.lsx.vmulwev.d.wu"
+ "llvm.loongarch.lsx.vmulwev.d.wu.w"
+ "llvm.loongarch.lsx.vmulwev.h.b"
+ "llvm.loongarch.lsx.vmulwev.h.bu"
+ "llvm.loongarch.lsx.vmulwev.h.bu.b"
+ "llvm.loongarch.lsx.vmulwev.q.d"
+ "llvm.loongarch.lsx.vmulwev.q.du"
+ "llvm.loongarch.lsx.vmulwev.q.du.d"
+ "llvm.loongarch.lsx.vmulwev.w.h"
+ "llvm.loongarch.lsx.vmulwev.w.hu"
+ "llvm.loongarch.lsx.vmulwev.w.hu.h"
+ "llvm.loongarch.lsx.vmulwod.d.w"
+ "llvm.loongarch.lsx.vmulwod.d.wu"
+ "llvm.loongarch.lsx.vmulwod.d.wu.w"
+ "llvm.loongarch.lsx.vmulwod.h.b"
+ "llvm.loongarch.lsx.vmulwod.h.bu"
+ "llvm.loongarch.lsx.vmulwod.h.bu.b"
+ "llvm.loongarch.lsx.vmulwod.q.d"
+ "llvm.loongarch.lsx.vmulwod.q.du"
+ "llvm.loongarch.lsx.vmulwod.q.du.d"
+ "llvm.loongarch.lsx.vmulwod.w.h"
+ "llvm.loongarch.lsx.vmulwod.w.hu"
+ "llvm.loongarch.lsx.vmulwod.w.hu.h"
+ "llvm.loongarch.lsx.vneg.b"
+ "llvm.loongarch.lsx.vneg.d"
+ "llvm.loongarch.lsx.vneg.h"
+ "llvm.loongarch.lsx.vneg.w"
+ "llvm.loongarch.lsx.vnor.v"
+ "llvm.loongarch.lsx.vnori.b"
+ "llvm.loongarch.lsx.vor.v"
+ "llvm.loongarch.lsx.vori.b"
+ "llvm.loongarch.lsx.vorn.v"
+ "llvm.loongarch.lsx.vpackev.b"
+ "llvm.loongarch.lsx.vpackev.d"
+ "llvm.loongarch.lsx.vpackev.h"
+ "llvm.loongarch.lsx.vpackev.w"
+ "llvm.loongarch.lsx.vpackod.b"
+ "llvm.loongarch.lsx.vpackod.d"
+ "llvm.loongarch.lsx.vpackod.h"
+ "llvm.loongarch.lsx.vpackod.w"
+ "llvm.loongarch.lsx.vpcnt.b"
+ "llvm.loongarch.lsx.vpcnt.d"
+ "llvm.loongarch.lsx.vpcnt.h"
+ "llvm.loongarch.lsx.vpcnt.w"
+ "llvm.loongarch.lsx.vpermi.w"
+ "llvm.loongarch.lsx.vpickev.b"
+ "llvm.loongarch.lsx.vpickev.d"
+ "llvm.loongarch.lsx.vpickev.h"
+ "llvm.loongarch.lsx.vpickev.w"
+ "llvm.loongarch.lsx.vpickod.b"
+ "llvm.loongarch.lsx.vpickod.d"
+ "llvm.loongarch.lsx.vpickod.h"
+ "llvm.loongarch.lsx.vpickod.w"
+ "llvm.loongarch.lsx.vpickve2gr.b"
+ "llvm.loongarch.lsx.vpickve2gr.bu"
+ "llvm.loongarch.lsx.vpickve2gr.d"
+ "llvm.loongarch.lsx.vpickve2gr.du"
+ "llvm.loongarch.lsx.vpickve2gr.h"
+ "llvm.loongarch.lsx.vpickve2gr.hu"
+ "llvm.loongarch.lsx.vpickve2gr.w"
+ "llvm.loongarch.lsx.vpickve2gr.wu"
+ "llvm.loongarch.lsx.vreplgr2vr.b"
+ "llvm.loongarch.lsx.vreplgr2vr.d"
+ "llvm.loongarch.lsx.vreplgr2vr.h"
+ "llvm.loongarch.lsx.vreplgr2vr.w"
+ "llvm.loongarch.lsx.vrepli.b"
+ "llvm.loongarch.lsx.vrepli.d"
+ "llvm.loongarch.lsx.vrepli.h"
+ "llvm.loongarch.lsx.vrepli.w"
+ "llvm.loongarch.lsx.vreplve.b"
+ "llvm.loongarch.lsx.vreplve.d"
+ "llvm.loongarch.lsx.vreplve.h"
+ "llvm.loongarch.lsx.vreplve.w"
+ "llvm.loongarch.lsx.vreplvei.b"
+ "llvm.loongarch.lsx.vreplvei.d"
+ "llvm.loongarch.lsx.vreplvei.h"
+ "llvm.loongarch.lsx.vreplvei.w"
+ "llvm.loongarch.lsx.vrotr.b"
+ "llvm.loongarch.lsx.vrotr.d"
+ "llvm.loongarch.lsx.vrotr.h"
+ "llvm.loongarch.lsx.vrotr.w"
+ "llvm.loongarch.lsx.vrotri.b"
+ "llvm.loongarch.lsx.vrotri.d"
+ "llvm.loongarch.lsx.vrotri.h"
+ "llvm.loongarch.lsx.vrotri.w"
+ "llvm.loongarch.lsx.vsadd.b"
+ "llvm.loongarch.lsx.vsadd.bu"
+ "llvm.loongarch.lsx.vsadd.d"
+ "llvm.loongarch.lsx.vsadd.du"
+ "llvm.loongarch.lsx.vsadd.h"
+ "llvm.loongarch.lsx.vsadd.hu"
+ "llvm.loongarch.lsx.vsadd.w"
+ "llvm.loongarch.lsx.vsadd.wu"
+ "llvm.loongarch.lsx.vsat.b"
+ "llvm.loongarch.lsx.vsat.bu"
+ "llvm.loongarch.lsx.vsat.d"
+ "llvm.loongarch.lsx.vsat.du"
+ "llvm.loongarch.lsx.vsat.h"
+ "llvm.loongarch.lsx.vsat.hu"
+ "llvm.loongarch.lsx.vsat.w"
+ "llvm.loongarch.lsx.vsat.wu"
+ "llvm.loongarch.lsx.vseq.b"
+ "llvm.loongarch.lsx.vseq.d"
+ "llvm.loongarch.lsx.vseq.h"
+ "llvm.loongarch.lsx.vseq.w"
+ "llvm.loongarch.lsx.vseqi.b"
+ "llvm.loongarch.lsx.vseqi.d"
+ "llvm.loongarch.lsx.vseqi.h"
+ "llvm.loongarch.lsx.vseqi.w"
+ "llvm.loongarch.lsx.vshuf.b"
+ "llvm.loongarch.lsx.vshuf.d"
+ "llvm.loongarch.lsx.vshuf.h"
+ "llvm.loongarch.lsx.vshuf.w"
+ "llvm.loongarch.lsx.vshuf4i.b"
+ "llvm.loongarch.lsx.vshuf4i.d"
+ "llvm.loongarch.lsx.vshuf4i.h"
+ "llvm.loongarch.lsx.vshuf4i.w"
+ "llvm.loongarch.lsx.vsigncov.b"
+ "llvm.loongarch.lsx.vsigncov.d"
+ "llvm.loongarch.lsx.vsigncov.h"
+ "llvm.loongarch.lsx.vsigncov.w"
+ "llvm.loongarch.lsx.vsle.b"
+ "llvm.loongarch.lsx.vsle.bu"
+ "llvm.loongarch.lsx.vsle.d"
+ "llvm.loongarch.lsx.vsle.du"
+ "llvm.loongarch.lsx.vsle.h"
+ "llvm.loongarch.lsx.vsle.hu"
+ "llvm.loongarch.lsx.vsle.w"
+ "llvm.loongarch.lsx.vsle.wu"
+ "llvm.loongarch.lsx.vslei.b"
+ "llvm.loongarch.lsx.vslei.bu"
+ "llvm.loongarch.lsx.vslei.d"
+ "llvm.loongarch.lsx.vslei.du"
+ "llvm.loongarch.lsx.vslei.h"
+ "llvm.loongarch.lsx.vslei.hu"
+ "llvm.loongarch.lsx.vslei.w"
+ "llvm.loongarch.lsx.vslei.wu"
+ "llvm.loongarch.lsx.vsll.b"
+ "llvm.loongarch.lsx.vsll.d"
+ "llvm.loongarch.lsx.vsll.h"
+ "llvm.loongarch.lsx.vsll.w"
+ "llvm.loongarch.lsx.vslli.b"
+ "llvm.loongarch.lsx.vslli.d"
+ "llvm.loongarch.lsx.vslli.h"
+ "llvm.loongarch.lsx.vslli.w"
+ "llvm.loongarch.lsx.vsllwil.d.w"
+ "llvm.loongarch.lsx.vsllwil.du.wu"
+ "llvm.loongarch.lsx.vsllwil.h.b"
+ "llvm.loongarch.lsx.vsllwil.hu.bu"
+ "llvm.loongarch.lsx.vsllwil.w.h"
+ "llvm.loongarch.lsx.vsllwil.wu.hu"
+ "llvm.loongarch.lsx.vslt.b"
+ "llvm.loongarch.lsx.vslt.bu"
+ "llvm.loongarch.lsx.vslt.d"
+ "llvm.loongarch.lsx.vslt.du"
+ "llvm.loongarch.lsx.vslt.h"
+ "llvm.loongarch.lsx.vslt.hu"
+ "llvm.loongarch.lsx.vslt.w"
+ "llvm.loongarch.lsx.vslt.wu"
+ "llvm.loongarch.lsx.vslti.b"
+ "llvm.loongarch.lsx.vslti.bu"
+ "llvm.loongarch.lsx.vslti.d"
+ "llvm.loongarch.lsx.vslti.du"
+ "llvm.loongarch.lsx.vslti.h"
+ "llvm.loongarch.lsx.vslti.hu"
+ "llvm.loongarch.lsx.vslti.w"
+ "llvm.loongarch.lsx.vslti.wu"
+ "llvm.loongarch.lsx.vsra.b"
+ "llvm.loongarch.lsx.vsra.d"
+ "llvm.loongarch.lsx.vsra.h"
+ "llvm.loongarch.lsx.vsra.w"
+ "llvm.loongarch.lsx.vsrai.b"
+ "llvm.loongarch.lsx.vsrai.d"
+ "llvm.loongarch.lsx.vsrai.h"
+ "llvm.loongarch.lsx.vsrai.w"
+ "llvm.loongarch.lsx.vsran.b.h"
+ "llvm.loongarch.lsx.vsran.h.w"
+ "llvm.loongarch.lsx.vsran.w.d"
+ "llvm.loongarch.lsx.vsrani.b.h"
+ "llvm.loongarch.lsx.vsrani.d.q"
+ "llvm.loongarch.lsx.vsrani.h.w"
+ "llvm.loongarch.lsx.vsrani.w.d"
+ "llvm.loongarch.lsx.vsrar.b"
+ "llvm.loongarch.lsx.vsrar.d"
+ "llvm.loongarch.lsx.vsrar.h"
+ "llvm.loongarch.lsx.vsrar.w"
+ "llvm.loongarch.lsx.vsrari.b"
+ "llvm.loongarch.lsx.vsrari.d"
+ "llvm.loongarch.lsx.vsrari.h"
+ "llvm.loongarch.lsx.vsrari.w"
+ "llvm.loongarch.lsx.vsrarn.b.h"
+ "llvm.loongarch.lsx.vsrarn.h.w"
+ "llvm.loongarch.lsx.vsrarn.w.d"
+ "llvm.loongarch.lsx.vsrarni.b.h"
+ "llvm.loongarch.lsx.vsrarni.d.q"
+ "llvm.loongarch.lsx.vsrarni.h.w"
+ "llvm.loongarch.lsx.vsrarni.w.d"
+ "llvm.loongarch.lsx.vsrl.b"
+ "llvm.loongarch.lsx.vsrl.d"
+ "llvm.loongarch.lsx.vsrl.h"
+ "llvm.loongarch.lsx.vsrl.w"
+ "llvm.loongarch.lsx.vsrli.b"
+ "llvm.loongarch.lsx.vsrli.d"
+ "llvm.loongarch.lsx.vsrli.h"
+ "llvm.loongarch.lsx.vsrli.w"
+ "llvm.loongarch.lsx.vsrln.b.h"
+ "llvm.loongarch.lsx.vsrln.h.w"
+ "llvm.loongarch.lsx.vsrln.w.d"
+ "llvm.loongarch.lsx.vsrlni.b.h"
+ "llvm.loongarch.lsx.vsrlni.d.q"
+ "llvm.loongarch.lsx.vsrlni.h.w"
+ "llvm.loongarch.lsx.vsrlni.w.d"
+ "llvm.loongarch.lsx.vsrlr.b"
+ "llvm.loongarch.lsx.vsrlr.d"
+ "llvm.loongarch.lsx.vsrlr.h"
+ "llvm.loongarch.lsx.vsrlr.w"
+ "llvm.loongarch.lsx.vsrlri.b"
+ "llvm.loongarch.lsx.vsrlri.d"
+ "llvm.loongarch.lsx.vsrlri.h"
+ "llvm.loongarch.lsx.vsrlri.w"
+ "llvm.loongarch.lsx.vsrlrn.b.h"
+ "llvm.loongarch.lsx.vsrlrn.h.w"
+ "llvm.loongarch.lsx.vsrlrn.w.d"
+ "llvm.loongarch.lsx.vsrlrni.b.h"
+ "llvm.loongarch.lsx.vsrlrni.d.q"
+ "llvm.loongarch.lsx.vsrlrni.h.w"
+ "llvm.loongarch.lsx.vsrlrni.w.d"
+ "llvm.loongarch.lsx.vssran.b.h"
+ "llvm.loongarch.lsx.vssran.bu.h"
+ "llvm.loongarch.lsx.vssran.h.w"
+ "llvm.loongarch.lsx.vssran.hu.w"
+ "llvm.loongarch.lsx.vssran.w.d"
+ "llvm.loongarch.lsx.vssran.wu.d"
+ "llvm.loongarch.lsx.vssrani.b.h"
+ "llvm.loongarch.lsx.vssrani.bu.h"
+ "llvm.loongarch.lsx.vssrani.d.q"
+ "llvm.loongarch.lsx.vssrani.du.q"
+ "llvm.loongarch.lsx.vssrani.h.w"
+ "llvm.loongarch.lsx.vssrani.hu.w"
+ "llvm.loongarch.lsx.vssrani.w.d"
+ "llvm.loongarch.lsx.vssrani.wu.d"
+ "llvm.loongarch.lsx.vssrarn.b.h"
+ "llvm.loongarch.lsx.vssrarn.bu.h"
+ "llvm.loongarch.lsx.vssrarn.h.w"
+ "llvm.loongarch.lsx.vssrarn.hu.w"
+ "llvm.loongarch.lsx.vssrarn.w.d"
+ "llvm.loongarch.lsx.vssrarn.wu.d"
+ "llvm.loongarch.lsx.vssrarni.b.h"
+ "llvm.loongarch.lsx.vssrarni.bu.h"
+ "llvm.loongarch.lsx.vssrarni.d.q"
+ "llvm.loongarch.lsx.vssrarni.du.q"
+ "llvm.loongarch.lsx.vssrarni.h.w"
+ "llvm.loongarch.lsx.vssrarni.hu.w"
+ "llvm.loongarch.lsx.vssrarni.w.d"
+ "llvm.loongarch.lsx.vssrarni.wu.d"
+ "llvm.loongarch.lsx.vssrln.b.h"
+ "llvm.loongarch.lsx.vssrln.bu.h"
+ "llvm.loongarch.lsx.vssrln.h.w"
+ "llvm.loongarch.lsx.vssrln.hu.w"
+ "llvm.loongarch.lsx.vssrln.w.d"
+ "llvm.loongarch.lsx.vssrln.wu.d"
+ "llvm.loongarch.lsx.vssrlni.b.h"
+ "llvm.loongarch.lsx.vssrlni.bu.h"
+ "llvm.loongarch.lsx.vssrlni.d.q"
+ "llvm.loongarch.lsx.vssrlni.du.q"
+ "llvm.loongarch.lsx.vssrlni.h.w"
+ "llvm.loongarch.lsx.vssrlni.hu.w"
+ "llvm.loongarch.lsx.vssrlni.w.d"
+ "llvm.loongarch.lsx.vssrlni.wu.d"
+ "llvm.loongarch.lsx.vssrlrn.b.h"
+ "llvm.loongarch.lsx.vssrlrn.bu.h"
+ "llvm.loongarch.lsx.vssrlrn.h.w"
+ "llvm.loongarch.lsx.vssrlrn.hu.w"
+ "llvm.loongarch.lsx.vssrlrn.w.d"
+ "llvm.loongarch.lsx.vssrlrn.wu.d"
+ "llvm.loongarch.lsx.vssrlrni.b.h"
+ "llvm.loongarch.lsx.vssrlrni.bu.h"
+ "llvm.loongarch.lsx.vssrlrni.d.q"
+ "llvm.loongarch.lsx.vssrlrni.du.q"
+ "llvm.loongarch.lsx.vssrlrni.h.w"
+ "llvm.loongarch.lsx.vssrlrni.hu.w"
+ "llvm.loongarch.lsx.vssrlrni.w.d"
+ "llvm.loongarch.lsx.vssrlrni.wu.d"
+ "llvm.loongarch.lsx.vssub.b"
+ "llvm.loongarch.lsx.vssub.bu"
+ "llvm.loongarch.lsx.vssub.d"
+ "llvm.loongarch.lsx.vssub.du"
+ "llvm.loongarch.lsx.vssub.h"
+ "llvm.loongarch.lsx.vssub.hu"
+ "llvm.loongarch.lsx.vssub.w"
+ "llvm.loongarch.lsx.vssub.wu"
+ "llvm.loongarch.lsx.vst"
+ "llvm.loongarch.lsx.vstelm.b"
+ "llvm.loongarch.lsx.vstelm.d"
+ "llvm.loongarch.lsx.vstelm.h"
+ "llvm.loongarch.lsx.vstelm.w"
+ "llvm.loongarch.lsx.vstx"
+ "llvm.loongarch.lsx.vsub.b"
+ "llvm.loongarch.lsx.vsub.d"
+ "llvm.loongarch.lsx.vsub.h"
+ "llvm.loongarch.lsx.vsub.q"
+ "llvm.loongarch.lsx.vsub.w"
+ "llvm.loongarch.lsx.vsubi.bu"
+ "llvm.loongarch.lsx.vsubi.du"
+ "llvm.loongarch.lsx.vsubi.hu"
+ "llvm.loongarch.lsx.vsubi.wu"
+ "llvm.loongarch.lsx.vsubwev.d.w"
+ "llvm.loongarch.lsx.vsubwev.d.wu"
+ "llvm.loongarch.lsx.vsubwev.h.b"
+ "llvm.loongarch.lsx.vsubwev.h.bu"
+ "llvm.loongarch.lsx.vsubwev.q.d"
+ "llvm.loongarch.lsx.vsubwev.q.du"
+ "llvm.loongarch.lsx.vsubwev.w.h"
+ "llvm.loongarch.lsx.vsubwev.w.hu"
+ "llvm.loongarch.lsx.vsubwod.d.w"
+ "llvm.loongarch.lsx.vsubwod.d.wu"
+ "llvm.loongarch.lsx.vsubwod.h.b"
+ "llvm.loongarch.lsx.vsubwod.h.bu"
+ "llvm.loongarch.lsx.vsubwod.q.d"
+ "llvm.loongarch.lsx.vsubwod.q.du"
+ "llvm.loongarch.lsx.vsubwod.w.h"
+ "llvm.loongarch.lsx.vsubwod.w.hu"
+ "llvm.loongarch.lsx.vxor.v"
+ "llvm.loongarch.lsx.vxori.b"
+ "llvm.lrint, llvm.llrint: argument and result disagree on vector use"
+ "llvm.lrint, llvm.llrint: argument must be floating-point or vector of floating-points, and result must be integer or vector of integers"
+ "llvm.lrint, llvm.llrint: argument must be same length as result"
+ "llvm.nvvm.bf2h.rn"
+ "llvm.nvvm.bf2h.rn.ftz"
+ "llvm.nvvm.fma.rn.ftz.bf16"
+ "llvm.nvvm.fma.rn.ftz.bf16x2"
+ "llvm.nvvm.fma.rn.ftz.relu.bf16"
+ "llvm.nvvm.fma.rn.ftz.relu.bf16x2"
+ "llvm.nvvm.fma.rn.ftz.sat.bf16"
+ "llvm.nvvm.fma.rn.ftz.sat.bf16x2"
+ "llvm.nvvm.fma.rn.sat.bf16"
+ "llvm.nvvm.fma.rn.sat.bf16x2"
+ "llvm.nvvm.fmax.ftz.bf16"
+ "llvm.nvvm.fmax.ftz.bf16x2"
+ "llvm.nvvm.fmax.ftz.nan.bf16"
+ "llvm.nvvm.fmax.ftz.nan.bf16x2"
+ "llvm.nvvm.fmax.ftz.nan.xorsign.abs.bf16"
+ "llvm.nvvm.fmax.ftz.nan.xorsign.abs.bf16x2"
+ "llvm.nvvm.fmax.ftz.xorsign.abs.bf16"
+ "llvm.nvvm.fmax.ftz.xorsign.abs.bf16x2"
+ "llvm.nvvm.fmin.ftz.bf16"
+ "llvm.nvvm.fmin.ftz.bf16x2"
+ "llvm.nvvm.fmin.ftz.nan.bf16"
+ "llvm.nvvm.fmin.ftz.nan.bf16x2"
+ "llvm.nvvm.fmin.ftz.nan.xorsign.abs.bf16"
+ "llvm.nvvm.fmin.ftz.nan.xorsign.abs.bf16x2"
+ "llvm.nvvm.fmin.ftz.xorsign.abs.bf16"
+ "llvm.nvvm.fmin.ftz.xorsign.abs.bf16x2"
+ "llvm.objc.claimAutoreleasedReturnValue"
+ "llvm.ppc.mffsl"
+ "llvm.ptrmask intrinsic arguments must be both scalars or both vectors"
+ "llvm.ptrmask intrinsic arguments must have the same number of elements"
+ "llvm.ptrmask intrinsic first argument must be pointer or vector of pointers"
+ "llvm.ptrmask intrinsic second argument bitwidth must match pointer index type size of first argument"
+ "llvm.reset.fpenv"
+ "llvm.reset.fpmode"
+ "llvm.riscv.sf.vfnrclip.x.f.qf"
+ "llvm.riscv.sf.vfnrclip.x.f.qf.mask"
+ "llvm.riscv.sf.vfnrclip.xu.f.qf"
+ "llvm.riscv.sf.vfnrclip.xu.f.qf.mask"
+ "llvm.riscv.sf.vfwmacc.4x4x4"
+ "llvm.riscv.sf.vqmacc.2x8x2"
+ "llvm.riscv.sf.vqmacc.4x8x4"
+ "llvm.riscv.sf.vqmaccsu.2x8x2"
+ "llvm.riscv.sf.vqmaccsu.4x8x4"
+ "llvm.riscv.sf.vqmaccu.2x8x2"
+ "llvm.riscv.sf.vqmaccu.4x8x4"
+ "llvm.riscv.sf.vqmaccus.2x8x2"
+ "llvm.riscv.sf.vqmaccus.4x8x4"
+ "llvm.riscv.vaesdf.vs"
+ "llvm.riscv.vaesdf.vv"
+ "llvm.riscv.vaesdm.vs"
+ "llvm.riscv.vaesdm.vv"
+ "llvm.riscv.vaesef.vs"
+ "llvm.riscv.vaesef.vv"
+ "llvm.riscv.vaesem.vs"
+ "llvm.riscv.vaesem.vv"
+ "llvm.riscv.vaeskf1"
+ "llvm.riscv.vaeskf2"
+ "llvm.riscv.vaesz.vs"
+ "llvm.riscv.vandn"
+ "llvm.riscv.vandn.mask"
+ "llvm.riscv.vbrev"
+ "llvm.riscv.vbrev.mask"
+ "llvm.riscv.vbrev8"
+ "llvm.riscv.vbrev8.mask"
+ "llvm.riscv.vclmul"
+ "llvm.riscv.vclmul.mask"
+ "llvm.riscv.vclmulh"
+ "llvm.riscv.vclmulh.mask"
+ "llvm.riscv.vclz"
+ "llvm.riscv.vclz.mask"
+ "llvm.riscv.vcpopv"
+ "llvm.riscv.vcpopv.mask"
+ "llvm.riscv.vctz"
+ "llvm.riscv.vctz.mask"
+ "llvm.riscv.vfncvtbf16.f.f.w"
+ "llvm.riscv.vfncvtbf16.f.f.w.mask"
+ "llvm.riscv.vfwcvtbf16.f.f.v"
+ "llvm.riscv.vfwcvtbf16.f.f.v.mask"
+ "llvm.riscv.vfwmaccbf16"
+ "llvm.riscv.vfwmaccbf16.mask"
+ "llvm.riscv.vghsh"
+ "llvm.riscv.vgmul.vv"
+ "llvm.riscv.vrev8"
+ "llvm.riscv.vrev8.mask"
+ "llvm.riscv.vrol"
+ "llvm.riscv.vrol.mask"
+ "llvm.riscv.vror"
+ "llvm.riscv.vror.mask"
+ "llvm.riscv.vsha2ch"
+ "llvm.riscv.vsha2cl"
+ "llvm.riscv.vsha2ms"
+ "llvm.riscv.vsm3c"
+ "llvm.riscv.vsm3me"
+ "llvm.riscv.vsm4k"
+ "llvm.riscv.vsm4r.vs"
+ "llvm.riscv.vsm4r.vv"
+ "llvm.riscv.vwsll"
+ "llvm.riscv.vwsll.mask"
+ "llvm.set.fpenv"
+ "llvm.set.fpmode"
+ "llvm.spv.assign.ptr.type"
+ "llvm.spv.assume"
+ "llvm.spv.expect"
+ "llvm.type.checked.load.relative"
+ "llvm.vector.reduce.fmaximum"
+ "llvm.vector.reduce.fminimum"
+ "llvm.vp.is.fpclass"
+ "llvm.x86.avx2.vpdpwsud.128"
+ "llvm.x86.avx2.vpdpwsud.256"
+ "llvm.x86.avx2.vpdpwsuds.128"
+ "llvm.x86.avx2.vpdpwsuds.256"
+ "llvm.x86.avx2.vpdpwusd.128"
+ "llvm.x86.avx2.vpdpwusd.256"
+ "llvm.x86.avx2.vpdpwusds.128"
+ "llvm.x86.avx2.vpdpwusds.256"
+ "llvm.x86.avx2.vpdpwuud.128"
+ "llvm.x86.avx2.vpdpwuud.256"
+ "llvm.x86.avx2.vpdpwuuds.128"
+ "llvm.x86.avx2.vpdpwuuds.256"
+ "llvm.x86.urdmsr"
+ "llvm.x86.uwrmsr"
+ "llvm.x86.vsha512msg1"
+ "llvm.x86.vsha512msg2"
+ "llvm.x86.vsha512rnds2"
+ "llvm.x86.vsm3msg1"
+ "llvm.x86.vsm3msg2"
+ "llvm.x86.vsm3rnds2"
+ "llvm.x86.vsm4key4128"
+ "llvm.x86.vsm4key4256"
+ "llvm.x86.vsm4rnds4128"
+ "llvm.x86.vsm4rnds4256"
+ "llvm_lto"
+ "loadIntoResourceManager:"
+ "loadResources:"
+ "lowerBoundMap"
+ "lut"
+ "lut ndarray should have been allocated"
+ "m68k_rtdcc"
+ "macOS14.4.0"
+ "maskAdd"
+ "maskTensor"
+ "matchPattern"
+ "max."
+ "maximumf"
+ "maxnumf"
+ "memrefToTensorOp"
+ "milOp.GetNumOutputs() == callOp->getNumResults()"
+ "min ndarray should have been allocated"
+ "min."
+ "minimumf"
+ "minnumf"
+ "missing "
+ "missing ']' closing scalable dimension"
+ "missing UUID"
+ "missing alias length"
+ "missing bitcode offset"
+ "missing bitcode size"
+ "missing bitcode wrapper magic"
+ "missing bitcode wrapper version"
+ "missing bug fix version number"
+ "missing closing '}' while processing pass options"
+ "missing file size"
+ "missing file type"
+ "missing function AIR major version"
+ "missing function AIR minor version"
+ "missing function Metal major version"
+ "missing function Metal minor version"
+ "missing function count"
+ "missing function data size"
+ "missing function external offset path"
+ "missing function hash"
+ "missing function list offset"
+ "missing function list size"
+ "missing function module offset"
+ "missing function module size"
+ "missing function name length"
+ "missing function patch info"
+ "missing function private metadata offset"
+ "missing function public metadata offset"
+ "missing function reflection offset"
+ "missing function render target array index data type"
+ "missing function source offset"
+ "missing function token"
+ "missing function type"
+ "missing function variable offset"
+ "missing header extension token"
+ "missing imported symbol count"
+ "missing imported symbol data size"
+ "missing imported symbol name length"
+ "missing imported symbol token"
+ "missing magic number"
+ "missing major version number"
+ "missing minor version number"
+ "missing module list offset"
+ "missing module list size"
+ "missing platform major"
+ "missing platform minor"
+ "missing platform update"
+ "missing private metadata offset"
+ "missing private metadata size"
+ "missing public metadata offset"
+ "missing public metadata size"
+ "missing reflection count"
+ "missing reserved bytes"
+ "missing script count"
+ "missing specialization info length"
+ "missing specialization script offset"
+ "missing tail padding"
+ "missing token data"
+ "missing token data size"
+ "missing triple arch/platform"
+ "missing unspecialized function hash"
+ "missing variable AIR major version"
+ "missing variable AIR minor version"
+ "missing variable Metal major version"
+ "missing variable Metal minor version"
+ "missing variable count"
+ "missing variable data size"
+ "missing variable external offset path"
+ "missing variable hash"
+ "missing variable module offset"
+ "missing variable module size"
+ "missing variable name length"
+ "missing variable private metadata offset"
+ "missing variable public metadata offset"
+ "missing variable reflection offset"
+ "missing variable source offset"
+ "missing variable token"
+ "mlir-elide-resource-strings-if-larger"
+ "more than two LC_BUILD_VERSION load commands"
+ "mps-call-conversion"
+ "mps-refine-dynamic-shapes"
+ "mps.call"
+ "mps.callables"
+ "mps.dequantize_lut"
+ "mps.entryFunctionName"
+ "mps.fullyPlacedOnANE"
+ "mps.strided_slice_update"
+ "mps.unrealized_fold"
+ "mps.variable_from_tensor"
+ "mpsgraphPackageURL"
+ "mpsx.cpu"
+ "mpsx.deinterleave"
+ "mpsx.file_backed_constant"
+ "mpsx.fp_to_int_clamped"
+ "mpsx.gpu"
+ "mpsx.interleave"
+ "mpsx.list_pop_back"
+ "mpsx.list_push_back"
+ "mpsx.make_list"
+ "mpsx.region_return"
+ "mpsx.rms_norm"
+ "mpsx.sdpa"
+ "mpsx.sparse_dense_matmul"
+ "mpsx.stitched"
+ "mpsx.stitched_return"
+ "mpsx.var_handle"
+ "non-private labels cannot appear between .cfi_startproc / .cfi_endproc pairs"
+ "nvidiagpu_gk"
+ "nvidiagpu_gm"
+ "nvidiagpu_gp"
+ "nvidiagpu_gv"
+ "nvvm."
+ "objc_claimAutoreleasedReturnValue"
+ "off"
+ "on"
+ "only support cases when total number of inputs and outputs <= 8"
+ "op"
+ "op-pipelines"
+ "opMayAliasInputToOutput"
+ "operand must be int/float/complex"
+ "operandSegmentSizes"
+ "optdebug"
+ "optimizeOriginalModule"
+ "output.hasOneUse()"
+ "pass-manager"
+ "pattern should not be applied if matmul ops have more than one use"
+ "pblendw"
+ "pmaxsb"
+ "pmul.dq.512"
+ "pmulu.dq"
+ "pmulu.dq.512"
+ "preEvalOptimizeOp"
+ "pre_encode_ti_tmp"
+ "predicateTensorData.mpsndarray != nil"
+ "previous .cfi_startproc was here"
+ "ptrAuthAuthenticatesNullValues"
+ "ptrAuthAuthenticationMode"
+ "ptrAuthExtraDiscriminator"
+ "ptrAuthIsAddressDiscriminated"
+ "ptrAuthIsaPointer"
+ "ptrAuthKey"
+ "ptrauth.abi-version"
+ "q2\xf0\xc1\xf4\x114"
+ "quant-ops-max-inputs-count"
+ "query"
+ "queryTensor"
+ "readResources:fromURL:usingAllocator:"
+ "reading a sparse array found index "
+ "reading sparse array with indexing above 8 bits: "
+ "reduce."
+ "referenced attribute does not match previous definition: "
+ "requires attribute 'inlineMode'"
+ "requires attribute 'lowerBoundMap'"
+ "requires attribute 'operandSegmentSizes'"
+ "requires attribute 'symbolName'"
+ "requires attribute 'upperBoundMap'"
+ "requires epsilon to be a scalar"
+ "requires input or output is a complex type"
+ "resourceManager"
+ "resourceOffsets"
+ "resources.bin"
+ "result must be int/float/complex"
+ "result type cannot match reduction attribute"
+ "result.hasOneUse()"
+ "resultSegmentSizes"
+ "resultType"
+ "returnOpForFunctionInModule:"
+ "rhs is not a non-broadcasting scalar constant"
+ "riscv."
+ "scale != nullptr && zeroPoint != nullptr && \"affine quantization parameters missing\""
+ "scale must be a scalar"
+ "scaleMul"
+ "scaleTensor"
+ "scaled_dot_product_attention"
+ "section_frame"
+ "sepos"
+ "serenity"
+ "setAlpha:"
+ "setCallables:"
+ "setCallablesDescription:"
+ "setConstantData:"
+ "setEntryFunctionName:"
+ "setIoSurfaces:"
+ "setMpsgraphPackageURL:"
+ "setResourceOffsets:"
+ "setResourceStorageMode:"
+ "setSparseFormat:"
+ "setSpecializationCountMax:"
+ "setWithObjects:"
+ "size mismatch for operand/result_segment_size"
+ "size mismatch in attribute conversion: "
+ "sliceUpdateDataTensor:updateTensor:starts:ends:strides:startMask:endMask:squeezeMask:name:"
+ "sliceUpdateDataTensor:updateTensor:startsTensor:endsTensor:stridesTensor:startMask:endMask:squeezeMask:name:"
+ "specializedModuleWithDevice:inputShapes:compilationDescriptor:fallingBack:fallbackRuntimeKey:"
+ "spirv1.2"
+ "spirv1.3"
+ "spirv1.4"
+ "spirv1.5"
+ "squeezed dimension and expanded dimension did not match"
+ "srcOp && \"ANERegionCall inputs must always be TensorToMemRef\""
+ "sse2."
+ "sse41."
+ "sse42."
+ "sse4a."
+ "ssse3."
+ "storel.dq"
+ "supportsSecureCoding"
+ "symbolName"
+ "system command for generating dylib for the CPU Region"
+ "target-features"
+ "tensor dimensions must be non-negative"
+ "tf32"
+ "the LC_BUILD_VERSION, command "
+ "the pattern did not define a valid expand operation"
+ "the pattern did not define a valid squeeze operation"
+ "the two LC_BUILD_VERSION load commands are not for the platforms MACOS and MACCATALYST"
+ "this operation does not support properties"
+ "toDictionary"
+ "transposedArray && \"transpose failed\""
+ "trying to read an array of "
+ "trying to schedule a pass on an unsupported operation"
+ "tvOS17.4.0"
+ "ub.poison"
+ "uefi"
+ "underlying element-types must match in order to produce exactly the same output"
+ "unhandled sparse matrix format"
+ "unsignedLongLongValue"
+ "unsupported bias shape"
+ "unsupported bits for llvm.vp.is.fpclass test mask"
+ "unsupported concatenation dimension"
+ "unsupported dequantizeop"
+ "unsupported kernel and bias fusion"
+ "unsupported kernel shape"
+ "unsupported version requested "
+ "upperBoundMap"
+ "v24@0:8@\"NSCoder\"16"
+ "v24@?0^v8Q16"
+ "v32@?0@\"<MPSGraphShapedTypeProvider>\"8Q16^B24"
+ "v32@?0@\"NSString\"8@\"NSString\"16^B24"
+ "v40@0:8{ModuleOp=^{Operation}}16@24Q32"
+ "v48@0:8@16@24{function_ref<char *(llvm::StringRef, unsigned long long, unsigned long long)>=^?q}32"
+ "v64@0:8{ModuleOp=^{Operation}}16@24@32Q40{function_ref<void (mlir::mps::serialization::ModuleOp)>=^?q}48"
+ "v9.5-a"
+ "validation failed"
+ "value isn't a global"
+ "varFromTensorWithTensor:name:"
+ "variable AIR major version required"
+ "variable AIR minor version required"
+ "variable Metal major version corrupted"
+ "variable Metal minor version corrupted"
+ "variable hash required"
+ "variable module offset required"
+ "variable name required"
+ "variable private metadata offset required"
+ "variable public metadata offset corrupted"
+ "vector_axis"
+ "verify() failed for `"
+ "vextracti128"
+ "vinserti128"
+ "visionOS"
+ "visionOS1.1.0"
+ "vpcmov.256"
+ "vperm2i128"
+ "wasm."
+ "when `axis` is not defined, `"
+ "writable"
+ "writeResources:"
+ "xcvalu"
+ "xcvbi"
+ "xcvbitmanip"
+ "xcvelw"
+ "xcvmac"
+ "xcvmem"
+ "xcvsimd"
+ "xop."
+ "xros"
+ "xsfcie"
+ "xsfvfnrclipxfqf"
+ "xsfvfwmaccqqq"
+ "xsfvqmaccdod"
+ "xsfvqmaccqoq"
+ "yptnk"
+ "zacas"
+ "zce"
+ "zeroPoint"
+ "zicfilp"
+ "zvfhmin"
+ "zvkb"
+ "zvknc"
+ "zvksc"
+ "{DenseMap<MPSGraphModuleKey, std::unique_ptr<LazyLoadableModuleRef>, MPSGraphModuleKeyInfo, llvm::detail::DenseMapPair<MPSGraphModuleKey, std::unique_ptr<LazyLoadableModuleRef>>>=\"Buckets\"^v\"NumEntries\"I\"NumTombstones\"I\"NumBuckets\"I}"
+ "{DenseMap<const void *, std::unique_ptr<RuntimeCacheEntry>, llvm::DenseMapInfo<const void *>, llvm::detail::DenseMapPair<const void *, std::unique_ptr<RuntimeCacheEntry>>>=\"Buckets\"^v\"NumEntries\"I\"NumTombstones\"I\"NumBuckets\"I}"
+ "{FuncOp=^{Operation}}16@0:8"
+ "{FuncOp=^{Operation}}24@0:8{ModuleOp=^{Operation}}16"
+ "{LazyLoadableModuleRef=\"_ctx\"{shared_ptr<mlir::MLIRContext>=\"__ptr_\"^{MLIRContext}\"__cntrl_\"^{__shared_weak_count}}\"_originalModule\"{OwningOpRef<mlir::ModuleOp>=\"op\"{ModuleOp=\"state\"^{Operation}}}\"_moduleURL\"@\"NSURL\"\"_resourceLoader\"{shared_ptr<ModuleResourcesLoader>=\"__ptr_\"^{ModuleResourcesLoader}\"__cntrl_\"^{__shared_weak_count}}}"
+ "{MPSGraphOperatingSystemVersion=qqq}16@0:8"
+ "{ModuleOp=^{Operation}}52@0:8@16@24@32B40r^v44"
+ "{ReturnOp=^{Operation}}24@0:8{ModuleOp=^{Operation}}16"
+ "{vector<mlir::NamedAttribute, std::allocator<mlir::NamedAttribute>>=^{NamedAttribute}^{NamedAttribute}{__compressed_pair<mlir::NamedAttribute *, std::allocator<mlir::NamedAttribute>>=^{NamedAttribute}}}40@0:8@16^v24@32"
+ "~ANERegionCallOpHandler"
- " ! = "
- " Expected "
- " does not support reading attributes from bytecode"
- " does not support reading types from bytecode"
- " does not support reading versioned attributes from bytecode"
- " does not support reading versioned types from bytecode"
- " index_tensor1 shape is: "
- " is not consistent with rank deduced from `fwd_shape` "
- " must be 0D tensor of 32-bit unsigned integer or 64-bit unsigned integer values or 0D memref of 32-bit unsigned integer or 64-bit unsigned integer values or static-shape defined tensor with shape equal to [1] or static-shape defined memref with shape equal to [1] or unranked tensor of 32-bit unsigned integer or 64-bit unsigned integer values or unranked.memref of 32-bit unsigned integer or 64-bit unsigned integer values, but got "
- " must be 0D tensor of 32-bit unsigned integer values or 1D tensor of 32-bit unsigned integer values or unranked tensor of 32-bit unsigned integer values or 0D memref of 32-bit unsigned integer values or 1D memref of 32-bit unsigned integer values or unranked.memref of 32-bit unsigned integer values, but got "
- " must be 0D tensor of 64-bit signed integer values or 0D memref of 64-bit signed integer values or static-shape defined tensor with shape equal to [1] or static-shape defined memref with shape equal to [1] or unranked tensor of 64-bit signed integer values or unranked.memref of 64-bit signed integer values, but got "
- " must be 0D tensor of MPS index type (si32 or si64) values or 0D memref of MPS index type (si32 or si64) values or static-shape defined tensor with shape equal to [1] or static-shape defined memref with shape equal to [1] or unranked tensor of MPS index type (si32 or si64) values or unranked.memref of MPS index type (si32 or si64) values, but got "
- " must be 0D tensor of MPS index type (si32 or si64) values or 1D tensor of MPS index type (si32 or si64) values or unranked tensor of MPS index type (si32 or si64) values or 0D memref of MPS index type (si32 or si64) values or 1D memref of MPS index type (si32 or si64) values or unranked.memref of MPS index type (si32 or si64) values, but got "
- " must be 0D tensor of MPS type values or 0D memref of MPS type values or static-shape defined tensor with shape equal to [1] or static-shape defined memref with shape equal to [1] or unranked tensor of MPS type values or unranked.memref of MPS type values, but got "
- " must be 0D tensor of floating point or 32/64-bit signed integer values or 0D memref of floating point or 32/64-bit signed integer values or static-shape defined tensor with shape equal to [1] or static-shape defined memref with shape equal to [1] or unranked tensor of floating point or 32/64-bit signed integer values or unranked.memref of floating point or 32/64-bit signed integer values, but got "
- " must be 0D tensor of floating point values or 0D memref of floating point values or static-shape defined tensor with shape equal to [1] or static-shape defined memref with shape equal to [1] or unranked tensor of floating point values or unranked.memref of floating point values, but got "
- " must be 0D tensor of floating point values or 1D tensor of floating point values or unranked tensor of floating point values or 0D memref of floating point values or 1D memref of floating point values or unranked.memref of floating point values, but got "
- " must be 0D/1D tensor of floating point or quantized values, but got "
- " must be MPS list element type, but got "
- " must be MPS list type, but got "
- " must be palette LUT, but got "
- " must be static-shape defined tensor with shape equal to [7] or static-shape defined memref with shape equal to [7] or unranked tensor of 32-bit signed integer values or unranked.memref of 32-bit signed integer values, but got "
- " must be tensor of 1-bit signless integer values or memref of 1-bit signless integer values, but got "
- " must be tensor of 16-bit float or 32-bit float values or memref of 16-bit float or 32-bit float values, but got "
- " must be tensor of 32-bit unsigned integer values or memref of 32-bit unsigned integer values, but got "
- " must be tensor of MPS index type (si32 or si64) values or memref of MPS index type (si32 or si64) values, but got "
- " must be tensor of MPS type values or memref of MPS type values or tensor of complex values or memref of complex values or tensor of quantized values or memref of quantized values, but got "
- " must be tensor of MPS type values or memref of MPS type values or tensor of complex values or memref of complex values, but got "
- " must be tensor of MPS type values or memref of MPS type values or tensor of quantized values or memref of quantized values, but got "
- " must be tensor of MPS type values or memref of MPS type values, but got "
- " must be tensor of any type values or memref of any type values, but got "
- " must be tensor of complex values or memref of complex values, but got "
- " must be tensor of floating point values or memref of floating point values or tensor of complex values or memref of complex values, but got "
- " must be tensor of floating point values or memref of floating point values, but got "
- " must be tensor of int values or memref of int values, but got "
- " must be tensor of unsigned quantized values, but got "
- " must be unranked tensor of 32-bit float values or 1D tensor of 32-bit float values or unranked.memref of 32-bit float values or 1D memref of 32-bit float values, but got "
- " must be unranked tensor of 32-bit float values or 2D tensor of 32-bit float values or unranked.memref of 32-bit float values or 2D memref of 32-bit float values, but got "
- " must be unranked tensor of 32-bit float values or 3D tensor of 32-bit float values or unranked.memref of 32-bit float values or 3D memref of 32-bit float values, but got "
- " must be unranked tensor of 32-bit float values or 4D tensor of 32-bit float values or unranked.memref of 32-bit float values or 4D memref of 32-bit float values, but got "
- " must be unranked tensor of 32-bit signed integer values or 1D tensor of 32-bit signed integer values or unranked.memref of 32-bit signed integer values or 1D memref of 32-bit signed integer values, but got "
- " must be unranked tensor of 32-bit signed integer values or unranked.memref of 32-bit signed integer values or static-shape defined tensor with shape equal to [5] or static-shape defined memref with shape equal to [5], but got "
- " must be unranked tensor of 32-bit unsigned integer values or ranked tensor type with rank equal to or greater than :2 or unranked.memref of 32-bit unsigned integer values or ranked memref type with rank equal to or greater than :2, but got "
- " must be unranked tensor of 32/64-bit signed integer values or 1D tensor of 32/64-bit signed integer values or unranked.memref of 32/64-bit signed integer values or 1D memref of 32/64-bit signed integer values, but got "
- " must be unranked tensor of MPS index type (si32 or si64) values or 3D tensor of MPS index type (si32 or si64) values or unranked.memref of MPS index type (si32 or si64) values or 3D memref of MPS index type (si32 or si64) values, but got "
- " must be unranked tensor of MPS type or complex values or unranked.memref of MPS type or complex values or ranked tensor type with rank equal to or greater than :1 or ranked memref type with rank equal to or greater than :1, but got "
- " must be unranked tensor of MPS type values or 4D tensor of MPS type values or unranked.memref of MPS type values or 4D memref of MPS type values, but got "
- " must be unranked tensor of MPS type values or 5D tensor of MPS type values or unranked.memref of MPS type values or 5D memref of MPS type values, but got "
- " must be unranked tensor of MPS type values or ranked tensor type with rank equal to or greater than :5 or unranked.memref of MPS type values or ranked memref type with rank equal to or greater than :5, but got "
- " must be unranked tensor of floating point values or 4D tensor of floating point values or unranked.memref of floating point values or 4D memref of floating point values, but got "
- " must be unranked tensor of floating point values or ranked tensor type with rank equal to or greater than :2 or unranked.memref of floating point values or ranked memref type with rank equal to or greater than :2, but got "
- " must be unranked tensor of int values or 1D tensor of int values or unranked.memref of int values or 1D memref of int values, but got "
- "!= "
- "!hasMin && \"min operand not supported\""
- "&getSemantics() == &RHS.getSemantics() && \"Should only compare APFloats with the same semantics\""
- "' failed to satisfy constraint: allowed 64-bit signless integer cases: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12"
- "' failed to satisfy constraint: any type attribute MPS list element type attribute"
- "'memref' must be ranked or unranked memref of any type values, but got "
- "'mps.deinterleave' op attribute 'interleave_factor' failed to satisfy constraint: 32-bit unsigned integer attribute"
- "'mps.deinterleave' op requires attribute 'interleave_factor'"
- "'mps.gru' op 'operand_segment_sizes' attribute for specifying operand segments must have 5 elements, but got "
- "'mps.gru' op requires attribute 'operand_segment_sizes'"
- "'mps.interleave' op attribute 'interleave_factor' failed to satisfy constraint: 32-bit unsigned integer attribute"
- "'mps.interleave' op requires attribute 'interleave_factor'"
- "'mps.lstm' op 'operand_segment_sizes' attribute for specifying operand segments must have 7 elements, but got "
- "'mps.lstm' op requires attribute 'operand_segment_sizes'"
- "'mps.make_list' op attribute 'element_type' failed to satisfy constraint: any type attribute MPS list element type attribute"
- "'mps.make_list' op attribute 'max_size' failed to satisfy constraint: 32-bit unsigned integer attribute"
- "'mps.make_list' op requires attribute 'element_type'"
- "'mps.resize' op 'operand_segment_sizes' attribute for specifying operand segments must have 4 elements, but got "
- "'mps.resize' op requires attribute 'operand_segment_sizes'"
- "'mps.resize_gradient' op 'operand_segment_sizes' attribute for specifying operand segments must have 4 elements, but got "
- "'mps.resize_gradient' op requires attribute 'operand_segment_sizes'"
- "'mps.singlegate_rnn' op 'operand_segment_sizes' attribute for specifying operand segments must have 4 elements, but got "
- "'mps.singlegate_rnn' op requires attribute 'operand_segment_sizes'"
- "'mps.singlegate_rnn_gradient' op 'operand_segment_sizes' attribute for specifying operand segments must have 5 elements, but got "
- "'mps.singlegate_rnn_gradient' op requires attribute 'operand_segment_sizes'"
- "'mpsx.dequantize_lut' op attribute 'axis' failed to satisfy constraint: 32-bit signed integer attribute"
- "'mpsx.quantized_matmul' op 'operand_segment_sizes' attribute for specifying operand segments must have 11 elements, but got "
- "'mpsx.quantized_matmul' op requires attribute 'operand_segment_sizes'"
- "'operand_segment_sizes' attribute for specifying operand segments must have 10 elements, but got "
- "'operand_segment_sizes' attribute for specifying operand segments must have 11 elements, but got "
- "'operand_segment_sizes' attribute for specifying operand segments must have 2 elements, but got "
- "'operand_segment_sizes' attribute for specifying operand segments must have 3 elements, but got "
- "'operand_segment_sizes' attribute for specifying operand segments must have 4 elements, but got "
- "'operand_segment_sizes' attribute for specifying operand segments must have 5 elements, but got "
- "'operand_segment_sizes' attribute for specifying operand segments must have 7 elements, but got "
- ", but got pointer: '0x"
- ", deduced from `fwd_shape`."
- ", should match dimension size: "
- "-[MPSGraphExecutable getOutputTypesWithDevice:inputTypes:compilationDescriptor:]_block_invoke"
- "-[MPSGraphExecutable specializeWithDevice:inputTypes:compilationDescriptor:]_block_invoke"
- "-[MPSGraphExecutable specializedModuleWithDevice:inputShapes:compilationDescriptor:fallingBack:]"
- "../mlir-mps/include/mlir-mps/Dialect/MPS/IR/MPSOps.h"
- "../mlir-mps/include/mlir-mps/Dialect/MPS/IR/TypeUtils.h"
- "../mlir-mps/include/mlir-mps/Support/RawAttributeUtils.h"
- "// autogenerated main\n\n#import \"modelObjC.h\"\n\nstatic inline size_t sizeofMPSDataTypeLocal(MPSDataType t)\n{\n    return (t & 0xFFFF) >> 3;\n}\n\nMPSGraphTensorData *allocateRandTensorDataWithType(MPSGraphShapedType *type,\n                                                   id<MTLDevice> runDevice,\n                                                   BOOL forceOnes){\n    srand(1);\n    NSInteger numberOfElements = 1;\n    for (int i = 0; i < type.shape.count; i++) {\n        numberOfElements *= type.shape[i].integerValue;\n    }\n    \n    size_t bufferLength = 1;\n    bufferLength *= (((type.shape[type.shape.count - 1].integerValue * sizeofMPSDataTypeLocal(type.dataType) + 63) / 64) * 64);\n    size_t rowBytes = bufferLength;\n    for (int i = 0; i < type.shape.count - 1; i++) {\n        bufferLength *= type.shape[i].integerValue;\n    }\n    \n    id<MTLBuffer> buffer = [runDevice newBufferWithLength:bufferLength options:MTLResourceStorageModeShared];\n    MPSGraphTensorData *tensorData = [[MPSGraphTensorData alloc] initWithMTLBuffer:buffer\n                                                                             shape:type.shape\n                                                                          dataType:type.dataType\n                                                                          rowBytes:rowBytes];\n    \n    MPSNDArray *ndArray = tensorData.mpsndarray;\n                                      \n    void *data = malloc(sizeofMPSDataTypeLocal(type.dataType) * numberOfElements);\n    if(type.dataType == MPSDataTypeFloat32){\n        float *dataIn = (float *)data;\n        for (int i = 0; i < numberOfElements; i++) {\n            dataIn[i] = forceOnes ? 1.0f : ((float)rand()/(float)RAND_MAX);\n        }\n    } else if (type.dataType == MPSDataTypeFloat16){\n        __fp16 *dataIn = (__fp16 *)data;\n        for (int i = 0; i < numberOfElements; i++) {\n            dataIn[i] = forceOnes ? 1.0f : ((float)rand()/(float)RAND_MAX);\n        }\n    } else if(type.dataType == MPSDataTypeComplexFloat32){\n        float *dataIn = (float *)data;\n        for (int i = 0; i < numberOfElements * 2; i++) {\n            dataIn[i] = forceOnes ? 1.0f : ((float)rand()/(float)RAND_MAX);\n        }\n    } else if (type.dataType == MPSDataTypeComplexFloat16){\n        __fp16 *dataIn = (__fp16 *)data;\n        for (int i = 0; i < numberOfElements * 2; i++) {\n            dataIn[i] = forceOnes ? 1.0f : ((float)rand()/(float)RAND_MAX);\n        }\n    } else if (type.dataType == MPSDataTypeBFloat16){\n        _bfloat16x1 *dataIn = (_bfloat16x1 *)data;\n        float val = 0.0f;\n        for (int i = 0; i < numberOfElements; i++) {\n            dataIn[i] = forceOnes ? 1.0f : ((float)rand()/(float)RAND_MAX);\n        }\n    }  else if (type.dataType == MPSDataTypeInt32){\n        int *dataIn = (int *)data;\n        for (int i = 0; i < numberOfElements; i++) {\n            dataIn[i] = rand();\n        }\n    } else {\n        assert(0);\n    }\n    \n    [ndArray writeBytes:data strideBytes:nil];\n    free(data);\n    \n    return tensorData;\n}\n\nvoid runPerfLoop(id<MTLCommandQueue> commandQueue,\n                 MPSGraphExecutable *graph,\n                 NSArray<MPSGraphTensorData *> * inputArray,\n                 NSArray<MPSGraphTensorData *> * _Nullable fetch,\n                 uint32_t iterations,\n                 uint32_t printEvery)\n{\n    // Build graph descriptor.\n    MPSGraphExecutableExecutionDescriptor *executableExecutionDescriptor = [MPSGraphExecutableExecutionDescriptor new];\n    executableExecutionDescriptor.waitUntilCompleted = YES;\n    // pre-warm up iteration\n    // If the outputs are dynamic, they'll be allocated by the runtime in the first run.\n    fetch = [graph runAsyncWithMTLCommandQueue:commandQueue\n                                   inputsArray:inputArray\n                                  resultsArray:fetch\n                           executionDescriptor:executableExecutionDescriptor];\n    \n    __block double sumOfTimes = 0;\n    CFAbsoluteTime startTime = CFAbsoluteTimeGetCurrent();\n    __block CFAbsoluteTime tic = CFAbsoluteTimeGetCurrent();\n    dispatch_semaphore_t doubleBufferingSemaphore = dispatch_semaphore_create(2);\n    __block uint32_t currentIteration = 0;\n    for (uint32_t i = 1; i <= iterations; i++) {\n        @autoreleasepool {\n            dispatch_semaphore_wait(doubleBufferingSemaphore, DISPATCH_TIME_FOREVER);\n            MPSGraphExecutableExecutionDescriptor *executableExecutionDescriptor = [MPSGraphExecutableExecutionDescriptor new];\n            if (i == iterations)\n                executableExecutionDescriptor.waitUntilCompleted = YES;\n            \n            executableExecutionDescriptor.completionHandler =\n            ^(NSArray<MPSGraphTensorData *> *_Nonnull results, NSError *_Nullable error) {\n                if (error) {\n                    NSLog(@\"%@\\n%@\", error.localizedDescription, error.description);\n                    exit(1);\n                }\n                dispatch_semaphore_signal(doubleBufferingSemaphore);\n                currentIteration++;\n                CFAbsoluteTime toc = CFAbsoluteTimeGetCurrent();\n                double time = 1000 * (toc - tic) / currentIteration;\n                // Exclude the first iteration from the average.\n                sumOfTimes += time;\n                \n                if (currentIteration % printEvery == 0)\n                    NSLog(@\"Time taken by MPSGraph eval in iteration %d = %f ms\\n\", currentIteration, time);\n            };\n            \n            // If the outputs are dynamic, they'll be allocated by the runtime in the first run.\n            fetch = [graph runAsyncWithMTLCommandQueue:commandQueue\n                                           inputsArray:inputArray\n                                          resultsArray:fetch\n                                   executionDescriptor:executableExecutionDescriptor];\n        }\n    }\n    CFAbsoluteTime endTime = CFAbsoluteTimeGetCurrent();\n    double numIter = MAX(iterations, 1);\n    NSLog(@\"Average MPSGraph full execution time = %f ms\\n\", 1000 * (endTime - startTime) / numIter);\n}\n\nBOOL isDynamicShape(MPSGraphShapedType *type){\n    if(!type.shape){\n        return YES;\n    }\n    for (int i = 0; i < type.shape.count; i++) {\n        if(type.shape[i].integerValue == -1){\n            return YES;\n        }\n    }\n    return NO;\n}\n\nint main(int argc, const char * argv[]) {\n    @autoreleasepool {\n        // Replace nil with path to .dat file if model has constants\n        NSString* constantsPath = [[NSBundle mainBundle] pathForResource: @\"modelObjCConstants\" ofType: @\"dat\"];\n        NSData *loadedData = constantsPath ? [NSData dataWithContentsOfFile:constantsPath] : nil;\n        \n        NSMutableArray<MPSGraphShapedType *> *inputTypes = [NSMutableArray new];\n        NSMutableArray<MPSGraphShapedType *> *outputTypes = [NSMutableArray new];\n        \n        MPSGraphExecutable *executable = getMPSGraphExecutable(loadedData, inputTypes, outputTypes, MPSGraphOptimizationLevel1);\n//        printf(\"%s\\n\", [executable debugDescription].UTF8String);\n        \n        id<MTLDevice> device = MTLCreateSystemDefaultDevice();\n        id<MTLCommandQueue> commandQueue = device.newCommandQueue;\n        \n        NSMutableArray<MPSGraphTensorData *> *inputArray = @[].mutableCopy;\n        for(NSUInteger i = 0; i < inputTypes.count; i++){\n            MPSGraphShapedType *type = inputTypes[i];\n            assert(!isDynamicShape(type) && \"Please provide the shape since the model supports dynamic shapes\");\n            [inputArray addObject:allocateRandTensorDataWithType(type, device, NO)];\n        }\n        NSMutableArray<MPSGraphTensorData *> *outputArray = @[].mutableCopy;\n        for(NSUInteger i = 0; i < outputTypes.count; i++){\n            MPSGraphShapedType *type = outputTypes[i];\n            assert(!isDynamicShape(type) && \"Please provide the shape since the model supports dynamic shapes\");\n            [outputArray addObject:allocateRandTensorDataWithType(type, device, NO)];\n        }\n        \n        runPerfLoop(commandQueue, executable, inputArray, outputArray, 100, 20);\n        \n    }\n    return 0;\n}\n        "
- "0 && \"ERROR: argument of a different block?\""
- "17.0.0git"
- "17.0.0git')"
- "3D Pooling only supports NDCHW, kernel/stride indicated NCDHW"
- "4.3"
- "@48@0:8Q16{MPSGraphVersion=qqq}24"
- "@[]"
- "ANE Scheduling Error = %@"
- "APFloat.h"
- "Adjustment + Size >= Size && \"Adjustment + Size must not overflow\""
- "AlignedAddr + Size <= (uintptr_t)NewSlab + PaddedSize"
- "AlignedAddr + SizeToAllocate <= (uintptr_t)End && \"Unable to allocate memory!\""
- "All sparse tensors should have rank == 1 Sparse tensors here have rank "
- "Allocate"
- "Allocator.h"
- "AveragePool"
- "DIArgList should have no operands other than a list of ValueAsMetadata"
- "Device ID: "
- "Error importing MLIR bytecode.\n"
- "Error: MLIR pass manager failed for debug conversion"
- "Execute graph viewer in the background. Creates tmp file litter."
- "FunctionOpInterfaces.h.inc"
- "GPURegionOps.mm"
- "Globals cannot contain scalable vectors"
- "INFINITY"
- "IR upgrade to version {0}.{1}.{2} failed"
- "Invalid attribute `archive` in property conversion: "
- "Invalid attribute `data_size` in property conversion: "
- "Invalid attribute `element_type` in property conversion: "
- "Invalid attribute `identifier` in property conversion: "
- "Invalid attribute `interleave_factor` in property conversion: "
- "Invalid attribute `max_size` in property conversion: "
- "Invalid attribute `operand_segment_sizes` in property conversion: "
- "Invalid attribute `transpose_dense` in property conversion: "
- "Invalid attribute `transpose_sparse` in property conversion: "
- "L2NormPool"
- "MLIR17.0.0git"
- "MPSGRAPH_DUMP_MODULE EV is set to %s.\n"
- "NAN"
- "Name.endswith(\"]\") && \"Name doesn't end in the substitution key!\""
- "OFFSET_CORNERS"
- "Only CSC storage format supported."
- "Parser.h"
- "PointerUnion.h"
- "Ptr != End && \"incrementing end() iterator\""
- "RawAttributeUtils.h"
- "Sparse Tensor shape is : ["
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::(anonymous namespace)::ConvertElementwiseUnary<mlir::mps::SignbitOp, mlir::anec::ElementwiseLessThanZero>]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::(anonymous namespace)::FoldOperation<mlir::anec::LiveIOReshape>]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::LifeCycleOpInterface]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::OpTrait::HasParent<mlir::mps::StitchedOp>::Impl<Empty>]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::OpTrait::mps::Reduction<Empty>]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::OpTrait::mps::Stitchable<Empty>]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::anec::(anonymous namespace)::ANEC_MaxToRelu1]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::anec::(anonymous namespace)::ANEC_MaxToRelu2]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::anec::(anonymous namespace)::CanonicalizeANEIOWithInterleave<mlir::mps::InterleaveOp, mlir::placement::MemrefToTensor>]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::anec::(anonymous namespace)::CanonicalizeANEIOWithInterleave<mlir::placement::TensorToMemref, mlir::mps::DeinterleaveOp>]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::arith::detail::MaxFOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::arith::detail::MinFOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::LowerInterleave<mlir::mps::DeinterleaveOp>]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::LowerInterleave<mlir::mps::InterleaveOp>]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::MPS_LowerSigmoid]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::MPS_Sparse_DenseSparseMatMulFusedOp]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::MPS_Sparse_SparseDenseMatMulFusedOp]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::(anonymous namespace)::MPS_Sparse_SparseTransposeDenseMatMulFusedOp]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::ListType]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::CPUOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::CreateSparseTensorOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::DeinterleaveOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::DenseSparseMatMulOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::FileBackedConstantOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::GPUOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::InterleaveOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::MakeListOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::SparseDenseMatMulOpGenericAdaptorBase::Properties]"
- "StringRef llvm::getTypeName() [DesiredTypeName = mlir::mps::detail::VarHandleOpGenericAdaptorBase::Properties]"
- "This operation does not support properties"
- "Twine"
- "Twine.h"
- "Types.h"
- "]x"
- "^experimental.vector.reduce.([a-z]+)\\.[a-z][0-9]+"
- "^experimental.vector.reduce.v2.([a-z]+)\\.[fi][0-9]+"
- "^v52@0:8@16{ModuleOp=^{Operation}}24@32@40B48"
- "__arg%d"
- "__out:%d"
- "_enableGPUQuantizationOps"
- "_relu"
- "` but it isn't registered in this MLIRContext: the dialect may not be loaded or this operation isn't registered by the dialect. See also https://mlir.llvm.org/getting_started/Faq/#registered-loaded-dependent-whats-up-with-dialects-management"
- "`grad_input` rank "
- "`grad_input`["
- "abs.i"
- "addEntryBlock"
- "amdgcn.alignbit"
- "ananas"
- "anec.live_io_reshape"
- "approx-func-fp-math"
- "argsort"
- "arith.maxf"
- "arith.minf"
- "atomic.load.add.f32.p"
- "atomic.load.add.f64.p"
- "avx2.movntdqa"
- "avx2.pmul.dq"
- "avx2.pmulu.dq"
- "avx512.kand.w"
- "avx512.kortestc.w"
- "avx512.mask.cvtpd2dq.256"
- "avx512.mask.cvtqq2ps.256"
- "avx512.mask.cvttpd2dq.256"
- "avx512.movntdqa"
- "band_part"
- "batch_norm"
- "batch_to_space"
- "begins"
- "biquantWeightsScale == nullptr && biquantWeightsZeroPoint == nullptr && \"double quantization not supported\""
- "bitPosition < getBitWidth() && \"Bit position out of bounds!\""
- "cast_if_present"
- "castedOp1 is not ::mlir::mps::CreateSparseTensorOp type"
- "castedOp1 is not ::mlir::mps::TransposeOp type"
- "clamped_relu"
- "cloudabi"
- "compare"
- "concat"
- "const"
- "constexpr_cast"
- "constexpr_lut_to_dense"
- "constructContainerOpForParserIfNecessary"
- "contiki"
- "conv"
- "conv_transpose"
- "createBytecodeFromMlirModule:fileHandle:"
- "createVersionedBytecodeFromMlirModule:packageKey:fileHandle:"
- "crop"
- "crop_resize"
- "crops"
- "cumsum"
- "current mps dialect version is {0}.{1}.{2}, can't parse version {3}.{4}.{5}"
- "dagId >= 0 && dagId < LstmDagId_count"
- "dagId >= 0 && dagId < LstmGradDagId_count"
- "dbg.addr"
- "dbg.value"
- "denseCols and sparseRows have mismatch shape. denseCols != sparseRows "
- "denseCols and sparseRows have mismatch shape. sparseCols != denseRows "
- "depth_to_space"
- "dequantize"
- "dialect "
- "div"
- "dynamic_dequantize"
- "dynamic_quantize"
- "einsum"
- "elu"
- "empty() && \"function already has an entry block\""
- "ends"
- "entities 'storageType' failed to satisfy constraint: 'is sparse format CSC'"
- "entities 'zeroVal' failed to satisfy constraint: ''"
- "exp"
- "exp2"
- "expand_dims"
- "expected data iterator aligned to "
- "expected key entry for archive in DictionaryAttr to set Properties."
- "expected key entry for data_size in DictionaryAttr to set Properties."
- "expected key entry for element_type in DictionaryAttr to set Properties."
- "expected key entry for identifier in DictionaryAttr to set Properties."
- "expected key entry for interleave_factor in DictionaryAttr to set Properties."
- "expected key entry for operand_segment_sizes in DictionaryAttr to set Properties."
- "expected op 'mps.materialize_sparse_tensor' to have attribute 'storage_type' of type '::mlir::mps::SparseTensorStorageAttr'"
- "expected string or keyword containing one of the following enum values for attribute 'kind' [addf, addi, assign, maxf, maxs, maxu, minf, mins, minu, mulf, muli, ori, andi]"
- "exports trie"
- "failed to read sparse shape of size: "
- "failed to verify that type of 'value' matches tensor equivalent of 'memref'"
- "failed: could not extract a valid constant axis within the input tensor rank."
- "fill"
- "fill_like"
- "flatten2d"
- "floor_div"
- "front"
- "gather_along_axis"
- "gather_nd"
- "gelu"
- "get"
- "getActiveBits() <= 64 && \"Too many bits for uint64_t\""
- "getNewRuntimeForDevice:module:inputShapes:compilationDescriptor:fallingBack:"
- "getSanitizedOpName"
- "getSplatValue"
- "getZExtValue"
- "greater"
- "greater_equal"
- "gru"
- "index_tensor0 and sparse_vals shape mismatch "
- "initWithDataType:isPerChannel:isAsymmetric:supportsMinValue:"
- "innerproduct"
- "input and min shapes not compatible for `axis` dim"
- "input and scale shapes not compatible for `axis` dim"
- "input and zeroPoint shapes not compatible for `axis` dim"
- "instance_norm"
- "invalid data symbol index"
- "invalid retained nodes, expected DILocalVariable or DILabel"
- "ios16."
- "ios17."
- "isSplat() && \"expected the attribute to be a splat\""
- "isValid() && \"Invalid twine!\""
- "isa<T>(*this) && \"Invalid accessor called\""
- "isa<X>(Val) && \"cast_if_present<Ty>() argument of incompatible type!\""
- "l2_norm"
- "l2_pool"
- "layer_norm"
- "lazyInitWithModuleURL:executableDescriptor:"
- "leaky_relu"
- "less-precise-fpmad"
- "less_equal"
- "linear"
- "linear_activation"
- "llvm.amdgcn.atomic.dec"
- "llvm.amdgcn.atomic.inc"
- "llvm.amdgcn.ldexp"
- "llvm::divideCeil(numElements, 8) == outData.size()"
- "local_response_norm"
- "log"
- "logical_and"
- "logical_not"
- "logical_or"
- "logical_xor"
- "lower_bound"
- "lstm"
- "match"
- "matmul"
- "max.i"
- "max.ui"
- "maxf"
- "memref.tensor_store"
- "milId"
- "min.ui"
- "minf"
- "minix"
- "missing ']' closing set of scalable dimensions"
- "mps.cpu"
- "mps.deinterleave"
- "mps.dense_sparse_matmul"
- "mps.file_backed_constant"
- "mps.gpu"
- "mps.interleave"
- "mps.list_pop_back"
- "mps.list_push_back"
- "mps.make_list"
- "mps.matmulPrimaryGrad"
- "mps.matmulSecondaryGrad"
- "mps.region_return"
- "mps.sparse_dense_matmul"
- "mps.stitched"
- "mps.stitched_return"
- "mps.var_handle"
- "mpsx.dequantize_lut"
- "nil"
- "no-infs-fp-math"
- "no-inline-line-tables"
- "no-jump-tables"
- "no-nans-fp-math"
- "no-signed-zeros-fp-math"
- "not_equal"
- "number of channels must be 1"
- "number of channels must be 2"
- "number of channels must be 4"
- "one_hot"
- "op->getNumRegions() == 1 && llvm::hasSingleElement(op->getRegion(0)) && \"expected generated operation to have a single region with a single \" \"block\""
- "optimizeOriginalModuleWithCompilationDescriptor:"
- "packBooleanData"
- "pad"
- "pad_type"
- "paddings"
- "paletteLut == nullptr && \"quantized LUT not supported\""
- "pixel_unshuffle"
- "pow"
- "prelu"
- "profile-sample-accurate"
- "q\x11\xf0\xf0\x11\xf4\x114"
- "quantize"
- "random_bernoulli"
- "random_normal"
- "random_uniform"
- "range_1d"
- "real_div"
- "region call does not implement runtime for regions different than ANE"
- "relu6"
- "repeatReturnCopy"
- "requires attribute 'operand_segment_sizes'"
- "resample"
- "reshape_like"
- "resize_bilinear"
- "result_segment_sizes"
- "rnn"
- "rsqrt"
- "scalar types differ for the values: "
- "scale != nullptr && zeroPoint != nullptr && \"only affine quantization supported\""
- "scatter_along_axis"
- "scatter_nd"
- "sigmoid_hard"
- "silu"
- "size() >= n && \"Dropping more elements than exist\""
- "slice_by_index"
- "slice_by_size"
- "softmax"
- "softplus"
- "softplus_parametric"
- "softsign"
- "space_to_batch"
- "space_to_depth"
- "specializedModuleWithDevice:inputShapes:compilationDescriptor:fallingBack:"
- "split"
- "sqrt"
- "squeeze"
- "sse.cvtsi2ss"
- "sse2.storel.dq"
- "sse41.pmaxsb"
- "sse41.pmuldq"
- "stddev"
- "subtypes.size() == SubtypesCount && \"subtypes array size differs from the expected number of subtypes\""
- "succeeded(result) && \"expected ConstantLike op to be foldable\""
- "this != &that && \"Self-move not supported\""
- "this->size() >= N && \"Cannot increase size with truncate\""
- "threshold"
- "thresholded_relu"
- "tile"
- "topk"
- "transformation was requested, but could not create a valid transpose op "
- "transpose_dense"
- "transpose_sparse"
- "transpose_sparse when sparse is right matrix is not supported"
- "truncate"
- "uint16"
- "uint32"
- "uint64"
- "unrecognized vendor-name: "
- "unsafe-fp-math"
- "upper_bound"
- "upsample_bilinear"
- "v32@0:8{ModuleOp=^{Operation}}16@24"
- "v40@0:8{ModuleOp=^{Operation}}16@24@32"
- "val"
- "variableFinalUpdate"
- "view-background"
- "wasm.dot.i8x16.i7x16.add.signed"
- "wasm.dot.i8x16.i7x16.signed"
- "webkit_jscc"
- "when `axis` is not defined, `min` must be a of size 1"
- "when `axis` is not defined, `scale` must be a of size 1"
- "when `axis` is not defined, `zeroPoint` must be a of size 1"
- "xTensor"
- "xop.vpcmov.256"
- "yptn"
- "{LazyLoadableModuleRef=\"_ctx\"{shared_ptr<mlir::MLIRContext>=\"__ptr_\"^{MLIRContext}\"__cntrl_\"^{__shared_weak_count}}\"_originalModule\"{OwningOpRef<mlir::ModuleOp>=\"op\"{ModuleOp=\"state\"^{Operation}}}\"_moduleURL\"@\"NSURL\"}"
- "{MPSGraphVersion=qqq}16@0:8"
- "{ModuleOp=^{Operation}}44@0:8@16@24@32B40"
- "{unordered_map<std::string, std::unique_ptr<RuntimeCacheEntry>, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<const std::string, std::unique_ptr<RuntimeCacheEntry>>>>=\"__table_\"{__hash_table<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, std::__unordered_map_hasher<std::string, std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, std::hash<std::string>, std::equal_to<std::string>>, std::__unordered_map_equal<std::string, std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, std::equal_to<std::string>, std::hash<std::string>>, std::allocator<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>>>=\"__bucket_list_\"{unique_ptr<std::__hash_node_base<std::__hash_node<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, void *> *> *[], std::__bucket_list_deallocator<std::allocator<std::__hash_node_base<std::__hash_node<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, void *> *> *>>>=\"__ptr_\"{__compressed_pair<std::__hash_node_base<std::__hash_node<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, void *> *> **, std::__bucket_list_deallocator<std::allocator<std::__hash_node_base<std::__hash_node<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, void *> *> *>>>=\"__value_\"^^v\"__value_\"{__bucket_list_deallocator<std::allocator<std::__hash_node_base<std::__hash_node<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, void *> *> *>>=\"__data_\"{__compressed_pair<unsigned long, std::allocator<std::__hash_node_base<std::__hash_node<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, void *> *> *>>=\"__value_\"Q}}}}\"__p1_\"{__compressed_pair<std::__hash_node_base<std::__hash_node<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, void *> *>, std::allocator<std::__hash_node<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, void *>>>=\"__value_\"{__hash_node_base<std::__hash_node<std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, void *> *>=\"__next_\"^v}}\"__p2_\"{__compressed_pair<unsigned long, std::__unordered_map_hasher<std::string, std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, std::hash<std::string>, std::equal_to<std::string>>>=\"__value_\"Q}\"__p3_\"{__compressed_pair<float, std::__unordered_map_equal<std::string, std::__hash_value_type<std::string, std::unique_ptr<RuntimeCacheEntry>>, std::equal_to<std::string>, std::hash<std::string>>>=\"__value_\"f}}}"
- "~RegionCallOpHandler"

```
