## Speech

> `/System/Library/Frameworks/Speech.framework/Speech`

```diff

-3403.3.1.0.0
-  __TEXT.__text: 0x1954f8
-  __TEXT.__auth_stubs: 0x2c60
-  __TEXT.__objc_methlist: 0x33b4
-  __TEXT.__cstring: 0x9c7b
-  __TEXT.__const: 0x7fa0
-  __TEXT.__constg_swiftt: 0x419c
-  __TEXT.__swift5_typeref: 0x541e
-  __TEXT.__swift5_reflstr: 0x34c8
-  __TEXT.__swift5_fieldmd: 0x2d9c
+3404.69.2.0.0
+  __TEXT.__text: 0x1b388c
+  __TEXT.__auth_stubs: 0x2f20
+  __TEXT.__objc_methlist: 0x4084
+  __TEXT.__const: 0xaa90
+  __TEXT.__dlopen_cstrs: 0x60
+  __TEXT.__cstring: 0xa361
+  __TEXT.__constg_swiftt: 0x4598
+  __TEXT.__swift5_typeref: 0x6154
+  __TEXT.__swift5_reflstr: 0x3837
+  __TEXT.__swift5_fieldmd: 0x316c
   __TEXT.__swift5_builtin: 0xdc
-  __TEXT.__swift5_assocty: 0x828
-  __TEXT.__swift5_proto: 0x554
-  __TEXT.__swift5_types: 0x2ec
-  __TEXT.__oslogstring: 0x3240
-  __TEXT.__swift5_capture: 0x1f28
-  __TEXT.__swift5_acfuncs: 0x460
-  __TEXT.__swift5_protos: 0x54
+  __TEXT.__swift5_assocty: 0xde0
+  __TEXT.__swift5_proto: 0x754
+  __TEXT.__swift5_types: 0x338
+  __TEXT.__oslogstring: 0x354a
+  __TEXT.__swift5_capture: 0x21bc
+  __TEXT.__swift5_acfuncs: 0x500
+  __TEXT.__swift_as_entry: 0x854
+  __TEXT.__swift_as_ret: 0x82c
+  __TEXT.__swift5_protos: 0x5c
   __TEXT.__swift5_mpenum: 0x38
   __TEXT.__gcc_except_tab: 0x80c
-  __TEXT.__dlopen_cstrs: 0x60
-  __TEXT.__unwind_info: 0x7338
-  __TEXT.__eh_frame: 0xef50
-  __TEXT.__objc_classname: 0x920
-  __TEXT.__objc_methname: 0xc6b0
-  __TEXT.__objc_methtype: 0x2534
-  __TEXT.__objc_stubs: 0x4a40
-  __DATA_CONST.__got: 0xce0
-  __DATA_CONST.__const: 0x1450
-  __DATA_CONST.__objc_classlist: 0x3c0
+  __TEXT.__unwind_info: 0x7ef0
+  __TEXT.__eh_frame: 0x107d8
+  __TEXT.__objc_classname: 0xa15
+  __TEXT.__objc_methname: 0xd1ca
+  __TEXT.__objc_methtype: 0x2726
+  __TEXT.__objc_stubs: 0x4b20
+  __DATA_CONST.__got: 0xe00
+  __DATA_CONST.__const: 0x1428
+  __DATA_CONST.__objc_classlist: 0x408
   __DATA_CONST.__objc_catlist: 0x18
-  __DATA_CONST.__objc_protolist: 0x148
+  __DATA_CONST.__objc_protolist: 0x178
   __DATA_CONST.__objc_imageinfo: 0x8
-  __DATA_CONST.__objc_selrefs: 0x2700
-  __DATA_CONST.__objc_protorefs: 0xb8
-  __DATA_CONST.__objc_superrefs: 0x1d8
-  __DATA_CONST.__objc_arraydata: 0x20
-  __AUTH_CONST.__auth_got: 0x1640
-  __AUTH_CONST.__auth_ptr: 0xce8
-  __AUTH_CONST.__const: 0x8a80
-  __AUTH_CONST.__cfstring: 0x3420
-  __AUTH_CONST.__objc_const: 0xc628
-  __AUTH_CONST.__objc_dictobj: 0x28
+  __DATA_CONST.__objc_selrefs: 0x2958
+  __DATA_CONST.__objc_protorefs: 0xe0
+  __DATA_CONST.__objc_superrefs: 0x1f8
+  __DATA_CONST.__objc_arraydata: 0x30
+  __AUTH_CONST.__auth_got: 0x17a0
+  __AUTH_CONST.__auth_ptr: 0xf90
+  __AUTH_CONST.__const: 0x9e08
+  __AUTH_CONST.__cfstring: 0x3640
+  __AUTH_CONST.__objc_const: 0xbdc0
+  __AUTH_CONST.__objc_dictobj: 0x50
   __AUTH_CONST.__objc_intobj: 0xf0
-  __AUTH.__objc_data: 0x8b0
-  __AUTH.__data: 0xc88
-  __DATA.__objc_ivar: 0x4e0
-  __DATA.__data: 0x4c70
-  __DATA.__bss: 0x8820
-  __DATA.__common: 0x2e8
-  __DATA_DIRTY.__objc_data: 0x1968
-  __DATA_DIRTY.__data: 0x3b08
+  __AUTH.__objc_data: 0xb50
+  __AUTH.__data: 0x1268
+  __DATA.__objc_ivar: 0x50c
+  __DATA.__data: 0x5758
+  __DATA.__common: 0x320
+  __DATA.__bss: 0xc580
+  __DATA_DIRTY.__objc_data: 0x16a8
+  __DATA_DIRTY.__data: 0x3948
+  __DATA_DIRTY.__bss: 0x448
   __DATA_DIRTY.__common: 0x60
-  __DATA_DIRTY.__bss: 0x468
   - /System/Library/Frameworks/AVFAudio.framework/AVFAudio
   - /System/Library/Frameworks/AVFoundation.framework/AVFoundation
   - /System/Library/Frameworks/CFNetwork.framework/CFNetwork

   - /usr/lib/swift/libswiftOSLog.dylib
   - /usr/lib/swift/libswiftObjectiveC.dylib
   - /usr/lib/swift/libswiftQuartzCore.dylib
+  - /usr/lib/swift/libswiftSynchronization.dylib
   - /usr/lib/swift/libswiftUniformTypeIdentifiers.dylib
   - /usr/lib/swift/libswiftXPC.dylib
   - /usr/lib/swift/libswift_Builtin_float.dylib

   - /usr/lib/swift/libswiftsimd.dylib
   - /usr/lib/swift/libswiftsys_time.dylib
   - /usr/lib/swift/libswiftunistd.dylib
-  Functions: 10221
-  Symbols:   4815
-  CStrings:  3575
+  Functions: 11661
+  Symbols:   5281
+  CStrings:  3694
 
Symbols:
+ _CMTimeRangeGetIntersection
+ _NSProgressFileOperationKindDownloading
+ _NSProgressKindFile
+ _NSProgressUseItemDescriptionKey
+ _OBJC_CLASS_$_EARFullPayloadCorrectionResult
+ _OBJC_CLASS_$_SFSpeechAnalyzerSpeechDetectorOptions
+ _OBJC_CLASS_$_SFSpeechDetectorResult
+ _OBJC_CLASS_$__SFSpeechRecognizerEndpointDetectorOptions
+ _OBJC_CLASS_$__SFSpeechRecognizerSpeechDetectorOptions
+ _OBJC_CLASS_$__TtC6Speech25AssetsInstallationRequest
+ _OBJC_METACLASS_$_EARFullPayloadCorrectionResult
+ _OBJC_METACLASS_$_SFSpeechAnalyzerSpeechDetectorOptions
+ _OBJC_METACLASS_$_SFSpeechDetectorResult
+ _OBJC_METACLASS_$__SFSpeechRecognizerEndpointDetectorOptions
+ _OBJC_METACLASS_$__SFSpeechRecognizerSpeechDetectorOptions
+ _OBJC_METACLASS_$__TtC6Speech25AssetsInstallationRequest
+ _SFEndModelRetention
+ _SFGeneralASRReplacementLanguageCodeForLocaleIdentifier
+ _SFLanguageDetectorLanguagePlaceholder
+ _SFModelAttributeCodeAutoPunctuation
+ _SFModelAttributeCodeContinuousListening
+ _SFModelAttributeCodeEmojiRecognition
+ _SFModelAttributeCodeOfflineOnly
+ _SFModelAttributeCodeOnDeviceSearch
+ _SFModelAttributeCodePreferOverServer
+ _SFModelAttributeCodeVoiceCommands
+ __class_setCustomDeallocInitiation
+ _dispatch_async_f
+ _objc_unsafeClaimAutoreleasedReturnValue
+ _pthread_main_np
+ _swift_asyncLet_begin
+ _swift_asyncLet_finish
+ _swift_asyncLet_get
+ _swift_getKeyPath
+ _swift_runtimeSupportsNoncopyableTypes
+ _xpcInterface_EARFullPayloadCorrector
+ _xpcInterface_EARTranscriptionEvaluator
- _OBJC_CLASS_$__SFSpeechRecognizerDetectorOptions
- _OBJC_CLASS_$__TtC6Speech15AnalysisContext
- _OBJC_CLASS_$__TtC6Speech20ModelDownloadRequest
- _OBJC_METACLASS_$__SFSpeechRecognizerDetectorOptions
- _OBJC_METACLASS_$__TtC6Speech15AnalysisContext
- _OBJC_METACLASS_$__TtC6Speech20ModelDownloadRequest
- _SFAnalysisContextTagGeoLMRegionID
- _kCMTimeRangeZero
CStrings:
+ "\x04\x11!"
+ "\x14\x12"
+ "\x15"
+ "\x17"
+ " Transcriber cannot be initialized with both `.emoji` and `.normalizedTranscription`."
+ " Transcriber cannot be initialized with both `.longerContextualization` and `.normalizedTranscription`."
+ " Transcriber cannot be initialized with both `.normalizedTranscription` and `.contextualizedTranscription`."
+ " Transcriber cannot be initialized with both `.punctuation` and `.normalizedTranscription`."
+ " Transcriber must be initialized with either `.normalizedTranscription` or `.contextualizedTranscription`."
+ " using Transcriber that is not associated with same SpeechAnalyzer"
+ "\""
+ "%@"
+ "%s Adding known usages for %@ assets: %@"
+ "%s All usages for %@ assets: %@"
+ "%s Client (%@) async fetching the path to the %@ asset for: %@"
+ "%s Client (%@) async unsubscribing from the %@ asset for: %@"
+ "%s Device %@ %@%d+"
+ "%s Device aneSubType: %@"
+ "%s MUX: Checking %zu container(s) for valid speech profiles."
+ "%s MUX: Failed to load speech profile at path: %@, error: %@"
+ "%s MUX: Failed to retrieve file attributes at path: %@, error: %@"
+ "%s MUX: Loaded %zu speech profile(s) in total."
+ "%s MUX: Loaded speech profile at path: %@"
+ "%s MUX: No cached speech profile for path: %@"
+ "%s MUX: No speech profile exists at path: %@"
+ "%s MUX: No speech profile site URL for personaId: %@"
+ "%s MUX: Out of %zu container(s), %zu speech profile(s) are present on disk."
+ "%s MUX: Reusing cached speech profile at path: %@"
+ "%s MUX: loadedProfiles cannot be nil."
+ "%s Refreshed %@ asset set with usage: %@"
+ "%s Refreshed all %@ asset sets."
+ "+[SFEntitledAssetManager assetSetUsagesForAssetType:]_block_invoke"
+ "+[SFSpeechAssetManager pathToAssetWithConfig:clientIdentifier:completion:]"
+ "+[SFSpeechAssetManager unsubscribeFromAssetWithConfig:clientIdentifier:completion:]"
+ "+[SFUtilities isANETypeAtLeastVersion:prefix:]"
+ ", resultsFinalizationTime "
+ "-[SFEntitledAssetManager _refreshAssetSetsWithName:]"
+ ".etiquetteReplacements flags desired "
+ "/Library/Caches/com.apple.xbs/Sources/SpeechFramework/SpeechAnalyzer/Assets/Assets.swift"
+ "/Library/Caches/com.apple.xbs/Sources/SpeechFramework/SpeechAnalyzer/Assets/AssetsInstallationRequest.swift"
+ "/Library/Caches/com.apple.xbs/Sources/SpeechFramework/SpeechAnalyzer/SpeechDetector.swift"
+ "/Library/Caches/com.apple.xbs/Sources/SpeechFramework/SpeechAnalyzer/TranscriberResults.swift"
+ "<Speech.SpeechDetector.Result: range "
+ "<_SFSpeechRecognizerEndpointDetectorOptions: detectAfterTime %f>"
+ "<_SFSpeechRecognizerModelOptions: farField %d, geoLMRegionID %@, supplementalModelURL %@, modelOverrideURL %@, speechProfileURLs %@, userIdMask %@, taskForMemoryLock %@, atypicalSpeech %d, enableParallelLoading %d, speechProfileContainers %@>"
+ "<_SFSpeechRecognizerSpeechDetectorOptions: sensitivityLevel %lu>"
+ "<_SFSpeechRecognizerSupportedFeatures (%p): locale %@, taskNames %@, singleUtterance %d, concatenateUtterances %d, modelOptions %@, endpointDetectionOptions %@, speechDetectionOptions %@, flags %#lx>"
+ "@\"_SFSpeechRecognizerEndpointDetectorOptions\""
+ "@\"_SFSpeechRecognizerSpeechDetectorOptions\""
+ "@108@0:8@16@24@32@40@48@56@64@72@80B88@92@?100"
+ "@112@0:8@16@24Q32@40{?={?=qiIq}{?=qiIq}}48@96d104"
+ "@116@0:8@16@24@32@40@48@56@64@72@80@88@96B104@?108"
+ "@124@0:8@16@24@32@40@48@56@64@72@80@88@96B104@108@?116"
+ "@132@0:8@16@24@32@40@48@56@64@72@80@88@96@104B112@116@?124"
+ "@44@0:8B16Q20@28@36"
+ "@52@0:8@16@24B32@36@?44"
+ "@64@0:8{?={?=qiIq}{?=qiIq}}16"
+ "@68@0:8@16B24@28@36@44@52B60B64"
+ "@80@0:8@16@24B32B36@40@48@56@64Q72"
+ "@84@0:8B16@20@28@36@44@52@60B68B72@76"
+ "Assets must be initialized with a locale that specifies a language"
+ "Assets needed for PhoneticEmbedder in %s are not installed on this device."
+ "Audio input time range is invalid"
+ "B28@0:8i16@20"
+ "CMTimeRangeValue"
+ "Cannot create SpeechDetector-only worker; use Transcriber worker instead"
+ "Custom transcriptionOptions provided: "
+ "EARFullPayloadCorrectionResult"
+ "LanguageDetector.%s: No available model info; fetching `Assets` to query"
+ "M"
+ "No assets to install supplied"
+ "SFSpeechAnalyzerSpeechDetectorOptions"
+ "SFSpeechAnalyzerSpeechDetectorResultDelegate"
+ "SFSpeechDetectorResult"
+ "Speech.AssetsInstallationRequest"
+ "Speech.ConfidenceAttribute"
+ "Speech.EARFullPayloadCorrector.correctPostITNOutput(_:withOptions:)"
+ "Speech.EARFullPayloadCorrector.hello()"
+ "Speech.EARSpeechRecognizer.isVADSupportedForRequest()"
+ "Speech.EARTranscriptionEvaluator.evaluateMessagesContext(_:recognizedText:correctedText:asrID:speechProfilePath:)"
+ "Speech.EARTranscriptionEvaluator.hello()"
+ "Speech.LocalSpeechRecognitionService.isEuclidAvailable(forConfigPath:)"
+ "Speech.LocalSpeechRecognitionService.makeFullPayloadCorrectorInstance(withLocale:clientID:)"
+ "Speech.LocalSpeechRecognitionService.makeLSRAssets(for:shouldSubscribe:clientID:modelOverridePath:isSpelling:)"
+ "Speech.LocalSpeechRecognitionService.makeTranscriptionEvaluator()"
+ "Speech.TimeRangeAttribute"
+ "Speech/Chunked.swift"
+ "Speech/EARFullPayloadCorrector.swift"
+ "Speech/ObjCSpeechAnalyzer.swift"
+ "Speech/SpeechDetector.swift"
+ "SpeechAnalyzer: Executing finalize/finish input barrier at %@"
+ "SpeechDetector encountered an error during recognition: %@"
+ "SpeechDetector.isWorkerUsable: Worker not usable because %s"
+ "T@\"NSDictionary\",R,C,N,V_loggingInfo"
+ "T@\"NSError\",R,C,N,V_error"
+ "T@\"NSString\",R,C,N,V_bestFormattedString"
+ "T@\"NSString\",R,C,N,V_result"
+ "T@\"_SFSpeechRecognizerEndpointDetectorOptions\",R,C,N,V_endpointDetectionOptions"
+ "T@\"_SFSpeechRecognizerSpeechDetectorOptions\",R,C,N,V_speechDetectionOptions"
+ "TB,N,V_reportResults"
+ "TQ,N,V_sensitivityLevel"
+ "TQ,R,N,V_modelRetention"
+ "TQ,R,N,V_sensitivityLevel"
+ "Td,R,N,V_bestFormattedStringSegmentConfidence"
+ "Transcriber must be initialized with a locale that specifies a language"
+ "Transcriber.makeWorkerUsable: Concatenate-utterances option different, cannot make usable"
+ "Transcriber.makeWorkerUsable: Must-match flags different, cannot make usable"
+ "Transcriber: Saved end-of-recognition result %s"
+ "Transcriber: Skipped empty/internal results"
+ "Unable to unarchive object, result was nil"
+ "Unable to unsubscribe %s from %@: %@"
+ "VAD sensitivity level desired "
+ "Voice activity detection model was unable to be enabled."
+ "Vv24@0:8@?<v@?@\"NSXPCListenerEndpoint\"@\"NSError\">16"
+ "Vv32@0:8@\"NSString\"16@?<v@?B@\"NSError\">24"
+ "Vv40@0:8@\"NSLocale\"16@\"NSString\"24@?<v@?@\"NSXPCListenerEndpoint\"@\"NSError\">32"
+ "Vv40@0:8@\"NSString\"16@\"NSDictionary\"24@?<v@?@\"EARFullPayloadCorrectionResult\">32"
+ "Vv56@0:8@\"NSArray\"16@\"NSString\"24@\"NSString\"32@\"NSUUID\"40@\"NSString\"48"
+ "Vv56@0:8@\"SFEntitledAssetConfig\"16B24@\"NSString\"28@\"NSURL\"36B44@?<v@?@\"NSXPCListenerEndpoint\"@\"NSError\">48"
+ "Vv56@0:8@16@24@32@40@48"
+ "Vv56@0:8@16B24@28@36B44@?48"
+ "[To be implemented in rdar://105899082] - SpeechDetector: Yielded (dummy) result"
+ "_SFSpeechRecognizerEndpointDetectorOptions"
+ "_SFSpeechRecognizerSpeechDetectorOptions"
+ "_SFXPCEARFullPayloadCorrector"
+ "_SFXPCEARTranscriptionEvaluator"
+ "_TtC6Speech14EARXPCRegistry"
+ "_TtC6Speech20FullPayloadCorrector"
+ "_TtC6Speech22TranscriptionEvaluator"
+ "_TtC6Speech23EARFullPayloadCorrector"
+ "_TtC6Speech25AssetsInstallationRequest"
+ "_TtC6Speech25EARTranscriptionEvaluator"
+ "_TtC6SpeechP33_62876FE84B3142C220F84970CC8D7F0025FullPayloadCorrectorActor"
+ "_bestFormattedString"
+ "_bestFormattedStringSegmentConfidence"
+ "_dealloc"
+ "_endModelRetentionWithCompletion:"
+ "_endpointDetectionOptions"
+ "_fullPayloadCorrectorService"
+ "_modelRetention"
+ "_objc_initiateDealloc"
+ "_reportResults"
+ "_result"
+ "_sensitivityLevel"
+ "_speechDetectionOptions"
+ "_transcriptionEvaluatorService"
+ "assetConfig"
+ "assetConfigs"
+ "assetName cannot be nil."
+ "assetPathFromStatus:"
+ "assetsDownloadRequestForClientIdentifier:transcriberOptions:completion:"
+ "bestFormattedString"
+ "bestFormattedStringSegmentConfidence"
+ "concatenate utterances desired "
+ "configurationForClientIdentifier:queue:transcriberOptions:languageDetectorOptions:speechDetectorOptions:transcriberResultDelegate:endpointingResultDelegate:languageDetectorResultDelegate:speechDetectorResultDelegate:considering:completion:"
+ "correctPostITNOutput:withOptions:reply:"
+ "default"
+ "detector.json"
+ "didFinalizeToRange"
+ "endpointDetectionOptions"
+ "evaluateMessagesContext:recognizedText:correctedText:asrID:speechProfilePath:"
+ "fullPayloadCorrectorActor"
+ "generalASRLanguageForLocale:"
+ "generalASRLanguageForLocaleIdentifier:"
+ "hasANE"
+ "index element "
+ "initWithAssetType:locale:regionId:"
+ "initWithBool:"
+ "initWithClientIdentifier:audioFormat:transcriberResultDelegate:endpointingResultDelegate:languageDetectorResultDelegate:queue:transcriberOptions:options:languageDetectorOptions:restrictedLogging:contextualNamedEntities:didChangeVolatileRange:"
+ "initWithClientIdentifier:audioFormat:transcriberResultDelegate:endpointingResultDelegate:languageDetectorResultDelegate:speechDetectorResultDelegate:queue:transcriberOptions:options:languageDetectorOptions:speechDetectorOptions:restrictedLogging:contextualNamedEntities:didChangeVolatileRange:"
+ "initWithClientIdentifier:audioFormat:transcriberResultDelegate:endpointingResultDelegate:languageDetectorResultDelegate:speechDetectorResultDelegate:queue:transcriberOptions:options:languageDetectorOptions:speechDetectorOptions:restrictedLogging:didChangeVolatileRange:"
+ "initWithConfiguration:options:restrictedLogging:contextualNamedEntities:didChangeVolatileRange:"
+ "initWithFarField:geoLMRegionID:supplementalModelURL:modelOverrideURL:speechProfileURLs:userIdMask:taskForMemoryLock:atypicalSpeech:enableParallelLoading:speechProfileContainers:"
+ "initWithHighPriority:modelRetention:loggingInfo:powerContext:"
+ "initWithLeftContext:rightContext:selectedText:contextualStrings:contextualNamedEntities:profileData:jitProfileData:"
+ "initWithLocale:taskNames:singleUtterance:concatenateUtterances:voiceCommandActiveSet:modelOptions:endpointDetectionOptions:speechDetectionOptions:flags:"
+ "initWithObjCSpeechAnalyzer:clientIdentifier:audioFormat:transcriberResultDelegate:endpointingResultDelegate:languageDetectorResultDelegate:speechDetectorResultDelegate:queue:transcriberOptions:options:languageDetectorOptions:speechDetectorOptions:restrictedLogging:contextualNamedEntities:didChangeVolatileRange:"
+ "initWithRange:"
+ "initWithReportResults:"
+ "initWithResult:loggingInfo:error:"
+ "initWithSegments:transcriptions:earResultType:nBestChoices:recognitionAudioRange:bestFormattedString:bestFormattedStringSegmentConfidence:"
+ "initWithSensitivityLevel:"
+ "initWithSupplementalModelURL:farField:geoLMRegionID:modelOverrideURL:speechProfileURLs:taskForMemoryLock:atypicalSpeech:enableParallelLoading:"
+ "initWithText:alternatives:confidence:"
+ "inputSequence"
+ "is"
+ "is NOT"
+ "isANETypeAtLeastVersion:prefix:"
+ "isEuclidAvailableForConfigPath:reply:"
+ "isVADSupportedForRequestWithReply:"
+ "language cannot be nil."
+ "lower upper "
+ "makeFullPayloadCorrectorInstanceWithLocale:clientID:reply:"
+ "makeLSRAssetsForAssetConfig:shouldSubscribe:clientID:modelOverridePath:isSpelling:reply:"
+ "makeTranscriptionEvaluatorWithReply:"
+ "modelRetention"
+ "pathToAssetWithConfig:clientID:asyncCompletion:"
+ "pathToAssetWithConfig:clientIdentifier:completion:"
+ "prepareToAnalyze(withProgressReadyHandler:)"
+ "progressWithTotalUnitCount:"
+ "reportResults"
+ "sensitivityLevel"
+ "setAnalysisContextWithContextualNamedEntities:completionHandler:"
+ "setFileOperationKind:"
+ "setInputSequence(_:audioFormat:)"
+ "setKind:"
+ "setLocalizedAdditionalDescription:"
+ "setReportResults:"
+ "setSensitivityLevel:"
+ "setUserInfoObject:forKey:"
+ "shouldEmitOndeviceRecordLogs"
+ "shouldSubscribe"
+ "speechAnalyzer:didProduceSpeechDetectorResult:"
+ "speechAnalyzer:didStopSpeechDetectorWithError:"
+ "speechAnalyzerWithConfiguration:inputAudioFile:options:restrictedLogging:contextualNamedEntities:didChangeVolatileRange:completion:"
+ "speechDetectionOptions"
+ "speechDetector"
+ "speechProfilePath"
+ "subscriberId cannot be nil."
+ "unsubscribeFromAssetWithConfig:clientID:asyncCompletion:"
+ "unsubscribeFromAssetWithConfig:clientIdentifier:completion:"
+ "v104@0:8@16@24@32@40@48@56@64@72@80@88@?96"
+ "v16@?0@\"EARFullPayloadCorrectionResult\"8"
+ "v32@0:8@\"NSArray\"16@?<v@?@\"NSError\">24"
+ "v32@0:8@\"SFSpeechAnalyzer\"16@\"SFSpeechDetectorResult\"24"
+ "v40@0:8@\"NSString\"16@\"SFSpeechAnalyzerTranscriberOptions\"24@?<v@?@\"_TtC6Speech25AssetsInstallationRequest\">32"
+ "v68@0:8@16@24@32B40@44@?52@?60"
+ "valueWithCMTimeRange:"
+ "yue"
+ "zh-HK"
- "\x02\x11"
- "\x06\x11!"
- "\b"
- "\x14\x11"
- "\x16"
- "%s GeneralASR is not supported on this device"
- "%s Language cannot be nil."
- "%s MUX: %zu speech profiles (out of %zu containers) are present on disk"
- "%s MUX: %zu speech profiles are loaded"
- "%s MUX: cached speech profile is nil for path=%{private}@"
- "%s MUX: checking speech profile existence of %zu containers..."
- "%s MUX: empty data site URL for Container from Persona: %@"
- "%s MUX: failed to load speech profile: path=%{private}@ error=%@"
- "%s MUX: fileAttribs is nil for cached speech profile at path=%{private}@"
- "%s MUX: loaded speech profile: path=%{private}@"
- "%s MUX: no (%@) speech profile at path: %@"
- "%s MUX: profiles cannot be nil."
- "%s MUX: reusing cached speech profile: path=%{private}@"
- ", resultsFinalToTime "
- ", transcription "
- "/Library/Caches/com.apple.xbs/Sources/SpeechFramework/SpeechAnalyzer/ModelDownloadRequest.swift"
- "<_SFSpeechRecognizerDetectorOptions: detectAfterTime %f>"
- "<_SFSpeechRecognizerModelOptions: farField %d, supplementalModelURL %@, modelOverrideURL %@, speechProfileURLs %@, userIdMask %@, taskForMemoryLock %@, atypicalSpeech %d, enableParallelLoading %d, speechProfileContainers %@>"
- "<_SFSpeechRecognizerSupportedFeatures (%p): locale %@, taskNames %@, singleUtterance %d, concatenateUtterances %d, modelOptions %@, detectionOptions %@, flags %#lx>"
- "@\"_SFSpeechRecognizerDetectorOptions\""
- "@124@0:8@16@24@32@40@48@56@64@72@80@88B96@100@108@?116"
- "@72@0:8@16@24B32B36@40@48@56Q64"
- "@76@0:8B16@20@28@36@44@52B60B64@68"
- "@80@0:8@16@24@32@40@48@56@64@72"
- "@96@0:8@16@24Q32@40{?={?=qiIq}{?=qiIq}}48"
- "Attempt to modify value after it was locked"
- "Can't construct Array with count < 0"
- "CommandRecognizer using Transcriber that is not associated with same SpeechAnalyzer"
- "Division by zero"
- "Division results in an overflow"
- "Insufficient space allocated to copy string contents"
- "LanguageDetector.%s: No available model info; using hard-coded values"
- "Must take zero or more splits"
- "Negative value is not representable"
- "No models to download supplied"
- "Not enough bits to represent the passed value"
- "Range requires lowerBound <= upperBound"
- "Speech.LocalSpeechRecognitionService.makeLSRAssets(locale:taskName:clientID:modelOverridePath:isSpelling:)"
- "Speech.ModelDownloadRequest"
- "SpeechAnalyzer: Executing input barrier at %@"
- "Swift/Array.swift"
- "Swift/Collection.swift"
- "Swift/ContiguousArrayBuffer.swift"
- "Swift/IntegerTypes.swift"
- "Swift/Integers.swift"
- "Swift/Range.swift"
- "Swift/StringTesting.swift"
- "Swift/StringUTF8View.swift"
- "Swift/UnsafeBufferPointer.swift"
- "Swift/UnsafePointer.swift"
- "Swift/UnsafeRawPointer.swift"
- "T@\"NSString\",N,C"
- "T@\"_SFSpeechRecognizerDetectorOptions\",R,C,N,V_detectionOptions"
- "Transcriber cannot be initialized with both `.emoji` and `.normalizedTranscription`."
- "Transcriber cannot be initialized with both `.longerContextualization` and `.normalizedTranscription`."
- "Transcriber cannot be initialized with both `.normalizedTranscription` and `.contextualizedTranscription`."
- "Transcriber cannot be initialized with both `.punctuation` and `.normalizedTranscription`."
- "Transcriber.%s: No available model info; using hard-coded values"
- "Unexpectedly found nil while unwrapping an Optional value"
- "UnsafeMutableBufferPointer with negative count"
- "UnsafeMutablePointer.initialize overlapping range"
- "UnsafeMutablePointer.initialize with negative count"
- "UnsafeMutablePointer.moveInitialize with negative count"
- "UnsafeMutableRawPointer.initializeMemory overlapping range"
- "UnsafeMutableRawPointer.initializeMemory with negative count"
- "Vv60@0:8@\"NSLocale\"16@\"NSString\"24@\"NSString\"32@\"NSURL\"40B48@?<v@?@\"NSXPCListenerEndpoint\"@\"NSError\">52"
- "Vv60@0:8@16@24@32@40B48@?52"
- "_SFSpeechRecognizerDetectorOptions"
- "_TtC6Speech20ModelDownloadRequest"
- "_TtCC6Speech15AnalysisContextP33_813AAEFDCCF2FB11F9DE9A1343F1F01821ContextDelegatesMutex"
- "_assetPathWithStatus:"
- "_contextualStringsForKey:"
- "_detectionOptions"
- "_didFinalizeToRange"
- "_facetimeObserver"
- "_foregroundObserver"
- "_isDeviceH13Plus"
- "_setContextualStrings:forKey:"
- "_setUserData:forKey:"
- "_userDataForKey:"
- "asr_speech_profile_shared_data"
- "assetTypesPerLocale"
- "availableCompatibleAudioFormats(clientID:)"
- "configurationForClientIdentifier:queue:transcriberOptions:languageDetectorOptions:transcriberResultDelegate:endpointingResultDelegate:languageDetectorResultDelegate:considering:completion:"
- "contextDelegatesMutex"
- "downloadWithCompletion:"
- "geoLM region desired "
- "initWithFarField:supplementalModelURL:modelOverrideURL:speechProfileURLs:userIdMask:taskForMemoryLock:atypicalSpeech:enableParallelLoading:speechProfileContainers:"
- "initWithLeftContext:rightContext:selectedText:geoLMRegionID:contextualStrings:contextualNamedEntities:profileData:jitProfileData:"
- "initWithLocale:taskNames:singleUtterance:concatenateUtterances:voiceCommandActiveSet:modelOptions:detectionOptions:flags:"
- "initWithObjCSpeechAnalyzer:clientIdentifier:audioFormat:transcriberResultDelegate:endpointingResultDelegate:languageDetectorResultDelegate:queue:transcriberOptions:options:languageDetectorOptions:restrictedLogging:geoLMRegionID:contextualNamedEntities:didChangeVolatileRange:"
- "initWithSegments:transcriptions:earResultType:nBestChoices:recognitionAudioRange:"
- "initWithText:alternatives:"
- "internalInputSequence"
- "invalid Collection: less than 'count' elements in collection"
- "makeLSRAssetsForLocale:taskName:clientID:modelOverrideURL:isSpelling:reply:"
- "modelDownloadRequestForClientIdentifier:transcriberOptions:"
- "prepareToAnalyze(reportingInto:)"
- "setAnalysisContextWithGeoLMRegionID:contextualNamedEntities:completionHandler:"
- "setDialogContexts:"
- "setGeoLMRegionID:"
- "setInputSequence(_:withAudioFormat:)"
- "speechAnalyzerWithConfiguration:inputAudioFile:options:restrictedLogging:geoLMRegionID:contextualNamedEntities:didChangeVolatileRange:completion:"
- "startTimeOnAudioBuffer"
- "taskName"
- "v32@0:8@\"NSArray\"16@?<v@?@\"NSArray\">24"
- "v40@0:8@\"NSString\"16@\"NSArray\"24@?<v@?@\"NSError\">32"
- "v76@0:8@16@24@32B40@44@52@?60@?68"
- "v88@0:8@16@24@32@40@48@56@64@72@?80"
- "\x81"

```
