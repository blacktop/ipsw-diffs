## CorePhotogrammetry

> `/System/Library/PrivateFrameworks/CorePhotogrammetry.framework/CorePhotogrammetry`

```diff

-2.59.3.0.0
-  __TEXT.__text: 0xc195e4
-  __TEXT.__auth_stubs: 0x3520
-  __TEXT.__init_offsets: 0x18
-  __TEXT.__const: 0x4ccc1
-  __TEXT.__gcc_except_tab: 0x7ae80
-  __TEXT.__cstring: 0x1080c
-  __TEXT.__oslogstring: 0x2e3
-  __TEXT.__unwind_info: 0x22a38
+3.8.0.0.0
+  __TEXT.__text: 0xf203c8
+  __TEXT.__auth_stubs: 0x3870
+  __TEXT.__init_offsets: 0x8
+  __TEXT.__gcc_except_tab: 0xa8c18
+  __TEXT.__const: 0x565bc
+  __TEXT.__cstring: 0x799ed
+  __TEXT.__oslogstring: 0x487
+  __TEXT.__unwind_info: 0x30df0
   __TEXT.__eh_frame: 0xfe0
-  __TEXT.__objc_methname: 0x1678
-  __TEXT.__objc_stubs: 0x1aa0
-  __DATA_CONST.__got: 0x518
-  __DATA_CONST.__const: 0x1068
+  __TEXT.__objc_methname: 0x179e
+  __TEXT.__objc_stubs: 0x1c40
+  __DATA_CONST.__got: 0x6e0
+  __DATA_CONST.__const: 0x1230
   __DATA_CONST.__objc_imageinfo: 0x8
-  __DATA_CONST.__objc_selrefs: 0x6a8
-  __AUTH_CONST.__auth_got: 0x1aa0
-  __AUTH_CONST.__const: 0x26b20
-  __AUTH_CONST.__cfstring: 0x660
+  __DATA_CONST.__objc_selrefs: 0x710
+  __AUTH_CONST.__auth_got: 0x1c48
+  __AUTH_CONST.__const: 0x302f0
+  __AUTH_CONST.__cfstring: 0x700
   __AUTH_CONST.__objc_intobj: 0x60
-  __AUTH.__data: 0x30
+  __AUTH.__data: 0x38
   __AUTH.__thread_vars: 0x60
   __AUTH.__thread_bss: 0x8038
-  __DATA.__data: 0x4000
-  __DATA.__llvm_prf_cnts: 0x400
-  __DATA.__llvm_prf_data: 0x14c0
+  __DATA.__data: 0x4064
+  __DATA.__llvm_prf_cnts: 0x408
+  __DATA.__llvm_prf_data: 0x1500
   __DATA.__llvm_prf_names: 0x1890
-  __DATA.__crash_info: 0x40
-  __DATA.__bss: 0x1608
-  __DATA_DIRTY.__data: 0x18
-  __DATA_DIRTY.__bss: 0x90
+  __DATA.__crash_info: 0x148
+  __DATA.__bss: 0x7710
+  __DATA.__common: 0x1c0
+  __DATA_DIRTY.__common: 0x1d0
   __LLVM_COV.__llvm_covfun: 0xf318
   __LLVM_COV.__llvm_covmap: 0x448
   - /System/Library/Frameworks/Accelerate.framework/Accelerate

   - /System/Library/Frameworks/CoreGraphics.framework/CoreGraphics
   - /System/Library/Frameworks/CoreImage.framework/CoreImage
   - /System/Library/Frameworks/CoreMedia.framework/CoreMedia
+  - /System/Library/Frameworks/CoreServices.framework/CoreServices
   - /System/Library/Frameworks/CoreVideo.framework/CoreVideo
   - /System/Library/Frameworks/Foundation.framework/Foundation
   - /System/Library/Frameworks/IOSurface.framework/IOSurface

   - /System/Library/Frameworks/Metal.framework/Metal
   - /System/Library/Frameworks/MetalPerformanceShaders.framework/MetalPerformanceShaders
   - /System/Library/Frameworks/MetalPerformanceShadersGraph.framework/MetalPerformanceShadersGraph
+  - /System/Library/Frameworks/ModelIO.framework/ModelIO
+  - /System/Library/Frameworks/QuartzCore.framework/QuartzCore
   - /System/Library/Frameworks/VideoToolbox.framework/VideoToolbox
+  - /System/Library/PrivateFrameworks/ANSTKit.framework/ANSTKit
   - /System/Library/PrivateFrameworks/AppleCVHWA.framework/AppleCVHWA
   - /System/Library/PrivateFrameworks/AppleNeuralEngine.framework/AppleNeuralEngine
   - /System/Library/PrivateFrameworks/CMPhoto.framework/CMPhoto

   - /System/Library/PrivateFrameworks/LearnedFeatures.framework/LearnedFeatures
   - /System/Library/PrivateFrameworks/SignpostCollection.framework/SignpostCollection
   - /System/Library/PrivateFrameworks/SignpostSupport.framework/SignpostSupport
-  - /System/Library/PrivateFrameworks/USDKit.framework/USDKit
   - /usr/lib/libMobileGestalt.dylib
   - /usr/lib/libSystem.B.dylib
   - /usr/lib/libc++.1.dylib
   - /usr/lib/libobjc.A.dylib
   - /usr/lib/usd/libusd_ms.dylib
-  UUID: 9008C8F4-28E0-38CB-878D-ED7E9B505E70
-  Functions: 25198
-  Symbols:   1308
-  CStrings:  2218
+  UUID: F5924463-33CF-34E8-9C34-3B20CA4894F0
+  Functions: 33310
+  Symbols:   3238
+  CStrings:  5124
 
Symbols:
+ _ANSTSemanticCategoryPerson
+ _ANSTSemanticCategorySalientPerson
+ _ANSTSemanticCategorySky
+ _BundleDidLoadNotification
+ _BundleResourceRequestLowDiskSpaceNotification
+ _CPGAddSampleStreamProcessingOutputGetGaussian3D
+ _CPGCameraCopyInverseLensDistortionData
+ _CPGCameraCopyLensDistortionData
+ _CPGCheckDeviceCompatibilityForEnvironmentGS
+ _CPGCheckPhotogrammetryAssetsAvailability
+ _CPGEnvironmentGSRequestDescriptionCreate
+ _CPGEnvironmentGSRequestDescriptionRelease
+ _CPGEnvironmentGSRequestDescriptionRetain
+ _CPGEnvironmentGSSessionOptionsCreate
+ _CPGEnvironmentGSSessionOptionsRelease
+ _CPGEnvironmentGSSessionOptionsRetain
+ _CPGEnvironmentGSSessionOptionsSetWorkingDirectory
+ _CPGFeatureMatcherResultCopyDifferenceScores
+ _CPGFeatureMatcherResultCopyPositions
+ _CPGFeatureMatcherResultRelease
+ _CPGFeatureMatcherResultRetain
+ _CPGFeatureMatcherSessionCreateWithOptions
+ _CPGFeatureMatcherSessionOptionsCreate
+ _CPGFeatureMatcherSessionOptionsRelease
+ _CPGFeatureMatcherSessionOptionsRetain
+ _CPGFeatureMatcherSessionOptionsSetDifferenceCriteria
+ _CPGFeatureMatcherSessionOptionsSetModelType
+ _CPGFeatureMatcherSessionRelease
+ _CPGFeatureMatcherSessionRetain
+ _CPGFeatureMatcherSessionRun
+ _CPGGSOutputGetFailure
+ _CPGGSOutputGetGaussianSplattingPointCloudURL
+ _CPGGSOutputRelease
+ _CPGGSOutputRetain
+ _CPGGSProcessingCallbackBundleCreate
+ _CPGGSProcessingCallbackBundleRelease
+ _CPGGSProcessingCallbackBundleRetain
+ _CPGGSProcessingCallbackBundleSetCompleteOutputCallback
+ _CPGGSRequestCopyID
+ _CPGGSRequestCreateWithEnvironmentGSRequestDescription
+ _CPGGSRequestRelease
+ _CPGGSRequestRetain
+ _CPGGaussian3DGetColorDiffuse
+ _CPGGaussian3DGetColorRest
+ _CPGGaussian3DGetColorRestDimension
+ _CPGGaussian3DGetCoordinates
+ _CPGGaussian3DGetCountOfPoints
+ _CPGGaussian3DGetMutableColorDiffuse
+ _CPGGaussian3DGetMutableColorRest
+ _CPGGaussian3DGetMutableCoordinates
+ _CPGGaussian3DGetMutableOpacities
+ _CPGGaussian3DGetMutableQuaternions
+ _CPGGaussian3DGetMutableScales
+ _CPGGaussian3DGetOpacities
+ _CPGGaussian3DGetQuaternions
+ _CPGGaussian3DGetScales
+ _CPGGaussian3DGetSphericalHarmonicsDimension
+ _CPGGaussian3DGetStorageMode
+ _CPGGaussian3DLoadFromURL
+ _CPGGaussian3DMetadataCreate
+ _CPGGaussian3DMetadataGetColorSpace
+ _CPGGaussian3DMetadataGetDefaultExtrinsics
+ _CPGGaussian3DMetadataGetDefaultIntrinsics
+ _CPGGaussian3DMetadataGetSortingMode
+ _CPGGaussian3DMetadataGetSplattingMode
+ _CPGGaussian3DMetadataRelease
+ _CPGGaussian3DMetadataRetain
+ _CPGGaussian3DMetadataSetColorSpace
+ _CPGGaussian3DMetadataSetDefaultExtrinsics
+ _CPGGaussian3DMetadataSetDefaultIntrinsics
+ _CPGGaussian3DMetadataSetSortingMode
+ _CPGGaussian3DMetadataSetSplattingMode
+ _CPGGaussian3DRelease
+ _CPGGaussian3DRetain
+ _CPGGaussian3DSaveToURLWithMetadata
+ _CPGGaussianSplattingCallbackBundleCreate
+ _CPGGaussianSplattingCallbackBundleRelease
+ _CPGGaussianSplattingCallbackBundleRetain
+ _CPGGaussianSplattingCallbackBundleSetDataLoaderCallback
+ _CPGGaussianSplattingCallbackBundleSetIterationCallback
+ _CPGGaussianSplattingCallbackBundleSetMemoryDidChangeCallback
+ _CPGGaussianSplattingCallbackBundleSetMemoryWillChangeCallback
+ _CPGGaussianSplattingDataLoaderProgressCopyMessage
+ _CPGGaussianSplattingDataLoaderProgressGetPercentage
+ _CPGGaussianSplattingDataLoaderProgressRelease
+ _CPGGaussianSplattingDataLoaderProgressRetain
+ _CPGGaussianSplattingIterationResultGetLoss
+ _CPGGaussianSplattingIterationResultGetNumGaussians
+ _CPGGaussianSplattingIterationResultGetProgress
+ _CPGGaussianSplattingIterationResultGetStep
+ _CPGGaussianSplattingIterationResultGetStepTimeMS
+ _CPGGaussianSplattingIterationResultRelease
+ _CPGGaussianSplattingIterationResultRetain
+ _CPGGaussianSplattingOptionsCreate
+ _CPGGaussianSplattingOptionsGetAppearanceModelingEnabled
+ _CPGGaussianSplattingOptionsGetAppearanceModelingMethod
+ _CPGGaussianSplattingOptionsGetColorSpace
+ _CPGGaussianSplattingOptionsGetComputeBakingView
+ _CPGGaussianSplattingOptionsGetDensificationInterval
+ _CPGGaussianSplattingOptionsGetDensifyFromIter
+ _CPGGaussianSplattingOptionsGetDensifyUntilIter
+ _CPGGaussianSplattingOptionsGetEnableSparsification
+ _CPGGaussianSplattingOptionsGetEvalMode
+ _CPGGaussianSplattingOptionsGetIterations
+ _CPGGaussianSplattingOptionsGetLambdaSSIM
+ _CPGGaussianSplattingOptionsGetMaskedBackground
+ _CPGGaussianSplattingOptionsGetPositionFinalLearningRate
+ _CPGGaussianSplattingOptionsGetPositionInitialLearningRate
+ _CPGGaussianSplattingOptionsGetRenderDownsample
+ _CPGGaussianSplattingOptionsGetSHDegree
+ _CPGGaussianSplattingOptionsGetSortingMode
+ _CPGGaussianSplattingOptionsGetSparsificationInterval
+ _CPGGaussianSplattingOptionsGetSparsifyFromIter
+ _CPGGaussianSplattingOptionsGetSparsifyUntilIter
+ _CPGGaussianSplattingOptionsGetSphericalBackgroundLayers
+ _CPGGaussianSplattingOptionsGetUseMaskIfAvailable
+ _CPGGaussianSplattingOptionsGetVoxelSize
+ _CPGGaussianSplattingOptionsRelease
+ _CPGGaussianSplattingOptionsRetain
+ _CPGGaussianSplattingOptionsSetAppearanceModelingEnabled
+ _CPGGaussianSplattingOptionsSetAppearanceModelingMethod
+ _CPGGaussianSplattingOptionsSetColorSpace
+ _CPGGaussianSplattingOptionsSetComputeBakingView
+ _CPGGaussianSplattingOptionsSetDensificationInterval
+ _CPGGaussianSplattingOptionsSetDensifyFromIter
+ _CPGGaussianSplattingOptionsSetDensifyUntilIter
+ _CPGGaussianSplattingOptionsSetEnableSparsification
+ _CPGGaussianSplattingOptionsSetEvalMode
+ _CPGGaussianSplattingOptionsSetIterations
+ _CPGGaussianSplattingOptionsSetLambdaSSIM
+ _CPGGaussianSplattingOptionsSetMaskedBackground
+ _CPGGaussianSplattingOptionsSetPositionFinalLearningRate
+ _CPGGaussianSplattingOptionsSetPositionInitialLearningRate
+ _CPGGaussianSplattingOptionsSetRenderDownsample
+ _CPGGaussianSplattingOptionsSetSHDegree
+ _CPGGaussianSplattingOptionsSetSortingMode
+ _CPGGaussianSplattingOptionsSetSparsificationInterval
+ _CPGGaussianSplattingOptionsSetSparsifyFromIter
+ _CPGGaussianSplattingOptionsSetSparsifyUntilIter
+ _CPGGaussianSplattingOptionsSetSphericalBackgroundLayers
+ _CPGGaussianSplattingOptionsSetUseMaskIfAvailable
+ _CPGGaussianSplattingOptionsSetVoxelSize
+ _CPGGaussianSplattingOutputGetGaussian3D
+ _CPGGaussianSplattingOutputGetGaussian3DMetadata
+ _CPGGaussianSplattingOutputRelease
+ _CPGGaussianSplattingOutputRetain
+ _CPGGaussianSplattingSessionCreate
+ _CPGGaussianSplattingSessionPause
+ _CPGGaussianSplattingSessionRelease
+ _CPGGaussianSplattingSessionResume
+ _CPGGaussianSplattingSessionRetain
+ _CPGGaussianSplattingSessionRunWithInitialGaussianPoints
+ _CPGGaussianSplattingSessionRunWithInitialPointCloud
+ _CPGGaussianSplattingSessionStop
+ _CPGLensDistortionDataGetDistortionCenter
+ _CPGLensDistortionDataGetLUTData
+ _CPGLensDistortionDataGetLUTLength
+ _CPGLensDistortionDataRelease
+ _CPGLensDistortionDataRetain
+ _CPGLoadPhotogrammetryAssetsSync
+ _CPGMeshCreate
+ _CPGMeshCreateWithNumberOfTrianglesAndVertices
+ _CPGMeshGetMutableTriangles
+ _CPGMeshGetMutableVertices
+ _CPGMeshGetNumberOfTriangles
+ _CPGMeshGetNumberOfVertices
+ _CPGMeshGetTriangles
+ _CPGMeshGetVertices
+ _CPGMeshLoadFromURL
+ _CPGMeshRelease
+ _CPGMeshRetain
+ _CPGMeshSaveToURL
+ _CPGMeshSetNumberOfTriangles
+ _CPGMeshSetNumberOfVertices
+ _CPGOutputGetGaussianSplattingPointCloudURL
+ _CPGPointCloudGetColors
+ _CPGPointCloudGetNumberOfPoints
+ _CPGPointCloudGetPositions
+ _CPGPointCloudHasColor
+ _CPGPointCloudLoadFromURL
+ _CPGPointCloudRelease
+ _CPGPointCloudRetain
+ _CPGProcessingCallbackBundleSetGaussianSplattingCallback
+ _CPGSessionCreateWithEnvironmentGSOptions
+ _CPGSessionOptionsSetGaussianSplattingOptions
+ _CPGSessionOptionsSetMVSType
+ _CPGSessionOptionsSetSemanticMaskingEnabled
+ _CPGSessionProcessGSRequestWithCallbacks
+ _CPGSfmMapCreate
+ _CPGSfmMapLoadFromURL
+ _CPGSfmMapRemoveCameraBySampleID
+ _CPGSfmMapSetCameraBySampleID
+ _CPGSfmOptionsSetBuildGaussianSplatting
+ _CPGTexturedMeshCopyDiffuseMap
+ _CPGTexturedMeshCreate
+ _CPGTexturedMeshCreateWithNumberOfTrianglesAndVertices
+ _CPGTexturedMeshGetMutableTexCoords
+ _CPGTexturedMeshGetMutableTexIndices
+ _CPGTexturedMeshGetMutableTriangles
+ _CPGTexturedMeshGetMutableVertices
+ _CPGTexturedMeshGetNumberOfMaterials
+ _CPGTexturedMeshGetNumberOfTexCoords
+ _CPGTexturedMeshGetNumberOfTexIndices
+ _CPGTexturedMeshGetNumberOfTriangles
+ _CPGTexturedMeshGetNumberOfVertices
+ _CPGTexturedMeshGetTexCoords
+ _CPGTexturedMeshGetTexIndices
+ _CPGTexturedMeshGetTriangles
+ _CPGTexturedMeshGetVertices
+ _CPGTexturedMeshLoadFromURL
+ _CPGTexturedMeshRelease
+ _CPGTexturedMeshRetain
+ _CPGTexturedMeshSaveToURL
+ _CPGTexturedMeshSetDiffuseMap
+ _CPGTexturedMeshSetNumberOfMaterials
+ _CPGTexturedMeshSetNumberOfTexCoords
+ _CPGTexturedMeshSetNumberOfTexIndices
+ _CPGTexturedMeshSetNumberOfTriangles
+ _CPGTexturedMeshSetNumberOfVertices
+ _CPGTexturingOptionsCreate
+ _CPGTexturingOptionsRelease
+ _CPGTexturingOptionsRetain
+ _CPGTexturingOptionsSetContourMaskingPixels
+ _CPGTexturingOptionsSetUseMaskIfAvailable
+ _CPGTexturingOutputGetMutableTexturedMesh
+ _CPGTexturingOutputGetTexturedMesh
+ _CPGTexturingOutputRelease
+ _CPGTexturingOutputRetain
+ _CPGTexturingWithOptions
+ _CocoaErrorDomain
+ _CommandBufferEncoderInfoErrorKey
+ _CommonCounterClipperInvocations
+ _CommonCounterClipperPrimitivesOut
+ _CommonCounterComputeKernelInvocations
+ _CommonCounterFragmentCycles
+ _CommonCounterFragmentInvocations
+ _CommonCounterFragmentsPassed
+ _CommonCounterPostTessellationVertexCycles
+ _CommonCounterPostTessellationVertexInvocations
+ _CommonCounterRenderTargetWriteCycles
+ _CommonCounterSetStageUtilization
+ _CommonCounterSetStatistic
+ _CommonCounterSetTimestamp
+ _CommonCounterTessellationCycles
+ _CommonCounterTessellationInputPatches
+ _CommonCounterTimestamp
+ _CommonCounterTotalCycles
+ _CommonCounterVertexCycles
+ _CommonCounterVertexInvocations
+ _CounterErrorDomain
+ _DebugDescriptionErrorKey
+ _DeviceCertificationiPhonePerformanceGaming
+ _FilePathErrorKey
+ _HelpAnchorErrorKey
+ _LocalizedDescriptionKey
+ _LocalizedFailureErrorKey
+ _LocalizedFailureReasonErrorKey
+ _LocalizedRecoveryOptionsErrorKey
+ _LocalizedRecoverySuggestionErrorKey
+ _MTLCommandBufferEncoderInfoErrorKey
+ _MTLCommonCounterClipperInvocations
+ _MTLCommonCounterClipperPrimitivesOut
+ _MTLCommonCounterComputeKernelInvocations
+ _MTLCommonCounterFragmentCycles
+ _MTLCommonCounterFragmentInvocations
+ _MTLCommonCounterFragmentsPassed
+ _MTLCommonCounterPostTessellationVertexCycles
+ _MTLCommonCounterPostTessellationVertexInvocations
+ _MTLCommonCounterRenderTargetWriteCycles
+ _MTLCommonCounterSetStageUtilization
+ _MTLCommonCounterSetStatistic
+ _MTLCommonCounterSetTimestamp
+ _MTLCommonCounterTessellationCycles
+ _MTLCommonCounterTessellationInputPatches
+ _MTLCommonCounterTimestamp
+ _MTLCommonCounterTotalCycles
+ _MTLCommonCounterVertexCycles
+ _MTLCommonCounterVertexInvocations
+ _MTLCopyAllDevices
+ _MTLCounterErrorDomain
+ _MachErrorDomain
+ _NSBundleDidLoadNotification
+ _NSBundleResourceRequestLowDiskSpaceNotification
+ _NSCocoaErrorDomain
+ _NSDebugDescriptionErrorKey
+ _NSDeviceCertificationiPhonePerformanceGaming
+ _NSFilePathErrorKey
+ _NSHelpAnchorErrorKey
+ _NSLocalizedDescriptionKey
+ _NSLocalizedFailureErrorKey
+ _NSLocalizedFailureReasonErrorKey
+ _NSLocalizedRecoveryOptionsErrorKey
+ _NSLocalizedRecoverySuggestionErrorKey
+ _NSLog
+ _NSMachErrorDomain
+ _NSOSStatusErrorDomain
+ _NSPOSIXErrorDomain
+ _NSProcessInfoPerformanceProfileDidChangeNotification
+ _NSProcessInfoPowerStateDidChangeNotification
+ _NSProcessInfoThermalStateDidChangeNotification
+ _NSProcessPerformanceProfileDefault
+ _NSProcessPerformanceProfileSustained
+ _NSRecoveryAttempterErrorKey
+ _NSStringEncodingErrorKey
+ _NSURLErrorKey
+ _NSUnderlyingErrorKey
+ _OBJC_CLASS_$_ANSTISPAlgorithm
+ _OBJC_CLASS_$_ANSTISPAlgorithmConfiguration
+ _OBJC_CLASS_$_MDLUtility
+ _OBJC_CLASS_$_NSData
+ _OSStatusErrorDomain
+ _POSIXErrorDomain
+ _ProcessInfoPerformanceProfileDidChangeNotification
+ _ProcessInfoPowerStateDidChangeNotification
+ _ProcessInfoThermalStateDidChangeNotification
+ _ProcessPerformanceProfileDefault
+ _ProcessPerformanceProfileSustained
+ _RecoveryAttempterErrorKey
+ _StringEncodingErrorKey
+ _URLErrorKey
+ _UnderlyingErrorKey
+ __SparseSpMV_Double
+ __ZN2NS7Private5Class10s_kNSArrayEv
+ __ZN2NS7Private5Class10s_kNSErrorEv
+ __ZN2NS7Private5Class10s_kNSValueEv
+ __ZN2NS7Private5Class11s_kNSBundleEv
+ __ZN2NS7Private5Class11s_kNSNumberEv
+ __ZN2NS7Private5Class11s_kNSObjectEv
+ __ZN2NS7Private5Class11s_kNSStringEv
+ __ZN2NS7Private5Class14s_kNSConditionEv
+ __ZN2NS7Private5Class15s_kNSDictionaryEv
+ __ZN2NS7Private5Class16s_kNSProcessInfoEv
+ __ZN2NS7Private5Class20s_kNSAutoreleasePoolEv
+ __ZN2NS7Private5Class23s_kNSNotificationCenterEv
+ __ZN2NS7Private5Class8s_kNSSetEv
+ __ZN2NS7Private5Class8s_kNSURLEv
+ __ZN2NS7Private5Class9s_kNSDateEv
+ __ZN2NS7Private8Selector10s_kreleaseEv
+ __ZN2NS7Private8Selector11s_kcompare_Ev
+ __ZN2NS7Private8Selector11s_khostNameEv
+ __ZN2NS7Private8Selector11s_kintValueEv
+ __ZN2NS7Private8Selector11s_kisEqual_Ev
+ __ZN2NS7Private8Selector11s_kisLoadedEv
+ __ZN2NS7Private8Selector11s_kobjCTypeEv
+ __ZN2NS7Private8Selector11s_kuserInfoEv
+ __ZN2NS7Private8Selector11s_kuserNameEv
+ __ZN2NS7Private8Selector12s_kargumentsEv
+ __ZN2NS7Private8Selector12s_kboolValueEv
+ __ZN2NS7Private8Selector12s_kbroadcastEv
+ __ZN2NS7Private8Selector12s_kbundleURLEv
+ __ZN2NS7Private8Selector12s_kcharValueEv
+ __ZN2NS7Private8Selector12s_klongValueEv
+ __ZN2NS7Private8Selector12s_kshowPoolsEv
+ __ZN2NS7Private8Selector13s_kUTF8StringEv
+ __ZN2NS7Private8Selector13s_kaddObject_Ev
+ __ZN2NS7Private8Selector13s_kallBundlesEv
+ __ZN2NS7Private8Selector13s_kallObjectsEv
+ __ZN2NS7Private8Selector13s_kbundlePathEv
+ __ZN2NS7Private8Selector13s_kdictionaryEv
+ __ZN2NS7Private8Selector13s_kfloatValueEv
+ __ZN2NS7Private8Selector13s_kmainBundleEv
+ __ZN2NS7Private8Selector13s_knextObjectEv
+ __ZN2NS7Private8Selector13s_kshortValueEv
+ __ZN2NS7Private8Selector14s_kautoreleaseEv
+ __ZN2NS7Private8Selector14s_kdescriptionEv
+ __ZN2NS7Private8Selector14s_kdoubleValueEv
+ __ZN2NS7Private8Selector14s_kenvironmentEv
+ __ZN2NS7Private8Selector14s_kprocessInfoEv
+ __ZN2NS7Private8Selector14s_kprocessNameEv
+ __ZN2NS7Private8Selector14s_kresourceURLEv
+ __ZN2NS7Private8Selector14s_kretainCountEv
+ __ZN2NS7Private8Selector14s_kstringValueEv
+ __ZN2NS7Private8Selector15s_kendActivity_Ev
+ __ZN2NS7Private8Selector15s_kfullUserNameEv
+ __ZN2NS7Private8Selector15s_kinitWithInt_Ev
+ __ZN2NS7Private8Selector15s_kinitWithURL_Ev
+ __ZN2NS7Private8Selector15s_kintegerValueEv
+ __ZN2NS7Private8Selector15s_kmutableBytesEv
+ __ZN2NS7Private8Selector15s_kpointerValueEv
+ __ZN2NS7Private8Selector15s_kresourcePathEv
+ __ZN2NS7Private8Selector15s_ksystemUptimeEv
+ __ZN2NS7Private8Selector15s_kthermalStateEv
+ __ZN2NS7Private8Selector16s_kallFrameworksEv
+ __ZN2NS7Private8Selector16s_kdefaultCenterEv
+ __ZN2NS7Private8Selector16s_kexecutableURLEv
+ __ZN2NS7Private8Selector16s_kinitWithBool_Ev
+ __ZN2NS7Private8Selector16s_kinitWithChar_Ev
+ __ZN2NS7Private8Selector16s_kinitWithLong_Ev
+ __ZN2NS7Private8Selector16s_kinitWithPath_Ev
+ __ZN2NS7Private8Selector16s_kisiOSAppOnMacEv
+ __ZN2NS7Private8Selector16s_kkeyEnumeratorEv
+ __ZN2NS7Private8Selector16s_klongLongValueEv
+ __ZN2NS7Private8Selector16s_kobjectForKey_Ev
+ __ZN2NS7Private8Selector17s_kbundleWithURL_Ev
+ __ZN2NS7Private8Selector17s_kexecutablePathEv
+ __ZN2NS7Private8Selector17s_kgetValue_size_Ev
+ __ZN2NS7Private8Selector17s_kinfoDictionaryEv
+ __ZN2NS7Private8Selector17s_kinitWithCoder_Ev
+ __ZN2NS7Private8Selector17s_kinitWithFloat_Ev
+ __ZN2NS7Private8Selector17s_kinitWithShort_Ev
+ __ZN2NS7Private8Selector17s_knumberWithInt_Ev
+ __ZN2NS7Private8Selector17s_kobjectAtIndex_Ev
+ __ZN2NS7Private8Selector17s_kphysicalMemoryEv
+ __ZN2NS7Private8Selector17s_kprocessorCountEv
+ __ZN2NS7Private8Selector17s_kwaitUntilDate_Ev
+ __ZN2NS7Private8Selector18s_kbundleWithPath_Ev
+ __ZN2NS7Private8Selector18s_kinitWithDouble_Ev
+ __ZN2NS7Private8Selector18s_kinitWithString_Ev
+ __ZN2NS7Private8Selector18s_kisEqualToValue_Ev
+ __ZN2NS7Private8Selector18s_knumberWithBool_Ev
+ __ZN2NS7Private8Selector18s_knumberWithChar_Ev
+ __ZN2NS7Private8Selector18s_knumberWithLong_Ev
+ __ZN2NS7Private8Selector18s_koperatingSystemEv
+ __ZN2NS7Private8Selector18s_kremoveObserver_Ev
+ __ZN2NS7Private8Selector18s_ksetProcessName_Ev
+ __ZN2NS7Private8Selector19s_karrayWithObject_Ev
+ __ZN2NS7Private8Selector19s_kbundleIdentifierEv
+ __ZN2NS7Private8Selector19s_kdebugDescriptionEv
+ __ZN2NS7Private8Selector19s_kfileURLWithPath_Ev
+ __ZN2NS7Private8Selector19s_kisEqualToNumber_Ev
+ __ZN2NS7Private8Selector19s_kisEqualToString_Ev
+ __ZN2NS7Private8Selector19s_kisMacCatalystAppEv
+ __ZN2NS7Private8Selector19s_knumberWithFloat_Ev
+ __ZN2NS7Private8Selector19s_knumberWithShort_Ev
+ __ZN2NS7Private8Selector19s_kobjectEnumeratorEv
+ __ZN2NS7Private8Selector19s_ksharedSupportURLEv
+ __ZN2NS7Private8Selector19s_kunsignedIntValueEv
+ __ZN2NS7Private8Selector20s_kbuiltInPlugInsURLEv
+ __ZN2NS7Private8Selector20s_kcharacterAtIndex_Ev
+ __ZN2NS7Private8Selector20s_kinitWithLongLong_Ev
+ __ZN2NS7Private8Selector20s_knumberWithDouble_Ev
+ __ZN2NS7Private8Selector20s_kprocessIdentifierEv
+ __ZN2NS7Private8Selector20s_ksharedSupportPathEv
+ __ZN2NS7Private8Selector20s_kstringWithString_Ev
+ __ZN2NS7Private8Selector20s_kunsignedCharValueEv
+ __ZN2NS7Private8Selector20s_kunsignedLongValueEv
+ __ZN2NS7Private8Selector20s_kvalueWithPointer_Ev
+ __ZN2NS7Private8Selector21s_kappStoreReceiptURLEv
+ __ZN2NS7Private8Selector21s_kbuiltInPlugInsPathEv
+ __ZN2NS7Private8Selector21s_kisDeviceCertified_Ev
+ __ZN2NS7Private8Selector21s_kunsignedShortValueEv
+ __ZN2NS7Private8Selector22s_kloadAndReturnError_Ev
+ __ZN2NS7Private8Selector22s_knumberWithLongLong_Ev
+ __ZN2NS7Private8Selector22s_krespondsToSelector_Ev
+ __ZN2NS7Private8Selector22s_ksharedFrameworksURLEv
+ __ZN2NS7Private8Selector23s_kactiveProcessorCountEv
+ __ZN2NS7Private8Selector23s_kgloballyUniqueStringEv
+ __ZN2NS7Private8Selector23s_kinitFileURLWithPath_Ev
+ __ZN2NS7Private8Selector23s_kinitWithUnsignedInt_Ev
+ __ZN2NS7Private8Selector23s_klocalizedDescriptionEv
+ __ZN2NS7Private8Selector23s_kprivateFrameworksURLEv
+ __ZN2NS7Private8Selector23s_ksharedFrameworksPathEv
+ __ZN2NS7Private8Selector23s_kunsignedIntegerValueEv
+ __ZN2NS7Private8Selector24s_kcStringUsingEncoding_Ev
+ __ZN2NS7Private8Selector24s_kinitWithUnsignedChar_Ev
+ __ZN2NS7Private8Selector24s_kinitWithUnsignedLong_Ev
+ __ZN2NS7Private8Selector24s_kisLowPowerModeEnabledEv
+ __ZN2NS7Private8Selector24s_kprivateFrameworksPathEv
+ __ZN2NS7Private8Selector24s_kunsignedLongLongValueEv
+ __ZN2NS7Private8Selector25s_kdescriptionWithLocale_Ev
+ __ZN2NS7Private8Selector25s_khasPerformanceProfile_Ev
+ __ZN2NS7Private8Selector25s_kinitWithObjects_count_Ev
+ __ZN2NS7Private8Selector25s_kinitWithUnsignedShort_Ev
+ __ZN2NS7Private8Selector25s_klocalizedFailureReasonEv
+ __ZN2NS7Private8Selector25s_knumberWithUnsignedInt_Ev
+ __ZN2NS7Private8Selector25s_koperatingSystemVersionEv
+ __ZN2NS7Private8Selector25s_krangeOfString_options_Ev
+ __ZN2NS7Private8Selector26s_karrayWithObjects_count_Ev
+ __ZN2NS7Private8Selector26s_kcaseInsensitiveCompare_Ev
+ __ZN2NS7Private8Selector26s_kenableSuddenTerminationEv
+ __ZN2NS7Private8Selector26s_kinitWithBytes_objCType_Ev
+ __ZN2NS7Private8Selector26s_klocalizedInfoDictionaryEv
+ __ZN2NS7Private8Selector26s_knumberWithUnsignedChar_Ev
+ __ZN2NS7Private8Selector26s_knumberWithUnsignedLong_Ev
+ __ZN2NS7Private8Selector27s_kdisableSuddenTerminationEv
+ __ZN2NS7Private8Selector27s_kfileSystemRepresentationEv
+ __ZN2NS7Private8Selector27s_klocalizedRecoveryOptionsEv
+ __ZN2NS7Private8Selector27s_knumberWithUnsignedShort_Ev
+ __ZN2NS7Private8Selector27s_kpreflightAndReturnError_Ev
+ __ZN2NS7Private8Selector27s_kstringByAppendingString_Ev
+ __ZN2NS7Private8Selector27s_kvalueWithBytes_objCType_Ev
+ __ZN2NS7Private8Selector28s_kinitWithCString_encoding_Ev
+ __ZN2NS7Private8Selector28s_kinitWithUnsignedLongLong_Ev
+ __ZN2NS7Private8Selector29s_kURLForAuxiliaryExecutable_Ev
+ __ZN2NS7Private8Selector30s_kenableAutomaticTermination_Ev
+ __ZN2NS7Private8Selector30s_klengthOfBytesUsingEncoding_Ev
+ __ZN2NS7Private8Selector30s_klocalizedRecoverySuggestionEv
+ __ZN2NS7Private8Selector30s_kmethodSignatureForSelector_Ev
+ __ZN2NS7Private8Selector30s_knumberWithUnsignedLongLong_Ev
+ __ZN2NS7Private8Selector30s_kobjectForInfoDictionaryKey_Ev
+ __ZN2NS7Private8Selector30s_kpathForAuxiliaryExecutable_Ev
+ __ZN2NS7Private8Selector30s_kstringWithCString_encoding_Ev
+ __ZN2NS7Private8Selector31s_kdictionaryWithObject_forKey_Ev
+ __ZN2NS7Private8Selector31s_kdisableAutomaticTermination_Ev
+ __ZN2NS7Private8Selector31s_koperatingSystemVersionStringEv
+ __ZN2NS7Private8Selector32s_kdateWithTimeIntervalSinceNow_Ev
+ __ZN2NS7Private8Selector32s_kinitWithDomain_code_userInfo_Ev
+ __ZN2NS7Private8Selector33s_kerrorWithDomain_code_userInfo_Ev
+ __ZN2NS7Private8Selector33s_kinitWithObjects_forKeys_count_Ev
+ __ZN2NS7Private8Selector35s_kbeginActivityWithOptions_reason_Ev
+ __ZN2NS7Private8Selector35s_kisOperatingSystemAtLeastVersion_Ev
+ __ZN2NS7Private8Selector37s_kautomaticTerminationSupportEnabledEv
+ __ZN2NS7Private8Selector37s_klocalizedStringForKey_value_table_Ev
+ __ZN2NS7Private8Selector37s_kmaximumLengthOfBytesUsingEncoding_Ev
+ __ZN2NS7Private8Selector38s_kaddObserverName_object_queue_block_Ev
+ __ZN2NS7Private8Selector39s_kdictionaryWithObjects_forKeys_count_Ev
+ __ZN2NS7Private8Selector41s_ksetAutomaticTerminationSupportEnabled_Ev
+ __ZN2NS7Private8Selector45s_kcountByEnumeratingWithState_objects_count_Ev
+ __ZN2NS7Private8Selector48s_kperformActivityWithOptions_reason_usingBlock_Ev
+ __ZN2NS7Private8Selector48s_kperformExpiringActivityWithReason_usingBlock_Ev
+ __ZN2NS7Private8Selector52s_kinitWithBytesNoCopy_length_encoding_freeWhenDone_Ev
+ __ZN2NS7Private8Selector7s_kcodeEv
+ __ZN2NS7Private8Selector7s_kcopyEv
+ __ZN2NS7Private8Selector7s_khashEv
+ __ZN2NS7Private8Selector7s_kinitEv
+ __ZN2NS7Private8Selector7s_kloadEv
+ __ZN2NS7Private8Selector7s_klockEv
+ __ZN2NS7Private8Selector7s_knameEv
+ __ZN2NS7Private8Selector7s_kwaitEv
+ __ZN2NS7Private8Selector8s_kallocEv
+ __ZN2NS7Private8Selector8s_karrayEv
+ __ZN2NS7Private8Selector8s_kcountEv
+ __ZN2NS7Private8Selector8s_kdrainEv
+ __ZN2NS7Private8Selector9s_kdomainEv
+ __ZN2NS7Private8Selector9s_klengthEv
+ __ZN2NS7Private8Selector9s_kobjectEv
+ __ZN2NS7Private8Selector9s_kretainEv
+ __ZN2NS7Private8Selector9s_ksignalEv
+ __ZN2NS7Private8Selector9s_kstringEv
+ __ZN2NS7Private8Selector9s_kunloadEv
+ __ZN2NS7Private8Selector9s_kunlockEv
+ __ZN2gc11AOTRegistry20registerProgramTableERKNS_15AOTProgramTableE
+ __ZN2gc12ProgramTable3addERKNS0_5EntryE
+ __ZN2gc15AOTProgramTable3addENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEERKNS_12ProgramTableE
+ __ZN2gc15ProgramSchedule17optimizationLevelERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEE
+ __ZN2gc15ProgramSchedule18disableStripStoresERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEE
+ __ZN2gc15ProgramSchedule24minimalHIRSimplificationERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEE
+ __ZN2gc16GeneratorFactory7compileENS_4util10TaggedBoolINS_11AOTOnly_TagEEERKNS_8ScheduleE
+ __ZN2gc3est30createLeastSquaresLossFunctionERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEENS1_10shared_ptrINS0_8BoxMinusEEENSA_INS0_10NoiseModelEEE
+ __ZN2gc6DoubleEt
+ __ZN2gc6ExtentC1Eib
+ __ZN2gc6UInt32Et
+ __ZN2gc6detail12logTypeErrorERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEERK17loopkit_rt_type_tSC_
+ __ZN2gc7ExtentsC1ENS_9eOrderingE
+ __ZN2gc7ExtentsC1ESt16initializer_listINS_6ExtentEENS_9eOrderingE
+ __ZN32pxrInternal__aapl__pxrReserved__11TfMallocTag4_EndEiPNS0_11_ThreadDataE
+ __ZN32pxrInternal__aapl__pxrReserved__26USDInitializeConfigurationC1Ev
+ __ZN32pxrInternal__aapl__pxrReserved__26USDInitializeConfigurationD1Ev
+ __ZN3MTL14CopyAllDevicesEv
+ __ZN3MTL20RemoveDeviceObserverEPKN2NS6ObjectE
+ __ZN3MTL25CreateSystemDefaultDeviceEv
+ __ZN3MTL26CopyAllDevicesWithObserverEPPN2NS6ObjectERKNSt3__18functionIFvPNS_6DeviceEPNS0_6StringEEEE
+ __ZN3MTL26CopyAllDevicesWithObserverEPPN2NS6ObjectEU13block_pointerFvPNS_6DeviceEPNS0_6StringEE
+ __ZN3MTL7Private5Class10s_kMTLTypeEv
+ __ZN3MTL7Private5Class14s_kMTLArgumentEv
+ __ZN3MTL7Private5Class15s_kMTLArrayTypeEv
+ __ZN3MTL7Private5Class15s_kMTLAttributeEv
+ __ZN3MTL7Private5Class16s_kMTLStructTypeEv
+ __ZN3MTL7Private5Class17s_kMTLPointerTypeEv
+ __ZN3MTL7Private5Class18s_kMTLArchitectureEv
+ __ZN3MTL7Private5Class18s_kMTLStructMemberEv
+ __ZN3MTL7Private5Class20s_kMTLCaptureManagerEv
+ __ZN3MTL7Private5Class20s_kMTLCompileOptionsEv
+ __ZN3MTL7Private5Class20s_kMTLHeapDescriptorEv
+ __ZN3MTL7Private5Class21s_kMTLLinkedFunctionsEv
+ __ZN3MTL7Private5Class21s_kMTLVertexAttributeEv
+ __ZN3MTL7Private5Class22s_kMTLFunctionConstantEv
+ __ZN3MTL7Private5Class22s_kMTLVertexDescriptorEv
+ __ZN3MTL7Private5Class23s_kMTLCaptureDescriptorEv
+ __ZN3MTL7Private5Class23s_kMTLSamplerDescriptorEv
+ __ZN3MTL7Private5Class23s_kMTLSharedEventHandleEv
+ __ZN3MTL7Private5Class23s_kMTLStencilDescriptorEv
+ __ZN3MTL7Private5Class23s_kMTLTextureDescriptorEv
+ __ZN3MTL7Private5Class24s_kMTLArgumentDescriptorEv
+ __ZN3MTL7Private5Class24s_kMTLBlitPassDescriptorEv
+ __ZN3MTL7Private5Class24s_kMTLFunctionDescriptorEv
+ __ZN3MTL7Private5Class24s_kMTLLogStateDescriptorEv
+ __ZN3MTL7Private5Class24s_kMTLMotionKeyframeDataEv
+ __ZN3MTL7Private5Class25s_kMTLAttributeDescriptorEv
+ __ZN3MTL7Private5Class25s_kMTLSharedEventListenerEv
+ __ZN3MTL7Private5Class25s_kMTLSharedTextureHandleEv
+ __ZN3MTL7Private5Class26s_kMTLRenderPassDescriptorEv
+ __ZN3MTL7Private5Class26s_kMTLTextureReferenceTypeEv
+ __ZN3MTL7Private5Class27s_kMTLComputePassDescriptorEv
+ __ZN3MTL7Private5Class28s_kMTLBufferLayoutDescriptorEv
+ __ZN3MTL7Private5Class28s_kMTLCommandQueueDescriptorEv
+ __ZN3MTL7Private5Class28s_kMTLDepthStencilDescriptorEv
+ __ZN3MTL7Private5Class28s_kMTLFunctionConstantValuesEv
+ __ZN3MTL7Private5Class28s_kMTLFunctionStitchingGraphEv
+ __ZN3MTL7Private5Class28s_kMTLResidencySetDescriptorEv
+ __ZN3MTL7Private5Class29s_kMTLBinaryArchiveDescriptorEv
+ __ZN3MTL7Private5Class29s_kMTLCommandBufferDescriptorEv
+ __ZN3MTL7Private5Class30s_kMTLAttributeDescriptorArrayEv
+ __ZN3MTL7Private5Class30s_kMTLIOCommandQueueDescriptorEv
+ __ZN3MTL7Private5Class30s_kMTLPipelineBufferDescriptorEv
+ __ZN3MTL7Private5Class30s_kMTLRenderPipelineDescriptorEv
+ __ZN3MTL7Private5Class30s_kMTLRenderPipelineReflectionEv
+ __ZN3MTL7Private5Class31s_kMTLComputePipelineDescriptorEv
+ __ZN3MTL7Private5Class31s_kMTLComputePipelineReflectionEv
+ __ZN3MTL7Private5Class31s_kMTLStitchedLibraryDescriptorEv
+ __ZN3MTL7Private5Class31s_kMTLVertexAttributeDescriptorEv
+ __ZN3MTL7Private5Class32s_kMTLFunctionStitchingInputNodeEv
+ __ZN3MTL7Private5Class32s_kMTLStageInputOutputDescriptorEv
+ __ZN3MTL7Private5Class33s_kMTLBufferLayoutDescriptorArrayEv
+ __ZN3MTL7Private5Class33s_kMTLRasterizationRateLayerArrayEv
+ __ZN3MTL7Private5Class33s_kMTLResourceStatePassDescriptorEv
+ __ZN3MTL7Private5Class34s_kMTLMeshRenderPipelineDescriptorEv
+ __ZN3MTL7Private5Class34s_kMTLRasterizationRateSampleArrayEv
+ __ZN3MTL7Private5Class34s_kMTLTileRenderPipelineDescriptorEv
+ __ZN3MTL7Private5Class34s_kMTLVertexBufferLayoutDescriptorEv
+ __ZN3MTL7Private5Class35s_kMTLCounterSampleBufferDescriptorEv
+ __ZN3MTL7Private5Class35s_kMTLFunctionStitchingFunctionNodeEv
+ __ZN3MTL7Private5Class35s_kMTLPipelineBufferDescriptorArrayEv
+ __ZN3MTL7Private5Class36s_kMTLIntersectionFunctionDescriptorEv
+ __ZN3MTL7Private5Class36s_kMTLRasterizationRateMapDescriptorEv
+ __ZN3MTL7Private5Class36s_kMTLRenderPassAttachmentDescriptorEv
+ __ZN3MTL7Private5Class36s_kMTLVertexAttributeDescriptorArrayEv
+ __ZN3MTL7Private5Class36s_kMTLVisibleFunctionTableDescriptorEv
+ __ZN3MTL7Private5Class37s_kMTLAccelerationStructureDescriptorEv
+ __ZN3MTL7Private5Class37s_kMTLIndirectCommandBufferDescriptorEv
+ __ZN3MTL7Private5Class38s_kMTLRasterizationRateLayerDescriptorEv
+ __ZN3MTL7Private5Class39s_kMTLRenderPipelineFunctionsDescriptorEv
+ __ZN3MTL7Private5Class39s_kMTLVertexBufferLayoutDescriptorArrayEv
+ __ZN3MTL7Private5Class41s_kMTLAccelerationStructurePassDescriptorEv
+ __ZN3MTL7Private5Class41s_kMTLIntersectionFunctionTableDescriptorEv
+ __ZN3MTL7Private5Class41s_kMTLRenderPassColorAttachmentDescriptorEv
+ __ZN3MTL7Private5Class41s_kMTLRenderPassDepthAttachmentDescriptorEv
+ __ZN3MTL7Private5Class43s_kMTLRenderPassStencilAttachmentDescriptorEv
+ __ZN3MTL7Private5Class44s_kMTLFunctionStitchingAttributeAlwaysInlineEv
+ __ZN3MTL7Private5Class45s_kMTLAccelerationStructureGeometryDescriptorEv
+ __ZN3MTL7Private5Class45s_kMTLInstanceAccelerationStructureDescriptorEv
+ __ZN3MTL7Private5Class45s_kMTLRenderPipelineColorAttachmentDescriptorEv
+ __ZN3MTL7Private5Class46s_kMTLBlitPassSampleBufferAttachmentDescriptorEv
+ __ZN3MTL7Private5Class46s_kMTLPrimitiveAccelerationStructureDescriptorEv
+ __ZN3MTL7Private5Class46s_kMTLRenderPassColorAttachmentDescriptorArrayEv
+ __ZN3MTL7Private5Class48s_kMTLRenderPassSampleBufferAttachmentDescriptorEv
+ __ZN3MTL7Private5Class49s_kMTLComputePassSampleBufferAttachmentDescriptorEv
+ __ZN3MTL7Private5Class49s_kMTLTileRenderPipelineColorAttachmentDescriptorEv
+ __ZN3MTL7Private5Class50s_kMTLAccelerationStructureCurveGeometryDescriptorEv
+ __ZN3MTL7Private5Class50s_kMTLRenderPipelineColorAttachmentDescriptorArrayEv
+ __ZN3MTL7Private5Class51s_kMTLBlitPassSampleBufferAttachmentDescriptorArrayEv
+ __ZN3MTL7Private5Class53s_kMTLAccelerationStructureTriangleGeometryDescriptorEv
+ __ZN3MTL7Private5Class53s_kMTLIndirectInstanceAccelerationStructureDescriptorEv
+ __ZN3MTL7Private5Class53s_kMTLRenderPassSampleBufferAttachmentDescriptorArrayEv
+ __ZN3MTL7Private5Class54s_kMTLComputePassSampleBufferAttachmentDescriptorArrayEv
+ __ZN3MTL7Private5Class54s_kMTLTileRenderPipelineColorAttachmentDescriptorArrayEv
+ __ZN3MTL7Private5Class55s_kMTLResourceStatePassSampleBufferAttachmentDescriptorEv
+ __ZN3MTL7Private5Class56s_kMTLAccelerationStructureBoundingBoxGeometryDescriptorEv
+ __ZN3MTL7Private5Class56s_kMTLAccelerationStructureMotionCurveGeometryDescriptorEv
+ __ZN3MTL7Private5Class59s_kMTLAccelerationStructureMotionTriangleGeometryDescriptorEv
+ __ZN3MTL7Private5Class60s_kMTLResourceStatePassSampleBufferAttachmentDescriptorArrayEv
+ __ZN3MTL7Private5Class62s_kMTLAccelerationStructureMotionBoundingBoxGeometryDescriptorEv
+ __ZN3MTL7Private5Class63s_kMTLAccelerationStructurePassSampleBufferAttachmentDescriptorEv
+ __ZN3MTL7Private5Class68s_kMTLAccelerationStructurePassSampleBufferAttachmentDescriptorArrayEv
+ __ZN3MTL7Private8Protocol10s_kMTLHeapEv
+ __ZN3MTL7Private8Protocol11s_kMTLEventEv
+ __ZN3MTL7Private8Protocol11s_kMTLFenceEv
+ __ZN3MTL7Private8Protocol12s_kMTLBufferEv
+ __ZN3MTL7Private8Protocol12s_kMTLDeviceEv
+ __ZN3MTL7Private8Protocol13s_kMTLBindingEv
+ __ZN3MTL7Private8Protocol13s_kMTLCounterEv
+ __ZN3MTL7Private8Protocol13s_kMTLLibraryEv
+ __ZN3MTL7Private8Protocol13s_kMTLTextureEv
+ __ZN3MTL7Private8Protocol14s_kMTLDrawableEv
+ __ZN3MTL7Private8Protocol14s_kMTLFunctionEv
+ __ZN3MTL7Private8Protocol14s_kMTLLogStateEv
+ __ZN3MTL7Private8Protocol14s_kMTLResourceEv
+ __ZN3MTL7Private8Protocol16s_kMTLAllocationEv
+ __ZN3MTL7Private8Protocol16s_kMTLCounterSetEv
+ __ZN3MTL7Private8Protocol17s_kMTLFunctionLogEv
+ __ZN3MTL7Private8Protocol17s_kMTLSharedEventEv
+ __ZN3MTL7Private8Protocol18s_kMTLCommandQueueEv
+ __ZN3MTL7Private8Protocol18s_kMTLIOFileHandleEv
+ __ZN3MTL7Private8Protocol18s_kMTLLogContainerEv
+ __ZN3MTL7Private8Protocol18s_kMTLResidencySetEv
+ __ZN3MTL7Private8Protocol18s_kMTLSamplerStateEv
+ __ZN3MTL7Private8Protocol19s_kMTLBinaryArchiveEv
+ __ZN3MTL7Private8Protocol19s_kMTLBufferBindingEv
+ __ZN3MTL7Private8Protocol19s_kMTLCommandBufferEv
+ __ZN3MTL7Private8Protocol20s_kMTLCommandEncoderEv
+ __ZN3MTL7Private8Protocol20s_kMTLDynamicLibraryEv
+ __ZN3MTL7Private8Protocol20s_kMTLFunctionHandleEv
+ __ZN3MTL7Private8Protocol20s_kMTLIOCommandQueueEv
+ __ZN3MTL7Private8Protocol20s_kMTLTextureBindingEv
+ __ZN3MTL7Private8Protocol21s_kMTLArgumentEncoderEv
+ __ZN3MTL7Private8Protocol21s_kMTLIOCommandBufferEv
+ __ZN3MTL7Private8Protocol21s_kMTLIOScratchBufferEv
+ __ZN3MTL7Private8Protocol23s_kMTLDepthStencilStateEv
+ __ZN3MTL7Private8Protocol24s_kMTLBlitCommandEncoderEv
+ __ZN3MTL7Private8Protocol24s_kMTLThreadgroupBindingEv
+ __ZN3MTL7Private8Protocol25s_kMTLCounterSampleBufferEv
+ __ZN3MTL7Private8Protocol25s_kMTLRenderPipelineStateEv
+ __ZN3MTL7Private8Protocol26s_kMTLComputePipelineStateEv
+ __ZN3MTL7Private8Protocol26s_kMTLObjectPayloadBindingEv
+ __ZN3MTL7Private8Protocol26s_kMTLRasterizationRateMapEv
+ __ZN3MTL7Private8Protocol26s_kMTLRenderCommandEncoderEv
+ __ZN3MTL7Private8Protocol26s_kMTLVisibleFunctionTableEv
+ __ZN3MTL7Private8Protocol27s_kMTLAccelerationStructureEv
+ __ZN3MTL7Private8Protocol27s_kMTLComputeCommandEncoderEv
+ __ZN3MTL7Private8Protocol27s_kMTLFunctionStitchingNodeEv
+ __ZN3MTL7Private8Protocol27s_kMTLIndirectCommandBufferEv
+ __ZN3MTL7Private8Protocol27s_kMTLIndirectRenderCommandEv
+ __ZN3MTL7Private8Protocol28s_kMTLIndirectComputeCommandEv
+ __ZN3MTL7Private8Protocol30s_kMTLCommandBufferEncoderInfoEv
+ __ZN3MTL7Private8Protocol30s_kMTLFunctionLogDebugLocationEv
+ __ZN3MTL7Private8Protocol30s_kMTLIOScratchBufferAllocatorEv
+ __ZN3MTL7Private8Protocol31s_kMTLIntersectionFunctionTableEv
+ __ZN3MTL7Private8Protocol32s_kMTLFunctionStitchingAttributeEv
+ __ZN3MTL7Private8Protocol33s_kMTLResourceStateCommandEncoderEv
+ __ZN3MTL7Private8Protocol34s_kMTLParallelRenderCommandEncoderEv
+ __ZN3MTL7Private8Protocol41s_kMTLAccelerationStructureCommandEncoderEv
+ __ZN3MTL7Private8Selector100s_kgetTextureAccessCounters_region_mipLevel_slice_resetCounters_countersBuffer_countersBufferOffset_Ev
+ __ZN3MTL7Private8Selector101s_kdrawIndexedPrimitives_indexType_indexBuffer_indexBufferOffset_indirectBuffer_indirectBufferOffset_Ev
+ __ZN3MTL7Private8Selector104s_kdrawPatches_patchStart_patchCount_patchIndexBuffer_patchIndexBufferOffset_instanceCount_baseInstance_Ev
+ __ZN3MTL7Private8Selector109s_kcopyFromTexture_sourceSlice_sourceLevel_toTexture_destinationSlice_destinationLevel_sliceCount_levelCount_Ev
+ __ZN3MTL7Private8Selector10s_kbuffersEv
+ __ZN3MTL7Private8Selector10s_kdeallocEv
+ __ZN3MTL7Private8Selector10s_kenqueueEv
+ __ZN3MTL7Private8Selector10s_klayoutsEv
+ __ZN3MTL7Private8Selector10s_kmembersEv
+ __ZN3MTL7Private8Selector10s_koptionsEv
+ __ZN3MTL7Private8Selector10s_kpresentEv
+ __ZN3MTL7Private8Selector10s_ksetUrl_Ev
+ __ZN3MTL7Private8Selector10s_kswizzleEv
+ __ZN3MTL7Private8Selector10s_ktextureEv
+ __ZN3MTL7Private8Selector114s_kdrawIndexedPrimitives_indexCount_indexType_indexBuffer_indexBufferOffset_instanceCount_baseVertex_baseInstance_Ev
+ __ZN3MTL7Private8Selector117s_kdrawMeshThreadgroupsWithIndirectBuffer_indirectBufferOffset_threadsPerObjectThreadgroup_threadsPerMeshThreadgroup_Ev
+ __ZN3MTL7Private8Selector11s_kbindingsEv
+ __ZN3MTL7Private8Selector11s_kcontentsEv
+ __ZN3MTL7Private8Selector11s_kcountersEv
+ __ZN3MTL7Private8Selector11s_kdataSizeEv
+ __ZN3MTL7Private8Selector11s_kdataTypeEv
+ __ZN3MTL7Private8Selector11s_kendScopeEv
+ __ZN3MTL7Private8Selector11s_kfunctionEv
+ __ZN3MTL7Private8Selector11s_kisActiveEv
+ __ZN3MTL7Private8Selector11s_kisSparseEv
+ __ZN3MTL7Private8Selector11s_klocationEv
+ __ZN3MTL7Private8Selector11s_klogStateEv
+ __ZN3MTL7Private8Selector11s_kmathModeEv
+ __ZN3MTL7Private8Selector11s_knewEventEv
+ __ZN3MTL7Private8Selector11s_knewFenceEv
+ __ZN3MTL7Private8Selector11s_kpriorityEv
+ __ZN3MTL7Private8Selector11s_kreadMaskEv
+ __ZN3MTL7Private8Selector11s_krequiredEv
+ __ZN3MTL7Private8Selector11s_ksetName_Ev
+ __ZN3MTL7Private8Selector11s_ksetSize_Ev
+ __ZN3MTL7Private8Selector11s_ksetType_Ev
+ __ZN3MTL7Private8Selector11s_kstepRateEv
+ __ZN3MTL7Private8Selector11s_kuseHeap_Ev
+ __ZN3MTL7Private8Selector11s_kusedSizeEv
+ __ZN3MTL7Private8Selector11s_kverticalEv
+ __ZN3MTL7Private8Selector120s_kloadTexture_slice_level_size_sourceBytesPerRow_sourceBytesPerImage_destinationOrigin_sourceHandle_sourceHandleOffset_Ev
+ __ZN3MTL7Private8Selector129s_kcopyFromTexture_sourceSlice_sourceLevel_sourceOrigin_sourceSize_toTexture_destinationSlice_destinationLevel_destinationOrigin_Ev
+ __ZN3MTL7Private8Selector12s_kalignmentEv
+ __ZN3MTL7Private8Selector12s_kargumentsEv
+ __ZN3MTL7Private8Selector12s_karrayTypeEv
+ __ZN3MTL7Private8Selector12s_kcurveTypeEv
+ __ZN3MTL7Private8Selector12s_kfunctionsEv
+ __ZN3MTL7Private8Selector12s_kindexTypeEv
+ __ZN3MTL7Private8Selector12s_kiosurfaceEv
+ __ZN3MTL7Private8Selector12s_klibrariesEv
+ __ZN3MTL7Private8Selector12s_kmagFilterEv
+ __ZN3MTL7Private8Selector12s_kminFilterEv
+ __ZN3MTL7Private8Selector12s_kmipFilterEv
+ __ZN3MTL7Private8Selector12s_koutputURLEv
+ __ZN3MTL7Private8Selector12s_kpatchTypeEv
+ __ZN3MTL7Private8Selector12s_kpeerCountEv
+ __ZN3MTL7Private8Selector12s_kpeerIndexEv
+ __ZN3MTL7Private8Selector12s_ksetDepth_Ev
+ __ZN3MTL7Private8Selector12s_ksetIndex_Ev
+ __ZN3MTL7Private8Selector12s_ksetLabel_Ev
+ __ZN3MTL7Private8Selector12s_ksetLevel_Ev
+ __ZN3MTL7Private8Selector12s_ksetNodes_Ev
+ __ZN3MTL7Private8Selector12s_ksetSlice_Ev
+ __ZN3MTL7Private8Selector12s_ksetUsage_Ev
+ __ZN3MTL7Private8Selector12s_ksetWidth_Ev
+ __ZN3MTL7Private8Selector12s_ktileWidthEv
+ __ZN3MTL7Private8Selector12s_ktryCancelEv
+ __ZN3MTL7Private8Selector12s_kwriteMaskEv
+ __ZN3MTL7Private8Selector13s_kGPUEndTimeEv
+ __ZN3MTL7Private8Selector13s_kaddBarrierEv
+ __ZN3MTL7Private8Selector13s_kattributesEv
+ __ZN3MTL7Private8Selector13s_kbeginScopeEv
+ __ZN3MTL7Private8Selector13s_kbufferSizeEv
+ __ZN3MTL7Private8Selector13s_kclearColorEv
+ __ZN3MTL7Private8Selector13s_kclearDepthEv
+ __ZN3MTL7Private8Selector13s_kcounterSetEv
+ __ZN3MTL7Private8Selector13s_kcurveBasisEv
+ __ZN3MTL7Private8Selector13s_kdepthPlaneEv
+ __ZN3MTL7Private8Selector13s_kdescriptorEv
+ __ZN3MTL7Private8Selector13s_kdrawableIDEv
+ __ZN3MTL7Private8Selector13s_kerrorStateEv
+ __ZN3MTL7Private8Selector13s_kgpuAddressEv
+ __ZN3MTL7Private8Selector13s_kheapOffsetEv
+ __ZN3MTL7Private8Selector13s_khorizontalEv
+ __ZN3MTL7Private8Selector13s_kisArgumentEv
+ __ZN3MTL7Private8Selector13s_kisHeadlessEv
+ __ZN3MTL7Private8Selector13s_kisLowPowerEv
+ __ZN3MTL7Private8Selector13s_klayerCountEv
+ __ZN3MTL7Private8Selector13s_kloadActionEv
+ __ZN3MTL7Private8Selector13s_klodAverageEv
+ __ZN3MTL7Private8Selector13s_kmutabilityEv
+ __ZN3MTL7Private8Selector13s_koutputNodeEv
+ __ZN3MTL7Private8Selector13s_kregistryIDEv
+ __ZN3MTL7Private8Selector13s_kscreenSizeEv
+ __ZN3MTL7Private8Selector13s_ksetAccess_Ev
+ __ZN3MTL7Private8Selector13s_ksetBarrierEv
+ __ZN3MTL7Private8Selector13s_ksetBuffer_Ev
+ __ZN3MTL7Private8Selector13s_ksetFormat_Ev
+ __ZN3MTL7Private8Selector13s_ksetGroups_Ev
+ __ZN3MTL7Private8Selector13s_ksetHeight_Ev
+ __ZN3MTL7Private8Selector13s_ksetOffset_Ev
+ __ZN3MTL7Private8Selector13s_ksetOpaque_Ev
+ __ZN3MTL7Private8Selector13s_ksetStride_Ev
+ __ZN3MTL7Private8Selector13s_kstructTypeEv
+ __ZN3MTL7Private8Selector13s_ktileHeightEv
+ __ZN3MTL7Private8Selector142s_kcopyFromBuffer_sourceOffset_sourceBytesPerRow_sourceBytesPerImage_sourceSize_toTexture_destinationSlice_destinationLevel_destinationOrigin_Ev
+ __ZN3MTL7Private8Selector142s_kcopyFromTexture_sourceSlice_sourceLevel_sourceOrigin_sourceSize_toBuffer_destinationOffset_destinationBytesPerRow_destinationBytesPerImage_Ev
+ __ZN3MTL7Private8Selector144s_kmoveTextureMappingsFromTexture_sourceSlice_sourceLevel_sourceOrigin_sourceSize_toTexture_destinationSlice_destinationLevel_destinationOrigin_Ev
+ __ZN3MTL7Private8Selector14s_karrayLengthEv
+ __ZN3MTL7Private8Selector14s_kborderColorEv
+ __ZN3MTL7Private8Selector14s_kbufferIndexEv
+ __ZN3MTL7Private8Selector14s_kcounterSetsEv
+ __ZN3MTL7Private8Selector14s_kdestinationEv
+ __ZN3MTL7Private8Selector14s_kelementTypeEv
+ __ZN3MTL7Private8Selector14s_kendEncodingEv
+ __ZN3MTL7Private8Selector14s_kindexBufferEv
+ __ZN3MTL7Private8Selector14s_kinstallNameEv
+ __ZN3MTL7Private8Selector14s_kisAliasableEv
+ __ZN3MTL7Private8Selector14s_kisCapturingEv
+ __ZN3MTL7Private8Selector14s_kisPatchDataEv
+ __ZN3MTL7Private8Selector14s_kisRemovableEv
+ __ZN3MTL7Private8Selector14s_kisShareableEv
+ __ZN3MTL7Private8Selector14s_klibraryTypeEv
+ __ZN3MTL7Private8Selector14s_klodMaxClampEv
+ __ZN3MTL7Private8Selector14s_klodMinClampEv
+ __ZN3MTL7Private8Selector14s_kmeshBuffersEv
+ __ZN3MTL7Private8Selector14s_kpeerGroupIDEv
+ __ZN3MTL7Private8Selector14s_kpixelFormatEv
+ __ZN3MTL7Private8Selector14s_kpointerTypeEv
+ __ZN3MTL7Private8Selector14s_ksampleCountEv
+ __ZN3MTL7Private8Selector14s_ksetOptions_Ev
+ __ZN3MTL7Private8Selector14s_ksetSwizzle_Ev
+ __ZN3MTL7Private8Selector14s_ksetTexture_Ev
+ __ZN3MTL7Private8Selector14s_kstopCaptureEv
+ __ZN3MTL7Private8Selector14s_kstorageModeEv
+ __ZN3MTL7Private8Selector14s_kstoreActionEv
+ __ZN3MTL7Private8Selector14s_ktextureTypeEv
+ __ZN3MTL7Private8Selector14s_ktileBuffersEv
+ __ZN3MTL7Private8Selector150s_kcopyFromBuffer_sourceOffset_sourceBytesPerRow_sourceBytesPerImage_sourceSize_toTexture_destinationSlice_destinationLevel_destinationOrigin_options_Ev
+ __ZN3MTL7Private8Selector150s_kcopyFromTexture_sourceSlice_sourceLevel_sourceOrigin_sourceSize_toBuffer_destinationOffset_destinationBytesPerRow_destinationBytesPerImage_options_Ev
+ __ZN3MTL7Private8Selector152s_kdrawIndexedPatches_patchIndexBuffer_patchIndexBufferOffset_controlPointIndexBuffer_controlPointIndexBufferOffset_indirectBuffer_indirectBufferOffset_Ev
+ __ZN3MTL7Private8Selector15s_kGPUStartTimeEv
+ __ZN3MTL7Private8Selector15s_karchitectureEv
+ __ZN3MTL7Private8Selector15s_kbufferOffsetEv
+ __ZN3MTL7Private8Selector15s_kclearBarrierEv
+ __ZN3MTL7Private8Selector15s_kclearStencilEv
+ __ZN3MTL7Private8Selector15s_kcommandQueueEv
+ __ZN3MTL7Private8Selector15s_kcommandTypesEv
+ __ZN3MTL7Private8Selector15s_kcpuCacheModeEv
+ __ZN3MTL7Private8Selector15s_kcurveEndCapsEv
+ __ZN3MTL7Private8Selector15s_kdispatchTypeEv
+ __ZN3MTL7Private8Selector15s_kencoderLabelEv
+ __ZN3MTL7Private8Selector15s_kendResidencyEv
+ __ZN3MTL7Private8Selector15s_kerrorOptionsEv
+ __ZN3MTL7Private8Selector15s_kfunctionNameEv
+ __ZN3MTL7Private8Selector15s_kfunctionTypeEv
+ __ZN3MTL7Private8Selector15s_kmeshBindingsEv
+ __ZN3MTL7Private8Selector15s_kmeshFunctionEv
+ __ZN3MTL7Private8Selector15s_krAddressModeEv
+ __ZN3MTL7Private8Selector15s_kradiusBufferEv
+ __ZN3MTL7Private8Selector15s_kradiusFormatEv
+ __ZN3MTL7Private8Selector15s_kradiusStrideEv
+ __ZN3MTL7Private8Selector15s_kresolveLevelEv
+ __ZN3MTL7Private8Selector15s_kresolveSliceEv
+ __ZN3MTL7Private8Selector15s_krootResourceEv
+ __ZN3MTL7Private8Selector15s_ksAddressModeEv
+ __ZN3MTL7Private8Selector15s_ksampleBufferEv
+ __ZN3MTL7Private8Selector15s_ksegmentCountEv
+ __ZN3MTL7Private8Selector15s_ksetCullMode_Ev
+ __ZN3MTL7Private8Selector15s_ksetDataType_Ev
+ __ZN3MTL7Private8Selector15s_ksetLogState_Ev
+ __ZN3MTL7Private8Selector15s_ksetMathMode_Ev
+ __ZN3MTL7Private8Selector15s_ksetPriority_Ev
+ __ZN3MTL7Private8Selector15s_ksetReadMask_Ev
+ __ZN3MTL7Private8Selector15s_ksetStepRate_Ev
+ __ZN3MTL7Private8Selector15s_ksetViewport_Ev
+ __ZN3MTL7Private8Selector15s_kstepFunctionEv
+ __ZN3MTL7Private8Selector15s_ktAddressModeEv
+ __ZN3MTL7Private8Selector15s_ktileBindingsEv
+ __ZN3MTL7Private8Selector15s_ktileFunctionEv
+ __ZN3MTL7Private8Selector15s_kupdateFence_Ev
+ __ZN3MTL7Private8Selector15s_kvertexBufferEv
+ __ZN3MTL7Private8Selector15s_kvertexFormatEv
+ __ZN3MTL7Private8Selector15s_kvertexStrideEv
+ __ZN3MTL7Private8Selector165s_kdrawIndexedPatches_patchStart_patchCount_patchIndexBuffer_patchIndexBufferOffset_controlPointIndexBuffer_controlPointIndexBufferOffset_instanceCount_baseInstance_Ev
+ __ZN3MTL7Private8Selector16s_kallocatedSizeEv
+ __ZN3MTL7Private8Selector16s_kargumentIndexEv
+ __ZN3MTL7Private8Selector16s_kattributeTypeEv
+ __ZN3MTL7Private8Selector16s_kcaptureObjectEv
+ __ZN3MTL7Private8Selector16s_kcommandBufferEv
+ __ZN3MTL7Private8Selector16s_kdebugLocationEv
+ __ZN3MTL7Private8Selector16s_kdispatchQueueEv
+ __ZN3MTL7Private8Selector16s_kenableLoggingEv
+ __ZN3MTL7Private8Selector16s_kencodedLengthEv
+ __ZN3MTL7Private8Selector16s_kfunctionCountEv
+ __ZN3MTL7Private8Selector16s_kfunctionNamesEv
+ __ZN3MTL7Private8Selector16s_kgpuResourceIDEv
+ __ZN3MTL7Private8Selector16s_kinstanceCountEv
+ __ZN3MTL7Private8Selector16s_kkernelEndTimeEv
+ __ZN3MTL7Private8Selector16s_klayerAtIndex_Ev
+ __ZN3MTL7Private8Selector16s_kmakeAliasableEv
+ __ZN3MTL7Private8Selector16s_kmaxAnisotropyEv
+ __ZN3MTL7Private8Selector16s_kmemberByName_Ev
+ __ZN3MTL7Private8Selector16s_kmotionEndTimeEv
+ __ZN3MTL7Private8Selector16s_kobjectBuffersEv
+ __ZN3MTL7Private8Selector16s_kparentTextureEv
+ __ZN3MTL7Private8Selector16s_kpopDebugGroupEv
+ __ZN3MTL7Private8Selector16s_kpresentedTimeEv
+ __ZN3MTL7Private8Selector16s_kradiusBuffersEv
+ __ZN3MTL7Private8Selector16s_ksetArguments_Ev
+ __ZN3MTL7Private8Selector16s_ksetCurveType_Ev
+ __ZN3MTL7Private8Selector16s_ksetFunctions_Ev
+ __ZN3MTL7Private8Selector16s_ksetIndexType_Ev
+ __ZN3MTL7Private8Selector16s_ksetLibraries_Ev
+ __ZN3MTL7Private8Selector16s_ksetMagFilter_Ev
+ __ZN3MTL7Private8Selector16s_ksetMinFilter_Ev
+ __ZN3MTL7Private8Selector16s_ksetMipFilter_Ev
+ __ZN3MTL7Private8Selector16s_ksetOutputURL_Ev
+ __ZN3MTL7Private8Selector16s_ksetTileWidth_Ev
+ __ZN3MTL7Private8Selector16s_ksetWriteMask_Ev
+ __ZN3MTL7Private8Selector16s_ksignaledValueEv
+ __ZN3MTL7Private8Selector16s_ktileArgumentsEv
+ __ZN3MTL7Private8Selector16s_ktriangleCountEv
+ __ZN3MTL7Private8Selector16s_kvertexBuffersEv
+ __ZN3MTL7Private8Selector16s_kwaitForFence_Ev
+ __ZN3MTL7Private8Selector17s_kaddAllocation_Ev
+ __ZN3MTL7Private8Selector17s_kaddLogHandler_Ev
+ __ZN3MTL7Private8Selector17s_kallAllocationsEv
+ __ZN3MTL7Private8Selector17s_kattributeIndexEv
+ __ZN3MTL7Private8Selector17s_kbinaryArchivesEv
+ __ZN3MTL7Private8Selector17s_kbufferDataSizeEv
+ __ZN3MTL7Private8Selector17s_kbufferDataTypeEv
+ __ZN3MTL7Private8Selector17s_kconstantValuesEv
+ __ZN3MTL7Private8Selector17s_kdebugSignpostsEv
+ __ZN3MTL7Private8Selector17s_kenqueueBarrierEv
+ __ZN3MTL7Private8Selector17s_kfunctionGraphsEv
+ __ZN3MTL7Private8Selector17s_kinheritBuffersEv
+ __ZN3MTL7Private8Selector17s_kiosurfacePlaneEv
+ __ZN3MTL7Private8Selector17s_kisDepthTextureEv
+ __ZN3MTL7Private8Selector17s_klocationNumberEv
+ __ZN3MTL7Private8Selector17s_kmaxSampleCountEv
+ __ZN3MTL7Private8Selector17s_knewSharedEventEv
+ __ZN3MTL7Private8Selector17s_kobjectBindingsEv
+ __ZN3MTL7Private8Selector17s_kobjectFunctionEv
+ __ZN3MTL7Private8Selector17s_kpresentAtTime_Ev
+ __ZN3MTL7Private8Selector17s_kresolveTextureEv
+ __ZN3MTL7Private8Selector17s_ksetAttributes_Ev
+ __ZN3MTL7Private8Selector17s_ksetBufferSize_Ev
+ __ZN3MTL7Private8Selector17s_ksetClearColor_Ev
+ __ZN3MTL7Private8Selector17s_ksetClearDepth_Ev
+ __ZN3MTL7Private8Selector17s_ksetCounterSet_Ev
+ __ZN3MTL7Private8Selector17s_ksetCurveBasis_Ev
+ __ZN3MTL7Private8Selector17s_ksetDepthPlane_Ev
+ __ZN3MTL7Private8Selector17s_ksetLoadAction_Ev
+ __ZN3MTL7Private8Selector17s_ksetLodAverage_Ev
+ __ZN3MTL7Private8Selector17s_ksetMutability_Ev
+ __ZN3MTL7Private8Selector17s_ksetOutputNode_Ev
+ __ZN3MTL7Private8Selector17s_ksetScreenSize_Ev
+ __ZN3MTL7Private8Selector17s_ksetTileHeight_Ev
+ __ZN3MTL7Private8Selector17s_ksparsePageSizeEv
+ __ZN3MTL7Private8Selector17s_ktextureBarrierEv
+ __ZN3MTL7Private8Selector17s_kvertexBindingsEv
+ __ZN3MTL7Private8Selector17s_kvertexFunctionEv
+ __ZN3MTL7Private8Selector18s_kallocationCountEv
+ __ZN3MTL7Private8Selector18s_kbackFaceStencilEv
+ __ZN3MTL7Private8Selector18s_kbinaryFunctionsEv
+ __ZN3MTL7Private8Selector18s_kbufferAlignmentEv
+ __ZN3MTL7Private8Selector18s_kcompareFunctionEv
+ __ZN3MTL7Private8Selector18s_kcompressionTypeEv
+ __ZN3MTL7Private8Selector18s_kcomputeFunctionEv
+ __ZN3MTL7Private8Selector18s_kdepthAttachmentEv
+ __ZN3MTL7Private8Selector18s_kdidModifyRange_Ev
+ __ZN3MTL7Private8Selector18s_kfastMathEnabledEv
+ __ZN3MTL7Private8Selector18s_kfragmentBuffersEv
+ __ZN3MTL7Private8Selector18s_kinitialCapacityEv
+ __ZN3MTL7Private8Selector18s_kinsertLibrariesEv
+ __ZN3MTL7Private8Selector18s_kkernelStartTimeEv
+ __ZN3MTL7Private8Selector18s_klanguageVersionEv
+ __ZN3MTL7Private8Selector18s_klinkedFunctionsEv
+ __ZN3MTL7Private8Selector18s_kmaxBufferLengthEv
+ __ZN3MTL7Private8Selector18s_kmaxTransferRateEv
+ __ZN3MTL7Private8Selector18s_kmotionStartTimeEv
+ __ZN3MTL7Private8Selector18s_knewCommandQueueEv
+ __ZN3MTL7Private8Selector18s_kpushDebugGroup_Ev
+ __ZN3MTL7Private8Selector18s_kresetWithRange_Ev
+ __ZN3MTL7Private8Selector18s_kresourceOptionsEv
+ __ZN3MTL7Private8Selector18s_ksetArrayLength_Ev
+ __ZN3MTL7Private8Selector18s_ksetBorderColor_Ev
+ __ZN3MTL7Private8Selector18s_ksetBufferIndex_Ev
+ __ZN3MTL7Private8Selector18s_ksetDestination_Ev
+ __ZN3MTL7Private8Selector18s_ksetIndexBuffer_Ev
+ __ZN3MTL7Private8Selector18s_ksetInstallName_Ev
+ __ZN3MTL7Private8Selector18s_ksetLibraryType_Ev
+ __ZN3MTL7Private8Selector18s_ksetLodMaxClamp_Ev
+ __ZN3MTL7Private8Selector18s_ksetLodMinClamp_Ev
+ __ZN3MTL7Private8Selector18s_ksetPixelFormat_Ev
+ __ZN3MTL7Private8Selector18s_ksetSampleCount_Ev
+ __ZN3MTL7Private8Selector18s_ksetScissorRect_Ev
+ __ZN3MTL7Private8Selector18s_ksetStorageMode_Ev
+ __ZN3MTL7Private8Selector18s_ksetStoreAction_Ev
+ __ZN3MTL7Private8Selector18s_ksetTextureType_Ev
+ __ZN3MTL7Private8Selector18s_kspecializedNameEv
+ __ZN3MTL7Private8Selector18s_ksupportsFamily_Ev
+ __ZN3MTL7Private8Selector18s_ktailSizeInBytesEv
+ __ZN3MTL7Private8Selector18s_ktextureDataTypeEv
+ __ZN3MTL7Private8Selector18s_kuseHeap_stages_Ev
+ __ZN3MTL7Private8Selector18s_kuseHeaps_count_Ev
+ __ZN3MTL7Private8Selector18s_kvertexArgumentsEv
+ __ZN3MTL7Private8Selector199s_kdrawPatches_patchStart_patchCount_patchIndexBuffer_patchIndexBufferOffset_instanceCount_baseInstance_tessellationFactorBuffer_tessellationFactorBufferOffset_tessellationFactorBufferInstanceStride_Ev
+ __ZN3MTL7Private8Selector19s_kaddResidencySet_Ev
+ __ZN3MTL7Private8Selector19s_kboundingBoxCountEv
+ __ZN3MTL7Private8Selector19s_kbufferStructTypeEv
+ __ZN3MTL7Private8Selector19s_kcolorAttachmentsEv
+ __ZN3MTL7Private8Selector19s_kelementArrayTypeEv
+ __ZN3MTL7Private8Selector19s_kfragmentBindingsEv
+ __ZN3MTL7Private8Selector19s_kfragmentFunctionEv
+ __ZN3MTL7Private8Selector19s_kfrontFaceStencilEv
+ __ZN3MTL7Private8Selector19s_khasUnifiedMemoryEv
+ __ZN3MTL7Private8Selector19s_kindexBufferIndexEv
+ __ZN3MTL7Private8Selector19s_kmaxInstanceCountEv
+ __ZN3MTL7Private8Selector19s_kmipmapLevelCountEv
+ __ZN3MTL7Private8Selector19s_kpresentDrawable_Ev
+ __ZN3MTL7Private8Selector19s_kprivateFunctionsEv
+ __ZN3MTL7Private8Selector19s_krequestResidencyEv
+ __ZN3MTL7Private8Selector19s_ksetClearStencil_Ev
+ __ZN3MTL7Private8Selector19s_ksetCommandTypes_Ev
+ __ZN3MTL7Private8Selector19s_ksetCpuCacheMode_Ev
+ __ZN3MTL7Private8Selector19s_ksetCurveEndCaps_Ev
+ __ZN3MTL7Private8Selector19s_ksetDispatchType_Ev
+ __ZN3MTL7Private8Selector19s_ksetErrorOptions_Ev
+ __ZN3MTL7Private8Selector19s_ksetFunctionName_Ev
+ __ZN3MTL7Private8Selector19s_ksetMeshFunction_Ev
+ __ZN3MTL7Private8Selector19s_ksetRAddressMode_Ev
+ __ZN3MTL7Private8Selector19s_ksetRadiusBuffer_Ev
+ __ZN3MTL7Private8Selector19s_ksetRadiusFormat_Ev
+ __ZN3MTL7Private8Selector19s_ksetRadiusStride_Ev
+ __ZN3MTL7Private8Selector19s_ksetResolveLevel_Ev
+ __ZN3MTL7Private8Selector19s_ksetResolveSlice_Ev
+ __ZN3MTL7Private8Selector19s_ksetSAddressMode_Ev
+ __ZN3MTL7Private8Selector19s_ksetSampleBuffer_Ev
+ __ZN3MTL7Private8Selector19s_ksetSegmentCount_Ev
+ __ZN3MTL7Private8Selector19s_ksetStepFunction_Ev
+ __ZN3MTL7Private8Selector19s_ksetTAddressMode_Ev
+ __ZN3MTL7Private8Selector19s_ksetTileFunction_Ev
+ __ZN3MTL7Private8Selector19s_ksetVertexBuffer_Ev
+ __ZN3MTL7Private8Selector19s_ksetVertexFormat_Ev
+ __ZN3MTL7Private8Selector19s_ksetVertexStride_Ev
+ __ZN3MTL7Private8Selector19s_kshaderValidationEv
+ __ZN3MTL7Private8Selector19s_kuseResidencySet_Ev
+ __ZN3MTL7Private8Selector19s_kvertexAttributesEv
+ __ZN3MTL7Private8Selector19s_kvertexDescriptorEv
+ __ZN3MTL7Private8Selector20s_kboundingBoxBufferEv
+ __ZN3MTL7Private8Selector20s_kboundingBoxStrideEv
+ __ZN3MTL7Private8Selector20s_kbufferBytesPerRowEv
+ __ZN3MTL7Private8Selector20s_kbufferPointerTypeEv
+ __ZN3MTL7Private8Selector20s_kcontrolPointCountEv
+ __ZN3MTL7Private8Selector20s_kelementStructTypeEv
+ __ZN3MTL7Private8Selector20s_kfirstMipmapInTailEv
+ __ZN3MTL7Private8Selector20s_kfragmentArgumentsEv
+ __ZN3MTL7Private8Selector20s_kindexBufferOffsetEv
+ __ZN3MTL7Private8Selector20s_kisBlendingEnabledEv
+ __ZN3MTL7Private8Selector20s_kisFramebufferOnlyEv
+ __ZN3MTL7Private8Selector20s_kmaxCallStackDepthEv
+ __ZN3MTL7Private8Selector20s_knewDefaultLibraryEv
+ __ZN3MTL7Private8Selector20s_koptimizationLevelEv
+ __ZN3MTL7Private8Selector20s_krasterSampleCountEv
+ __ZN3MTL7Private8Selector20s_kremoveAllocation_Ev
+ __ZN3MTL7Private8Selector20s_krenderTargetWidthEv
+ __ZN3MTL7Private8Selector20s_kresolveDepthPlaneEv
+ __ZN3MTL7Private8Selector20s_krgbBlendOperationEv
+ __ZN3MTL7Private8Selector20s_ksetArgumentIndex_Ev
+ __ZN3MTL7Private8Selector20s_ksetCaptureObject_Ev
+ __ZN3MTL7Private8Selector20s_ksetDepthClipMode_Ev
+ __ZN3MTL7Private8Selector20s_ksetEnableLogging_Ev
+ __ZN3MTL7Private8Selector20s_ksetFunctionCount_Ev
+ __ZN3MTL7Private8Selector20s_ksetInstanceCount_Ev
+ __ZN3MTL7Private8Selector20s_ksetLayer_atIndex_Ev
+ __ZN3MTL7Private8Selector20s_ksetMaxAnisotropy_Ev
+ __ZN3MTL7Private8Selector20s_ksetMotionEndTime_Ev
+ __ZN3MTL7Private8Selector20s_ksetRadiusBuffers_Ev
+ __ZN3MTL7Private8Selector20s_ksetSignaledValue_Ev
+ __ZN3MTL7Private8Selector20s_ksetStageInRegion_Ev
+ __ZN3MTL7Private8Selector20s_ksetTriangleCount_Ev
+ __ZN3MTL7Private8Selector20s_ksetVertexBuffers_Ev
+ __ZN3MTL7Private8Selector20s_kstencilAttachmentEv
+ __ZN3MTL7Private8Selector20s_ksupportRayTracingEv
+ __ZN3MTL7Private8Selector20s_ksupports32BitMSAAEv
+ __ZN3MTL7Private8Selector21s_kargumentDescriptorEv
+ __ZN3MTL7Private8Selector21s_kblitCommandEncoderEv
+ __ZN3MTL7Private8Selector21s_kblitPassDescriptorEv
+ __ZN3MTL7Private8Selector21s_kboundingBoxBuffersEv
+ __ZN3MTL7Private8Selector21s_kcontrolPointBufferEv
+ __ZN3MTL7Private8Selector21s_kcontrolPointFormatEv
+ __ZN3MTL7Private8Selector21s_kcontrolPointStrideEv
+ __ZN3MTL7Private8Selector21s_kdepthResolveFilterEv
+ __ZN3MTL7Private8Selector21s_kelementPointerTypeEv
+ __ZN3MTL7Private8Selector21s_kfunctionDescriptorEv
+ __ZN3MTL7Private8Selector21s_khazardTrackingModeEv
+ __ZN3MTL7Private8Selector21s_kpreloadedLibrariesEv
+ __ZN3MTL7Private8Selector21s_kpreprocessorMacrosEv
+ __ZN3MTL7Private8Selector21s_kpreserveInvarianceEv
+ __ZN3MTL7Private8Selector21s_kradiusBufferOffsetEv
+ __ZN3MTL7Private8Selector21s_krenderTargetHeightEv
+ __ZN3MTL7Private8Selector21s_kretainedReferencesEv
+ __ZN3MTL7Private8Selector21s_ksetBinaryArchives_Ev
+ __ZN3MTL7Private8Selector21s_ksetConstantValues_Ev
+ __ZN3MTL7Private8Selector21s_ksetFunctionGraphs_Ev
+ __ZN3MTL7Private8Selector21s_ksetInheritBuffers_Ev
+ __ZN3MTL7Private8Selector21s_ksetObjectFunction_Ev
+ __ZN3MTL7Private8Selector21s_ksetPurgeableState_Ev
+ __ZN3MTL7Private8Selector21s_ksetResolveTexture_Ev
+ __ZN3MTL7Private8Selector21s_ksetSparsePageSize_Ev
+ __ZN3MTL7Private8Selector21s_ksetVertexFunction_Ev
+ __ZN3MTL7Private8Selector21s_ksignalEvent_value_Ev
+ __ZN3MTL7Private8Selector21s_kstoreActionOptionsEv
+ __ZN3MTL7Private8Selector21s_ksupportsRaytracingEv
+ __ZN3MTL7Private8Selector21s_kuseResource_usage_Ev
+ __ZN3MTL7Private8Selector21s_kvertexBufferOffsetEv
+ __ZN3MTL7Private8Selector21s_kwaitUntilCompletedEv
+ __ZN3MTL7Private8Selector21s_kwaitUntilScheduledEv
+ __ZN3MTL7Private8Selector22s_kalphaBlendOperationEv
+ __ZN3MTL7Private8Selector22s_kargumentIndexStrideEv
+ __ZN3MTL7Private8Selector22s_kcontainsAllocation_Ev
+ __ZN3MTL7Private8Selector22s_kcontrolDependenciesEv
+ __ZN3MTL7Private8Selector22s_kcontrolPointBuffersEv
+ __ZN3MTL7Private8Selector22s_kdefaultCaptureScopeEv
+ __ZN3MTL7Private8Selector22s_kgeometryDescriptorsEv
+ __ZN3MTL7Private8Selector22s_kinstanceCountBufferEv
+ __ZN3MTL7Private8Selector22s_kisAlphaToOneEnabledEv
+ __ZN3MTL7Private8Selector22s_kisDepthWriteEnabledEv
+ __ZN3MTL7Private8Selector22s_kmaxCommandsInFlightEv
+ __ZN3MTL7Private8Selector22s_kmeshLinkedFunctionsEv
+ __ZN3MTL7Private8Selector22s_kmotionEndBorderModeEv
+ __ZN3MTL7Private8Selector22s_kmotionKeyframeCountEv
+ __ZN3MTL7Private8Selector22s_kmotionTransformTypeEv
+ __ZN3MTL7Private8Selector22s_kparentRelativeLevelEv
+ __ZN3MTL7Private8Selector22s_kparentRelativeSliceEv
+ __ZN3MTL7Private8Selector22s_kpayloadMemoryLengthEv
+ __ZN3MTL7Private8Selector22s_kphysicalGranularityEv
+ __ZN3MTL7Private8Selector22s_kprimitiveDataBufferEv
+ __ZN3MTL7Private8Selector22s_kprimitiveDataStrideEv
+ __ZN3MTL7Private8Selector22s_kremoteStorageBufferEv
+ __ZN3MTL7Private8Selector22s_kremoveResidencySet_Ev
+ __ZN3MTL7Private8Selector22s_ksetBackFaceStencil_Ev
+ __ZN3MTL7Private8Selector22s_ksetBinaryFunctions_Ev
+ __ZN3MTL7Private8Selector22s_ksetBlendingEnabled_Ev
+ __ZN3MTL7Private8Selector22s_ksetCompareFunction_Ev
+ __ZN3MTL7Private8Selector22s_ksetCompressionType_Ev
+ __ZN3MTL7Private8Selector22s_ksetComputeFunction_Ev
+ __ZN3MTL7Private8Selector22s_ksetDepthAttachment_Ev
+ __ZN3MTL7Private8Selector22s_ksetFastMathEnabled_Ev
+ __ZN3MTL7Private8Selector22s_ksetInitialCapacity_Ev
+ __ZN3MTL7Private8Selector22s_ksetInsertLibraries_Ev
+ __ZN3MTL7Private8Selector22s_ksetLanguageVersion_Ev
+ __ZN3MTL7Private8Selector22s_ksetLinkedFunctions_Ev
+ __ZN3MTL7Private8Selector22s_ksetMotionStartTime_Ev
+ __ZN3MTL7Private8Selector22s_ksetResourceOptions_Ev
+ __ZN3MTL7Private8Selector22s_ksetSpecializedName_Ev
+ __ZN3MTL7Private8Selector22s_ksetTexture_atIndex_Ev
+ __ZN3MTL7Private8Selector22s_ksetViewports_count_Ev
+ __ZN3MTL7Private8Selector22s_ksupportsFeatureSet_Ev
+ __ZN3MTL7Private8Selector22s_kwaitForEvent_value_Ev
+ __ZN3MTL7Private8Selector23s_kaddCompletedHandler_Ev
+ __ZN3MTL7Private8Selector23s_kaddPresentedHandler_Ev
+ __ZN3MTL7Private8Selector23s_kaddScheduledHandler_Ev
+ __ZN3MTL7Private8Selector23s_kconstantDataAtIndex_Ev
+ __ZN3MTL7Private8Selector23s_kcurrentAllocatedSizeEv
+ __ZN3MTL7Private8Selector23s_kdepthCompareFunctionEv
+ __ZN3MTL7Private8Selector23s_kinheritPipelineStateEv
+ __ZN3MTL7Private8Selector23s_kinitWithSampleCount_Ev
+ __ZN3MTL7Private8Selector23s_kinsertDebugSignpost_Ev
+ __ZN3MTL7Private8Selector23s_kmotionTransformCountEv
+ __ZN3MTL7Private8Selector23s_knewFunctionWithName_Ev
+ __ZN3MTL7Private8Selector23s_knewSharedEventHandleEv
+ __ZN3MTL7Private8Selector23s_krasterizationRateMapEv
+ __ZN3MTL7Private8Selector23s_kremoteStorageTextureEv
+ __ZN3MTL7Private8Selector23s_kremoveAllAllocationsEv
+ __ZN3MTL7Private8Selector23s_krenderCommandEncoderEv
+ __ZN3MTL7Private8Selector23s_krenderPassDescriptorEv
+ __ZN3MTL7Private8Selector23s_kresolveCounterRange_Ev
+ __ZN3MTL7Private8Selector23s_ksetBoundingBoxCount_Ev
+ __ZN3MTL7Private8Selector23s_ksetDepthStoreAction_Ev
+ __ZN3MTL7Private8Selector23s_ksetFragmentFunction_Ev
+ __ZN3MTL7Private8Selector23s_ksetFrontFaceStencil_Ev
+ __ZN3MTL7Private8Selector23s_ksetFunction_atIndex_Ev
+ __ZN3MTL7Private8Selector23s_ksetIndexBufferIndex_Ev
+ __ZN3MTL7Private8Selector23s_ksetMaxInstanceCount_Ev
+ __ZN3MTL7Private8Selector23s_ksetMipmapLevelCount_Ev
+ __ZN3MTL7Private8Selector23s_ksetPrivateFunctions_Ev
+ __ZN3MTL7Private8Selector23s_ksetShaderValidation_Ev
+ __ZN3MTL7Private8Selector23s_ksetTriangleFillMode_Ev
+ __ZN3MTL7Private8Selector23s_ksetVertexDescriptor_Ev
+ __ZN3MTL7Private8Selector23s_ksharedCaptureManagerEv
+ __ZN3MTL7Private8Selector23s_ksourceRGBBlendFactorEv
+ __ZN3MTL7Private8Selector23s_kstageInputAttributesEv
+ __ZN3MTL7Private8Selector23s_kstageInputDescriptorEv
+ __ZN3MTL7Private8Selector23s_kstencilResolveFilterEv
+ __ZN3MTL7Private8Selector23s_ksupportsDestination_Ev
+ __ZN3MTL7Private8Selector23s_ksynchronizeResource_Ev
+ __ZN3MTL7Private8Selector23s_ktextureReferenceTypeEv
+ __ZN3MTL7Private8Selector23s_kthreadExecutionWidthEv
+ __ZN3MTL7Private8Selector24s_kaddAllocations_count_Ev
+ __ZN3MTL7Private8Selector24s_kaddDebugMarker_range_Ev
+ __ZN3MTL7Private8Selector24s_kcomputeCommandEncoderEv
+ __ZN3MTL7Private8Selector24s_kcomputePassDescriptorEv
+ __ZN3MTL7Private8Selector24s_kdepthFailureOperationEv
+ __ZN3MTL7Private8Selector24s_kmaxCommandBufferCountEv
+ __ZN3MTL7Private8Selector24s_kmaxTessellationFactorEv
+ __ZN3MTL7Private8Selector24s_kmotionStartBorderModeEv
+ __ZN3MTL7Private8Selector24s_kmotionTransformBufferEv
+ __ZN3MTL7Private8Selector24s_kmotionTransformStrideEv
+ __ZN3MTL7Private8Selector24s_knormalizedCoordinatesEv
+ __ZN3MTL7Private8Selector24s_kobjectLinkedFunctionsEv
+ __ZN3MTL7Private8Selector24s_kobjectPayloadDataSizeEv
+ __ZN3MTL7Private8Selector24s_kphysicalSizeForLayer_Ev
+ __ZN3MTL7Private8Selector24s_kremoveAllDebugMarkersEv
+ __ZN3MTL7Private8Selector24s_kserializeToURL_error_Ev
+ __ZN3MTL7Private8Selector24s_ksetAlphaToOneEnabled_Ev
+ __ZN3MTL7Private8Selector24s_ksetBoundingBoxBuffer_Ev
+ __ZN3MTL7Private8Selector24s_ksetBoundingBoxStride_Ev
+ __ZN3MTL7Private8Selector24s_ksetControlPointCount_Ev
+ __ZN3MTL7Private8Selector24s_ksetDepthStencilState_Ev
+ __ZN3MTL7Private8Selector24s_ksetDepthWriteEnabled_Ev
+ __ZN3MTL7Private8Selector24s_ksetIndexBufferOffset_Ev
+ __ZN3MTL7Private8Selector24s_ksetMaxCallStackDepth_Ev
+ __ZN3MTL7Private8Selector24s_ksetOptimizationLevel_Ev
+ __ZN3MTL7Private8Selector24s_ksetOwnerWithIdentity_Ev
+ __ZN3MTL7Private8Selector24s_ksetRasterSampleCount_Ev
+ __ZN3MTL7Private8Selector24s_ksetRenderTargetWidth_Ev
+ __ZN3MTL7Private8Selector24s_ksetResolveDepthPlane_Ev
+ __ZN3MTL7Private8Selector24s_ksetRgbBlendOperation_Ev
+ __ZN3MTL7Private8Selector24s_ksetStencilAttachment_Ev
+ __ZN3MTL7Private8Selector24s_ksetSupportRayTracing_Ev
+ __ZN3MTL7Private8Selector24s_ksparseTileSizeInBytesEv
+ __ZN3MTL7Private8Selector24s_kvertexLinkedFunctionsEv
+ __ZN3MTL7Private8Selector24s_kverticalSampleStorageEv
+ __ZN3MTL7Private8Selector25s_kargumentBuffersSupportEv
+ __ZN3MTL7Private8Selector25s_kconstantBlockAlignmentEv
+ __ZN3MTL7Private8Selector25s_kendOfVertexSampleIndexEv
+ __ZN3MTL7Private8Selector25s_kimageblockSampleLengthEv
+ __ZN3MTL7Private8Selector25s_kinitWithArgumentIndex_Ev
+ __ZN3MTL7Private8Selector25s_kinitWithDispatchQueue_Ev
+ __ZN3MTL7Private8Selector25s_kinputPrimitiveTopologyEv
+ __ZN3MTL7Private8Selector25s_kinstanceDescriptorTypeEv
+ __ZN3MTL7Private8Selector25s_kisRasterizationEnabledEv
+ __ZN3MTL7Private8Selector25s_kmaxMeshBufferBindCountEv
+ __ZN3MTL7Private8Selector25s_knewHeapWithDescriptor_Ev
+ __ZN3MTL7Private8Selector25s_knewSharedTextureHandleEv
+ __ZN3MTL7Private8Selector25s_kobjectPayloadAlignmentEv
+ __ZN3MTL7Private8Selector25s_kpatchControlPointCountEv
+ __ZN3MTL7Private8Selector25s_kscratchBufferAllocatorEv
+ __ZN3MTL7Private8Selector25s_ksetBoundingBoxBuffers_Ev
+ __ZN3MTL7Private8Selector25s_ksetControlPointBuffer_Ev
+ __ZN3MTL7Private8Selector25s_ksetControlPointFormat_Ev
+ __ZN3MTL7Private8Selector25s_ksetControlPointStride_Ev
+ __ZN3MTL7Private8Selector25s_ksetDepthResolveFilter_Ev
+ __ZN3MTL7Private8Selector25s_ksetFrontFacingWinding_Ev
+ __ZN3MTL7Private8Selector25s_ksetHazardTrackingMode_Ev
+ __ZN3MTL7Private8Selector25s_ksetPreloadedLibraries_Ev
+ __ZN3MTL7Private8Selector25s_ksetPreprocessorMacros_Ev
+ __ZN3MTL7Private8Selector25s_ksetPreserveInvariance_Ev
+ __ZN3MTL7Private8Selector25s_ksetRadiusBufferOffset_Ev
+ __ZN3MTL7Private8Selector25s_ksetRenderTargetHeight_Ev
+ __ZN3MTL7Private8Selector25s_ksetRetainedReferences_Ev
+ __ZN3MTL7Private8Selector25s_ksetScissorRects_count_Ev
+ __ZN3MTL7Private8Selector25s_ksetStencilStoreAction_Ev
+ __ZN3MTL7Private8Selector25s_ksetStoreActionOptions_Ev
+ __ZN3MTL7Private8Selector25s_ksetTextures_withRange_Ev
+ __ZN3MTL7Private8Selector25s_ksetVertexBufferOffset_Ev
+ __ZN3MTL7Private8Selector25s_ksourceAlphaBlendFactorEv
+ __ZN3MTL7Private8Selector25s_kstartCaptureWithScope_Ev
+ __ZN3MTL7Private8Selector25s_kstencilCompareFunctionEv
+ __ZN3MTL7Private8Selector25s_ksupportArgumentBuffersEv
+ __ZN3MTL7Private8Selector25s_kuseHeaps_count_stages_Ev
+ __ZN3MTL7Private8Selector25s_kvisibilityResultBufferEv
+ __ZN3MTL7Private8Selector260s_kdrawIndexedPatches_patchStart_patchCount_patchIndexBuffer_patchIndexBufferOffset_controlPointIndexBuffer_controlPointIndexBufferOffset_instanceCount_baseInstance_tessellationFactorBuffer_tessellationFactorBufferOffset_tessellationFactorBufferInstanceStride_Ev
+ __ZN3MTL7Private8Selector26s_kaddResidencySets_count_Ev
+ __ZN3MTL7Private8Selector26s_kboundingBoxBufferOffsetEv
+ __ZN3MTL7Private8Selector26s_kcompileSymbolVisibilityEv
+ __ZN3MTL7Private8Selector26s_kdispatchThreadsPerTile_Ev
+ __ZN3MTL7Private8Selector26s_kelementIsArgumentBufferEv
+ __ZN3MTL7Private8Selector26s_kendOfEncoderSampleIndexEv
+ __ZN3MTL7Private8Selector26s_kfillBuffer_range_value_Ev
+ __ZN3MTL7Private8Selector26s_kfragmentLinkedFunctionsEv
+ __ZN3MTL7Private8Selector26s_khorizontalSampleStorageEv
+ __ZN3MTL7Private8Selector26s_kisPatchControlPointDataEv
+ __ZN3MTL7Private8Selector26s_kmaxMotionTransformCountEv
+ __ZN3MTL7Private8Selector26s_kmaxVertexCallStackDepthEv
+ __ZN3MTL7Private8Selector26s_kmemoryBarrierWithScope_Ev
+ __ZN3MTL7Private8Selector26s_kpresentDrawable_atTime_Ev
+ __ZN3MTL7Private8Selector26s_kreadWriteTextureSupportEv
+ __ZN3MTL7Private8Selector26s_krenderTargetArrayLengthEv
+ __ZN3MTL7Private8Selector26s_ksampleBufferAttachmentsEv
+ __ZN3MTL7Private8Selector26s_ksetAlphaBlendOperation_Ev
+ __ZN3MTL7Private8Selector26s_ksetControlDependencies_Ev
+ __ZN3MTL7Private8Selector26s_ksetControlPointBuffers_Ev
+ __ZN3MTL7Private8Selector26s_ksetDefaultCaptureScope_Ev
+ __ZN3MTL7Private8Selector26s_ksetFunctions_withRange_Ev
+ __ZN3MTL7Private8Selector26s_ksetGeometryDescriptors_Ev
+ __ZN3MTL7Private8Selector26s_ksetInstanceCountBuffer_Ev
+ __ZN3MTL7Private8Selector26s_ksetMaxCommandsInFlight_Ev
+ __ZN3MTL7Private8Selector26s_ksetMeshLinkedFunctions_Ev
+ __ZN3MTL7Private8Selector26s_ksetMeshTexture_atIndex_Ev
+ __ZN3MTL7Private8Selector26s_ksetMotionEndBorderMode_Ev
+ __ZN3MTL7Private8Selector26s_ksetMotionKeyframeCount_Ev
+ __ZN3MTL7Private8Selector26s_ksetMotionTransformType_Ev
+ __ZN3MTL7Private8Selector26s_ksetPayloadMemoryLength_Ev
+ __ZN3MTL7Private8Selector26s_ksetPrimitiveDataBuffer_Ev
+ __ZN3MTL7Private8Selector26s_ksetPrimitiveDataStride_Ev
+ __ZN3MTL7Private8Selector26s_ksetRenderPipelineState_Ev
+ __ZN3MTL7Private8Selector26s_ksetTileTexture_atIndex_Ev
+ __ZN3MTL7Private8Selector26s_kstartCaptureWithDevice_Ev
+ __ZN3MTL7Private8Selector26s_kstencilFailureOperationEv
+ __ZN3MTL7Private8Selector26s_ksupportsQueryTextureLODEv
+ __ZN3MTL7Private8Selector26s_kthreadgroupMemoryLengthEv
+ __ZN3MTL7Private8Selector26s_kuseResidencySets_count_Ev
+ __ZN3MTL7Private8Selector27s_kcontrolPointBufferOffsetEv
+ __ZN3MTL7Private8Selector27s_kdefaultRasterSampleCountEv
+ __ZN3MTL7Private8Selector27s_kencodeSignalEvent_value_Ev
+ __ZN3MTL7Private8Selector27s_kendOfFragmentSampleIndexEv
+ __ZN3MTL7Private8Selector27s_kinstanceDescriptorBufferEv
+ __ZN3MTL7Private8Selector27s_kinstanceDescriptorStrideEv
+ __ZN3MTL7Private8Selector27s_kisAlphaToCoverageEnabledEv
+ __ZN3MTL7Private8Selector27s_kmaxKernelBufferBindCountEv
+ __ZN3MTL7Private8Selector27s_kmaxObjectBufferBindCountEv
+ __ZN3MTL7Private8Selector27s_kmaxThreadsPerThreadgroupEv
+ __ZN3MTL7Private8Selector27s_kmaxVertexBufferBindCountEv
+ __ZN3MTL7Private8Selector27s_kmeshThreadExecutionWidthEv
+ __ZN3MTL7Private8Selector27s_knewDynamicLibrary_error_Ev
+ __ZN3MTL7Private8Selector27s_knewLibraryWithURL_error_Ev
+ __ZN3MTL7Private8Selector27s_kprimitiveDataElementSizeEv
+ __ZN3MTL7Private8Selector27s_kremoveAllocations_count_Ev
+ __ZN3MTL7Private8Selector27s_ksegmentControlPointCountEv
+ __ZN3MTL7Private8Selector27s_ksetBufferOffset_atIndex_Ev
+ __ZN3MTL7Private8Selector27s_ksetBytes_length_atIndex_Ev
+ __ZN3MTL7Private8Selector27s_ksetComputePipelineState_Ev
+ __ZN3MTL7Private8Selector27s_ksetDepthCompareFunction_Ev
+ __ZN3MTL7Private8Selector27s_ksetInheritPipelineState_Ev
+ __ZN3MTL7Private8Selector27s_ksetMotionTransformCount_Ev
+ __ZN3MTL7Private8Selector27s_ksetRasterizationEnabled_Ev
+ __ZN3MTL7Private8Selector27s_ksetRasterizationRateMap_Ev
+ __ZN3MTL7Private8Selector27s_ksetSamplerState_atIndex_Ev
+ __ZN3MTL7Private8Selector27s_ksetSourceRGBBlendFactor_Ev
+ __ZN3MTL7Private8Selector27s_ksetStageInputDescriptor_Ev
+ __ZN3MTL7Private8Selector27s_ksetStencilResolveFilter_Ev
+ __ZN3MTL7Private8Selector27s_kstartOfVertexSampleIndexEv
+ __ZN3MTL7Private8Selector27s_ksupportsCounterSampling_Ev
+ __ZN3MTL7Private8Selector27s_ksupportsDynamicLibrariesEv
+ __ZN3MTL7Private8Selector27s_ksupportsFunctionPointersEv
+ __ZN3MTL7Private8Selector27s_ktessellationFactorFormatEv
+ __ZN3MTL7Private8Selector27s_kupdateFence_afterStages_Ev
+ __ZN3MTL7Private8Selector27s_kvertexPreloadedLibrariesEv
+ __ZN3MTL7Private8Selector28s_kallowGPUOptimizedContentsEv
+ __ZN3MTL7Private8Selector28s_kdepthStencilPassOperationEv
+ __ZN3MTL7Private8Selector28s_kdestinationRGBBlendFactorEv
+ __ZN3MTL7Private8Selector28s_kencodeWaitForEvent_value_Ev
+ __ZN3MTL7Private8Selector28s_kgetSamplePositions_count_Ev
+ __ZN3MTL7Private8Selector28s_kinstanceCountBufferOffsetEv
+ __ZN3MTL7Private8Selector28s_kmaxFragmentCallStackDepthEv
+ __ZN3MTL7Private8Selector28s_knewIOHandleWithURL_error_Ev
+ __ZN3MTL7Private8Selector28s_knewLibraryWithData_error_Ev
+ __ZN3MTL7Private8Selector28s_knewLibraryWithFile_error_Ev
+ __ZN3MTL7Private8Selector28s_knewSharedEventWithHandle_Ev
+ __ZN3MTL7Private8Selector28s_knewTextureWithDescriptor_Ev
+ __ZN3MTL7Private8Selector28s_kobjectAtIndexedSubscript_Ev
+ __ZN3MTL7Private8Selector28s_kprimitiveDataBufferOffsetEv
+ __ZN3MTL7Private8Selector28s_ksetArgumentBuffer_offset_Ev
+ __ZN3MTL7Private8Selector28s_ksetBuffer_offset_atIndex_Ev
+ __ZN3MTL7Private8Selector28s_ksetDepthFailureOperation_Ev
+ __ZN3MTL7Private8Selector28s_ksetMaxCommandBufferCount_Ev
+ __ZN3MTL7Private8Selector28s_ksetMaxTessellationFactor_Ev
+ __ZN3MTL7Private8Selector28s_ksetMotionStartBorderMode_Ev
+ __ZN3MTL7Private8Selector28s_ksetMotionTransformBuffer_Ev
+ __ZN3MTL7Private8Selector28s_ksetMotionTransformStride_Ev
+ __ZN3MTL7Private8Selector28s_ksetNormalizedCoordinates_Ev
+ __ZN3MTL7Private8Selector28s_ksetObjectLinkedFunctions_Ev
+ __ZN3MTL7Private8Selector28s_ksetObjectTexture_atIndex_Ev
+ __ZN3MTL7Private8Selector28s_ksetSamplePositions_count_Ev
+ __ZN3MTL7Private8Selector28s_ksetStencilReferenceValue_Ev
+ __ZN3MTL7Private8Selector28s_ksetVertexLinkedFunctions_Ev
+ __ZN3MTL7Private8Selector28s_ksetVertexTexture_atIndex_Ev
+ __ZN3MTL7Private8Selector28s_kstartOfEncoderSampleIndexEv
+ __ZN3MTL7Private8Selector28s_ktessellationPartitionModeEv
+ __ZN3MTL7Private8Selector28s_kthreadgroupMemoryDataSizeEv
+ __ZN3MTL7Private8Selector28s_kuseResource_usage_stages_Ev
+ __ZN3MTL7Private8Selector28s_kuseResources_count_usage_Ev
+ __ZN3MTL7Private8Selector29s_kcopyFromTexture_toTexture_Ev
+ __ZN3MTL7Private8Selector29s_kcopyStatusToBuffer_offset_Ev
+ __ZN3MTL7Private8Selector29s_kdepthAttachmentPixelFormatEv
+ __ZN3MTL7Private8Selector29s_kfragmentPreloadedLibrariesEv
+ __ZN3MTL7Private8Selector29s_kgenerateMipmapsForTexture_Ev
+ __ZN3MTL7Private8Selector29s_kinsertDebugCaptureBoundaryEv
+ __ZN3MTL7Private8Selector29s_kmathFloatingPointFunctionsEv
+ __ZN3MTL7Private8Selector29s_kmaxFragmentBufferBindCountEv
+ __ZN3MTL7Private8Selector29s_kmaxThreadgroupMemoryLengthEv
+ __ZN3MTL7Private8Selector29s_kmotionTransformCountBufferEv
+ __ZN3MTL7Private8Selector29s_knewCaptureScopeWithDevice_Ev
+ __ZN3MTL7Private8Selector29s_kobjectThreadExecutionWidthEv
+ __ZN3MTL7Private8Selector29s_kremoveResidencySets_count_Ev
+ __ZN3MTL7Private8Selector29s_ksetAlphaToCoverageEnabled_Ev
+ __ZN3MTL7Private8Selector29s_ksetConstantBlockAlignment_Ev
+ __ZN3MTL7Private8Selector29s_ksetEndOfVertexSampleIndex_Ev
+ __ZN3MTL7Private8Selector29s_ksetImageblockSampleLength_Ev
+ __ZN3MTL7Private8Selector29s_ksetImageblockWidth_height_Ev
+ __ZN3MTL7Private8Selector29s_ksetInputPrimitiveTopology_Ev
+ __ZN3MTL7Private8Selector29s_ksetInstanceDescriptorType_Ev
+ __ZN3MTL7Private8Selector29s_ksetMaxMeshBufferBindCount_Ev
+ __ZN3MTL7Private8Selector29s_ksetMeshTextures_withRange_Ev
+ __ZN3MTL7Private8Selector29s_ksetScratchBufferAllocator_Ev
+ __ZN3MTL7Private8Selector29s_ksetSourceAlphaBlendFactor_Ev
+ __ZN3MTL7Private8Selector29s_ksetStencilCompareFunction_Ev
+ __ZN3MTL7Private8Selector29s_ksetSupportArgumentBuffers_Ev
+ __ZN3MTL7Private8Selector29s_ksetTileTextures_withRange_Ev
+ __ZN3MTL7Private8Selector29s_ksetVisibilityResultBuffer_Ev
+ __ZN3MTL7Private8Selector29s_kstageInputOutputDescriptorEv
+ __ZN3MTL7Private8Selector29s_kstartOfFragmentSampleIndexEv
+ __ZN3MTL7Private8Selector29s_kthreadgroupMemoryAlignmentEv
+ __ZN3MTL7Private8Selector29s_ktransformationMatrixBufferEv
+ __ZN3MTL7Private8Selector29s_ktransformationMatrixLayoutEv
+ __ZN3MTL7Private8Selector29s_kwaitForFence_beforeStages_Ev
+ __ZN3MTL7Private8Selector30s_kdestinationAlphaBlendFactorEv
+ __ZN3MTL7Private8Selector30s_kelementTextureReferenceTypeEv
+ __ZN3MTL7Private8Selector30s_kfunctionConstantsDictionaryEv
+ __ZN3MTL7Private8Selector30s_kfunctionHandleWithFunction_Ev
+ __ZN3MTL7Private8Selector30s_kmaxVertexAmplificationCountEv
+ __ZN3MTL7Private8Selector30s_kmotionTransformBufferOffsetEv
+ __ZN3MTL7Private8Selector30s_knewSharedTextureWithHandle_Ev
+ __ZN3MTL7Private8Selector30s_kparameterBufferSizeAndAlignEv
+ __ZN3MTL7Private8Selector30s_kresourceStateCommandEncoderEv
+ __ZN3MTL7Private8Selector30s_kresourceStatePassDescriptorEv
+ __ZN3MTL7Private8Selector30s_ksetBoundingBoxBufferOffset_Ev
+ __ZN3MTL7Private8Selector30s_ksetCompileSymbolVisibility_Ev
+ __ZN3MTL7Private8Selector30s_ksetDepthStoreActionOptions_Ev
+ __ZN3MTL7Private8Selector30s_ksetEndOfEncoderSampleIndex_Ev
+ __ZN3MTL7Private8Selector30s_ksetFragmentLinkedFunctions_Ev
+ __ZN3MTL7Private8Selector30s_ksetFragmentTexture_atIndex_Ev
+ __ZN3MTL7Private8Selector30s_ksetMaxMotionTransformCount_Ev
+ __ZN3MTL7Private8Selector30s_ksetMaxVertexCallStackDepth_Ev
+ __ZN3MTL7Private8Selector30s_ksetRenderTargetArrayLength_Ev
+ __ZN3MTL7Private8Selector30s_ksetSamplerStates_withRange_Ev
+ __ZN3MTL7Private8Selector30s_ksetStencilFailureOperation_Ev
+ __ZN3MTL7Private8Selector30s_ksetTessellationFactorScale_Ev
+ __ZN3MTL7Private8Selector30s_ksetThreadgroupMemoryLength_Ev
+ __ZN3MTL7Private8Selector30s_ksupports32BitFloatFilteringEv
+ __ZN3MTL7Private8Selector30s_ksupportsPrimitiveMotionBlurEv
+ __ZN3MTL7Private8Selector30s_ksupportsTextureSampleCount_Ev
+ __ZN3MTL7Private8Selector31s_kcommandBufferWithDescriptor_Ev
+ __ZN3MTL7Private8Selector31s_knewBufferWithLength_options_Ev
+ __ZN3MTL7Private8Selector31s_kpresentAfterMinimumDuration_Ev
+ __ZN3MTL7Private8Selector31s_krecommendedMaxWorkingSetSizeEv
+ __ZN3MTL7Private8Selector31s_ksetColorStoreAction_atIndex_Ev
+ __ZN3MTL7Private8Selector31s_ksetControlPointBufferOffset_Ev
+ __ZN3MTL7Private8Selector31s_ksetDefaultRasterSampleCount_Ev
+ __ZN3MTL7Private8Selector31s_ksetEndOfFragmentSampleIndex_Ev
+ __ZN3MTL7Private8Selector31s_ksetInstanceDescriptorBuffer_Ev
+ __ZN3MTL7Private8Selector31s_ksetInstanceDescriptorStride_Ev
+ __ZN3MTL7Private8Selector31s_ksetMaxKernelBufferBindCount_Ev
+ __ZN3MTL7Private8Selector31s_ksetMaxObjectBufferBindCount_Ev
+ __ZN3MTL7Private8Selector31s_ksetMaxVertexBufferBindCount_Ev
+ __ZN3MTL7Private8Selector31s_ksetMeshBufferOffset_atIndex_Ev
+ __ZN3MTL7Private8Selector31s_ksetMeshBytes_length_atIndex_Ev
+ __ZN3MTL7Private8Selector31s_ksetMeshSamplerState_atIndex_Ev
+ __ZN3MTL7Private8Selector31s_ksetObjectTextures_withRange_Ev
+ __ZN3MTL7Private8Selector31s_ksetPrimitiveDataElementSize_Ev
+ __ZN3MTL7Private8Selector31s_ksetSegmentControlPointCount_Ev
+ __ZN3MTL7Private8Selector31s_ksetStartOfVertexSampleIndex_Ev
+ __ZN3MTL7Private8Selector31s_ksetTessellationFactorFormat_Ev
+ __ZN3MTL7Private8Selector31s_ksetTileBufferOffset_atIndex_Ev
+ __ZN3MTL7Private8Selector31s_ksetTileBytes_length_atIndex_Ev
+ __ZN3MTL7Private8Selector31s_ksetTileSamplerState_atIndex_Ev
+ __ZN3MTL7Private8Selector31s_ksetVertexPreloadedLibraries_Ev
+ __ZN3MTL7Private8Selector31s_ksetVertexTextures_withRange_Ev
+ __ZN3MTL7Private8Selector31s_kstencilAttachmentPixelFormatEv
+ __ZN3MTL7Private8Selector31s_ksupportAddingBinaryFunctionsEv
+ __ZN3MTL7Private8Selector31s_ksupportsBCTextureCompressionEv
+ __ZN3MTL7Private8Selector31s_ksupportsRaytracingFromRenderEv
+ __ZN3MTL7Private8Selector32s_kareBarycentricCoordsSupportedEv
+ __ZN3MTL7Private8Selector32s_kareRasterOrderGroupsSupportedEv
+ __ZN3MTL7Private8Selector32s_kindirectRenderCommandAtIndex_Ev
+ __ZN3MTL7Private8Selector32s_kmaxArgumentBufferSamplerCountEv
+ __ZN3MTL7Private8Selector32s_kmaxTotalThreadsPerThreadgroupEv
+ __ZN3MTL7Private8Selector32s_knewIOFileHandleWithURL_error_Ev
+ __ZN3MTL7Private8Selector32s_knewRemoteBufferViewForDevice_Ev
+ __ZN3MTL7Private8Selector32s_knotifyListener_atValue_block_Ev
+ __ZN3MTL7Private8Selector32s_koptimizeContentsForCPUAccess_Ev
+ __ZN3MTL7Private8Selector32s_koptimizeContentsForGPUAccess_Ev
+ __ZN3MTL7Private8Selector32s_ksetAllowGPUOptimizedContents_Ev
+ __ZN3MTL7Private8Selector32s_ksetBuffers_offsets_withRange_Ev
+ __ZN3MTL7Private8Selector32s_ksetDepthStencilPassOperation_Ev
+ __ZN3MTL7Private8Selector32s_ksetDestinationRGBBlendFactor_Ev
+ __ZN3MTL7Private8Selector32s_ksetInstanceCountBufferOffset_Ev
+ __ZN3MTL7Private8Selector32s_ksetMaxFragmentCallStackDepth_Ev
+ __ZN3MTL7Private8Selector32s_ksetMeshBuffer_offset_atIndex_Ev
+ __ZN3MTL7Private8Selector32s_ksetObject_atIndexedSubscript_Ev
+ __ZN3MTL7Private8Selector32s_ksetPrimitiveDataBufferOffset_Ev
+ __ZN3MTL7Private8Selector32s_ksetStartOfEncoderSampleIndex_Ev
+ __ZN3MTL7Private8Selector32s_ksetStencilStoreActionOptions_Ev
+ __ZN3MTL7Private8Selector32s_ksetTessellationPartitionMode_Ev
+ __ZN3MTL7Private8Selector32s_ksetTileBuffer_offset_atIndex_Ev
+ __ZN3MTL7Private8Selector32s_kstartCaptureWithCommandQueue_Ev
+ __ZN3MTL7Private8Selector32s_kstaticThreadgroupMemoryLengthEv
+ __ZN3MTL7Private8Selector32s_ksupportDynamicAttributeStrideEv
+ __ZN3MTL7Private8Selector32s_ksupportIndirectCommandBuffersEv
+ __ZN3MTL7Private8Selector32s_ktileAdditionalBinaryFunctionsEv
+ __ZN3MTL7Private8Selector33s_kindirectComputeCommandAtIndex_Ev
+ __ZN3MTL7Private8Selector33s_kinstanceDescriptorBufferOffsetEv
+ __ZN3MTL7Private8Selector33s_kmaxAvailableSizeWithAlignment_Ev
+ __ZN3MTL7Private8Selector33s_knewCommandQueueWithDescriptor_Ev
+ __ZN3MTL7Private8Selector33s_knewRemoteTextureViewForDevice_Ev
+ __ZN3MTL7Private8Selector33s_knewSamplerStateWithDescriptor_Ev
+ __ZN3MTL7Private8Selector33s_knewTextureViewWithPixelFormat_Ev
+ __ZN3MTL7Private8Selector33s_ksampleTimestamps_gpuTimestamp_Ev
+ __ZN3MTL7Private8Selector33s_ksetConstantValue_type_atIndex_Ev
+ __ZN3MTL7Private8Selector33s_ksetDepthAttachmentPixelFormat_Ev
+ __ZN3MTL7Private8Selector33s_ksetDepthBias_slopeScale_clamp_Ev
+ __ZN3MTL7Private8Selector33s_ksetFragmentPreloadedLibraries_Ev
+ __ZN3MTL7Private8Selector33s_ksetFragmentTextures_withRange_Ev
+ __ZN3MTL7Private8Selector33s_ksetMathFloatingPointFunctions_Ev
+ __ZN3MTL7Private8Selector33s_ksetMaxFragmentBufferBindCount_Ev
+ __ZN3MTL7Private8Selector33s_ksetMotionTransformCountBuffer_Ev
+ __ZN3MTL7Private8Selector33s_ksetObjectBufferOffset_atIndex_Ev
+ __ZN3MTL7Private8Selector33s_ksetObjectBytes_length_atIndex_Ev
+ __ZN3MTL7Private8Selector33s_ksetObjectSamplerState_atIndex_Ev
+ __ZN3MTL7Private8Selector33s_ksetStartOfFragmentSampleIndex_Ev
+ __ZN3MTL7Private8Selector33s_ksetTransformationMatrixBuffer_Ev
+ __ZN3MTL7Private8Selector33s_ksetTransformationMatrixLayout_Ev
+ __ZN3MTL7Private8Selector33s_ksetVertexBufferOffset_atIndex_Ev
+ __ZN3MTL7Private8Selector33s_ksetVertexBytes_length_atIndex_Ev
+ __ZN3MTL7Private8Selector33s_ksetVertexSamplerState_atIndex_Ev
+ __ZN3MTL7Private8Selector33s_ksupportsPullModelInterpolationEv
+ __ZN3MTL7Private8Selector33s_ksupportsRenderDynamicLibrariesEv
+ __ZN3MTL7Private8Selector33s_ktessellationFactorStepFunctionEv
+ __ZN3MTL7Private8Selector33s_ktessellationOutputWindingOrderEv
+ __ZN3MTL7Private8Selector33s_kthreadgroupSizeMatchesTileSizeEv
+ __ZN3MTL7Private8Selector33s_kvisibleFunctionTableDescriptorEv
+ __ZN3MTL7Private8Selector34s_kaddLibraryWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector34s_kinstancedAccelerationStructuresEv
+ __ZN3MTL7Private8Selector34s_kintersectionFunctionTableOffsetEv
+ __ZN3MTL7Private8Selector34s_kmaxTotalThreadgroupsPerMeshGridEv
+ __ZN3MTL7Private8Selector34s_knewDynamicLibraryWithURL_error_Ev
+ __ZN3MTL7Private8Selector34s_knewSharedTextureWithDescriptor_Ev
+ __ZN3MTL7Private8Selector34s_ksetConstantValue_type_withName_Ev
+ __ZN3MTL7Private8Selector34s_ksetDestinationAlphaBlendFactor_Ev
+ __ZN3MTL7Private8Selector34s_ksetKernelBuffer_offset_atIndex_Ev
+ __ZN3MTL7Private8Selector34s_ksetMaxVertexAmplificationCount_Ev
+ __ZN3MTL7Private8Selector34s_ksetMeshSamplerStates_withRange_Ev
+ __ZN3MTL7Private8Selector34s_ksetMotionTransformBufferOffset_Ev
+ __ZN3MTL7Private8Selector34s_ksetObjectBuffer_offset_atIndex_Ev
+ __ZN3MTL7Private8Selector34s_ksetRenderPipelineState_atIndex_Ev
+ __ZN3MTL7Private8Selector34s_ksetTileSamplerStates_withRange_Ev
+ __ZN3MTL7Private8Selector34s_ksetVertexBuffer_offset_atIndex_Ev
+ __ZN3MTL7Private8Selector34s_ksetVisibilityResultMode_offset_Ev
+ __ZN3MTL7Private8Selector34s_ksynchronizeTexture_slice_level_Ev
+ __ZN3MTL7Private8Selector34s_kvertexAdditionalBinaryFunctionsEv
+ __ZN3MTL7Private8Selector35s_kallowReferencingUndefinedSymbolsEv
+ __ZN3MTL7Private8Selector35s_kgetDefaultSamplePositions_count_Ev
+ __ZN3MTL7Private8Selector35s_kisTessellationFactorScaleEnabledEv
+ __ZN3MTL7Private8Selector35s_kmotionTransformCountBufferOffsetEv
+ __ZN3MTL7Private8Selector35s_knewArgumentEncoderWithArguments_Ev
+ __ZN3MTL7Private8Selector35s_knewCaptureScopeWithCommandQueue_Ev
+ __ZN3MTL7Private8Selector35s_knewFunctionWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector35s_knewLogStateWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector35s_knewScratchBufferWithMinimumSize_Ev
+ __ZN3MTL7Private8Selector35s_knewTextureWithDescriptor_offset_Ev
+ __ZN3MTL7Private8Selector35s_kresetCommandsInBuffer_withRange_Ev
+ __ZN3MTL7Private8Selector35s_ksetComputePipelineState_atIndex_Ev
+ __ZN3MTL7Private8Selector35s_ksetFragmentBufferOffset_atIndex_Ev
+ __ZN3MTL7Private8Selector35s_ksetFragmentBytes_length_atIndex_Ev
+ __ZN3MTL7Private8Selector35s_ksetFragmentSamplerState_atIndex_Ev
+ __ZN3MTL7Private8Selector35s_ksetStencilAttachmentPixelFormat_Ev
+ __ZN3MTL7Private8Selector35s_ksetSupportAddingBinaryFunctions_Ev
+ __ZN3MTL7Private8Selector35s_ksetVisibleFunctionTable_atIndex_Ev
+ __ZN3MTL7Private8Selector35s_ktransformationMatrixBufferOffsetEv
+ __ZN3MTL7Private8Selector35s_kuseResources_count_usage_stages_Ev
+ __ZN3MTL7Private8Selector36s_kblitCommandEncoderWithDescriptor_Ev
+ __ZN3MTL7Private8Selector36s_kcopyParameterDataToBuffer_offset_Ev
+ __ZN3MTL7Private8Selector36s_kfragmentAdditionalBinaryFunctionsEv
+ __ZN3MTL7Private8Selector36s_kfunctionHandleWithFunction_stage_Ev
+ __ZN3MTL7Private8Selector36s_kmaxTotalThreadsPerMeshThreadgroupEv
+ __ZN3MTL7Private8Selector36s_kmemoryBarrierWithResources_count_Ev
+ __ZN3MTL7Private8Selector36s_knewAccelerationStructureWithSize_Ev
+ __ZN3MTL7Private8Selector36s_ksetAccelerationStructure_atIndex_Ev
+ __ZN3MTL7Private8Selector36s_ksetConstantValues_type_withRange_Ev
+ __ZN3MTL7Private8Selector36s_ksetFragmentBuffer_offset_atIndex_Ev
+ __ZN3MTL7Private8Selector36s_ksetIndirectCommandBuffer_atIndex_Ev
+ __ZN3MTL7Private8Selector36s_ksetMaxTotalThreadsPerThreadgroup_Ev
+ __ZN3MTL7Private8Selector36s_ksetMeshBuffers_offsets_withRange_Ev
+ __ZN3MTL7Private8Selector36s_ksetObjectSamplerStates_withRange_Ev
+ __ZN3MTL7Private8Selector36s_ksetSupportDynamicAttributeStride_Ev
+ __ZN3MTL7Private8Selector36s_ksetSupportIndirectCommandBuffers_Ev
+ __ZN3MTL7Private8Selector36s_ksetTileAdditionalBinaryFunctions_Ev
+ __ZN3MTL7Private8Selector36s_ksetTileBuffers_offsets_withRange_Ev
+ __ZN3MTL7Private8Selector36s_ksetVertexSamplerStates_withRange_Ev
+ __ZN3MTL7Private8Selector36s_kstartCaptureWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector36s_ksupportsVertexAmplificationCount_Ev
+ __ZN3MTL7Private8Selector36s_ktessellationControlPointIndexTypeEv
+ __ZN3MTL7Private8Selector36s_kwaitUntilSignaledValue_timeoutMS_Ev
+ __ZN3MTL7Private8Selector37s_kexecuteCommandsInBuffer_withRange_Ev
+ __ZN3MTL7Private8Selector37s_kinstanceTransformationMatrixLayoutEv
+ __ZN3MTL7Private8Selector37s_knewArgumentEncoderWithBufferIndex_Ev
+ __ZN3MTL7Private8Selector37s_knewBufferWithBytes_length_options_Ev
+ __ZN3MTL7Private8Selector37s_knewDefaultLibraryWithBundle_error_Ev
+ __ZN3MTL7Private8Selector37s_ksetBlendColorRed_green_blue_alpha_Ev
+ __ZN3MTL7Private8Selector37s_ksetInstanceDescriptorBufferOffset_Ev
+ __ZN3MTL7Private8Selector37s_ksetRenderPipelineStates_withRange_Ev
+ __ZN3MTL7Private8Selector37s_ksetTessellationFactorScaleEnabled_Ev
+ __ZN3MTL7Private8Selector37s_ksetTessellationFactorStepFunction_Ev
+ __ZN3MTL7Private8Selector37s_ksetTessellationOutputWindingOrder_Ev
+ __ZN3MTL7Private8Selector37s_ksetThreadgroupSizeMatchesTileSize_Ev
+ __ZN3MTL7Private8Selector37s_ksupportAddingVertexBinaryFunctionsEv
+ __ZN3MTL7Private8Selector37s_ksupportsFunctionPointersFromRenderEv
+ __ZN3MTL7Private8Selector38s_kaccelerationStructureCommandEncoderEv
+ __ZN3MTL7Private8Selector38s_kaccelerationStructurePassDescriptorEv
+ __ZN3MTL7Private8Selector38s_kintersectionFunctionTableDescriptorEv
+ __ZN3MTL7Private8Selector38s_kmaxKernelThreadgroupMemoryBindCountEv
+ __ZN3MTL7Private8Selector38s_kmaxObjectThreadgroupMemoryBindCountEv
+ __ZN3MTL7Private8Selector38s_kmaxTotalThreadsPerObjectThreadgroupEv
+ __ZN3MTL7Private8Selector38s_knewArgumentEncoderForBufferAtIndex_Ev
+ __ZN3MTL7Private8Selector38s_knewBufferWithLength_options_offset_Ev
+ __ZN3MTL7Private8Selector38s_knewDepthStencilStateWithDescriptor_Ev
+ __ZN3MTL7Private8Selector38s_knewLibraryWithSource_options_error_Ev
+ __ZN3MTL7Private8Selector38s_krenderCommandEncoderWithDescriptor_Ev
+ __ZN3MTL7Private8Selector38s_ksetColorStoreActionOptions_atIndex_Ev
+ __ZN3MTL7Private8Selector38s_ksetComputePipelineStates_withRange_Ev
+ __ZN3MTL7Private8Selector38s_ksetFragmentSamplerStates_withRange_Ev
+ __ZN3MTL7Private8Selector38s_ksetInstancedAccelerationStructures_Ev
+ __ZN3MTL7Private8Selector38s_ksetIntersectionFunctionTableOffset_Ev
+ __ZN3MTL7Private8Selector38s_ksetMaxTotalThreadgroupsPerMeshGrid_Ev
+ __ZN3MTL7Private8Selector38s_ksetObjectBuffers_offsets_withRange_Ev
+ __ZN3MTL7Private8Selector38s_ksetThreadgroupMemoryLength_atIndex_Ev
+ __ZN3MTL7Private8Selector38s_ksetVertexAdditionalBinaryFunctions_Ev
+ __ZN3MTL7Private8Selector38s_ksetVertexBuffers_offsets_withRange_Ev
+ __ZN3MTL7Private8Selector38s_ksetVisibleFunctionTables_withRange_Ev
+ __ZN3MTL7Private8Selector38s_kshouldMaximizeConcurrentCompilationEv
+ __ZN3MTL7Private8Selector39s_kcomputeCommandEncoderWithDescriptor_Ev
+ __ZN3MTL7Private8Selector39s_kimageblockMemoryLengthForDimensions_Ev
+ __ZN3MTL7Private8Selector39s_knewArgumentEncoderWithBufferBinding_Ev
+ __ZN3MTL7Private8Selector39s_knewResidencySetWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector39s_ksetAllowReferencingUndefinedSymbols_Ev
+ __ZN3MTL7Private8Selector39s_ksetIndirectCommandBuffers_withRange_Ev
+ __ZN3MTL7Private8Selector39s_ksetMotionTransformCountBufferOffset_Ev
+ __ZN3MTL7Private8Selector39s_ksetTransformationMatrixBufferOffset_Ev
+ __ZN3MTL7Private8Selector39s_ksupportAddingFragmentBinaryFunctionsEv
+ __ZN3MTL7Private8Selector39s_ksupportsShaderBarycentricCoordinatesEv
+ __ZN3MTL7Private8Selector40s_kcommandBufferWithUnretainedReferencesEv
+ __ZN3MTL7Private8Selector40s_kisDepth24Stencil8PixelFormatSupportedEv
+ __ZN3MTL7Private8Selector40s_kmaximumConcurrentCompilationTaskCountEv
+ __ZN3MTL7Private8Selector40s_knewBinaryArchiveWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector40s_kpresentDrawable_afterMinimumDuration_Ev
+ __ZN3MTL7Private8Selector40s_ksetFragmentAdditionalBinaryFunctions_Ev
+ __ZN3MTL7Private8Selector40s_ksetFragmentBuffers_offsets_withRange_Ev
+ __ZN3MTL7Private8Selector40s_ksetIntersectionFunctionTable_atIndex_Ev
+ __ZN3MTL7Private8Selector40s_ksetMaxTotalThreadsPerMeshThreadgroup_Ev
+ __ZN3MTL7Private8Selector40s_ksetTessellationControlPointIndexType_Ev
+ __ZN3MTL7Private8Selector41s_kcomputeCommandEncoderWithDispatchType_Ev
+ __ZN3MTL7Private8Selector41s_kdispatchThreads_threadsPerThreadgroup_Ev
+ __ZN3MTL7Private8Selector41s_kheapTextureSizeAndAlignWithDescriptor_Ev
+ __ZN3MTL7Private8Selector41s_knewIOCommandQueueWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector41s_knewRasterizationRateMapWithDescriptor_Ev
+ __ZN3MTL7Private8Selector41s_knewVisibleFunctionTableWithDescriptor_Ev
+ __ZN3MTL7Private8Selector41s_ksetInstanceTransformationMatrixLayout_Ev
+ __ZN3MTL7Private8Selector41s_ksetSupportAddingVertexBinaryFunctions_Ev
+ __ZN3MTL7Private8Selector41s_ksetVisibleFunctionTable_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector42s_kareProgrammableSamplePositionsSupportedEv
+ __ZN3MTL7Private8Selector42s_kdrawPrimitives_vertexStart_vertexCount_Ev
+ __ZN3MTL7Private8Selector42s_knewAccelerationStructureWithDescriptor_Ev
+ __ZN3MTL7Private8Selector42s_knewLibraryWithStitchedDescriptor_error_Ev
+ __ZN3MTL7Private8Selector42s_ksetAccelerationStructure_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector42s_ksetMaxKernelThreadgroupMemoryBindCount_Ev
+ __ZN3MTL7Private8Selector42s_ksetMaxObjectThreadgroupMemoryBindCount_Ev
+ __ZN3MTL7Private8Selector42s_ksetMaxTotalThreadsPerObjectThreadgroup_Ev
+ __ZN3MTL7Private8Selector42s_ksetShouldMaximizeConcurrentCompilation_Ev
+ __ZN3MTL7Private8Selector42s_ksparseTileSizeInBytesForSparsePageSize_Ev
+ __ZN3MTL7Private8Selector43s_kaddFunctionWithDescriptor_library_error_Ev
+ __ZN3MTL7Private8Selector43s_kinitWithSampleCount_horizontal_vertical_Ev
+ __ZN3MTL7Private8Selector43s_kmapPhysicalToScreenCoordinates_forLayer_Ev
+ __ZN3MTL7Private8Selector43s_kmapScreenToPhysicalCoordinates_forLayer_Ev
+ __ZN3MTL7Private8Selector43s_knewAccelerationStructureWithSize_offset_Ev
+ __ZN3MTL7Private8Selector43s_koptimizeIndirectCommandBuffer_withRange_Ev
+ __ZN3MTL7Private8Selector43s_ksetBufferOffset_attributeStride_atIndex_Ev
+ __ZN3MTL7Private8Selector43s_ksetBytes_length_attributeStride_atIndex_Ev
+ __ZN3MTL7Private8Selector43s_ksetIntersectionFunctionTables_withRange_Ev
+ __ZN3MTL7Private8Selector43s_ksetSupportAddingFragmentBinaryFunctions_Ev
+ __ZN3MTL7Private8Selector44s_kaccelerationStructureSizesWithDescriptor_Ev
+ __ZN3MTL7Private8Selector44s_kheapBufferSizeAndAlignWithLength_options_Ev
+ __ZN3MTL7Private8Selector44s_knewCommandQueueWithMaxCommandBufferCount_Ev
+ __ZN3MTL7Private8Selector44s_knewFunctionWithName_constantValues_error_Ev
+ __ZN3MTL7Private8Selector44s_knewTextureWithDescriptor_iosurface_plane_Ev
+ __ZN3MTL7Private8Selector44s_koptimizeContentsForCPUAccess_slice_level_Ev
+ __ZN3MTL7Private8Selector44s_koptimizeContentsForGPUAccess_slice_level_Ev
+ __ZN3MTL7Private8Selector44s_ksetBuffer_offset_attributeStride_atIndex_Ev
+ __ZN3MTL7Private8Selector44s_ksetObjectThreadgroupMemoryLength_atIndex_Ev
+ __ZN3MTL7Private8Selector44s_ksetVertexAmplificationCount_viewMappings_Ev
+ __ZN3MTL7Private8Selector44s_ksetVisibleFunctionTables_withBufferRange_Ev
+ __ZN3MTL7Private8Selector45s_knewComputePipelineStateWithFunction_error_Ev
+ __ZN3MTL7Private8Selector45s_kresourceStateCommandEncoderWithDescriptor_Ev
+ __ZN3MTL7Private8Selector45s_ksetThreadgroupMemoryLength_offset_atIndex_Ev
+ __ZN3MTL7Private8Selector45s_ksetTileVisibleFunctionTable_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector46s_kdispatchThreadgroups_threadsPerThreadgroup_Ev
+ __ZN3MTL7Private8Selector46s_kinitWithName_arguments_controlDependencies_Ev
+ __ZN3MTL7Private8Selector46s_knewCounterSampleBufferWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector46s_knewIOHandleWithURL_compressionMethod_error_Ev
+ __ZN3MTL7Private8Selector46s_knewIntersectionFunctionTableWithDescriptor_Ev
+ __ZN3MTL7Private8Selector46s_knewRenderPipelineStateWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector46s_kparallelRenderCommandEncoderWithDescriptor_Ev
+ __ZN3MTL7Private8Selector46s_ksetArgumentBuffer_startOffset_arrayElement_Ev
+ __ZN3MTL7Private8Selector46s_ksetIntersectionFunctionTable_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector46s_ksetTileAccelerationStructure_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector46s_ksupportsRasterizationRateMapWithLayerCount_Ev
+ __ZN3MTL7Private8Selector47s_kallowDuplicateIntersectionFunctionInvocationEv
+ __ZN3MTL7Private8Selector47s_kgetBytes_bytesPerRow_fromRegion_mipmapLevel_Ev
+ __ZN3MTL7Private8Selector47s_kminimumLinearTextureAlignmentForPixelFormat_Ev
+ __ZN3MTL7Private8Selector47s_kminimumTextureBufferAlignmentForPixelFormat_Ev
+ __ZN3MTL7Private8Selector47s_knewFunctionWithDescriptor_completionHandler_Ev
+ __ZN3MTL7Private8Selector47s_knewIntersectionFunctionWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector47s_knewTextureWithDescriptor_offset_bytesPerRow_Ev
+ __ZN3MTL7Private8Selector47s_knewVisibleFunctionTableWithDescriptor_stage_Ev
+ __ZN3MTL7Private8Selector47s_ksetVertexVisibleFunctionTable_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector48s_knewArgumentEncoderWithBufferIndex_reflection_Ev
+ __ZN3MTL7Private8Selector48s_krasterizationRateMapDescriptorWithScreenSize_Ev
+ __ZN3MTL7Private8Selector48s_ksetTileVisibleFunctionTables_withBufferRange_Ev
+ __ZN3MTL7Private8Selector48s_ksetVertexAccelerationStructure_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector49s_kheapAccelerationStructureSizeAndAlignWithSize_Ev
+ __ZN3MTL7Private8Selector49s_knewAccelerationStructureWithDescriptor_offset_Ev
+ __ZN3MTL7Private8Selector49s_ksetBuffers_offsets_attributeStrides_withRange_Ev
+ __ZN3MTL7Private8Selector49s_ksetFragmentVisibleFunctionTable_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector49s_ksetIntersectionFunctionTables_withBufferRange_Ev
+ __ZN3MTL7Private8Selector49s_ksetVertexBufferOffset_attributeStride_atIndex_Ev
+ __ZN3MTL7Private8Selector49s_ksetVertexBytes_length_attributeStride_atIndex_Ev
+ __ZN3MTL7Private8Selector50s_kaddRenderPipelineFunctionsWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector50s_kloadBytes_size_sourceHandle_sourceHandleOffset_Ev
+ __ZN3MTL7Private8Selector50s_knewIOFileHandleWithURL_compressionMethod_error_Ev
+ __ZN3MTL7Private8Selector50s_knewLibraryWithSource_options_completionHandler_Ev
+ __ZN3MTL7Private8Selector50s_ksetFragmentAccelerationStructure_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector50s_ksetKernelBuffer_offset_attributeStride_atIndex_Ev
+ __ZN3MTL7Private8Selector50s_ksetTileIntersectionFunctionTable_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector50s_ksetVertexBuffer_offset_attributeStride_atIndex_Ev
+ __ZN3MTL7Private8Selector50s_ksetVertexVisibleFunctionTables_withBufferRange_Ev
+ __ZN3MTL7Private8Selector50s_kthreadGroupSizeIsMultipleOfThreadExecutionWidthEv
+ __ZN3MTL7Private8Selector51s_kaddComputePipelineFunctionsWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector51s_kconcurrentDispatchThreads_threadsPerThreadgroup_Ev
+ __ZN3MTL7Private8Selector51s_kmemoryBarrierWithScope_afterStages_beforeStages_Ev
+ __ZN3MTL7Private8Selector51s_kreplaceRegion_mipmapLevel_withBytes_bytesPerRow_Ev
+ __ZN3MTL7Private8Selector51s_ksetAllowDuplicateIntersectionFunctionInvocation_Ev
+ __ZN3MTL7Private8Selector51s_ksetSamplerState_lodMinClamp_lodMaxClamp_atIndex_Ev
+ __ZN3MTL7Private8Selector51s_kupdateTextureMapping_mode_region_mipLevel_slice_Ev
+ __ZN3MTL7Private8Selector52s_kinitWithFunctionName_nodes_outputNode_attributes_Ev
+ __ZN3MTL7Private8Selector52s_knewIntersectionFunctionTableWithDescriptor_stage_Ev
+ __ZN3MTL7Private8Selector52s_kresetTextureAccessCounters_region_mipLevel_slice_Ev
+ __ZN3MTL7Private8Selector52s_ksampleCountersInBuffer_atSampleIndex_withBarrier_Ev
+ __ZN3MTL7Private8Selector52s_ksetFragmentVisibleFunctionTables_withBufferRange_Ev
+ __ZN3MTL7Private8Selector52s_ksetStencilFrontReferenceValue_backReferenceValue_Ev
+ __ZN3MTL7Private8Selector52s_ksetVertexIntersectionFunctionTable_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector53s_kaccelerationStructureCommandEncoderWithDescriptor_Ev
+ __ZN3MTL7Private8Selector53s_kcopyAccelerationStructure_toAccelerationStructure_Ev
+ __ZN3MTL7Private8Selector53s_ksetTessellationFactorBuffer_offset_instanceStride_Ev
+ __ZN3MTL7Private8Selector53s_ksetTileIntersectionFunctionTables_withBufferRange_Ev
+ __ZN3MTL7Private8Selector54s_kaddMeshRenderPipelineFunctionsWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector54s_kaddTileRenderPipelineFunctionsWithDescriptor_error_Ev
+ __ZN3MTL7Private8Selector54s_kdrawPrimitives_indirectBuffer_indirectBufferOffset_Ev
+ __ZN3MTL7Private8Selector54s_kmeshThreadgroupSizeIsMultipleOfThreadExecutionWidthEv
+ __ZN3MTL7Private8Selector54s_knewLibraryWithStitchedDescriptor_completionHandler_Ev
+ __ZN3MTL7Private8Selector54s_krasterizationRateMapDescriptorWithScreenSize_layer_Ev
+ __ZN3MTL7Private8Selector54s_ksetFragmentIntersectionFunctionTable_atBufferIndex_Ev
+ __ZN3MTL7Private8Selector54s_ksetThreadGroupSizeIsMultipleOfThreadExecutionWidth_Ev
+ __ZN3MTL7Private8Selector55s_kheapAccelerationStructureSizeAndAlignWithDescriptor_Ev
+ __ZN3MTL7Private8Selector55s_knewBufferWithBytesNoCopy_length_options_deallocator_Ev
+ __ZN3MTL7Private8Selector55s_ksetMeshSamplerState_lodMinClamp_lodMaxClamp_atIndex_Ev
+ __ZN3MTL7Private8Selector55s_ksetTileSamplerState_lodMinClamp_lodMaxClamp_atIndex_Ev
+ __ZN3MTL7Private8Selector55s_ksetVertexBuffers_offsets_attributeStrides_withRange_Ev
+ __ZN3MTL7Private8Selector55s_ksetVertexIntersectionFunctionTables_withBufferRange_Ev
+ __ZN3MTL7Private8Selector55s_ktextureCubeDescriptorWithPixelFormat_size_mipmapped_Ev
+ __ZN3MTL7Private8Selector56s_kconcurrentDispatchThreadgroups_threadsPerThreadgroup_Ev
+ __ZN3MTL7Private8Selector56s_kdrawPrimitives_vertexStart_vertexCount_instanceCount_Ev
+ __ZN3MTL7Private8Selector56s_knewFunctionWithName_constantValues_completionHandler_Ev
+ __ZN3MTL7Private8Selector56s_kobjectThreadgroupSizeIsMultipleOfThreadExecutionWidthEv
+ __ZN3MTL7Private8Selector56s_ksetSamplerStates_lodMinClamps_lodMaxClamps_withRange_Ev
+ __ZN3MTL7Private8Selector57s_knewComputePipelineStateWithFunction_completionHandler_Ev
+ __ZN3MTL7Private8Selector57s_ksetFragmentIntersectionFunctionTables_withBufferRange_Ev
+ __ZN3MTL7Private8Selector57s_ksetObjectSamplerState_lodMinClamp_lodMaxClamp_atIndex_Ev
+ __ZN3MTL7Private8Selector57s_ksetVertexSamplerState_lodMinClamp_lodMaxClamp_atIndex_Ev
+ __ZN3MTL7Private8Selector57s_ksparseTileSizeWithTextureType_pixelFormat_sampleCount_Ev
+ __ZN3MTL7Private8Selector58s_kloadBuffer_offset_size_sourceHandle_sourceHandleOffset_Ev
+ __ZN3MTL7Private8Selector58s_knewRenderPipelineStateWithDescriptor_completionHandler_Ev
+ __ZN3MTL7Private8Selector58s_ksetMeshThreadgroupSizeIsMultipleOfThreadExecutionWidth_Ev
+ __ZN3MTL7Private8Selector59s_knewIntersectionFunctionWithDescriptor_completionHandler_Ev
+ __ZN3MTL7Private8Selector59s_knewTextureViewWithPixelFormat_textureType_levels_slices_Ev
+ __ZN3MTL7Private8Selector59s_ksetFragmentSamplerState_lodMinClamp_lodMaxClamp_atIndex_Ev
+ __ZN3MTL7Private8Selector59s_ksetOpaqueCurveIntersectionFunctionWithSignature_atIndex_Ev
+ __ZN3MTL7Private8Selector59s_ksetStageInRegionWithIndirectBuffer_indirectBufferOffset_Ev
+ __ZN3MTL7Private8Selector59s_kwriteCompactedAccelerationStructureSize_toBuffer_offset_Ev
+ __ZN3MTL7Private8Selector60s_ksetMeshSamplerStates_lodMinClamps_lodMaxClamps_withRange_Ev
+ __ZN3MTL7Private8Selector60s_ksetObjectThreadgroupSizeIsMultipleOfThreadExecutionWidth_Ev
+ __ZN3MTL7Private8Selector60s_ksetTileSamplerStates_lodMinClamps_lodMaxClamps_withRange_Ev
+ __ZN3MTL7Private8Selector61s_kmemoryBarrierWithResources_count_afterStages_beforeStages_Ev
+ __ZN3MTL7Private8Selector61s_knewRenderPipelineStateWithAdditionalBinaryFunctions_error_Ev
+ __ZN3MTL7Private8Selector61s_ksetOpaqueCurveIntersectionFunctionWithSignature_withRange_Ev
+ __ZN3MTL7Private8Selector61s_ktexture2DDescriptorWithPixelFormat_width_height_mipmapped_Ev
+ __ZN3MTL7Private8Selector62s_knewComputePipelineStateWithAdditionalBinaryFunctions_error_Ev
+ __ZN3MTL7Private8Selector62s_ksetObjectSamplerStates_lodMinClamps_lodMaxClamps_withRange_Ev
+ __ZN3MTL7Private8Selector62s_ksetOpaqueTriangleIntersectionFunctionWithSignature_atIndex_Ev
+ __ZN3MTL7Private8Selector62s_ksetVertexSamplerStates_lodMinClamps_lodMaxClamps_withRange_Ev
+ __ZN3MTL7Private8Selector63s_kcopyAndCompactAccelerationStructure_toAccelerationStructure_Ev
+ __ZN3MTL7Private8Selector63s_kcopyFromBuffer_sourceOffset_toBuffer_destinationOffset_size_Ev
+ __ZN3MTL7Private8Selector63s_kexecuteCommandsInBuffer_indirectBuffer_indirectBufferOffset_Ev
+ __ZN3MTL7Private8Selector63s_kresolveCounters_inRange_destinationBuffer_destinationOffset_Ev
+ __ZN3MTL7Private8Selector64s_knewComputePipelineStateWithFunction_options_reflection_error_Ev
+ __ZN3MTL7Private8Selector64s_ksetFragmentSamplerStates_lodMinClamps_lodMaxClamps_withRange_Ev
+ __ZN3MTL7Private8Selector64s_ksetOpaqueTriangleIntersectionFunctionWithSignature_withRange_Ev
+ __ZN3MTL7Private8Selector65s_knewComputePipelineStateWithFunction_options_completionHandler_Ev
+ __ZN3MTL7Private8Selector65s_knewRenderPipelineStateWithDescriptor_options_reflection_error_Ev
+ __ZN3MTL7Private8Selector65s_kupdateTextureMapping_mode_indirectBuffer_indirectBufferOffset_Ev
+ __ZN3MTL7Private8Selector66s_knewComputePipelineStateWithDescriptor_options_reflection_error_Ev
+ __ZN3MTL7Private8Selector66s_knewIndirectCommandBufferWithDescriptor_maxCommandCount_options_Ev
+ __ZN3MTL7Private8Selector66s_knewRenderPipelineStateWithDescriptor_options_completionHandler_Ev
+ __ZN3MTL7Private8Selector66s_krasterizationRateMapDescriptorWithScreenSize_layerCount_layers_Ev
+ __ZN3MTL7Private8Selector66s_kupdateTextureMappings_mode_regions_mipLevels_slices_numRegions_Ev
+ __ZN3MTL7Private8Selector67s_kconvertSparseTileRegions_toPixelRegions_withTileSize_numRegions_Ev
+ __ZN3MTL7Private8Selector67s_kgetBytes_bytesPerRow_bytesPerImage_fromRegion_mipmapLevel_slice_Ev
+ __ZN3MTL7Private8Selector67s_knewComputePipelineStateWithDescriptor_options_completionHandler_Ev
+ __ZN3MTL7Private8Selector67s_knewTextureViewWithPixelFormat_textureType_levels_slices_swizzle_Ev
+ __ZN3MTL7Private8Selector69s_kdrawPrimitives_vertexStart_vertexCount_instanceCount_baseInstance_Ev
+ __ZN3MTL7Private8Selector69s_knewRenderPipelineStateWithMeshDescriptor_options_reflection_error_Ev
+ __ZN3MTL7Private8Selector69s_knewRenderPipelineStateWithTileDescriptor_options_reflection_error_Ev
+ __ZN3MTL7Private8Selector6s_kURLEv
+ __ZN3MTL7Private8Selector6s_kurlEv
+ __ZN3MTL7Private8Selector70s_kcopyIndirectCommandBuffer_sourceRange_destination_destinationIndex_Ev
+ __ZN3MTL7Private8Selector70s_knewRenderPipelineStateWithMeshDescriptor_options_completionHandler_Ev
+ __ZN3MTL7Private8Selector70s_knewRenderPipelineStateWithTileDescriptor_options_completionHandler_Ev
+ __ZN3MTL7Private8Selector70s_ktextureBufferDescriptorWithPixelFormat_width_resourceOptions_usage_Ev
+ __ZN3MTL7Private8Selector71s_kreplaceRegion_mipmapLevel_slice_withBytes_bytesPerRow_bytesPerImage_Ev
+ __ZN3MTL7Private8Selector72s_ksparseTileSizeWithTextureType_pixelFormat_sampleCount_sparsePageSize_Ev
+ __ZN3MTL7Private8Selector72s_kwriteCompactedAccelerationStructureSize_toBuffer_offset_sizeDataType_Ev
+ __ZN3MTL7Private8Selector73s_kdrawMeshThreads_threadsPerObjectThreadgroup_threadsPerMeshThreadgroup_Ev
+ __ZN3MTL7Private8Selector75s_kbuildAccelerationStructure_descriptor_scratchBuffer_scratchBufferOffset_Ev
+ __ZN3MTL7Private8Selector76s_kdrawIndexedPrimitives_indexCount_indexType_indexBuffer_indexBufferOffset_Ev
+ __ZN3MTL7Private8Selector78s_kdrawMeshThreadgroups_threadsPerObjectThreadgroup_threadsPerMeshThreadgroup_Ev
+ __ZN3MTL7Private8Selector7s_kdataEv
+ __ZN3MTL7Private8Selector7s_kheapEv
+ __ZN3MTL7Private8Selector7s_kinitEv
+ __ZN3MTL7Private8Selector7s_klineEv
+ __ZN3MTL7Private8Selector7s_klogsEv
+ __ZN3MTL7Private8Selector7s_knameEv
+ __ZN3MTL7Private8Selector7s_ksizeEv
+ __ZN3MTL7Private8Selector7s_ktypeEv
+ __ZN3MTL7Private8Selector81s_kconvertSparsePixelRegions_toTileRegions_withTileSize_alignmentMode_numRegions_Ev
+ __ZN3MTL7Private8Selector85s_kdispatchThreadgroupsWithIndirectBuffer_indirectBufferOffset_threadsPerThreadgroup_Ev
+ __ZN3MTL7Private8Selector87s_krefitAccelerationStructure_descriptor_destination_scratchBuffer_scratchBufferOffset_Ev
+ __ZN3MTL7Private8Selector8s_kdepthEv
+ __ZN3MTL7Private8Selector8s_kerrorEv
+ __ZN3MTL7Private8Selector8s_kindexEv
+ __ZN3MTL7Private8Selector8s_klabelEv
+ __ZN3MTL7Private8Selector8s_klevelEv
+ __ZN3MTL7Private8Selector8s_knodesEv
+ __ZN3MTL7Private8Selector8s_kresetEv
+ __ZN3MTL7Private8Selector8s_ksliceEv
+ __ZN3MTL7Private8Selector8s_kusageEv
+ __ZN3MTL7Private8Selector8s_kwidthEv
+ __ZN3MTL7Private8Selector90s_kdrawIndexedPrimitives_indexCount_indexType_indexBuffer_indexBufferOffset_instanceCount_Ev
+ __ZN3MTL7Private8Selector91s_kdrawPatches_patchIndexBuffer_patchIndexBufferOffset_indirectBuffer_indirectBufferOffset_Ev
+ __ZN3MTL7Private8Selector95s_krefitAccelerationStructure_descriptor_destination_scratchBuffer_scratchBufferOffset_options_Ev
+ __ZN3MTL7Private8Selector9s_kaccessEv
+ __ZN3MTL7Private8Selector9s_kbufferEv
+ __ZN3MTL7Private8Selector9s_kcolumnEv
+ __ZN3MTL7Private8Selector9s_kcommitEv
+ __ZN3MTL7Private8Selector9s_kdeviceEv
+ __ZN3MTL7Private8Selector9s_kformatEv
+ __ZN3MTL7Private8Selector9s_kgroupsEv
+ __ZN3MTL7Private8Selector9s_kheightEv
+ __ZN3MTL7Private8Selector9s_kisUsedEv
+ __ZN3MTL7Private8Selector9s_klayersEv
+ __ZN3MTL7Private8Selector9s_klengthEv
+ __ZN3MTL7Private8Selector9s_koffsetEv
+ __ZN3MTL7Private8Selector9s_kopaqueEv
+ __ZN3MTL7Private8Selector9s_kstatusEv
+ __ZN3MTL7Private8Selector9s_kstrideEv
+ __ZNK2gc6Buffer4nameEv
+ __ZNKSt3__115error_condition7messageEv
+ __ZNKSt3__16locale9has_facetERNS0_2idE
+ __ZNSt13runtime_errorC2EPKc
+ __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEE6assignEmc
+ __ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEC1ERKS5_mmRKS4_
+ __ZNSt3__113basic_ostreamIcNS_11char_traitsIcEEElsEb
+ __ZNSt3__113shared_futureIvED1Ev
+ __ZNSt3__118condition_variable10notify_oneEv
+ __ZNSt3__119__shared_mutex_base11lock_sharedEv
+ __ZNSt3__119__shared_mutex_base13unlock_sharedEv
+ __ZNSt3__119__shared_mutex_base4lockEv
+ __ZNSt3__119__shared_mutex_base6unlockEv
+ __ZNSt3__119__shared_mutex_baseC1Ev
+ __ZNSt3__16__sortIRNS_6__lessIiiEEPiEEvT0_S5_T_
+ __ZNSt3__16locale5facet16__on_zero_sharedEv
+ __ZNSt3__16locale5facetD2Ev
+ __ZNSt3__16localeC1ERKS0_
+ __ZNSt3__17promiseIvE10get_futureEv
+ __ZNSt3__17promiseIvE9set_valueEv
+ __ZNSt3__17promiseIvEC1Ev
+ __ZNSt3__17promiseIvED1Ev
+ __ZTINSt3__16locale5facetE
+ __ZTIPKc
+ __ZTVNSt3__115basic_streambufIcNS_11char_traitsIcEEEE
+ __ZneRK17loopkit_rt_type_tS1_
+ ___toupper
+ ___udivti3
+ _atanf
+ _cblas_dgemm$NEWLAPACK
+ _cblas_dgemv$NEWLAPACK
+ _cblas_sgemv$NEWLAPACK
+ _clock
+ _dgesvd$NEWLAPACK
+ _dgetrf$NEWLAPACK
+ _dgetri$NEWLAPACK
+ _dpotrf$NEWLAPACK
+ _dpotrs$NEWLAPACK
+ _dsyev$NEWLAPACK
+ _dsysv$NEWLAPACK
+ _dsytrf$NEWLAPACK
+ _dsytrs$NEWLAPACK
+ _e5rt_e5_compiler_compile
+ _e5rt_e5_compiler_create
+ _e5rt_e5_compiler_is_new_compile_required
+ _e5rt_e5_compiler_options_create
+ _e5rt_e5_compiler_options_release
+ _e5rt_e5_compiler_options_set_compute_device_types_mask
+ _e5rt_e5_compiler_release
+ _e5rt_execution_stream_operation_create_precompiled_compute_operation_with_options
+ _e5rt_precompiled_compute_op_create_options_create_with_program_function
+ _e5rt_precompiled_compute_op_create_options_release
+ _e5rt_precompiled_compute_op_create_options_set_allocate_intermediate_buffers
+ _e5rt_program_function_release
+ _e5rt_program_library_retain_program_function
+ _fmod
+ _halide_copy_to_device
+ _ldiv
+ _modff
+ _nextafterf
+ _objc_getProtocol
+ _objc_lookUpClass
+ _sel_registerName
+ _ssyev$NEWLAPACK
+ _tanf
- __ZN2gc11AOTRegistry16registerScheduleERKNS_8ScheduleE
- __ZN2gc16GeneratorFactory7compileERKNS_8ScheduleE
- __ZN2gc3est30createLeastSquaresLossFunctionENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEENS1_10shared_ptrINS0_8BoxMinusEEENS8_INS0_10NoiseModelEEE
- __ZN2gc8Schedule15addCustomOptionERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEES9_S9_
- __ZN2gc8Schedule3addERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEERKNS_15ProgramScheduleE
- __ZN32pxrInternal__aapl__pxrReserved__11TfMallocTag4_EndEPNS0_11_ThreadDataE
- __ZNSt3__115basic_streambufIcNS_11char_traitsIcEEEC2Ev
- __ZNSt3__115basic_streambufIcNS_11char_traitsIcEEED2Ev
- _cblas_sgemv
- _dgesvd_
- _dgetrf_
- _dgetri_
- _dpotrf_
- _dpotrs_
- _dsyev_
- _dsysv_
- _dsytrf_
- _dsytrs_
- _e5rt_execution_stream_operation_create_precompiled_compute_operation
- _localeconv
- _objc_retain_x9
- _wmemchr
CStrings:
+ "\n\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT = int64_t,\n    int N_READS = REDUCE_N_READS>\n[[kernel]] void all_reduce(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& in_size [[buffer(2)]],\n    const constant size_t& row_size [[buffer(3)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  threadgroup U shared_vals[simd_size];\n  U total = Op::init;\n  IdxT start_idx = gid.y * IdxT(row_size);\n  IdxT actual_row =\n      (start_idx + row_size <= in_size) ? row_size : in_size - start_idx;\n  IdxT blocks = actual_row / (lsize.x * N_READS);\n  int extra = actual_row - blocks * (lsize.x * N_READS);\n  extra -= lid.x * N_READS;\n  start_idx += lid.x * N_READS;\n  in += start_idx;\n  if (extra >= N_READS) {\n    blocks++;\n    extra = 0;\n  }\n  for (IdxT b = 0; b < blocks; b++) {\n    for (int i = 0; i < N_READS; i++) {\n      total = op(static_cast<U>(in[i]), total);\n    }\n    in += lsize.x * N_READS;\n  }\n  if (extra > 0) {\n    for (int i = 0; i < extra; i++) {\n      total = op(static_cast<U>(in[i]), total);\n    }\n  }\n  total = op.simd_reduce(total);\n  if (simd_per_group > 1) {\n    if (simd_lane_id == 0) {\n      shared_vals[simd_group_id] = total;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    total = lid.x < simd_per_group ? shared_vals[lid.x] : op.init;\n    total = op.simd_reduce(total);\n  }\n  if (lid.x == 0) {\n    out[gid.y] = total;\n  }\n}\ntemplate <typename T, typename U, typename Op, typename IdxT, int NDIMS>\n[[kernel]] void col_reduce_small(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]]) {\n  constexpr int n_reads = 4;\n  Op op;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  U totals[n_reads];\n  for (int i = 0; i < n_reads; i++) {\n    totals[i] = Op::init;\n  }\n  IdxT column = IdxT(gid.x) * lsize.x * n_reads + lid.x * n_reads;\n  if (column >= reduction_stride) {\n    return;\n  }\n  bool safe = column + n_reads <= reduction_stride;\n  IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n  IdxT in_idx = elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + column;\n  IdxT total_rows = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(lid.y, reduce_shape, reduce_strides);\n  for (IdxT r = lid.y; r < total_rows; r += lsize.y) {\n    row = in + loop.location();\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(static_cast<U>(row[i]), totals[i]);\n      }\n    } else {\n      U vals[n_reads];\n      for (int i = 0; i < n_reads; i++) {\n        vals[i] =\n            (column + i < reduction_stride) ? static_cast<U>(row[i]) : op.init;\n      }\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(vals[i], totals[i]);\n      }\n    }\n    loop.next(lsize.y, reduce_shape, reduce_strides);\n  }\n  if (lsize.y > 1) {\n    threadgroup U shared_vals[32 * 8 * n_reads];\n    for (int i = 0; i < n_reads; i++) {\n      shared_vals[lid.y * lsize.x * n_reads + lid.x * n_reads + i] = totals[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (lid.y == 0) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = shared_vals[lid.x * n_reads + i];\n      }\n      for (uint j = 1; j < lsize.y; j++) {\n        for (int i = 0; i < n_reads; i++) {\n          totals[i] =\n              op(shared_vals[j * lsize.x * n_reads + lid.x * n_reads + i],\n                 totals[i]);\n        }\n      }\n    }\n  }\n  if (lid.y == 0) {\n    out += out_idx * IdxT(reduction_stride) + column;\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        out[i] = totals[i];\n      }\n    } else {\n      for (int i = 0; column + i < reduction_stride; i++) {\n        out[i] = totals[i];\n      }\n    }\n  }\n}\ntemplate <typename T, typename U, typename Op, typename IdxT, int NDIMS>\n[[kernel]] void col_reduce_longcolumn(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    const constant size_t& out_size [[buffer(11)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]]) {\n  Op op;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  IdxT out_idx = gid.x + gsize.x * IdxT(gid.y);\n  IdxT in_idx = elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + lid.x;\n  U total = Op::init;\n  IdxT total_rows = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(gid.z * lsize.y + lid.y, reduce_shape, reduce_strides);\n  for (IdxT r = gid.z * lsize.y + lid.y; r < total_rows;\n       r += lsize.y * gsize.z) {\n    row = in + loop.location();\n    total = op(static_cast<U>(*row), total);\n    loop.next(lsize.y * gsize.z, reduce_shape, reduce_strides);\n  }\n  threadgroup U shared_vals[32 * 32];\n  shared_vals[lid.y * lsize.x + lid.x] = total;\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  if (lid.y == 0) {\n    for (uint i = 1; i < lsize.y; i++) {\n      total = op(total, shared_vals[i * lsize.x + lid.x]);\n    }\n    out[gid.z * IdxT(out_size) + out_idx * IdxT(reduction_stride) + lid.x] =\n        total;\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int BM,\n    int BN>\n[[kernel]] void col_reduce_looped(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  constexpr int n_simdgroups = 8;\n  constexpr short tgp_size = n_simdgroups * simd_size;\n  constexpr short n_reads = (BM * BN) / tgp_size;\n  constexpr short n_read_blocks = BN / n_reads;\n  threadgroup U shared_vals[BN * BM];\n  U totals[n_reads];\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  for (int i = 0; i < n_reads; i++) {\n    totals[i] = Op::init;\n  }\n  short lid = simd_group_id * simd_size + simd_lane_id;\n  short2 offset((lid % n_read_blocks) * n_reads, lid / n_read_blocks);\n  IdxT column = BN * gid.x + offset.x;\n  bool safe = column + n_reads <= reduction_stride;\n  IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n  IdxT in_idx = elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + column;\n  IdxT total = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(offset.y, reduce_shape, reduce_strides);\n  for (IdxT r = offset.y; r < total; r += BM) {\n    row = in + loop.location();\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(static_cast<U>(row[i]), totals[i]);\n      }\n    } else {\n      U vals[n_reads];\n      for (int i = 0; i < n_reads; i++) {\n        vals[i] =\n            (column + i < reduction_stride) ? static_cast<U>(row[i]) : op.init;\n      }\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(vals[i], totals[i]);\n      }\n    }\n    loop.next(BM, reduce_shape, reduce_strides);\n  }\n  if (BM == 32) {\n    constexpr int n_outputs = BN / n_simdgroups;\n    static_assert(\n        BM != 32 || n_outputs == n_reads,\n        \"The tile should be selected such that n_outputs == n_reads\");\n    for (int i = 0; i < n_reads; i++) {\n      shared_vals[offset.y * BN + offset.x + i] = totals[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    short2 out_offset(simd_group_id * n_outputs, simd_lane_id);\n    for (int i = 0; i < n_outputs; i++) {\n      totals[i] =\n          op.simd_reduce(shared_vals[out_offset.y * BN + out_offset.x + i]);\n    }\n    if (simd_lane_id == 0) {\n      IdxT out_column = BN * gid.x + out_offset.x;\n      out += out_idx * IdxT(reduction_stride) + out_column;\n      if (out_column + n_outputs <= reduction_stride) {\n        for (int i = 0; i < n_outputs; i++) {\n          out[i] = totals[i];\n        }\n      } else {\n        for (int i = 0; out_column + i < reduction_stride; i++) {\n          out[i] = totals[i];\n        }\n      }\n    }\n  }\n  else {\n    short x_block = offset.x / n_reads;\n    for (int i = 0; i < n_reads; i++) {\n      shared_vals[x_block * BM * n_reads + i * BM + offset.y] = totals[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (offset.y == 0) {\n      for (int i = 0; i < n_reads; i++) {\n        for (int j = 1; j < BM; j++) {\n          totals[i] =\n              op(shared_vals[x_block * BM * n_reads + i * BM + j], totals[i]);\n        }\n      }\n    }\n    if (offset.y == 0) {\n      out += out_idx * IdxT(reduction_stride) + column;\n      if (safe) {\n        for (int i = 0; i < n_reads; i++) {\n          out[i] = totals[i];\n        }\n      } else {\n        for (int i = 0; column + i < reduction_stride; i++) {\n          out[i] = totals[i];\n        }\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int BM,\n    int BN>\n[[kernel]] void col_reduce_2pass(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& reduction_stride [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    const constant size_t& non_col_reductions [[buffer(10)]],\n    const constant size_t& out_size [[buffer(11)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  constexpr int n_simdgroups = 8;\n  constexpr short tgp_size = n_simdgroups * simd_size;\n  constexpr short n_reads = (BM * BN) / tgp_size;\n  constexpr short n_read_blocks = BN / n_reads;\n  constexpr int n_outputs = BN / n_simdgroups;\n  constexpr short outer_blocks = 32;\n  static_assert(BM == 32, \"BM should be equal to 32\");\n  threadgroup U shared_vals[BN * BM];\n  U totals[n_reads];\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  for (int i = 0; i < n_reads; i++) {\n    totals[i] = Op::init;\n  }\n  short lid = simd_group_id * simd_size + simd_lane_id;\n  short2 offset((lid % n_read_blocks) * n_reads, lid / n_read_blocks);\n  IdxT column = BN * gid.x + offset.x;\n  bool safe = column + n_reads <= reduction_stride;\n  IdxT full_idx = gid.y + gsize.y * IdxT(gid.z);\n  IdxT block_idx = full_idx / IdxT(out_size);\n  IdxT out_idx = full_idx % IdxT(out_size);\n  IdxT in_idx = elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n  in += in_idx + column;\n  IdxT total = IdxT(non_col_reductions) * IdxT(reduction_size);\n  loop.next(offset.y + block_idx * BM, reduce_shape, reduce_strides);\n  for (IdxT r = offset.y + block_idx * BM; r < total; r += outer_blocks * BM) {\n    row = in + loop.location();\n    if (safe) {\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(static_cast<U>(row[i]), totals[i]);\n      }\n    } else {\n      U vals[n_reads];\n      for (int i = 0; i < n_reads; i++) {\n        vals[i] =\n            (column + i < reduction_stride) ? static_cast<U>(row[i]) : op.init;\n      }\n      for (int i = 0; i < n_reads; i++) {\n        totals[i] = op(vals[i], totals[i]);\n      }\n    }\n    loop.next(outer_blocks * BM, reduce_shape, reduce_strides);\n  }\n  for (int i = 0; i < n_reads; i++) {\n    shared_vals[offset.y * BN + offset.x + i] = totals[i];\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  short2 out_offset(simd_group_id * n_outputs, simd_lane_id);\n  for (int i = 0; i < n_outputs; i++) {\n    totals[i] =\n        op.simd_reduce(shared_vals[out_offset.y * BN + out_offset.x + i]);\n  }\n  if (simd_lane_id == 0) {\n    IdxT out_column = BN * gid.x + out_offset.x;\n    out += full_idx * IdxT(reduction_stride) + out_column;\n    if (out_column + n_outputs <= reduction_stride) {\n      for (int i = 0; i < n_outputs; i++) {\n        out[i] = totals[i];\n      }\n    } else {\n      for (int i = 0; out_column + i < reduction_stride; i++) {\n        out[i] = totals[i];\n      }\n    }\n  }\n}\ntemplate <typename T, typename Op>\n[[kernel]] void init_reduce(\n    device T* out [[buffer(0)]],\n    uint tid [[thread_position_in_grid]]) {\n  out[tid] = Op::init;\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void per_thread_row_reduce(\n    thread U totals[N_WRITES],\n    const device T* inputs[N_WRITES],\n    int blocks,\n    int extra,\n    uint lsize_x,\n    uint lid_x) {\n  Op op;\n  for (int i = 0; i < N_WRITES; i++) {\n    totals[i] = Op::init;\n  }\n  for (int i = 0; i < blocks; i++) {\n    for (int j = 0; j < N_WRITES; j++) {\n      for (int i = 0; i < N_READS; i++) {\n        totals[j] = op(static_cast<U>(inputs[j][i]), totals[j]);\n      }\n      inputs[j] += lsize_x * N_READS;\n    }\n  }\n  int index = lid_x * N_READS;\n  if (index + N_READS <= extra) {\n    for (int j = 0; j < N_WRITES; j++) {\n      for (int i = 0; i < N_READS; i++) {\n        totals[j] = op(static_cast<U>(inputs[j][i]), totals[j]);\n      }\n    }\n  } else {\n    for (int j = 0; j < N_WRITES; j++) {\n      for (int i = 0; index + i < extra; i++) {\n        totals[j] = op(static_cast<U>(inputs[j][i]), totals[j]);\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void per_thread_row_reduce(\n    thread U totals[N_WRITES],\n    const device T* in,\n    const constant size_t& reduction_size,\n    int blocks,\n    int extra,\n    uint lsize_x,\n    uint lid_x) {\n  const device T* inputs[N_WRITES];\n  inputs[0] = in + lid_x * N_READS;\n  for (int i = 1; i < N_READS; i++) {\n    inputs[i] = inputs[i - 1] + reduction_size;\n  }\n  per_thread_row_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, inputs, blocks, extra, lsize_x, lid_x);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void per_thread_row_reduce(\n    thread U totals[N_WRITES],\n    const device T* in,\n    const size_t row_idx,\n    int blocks,\n    int extra,\n    const constant int* shape,\n    const constant size_t* strides,\n    const constant int& ndim,\n    uint lsize_x,\n    uint lid_x) {\n  const device T* inputs[N_WRITES];\n  in += lid_x * N_READS;\n  for (int i = 0; i < N_READS; i++) {\n    inputs[i] = in + elem_to_loc(row_idx + i, shape, strides, ndim);\n  }\n  per_thread_row_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, inputs, blocks, extra, lsize_x, lid_x);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\nMETAL_FUNC void threadgroup_reduce(\n    thread U totals[N_WRITES],\n    threadgroup U* shared_vals,\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  for (int i = 0; i < N_WRITES; i++) {\n    totals[i] = op.simd_reduce(totals[i]);\n  }\n  if (simd_per_group > 1) {\n    if (simd_lane_id == 0) {\n      for (int i = 0; i < N_WRITES; i++) {\n        shared_vals[simd_group_id * N_WRITES + i] = totals[i];\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    U values[N_WRITES];\n    for (int i = 0; i < N_WRITES; i++) {\n      values[i] = (lid.x < simd_per_group) ? shared_vals[lid.x * N_WRITES + i]\n                                           : op.init;\n    }\n    for (int i = 0; i < N_WRITES; i++) {\n      totals[i] = op.simd_reduce(values[i]);\n    }\n  }\n}\ntemplate <typename T, typename U, typename Op, int N_READS = REDUCE_N_READS>\nMETAL_FUNC void\nthread_reduce(thread U& total, const device T* row, int blocks, int extra) {\n  Op op;\n  for (int i = 0; i < blocks; i++) {\n    U vals[N_READS];\n    for (int j = 0; j < N_READS; j++) {\n      vals[j] = row[j];\n    }\n    for (int j = 0; j < N_READS; j++) {\n      total = op(vals[j], total);\n    }\n    row += N_READS;\n  }\n  for (int i = 0; i < extra; i++) {\n    total = op(*row++, total);\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int N_READS = REDUCE_N_READS>\n[[kernel]] void row_reduce_small(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& row_size [[buffer(2)]],\n    const constant size_t& non_row_reductions [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 tid [[thread_position_in_grid]],\n    uint3 tsize [[threads_per_grid]]) {\n  Op op;\n  U total_val = Op::init;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  int blocks = IdxT(row_size) / N_READS;\n  int extra = IdxT(row_size) % N_READS;\n  if ((non_row_reductions < 32 && row_size <= 8) || non_row_reductions <= 8) {\n    IdxT out_idx = tid.x + tsize.y * IdxT(tid.y);\n    in += elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n    for (uint r = 0; r < non_row_reductions; r++) {\n      row = in + loop.location();\n      thread_reduce<T, U, Op, N_READS>(total_val, row, blocks, extra);\n      loop.next(reduce_shape, reduce_strides);\n    }\n    out[out_idx] = total_val;\n  } else {\n    IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n    in += elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim);\n    loop.next(simd_lane_id, reduce_shape, reduce_strides);\n    for (uint r = simd_lane_id; r < non_row_reductions; r += simd_size) {\n      row = in + loop.location();\n      thread_reduce<T, U, Op, N_READS>(total_val, row, blocks, extra);\n      loop.next(simd_size, reduce_shape, reduce_strides);\n    }\n    total_val = op.simd_reduce(total_val);\n    if (simd_lane_id == 0) {\n      out[out_idx] = total_val;\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT = size_t,\n    int N_READS = REDUCE_N_READS,\n    int N_WRITES = REDUCE_N_WRITES>\n[[kernel]] void row_reduce_simple(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& reduction_size [[buffer(2)]],\n    const constant size_t& out_size [[buffer(3)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  threadgroup U shared_vals[simd_size * N_WRITES];\n  U totals[N_WRITES];\n  IdxT out_idx = N_WRITES * (gid.y + gsize.y * IdxT(gid.z));\n  if (out_idx + N_WRITES > out_size) {\n    out_idx = out_size - N_WRITES;\n  }\n  in += out_idx * IdxT(reduction_size);\n  out += out_idx;\n  int blocks = IdxT(reduction_size) / (lsize.x * N_READS);\n  int extra = reduction_size - blocks * (lsize.x * N_READS);\n  per_thread_row_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, in, reduction_size, blocks, extra, lsize.x, lid.x);\n  threadgroup_reduce<T, U, Op, N_READS, N_WRITES>(\n      totals, shared_vals, lid, simd_lane_id, simd_per_group, simd_group_id);\n  if (lid.x == 0) {\n    for (int i = 0; i < N_WRITES; i++) {\n      out[i] = totals[i];\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    typename IdxT,\n    int NDIMS,\n    int N_READS = REDUCE_N_READS>\n[[kernel]] void row_reduce_looped(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& row_size [[buffer(2)]],\n    const constant size_t& non_row_reductions [[buffer(3)]],\n    const constant int* shape [[buffer(4)]],\n    const constant size_t* strides [[buffer(5)]],\n    const constant int& ndim [[buffer(6)]],\n    const constant int* reduce_shape [[buffer(7)]],\n    const constant size_t* reduce_strides [[buffer(8)]],\n    const constant int& reduce_ndim [[buffer(9)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_per_group [[simdgroups_per_threadgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  Op op;\n  threadgroup U shared_vals[simd_size];\n  U total = Op::init;\n  IdxT out_idx = gid.y + gsize.y * IdxT(gid.z);\n  in += elem_to_loc<size_t, IdxT>(out_idx, shape, strides, ndim) +\n      lid.x * N_READS;\n  LoopedElemToLoc<NDIMS, IdxT, (NDIMS > 2)> loop(reduce_ndim);\n  const device T* row;\n  int blocks = IdxT(row_size) / (lsize.x * N_READS);\n  int extra = row_size - blocks * (lsize.x * N_READS);\n  for (IdxT i = 0; i < non_row_reductions; i++) {\n    row = in + loop.location();\n    U row_total;\n    per_thread_row_reduce<T, U, Op, N_READS, 1>(\n        &row_total, &row, blocks, extra, lsize.x, lid.x);\n    total = op(total, row_total);\n    loop.next(reduce_shape, reduce_strides);\n  }\n  threadgroup_reduce<T, U, Op, N_READS, 1>(\n      &total, shared_vals, lid, simd_lane_id, simd_per_group, simd_group_id);\n  if (lid.x == 0) {\n    out[out_idx] = total;\n  }\n}\n"
+ "\n\ntemplate <typename T>\n[[kernel]] void arange(\n    constant const T& start,\n    constant const T& step,\n    device T* out,\n    uint index [[thread_position_in_grid]]) {\n  out[index] = start + index * step;\n}\n"
+ "\n#if (__METAL_VERSION__ >= 310)\nusing namespace metal;\ntypedef bfloat bfloat16_t;\ninline uint16_t bfloat16_to_uint16(const bfloat16_t x) {\n  return as_type<uint16_t>(x);\n}\ninline bfloat16_t uint16_to_bfloat16(const uint16_t x) {\n  return as_type<bfloat16_t>(x);\n}\n\n #else\nusing namespace metal;\nconstexpr METAL_FUNC uint16_t float_to_bfloat_bits(float x) {\n  if ((as_type<uint32_t>(x) & ~_fp_encoding_traits<float>::sign_mask) >\n      _fp_encoding_traits<float>::inf_mask) {\n    return uint16_t(as_type<uint32_t>(0x7FC0));\n  }\n  uint32_t float_bits = as_type<uint32_t>(x);\n  float_bits += ((float_bits >> 16) & 1) + as_type<uint32_t>(0x7FFF);\n  return float_bits >> 16;\n}\nconstexpr METAL_FUNC float bfloat_bits_to_float(uint16_t x) {\n  return as_type<float>((uint32_t)x << 16);\n}\nstruct _MLX_BFloat16;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_to_bfloat =\n    !is_same_v<T, _MLX_BFloat16> && is_convertible_v<T, float>;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_from_bfloat =\n    !is_same_v<T, _MLX_BFloat16> && is_convertible_v<float, T>;\nstruct _MLX_BFloat16 {\n  uint16_t bits_;\n  _MLX_BFloat16() thread = default;\n  _MLX_BFloat16() threadgroup = default;\n  _MLX_BFloat16() device = default;\n  _MLX_BFloat16() constant = default;\n  struct bits_to_bfloat_struct {};\n  static constexpr METAL_FUNC bits_to_bfloat_struct bits_to_bfloat() {\n    return bits_to_bfloat_struct();\n  }\n  constexpr METAL_FUNC _MLX_BFloat16(uint16_t bits, bits_to_bfloat_struct)\n      : bits_(bits) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) thread\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) threadgroup\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) device\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_bfloat<T>>::type>\n  constexpr METAL_FUNC _MLX_BFloat16(T x) constant\n      : bits_(float_to_bfloat_bits(static_cast<float>(x))) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const thread {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const threadgroup {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const device {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_bfloat<T>>::type>\n  constexpr METAL_FUNC operator T() const constant {\n    return static_cast<T>(bfloat_bits_to_float(bits_));\n  }\n};\nconstexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 x) {\n  return -static_cast<float>(x);\n}\nconstexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC float operator+(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC float operator+(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC float operator+(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC float operator+(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator+(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator+(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) + static_cast<float>(rhs); };;\nconstexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC float operator-(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC float operator-(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC float operator-(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC float operator-(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator-(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator-(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) - static_cast<float>(rhs); };;\nconstexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC float operator*(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC float operator*(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC float operator*(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC float operator*(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator*(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator*(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) * static_cast<float>(rhs); };;\nconstexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC float operator/(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC float operator/(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC float operator/(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC float operator/(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); }; constexpr METAL_FUNC _MLX_BFloat16 operator/(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); } constexpr METAL_FUNC _MLX_BFloat16 operator/(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) / static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) > static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) < static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator>=(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator>=(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) >= static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator<=(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); } constexpr METAL_FUNC bool operator<=(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) <= static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator==(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); } constexpr METAL_FUNC bool operator==(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) == static_cast<float>(rhs); };;\nconstexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, float rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(float lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, half rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(half lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, int32_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(int32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, uint32_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(uint32_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, int64_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(int64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); }; constexpr METAL_FUNC bool operator!=(_MLX_BFloat16 lhs, uint64_t rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); } constexpr METAL_FUNC bool operator!=(uint64_t lhs, _MLX_BFloat16 rhs) { return static_cast<float>(lhs) != static_cast<float>(rhs); };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator+=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator+=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator+=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator-=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator-=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator-=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator*=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator*=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator*=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device float& operator/=( device float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread float& operator/=( thread float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, float rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup float& operator/=( threadgroup float& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator+=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator+=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator+=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator-=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator-=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator-=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator*=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator*=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator*=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device half& operator/=( device half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread half& operator/=( thread half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, half rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup half& operator/=( threadgroup half& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator+=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator+=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator+=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator-=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator-=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator-=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator*=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator*=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator*=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int16_t& operator/=( device int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int16_t& operator/=( thread int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, int16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int16_t& operator/=( threadgroup int16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator+=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator+=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator+=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator-=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator-=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator-=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator*=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator*=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator*=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int32_t& operator/=( device int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int32_t& operator/=( thread int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, int32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int32_t& operator/=( threadgroup int32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator+=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator+=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator+=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator-=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator-=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator-=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator*=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator*=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator*=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device int64_t& operator/=( device int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread int64_t& operator/=( thread int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, int64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup int64_t& operator/=( threadgroup int64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator+=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator+=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator+=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator-=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator-=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator-=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator*=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator*=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator*=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint16_t& operator/=( device uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint16_t& operator/=( thread uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, uint16_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint16_t& operator/=( threadgroup uint16_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator+=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator+=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator+=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator-=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator-=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator-=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator*=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator*=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator*=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint32_t& operator/=( device uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint32_t& operator/=( thread uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, uint32_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint32_t& operator/=( threadgroup uint32_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator+=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator+=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator+=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator-=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator-=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator-=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator*=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator*=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator*=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };; constexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC device uint64_t& operator/=( device uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC thread uint64_t& operator/=( thread uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, uint64_t rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; } constexpr METAL_FUNC threadgroup uint64_t& operator/=( threadgroup uint64_t& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator+=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator+=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator+=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) + static_cast<float>(rhs); return lhs; };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator-=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator-=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator-=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) - static_cast<float>(rhs); return lhs; };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator*=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator*=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator*=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) * static_cast<float>(rhs); return lhs; };;\nconstexpr METAL_FUNC device _MLX_BFloat16& operator/=( device _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC thread _MLX_BFloat16& operator/=( thread _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; }; constexpr METAL_FUNC threadgroup _MLX_BFloat16& operator/=( threadgroup _MLX_BFloat16& lhs, _MLX_BFloat16 rhs) { lhs = static_cast<float>(lhs) / static_cast<float>(rhs); return lhs; };;\ntypedef struct _MLX_BFloat16 bfloat16_t;\n#pragma METAL internals : enable\nnamespace metal {\ntemplate <>\nstruct _numeric_limits_impl<bfloat16_t> : _fp_numeric_limits_impl_base {\n  static constexpr constant int digits = 8;\n  static constexpr constant int digits10 = 2;\n  static constexpr constant int max_digits10 = 4;\n  static constexpr constant int radix = 2;\n  static constexpr constant int min_exponent = -125;\n  static constexpr constant int min_exponent10 = -37;\n  static constexpr constant int max_exponent = 128;\n  static constexpr constant int max_exponent10 = 38;\n  static constexpr bfloat16_t min() {\n    return _MLX_BFloat16(0x0080, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t lowest() {\n    return _MLX_BFloat16(0xFF7F, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t max() {\n    return _MLX_BFloat16(0x7F7F, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t epsilon() {\n    return _MLX_BFloat16(0x3C00, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t round_error() {\n    return _MLX_BFloat16(0x3F00, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t infinity() {\n    return _MLX_BFloat16(0x7F80, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t quiet_NaN() {\n    return _MLX_BFloat16(0x7FC0, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t signaling_NaN() {\n    return _MLX_BFloat16(0x7F80, _MLX_BFloat16::bits_to_bfloat());\n  }\n  static constexpr bfloat16_t denorm_min() {\n    return _MLX_BFloat16(0x0001, _MLX_BFloat16::bits_to_bfloat());\n  }\n};\nMETAL_FUNC bool isnan(_MLX_BFloat16 x) {\n  return x != x;\n}\n}\n#pragma METAL internals : disable\ninline uint16_t bfloat16_to_uint16(const bfloat16_t x) {\n  return x.bits_;\n}\ninline bfloat16_t uint16_to_bfloat16(const uint16_t x) {\n  return _MLX_BFloat16(x, _MLX_BFloat16::bits_to_bfloat());\n}\n\n #endif\n\nnamespace metal {\nMETAL_FUNC bfloat16_t abs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t acos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acos(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t acosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acosh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t asin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asin(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t asinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asinh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t atan(bfloat16_t y_over_x) { return static_cast<bfloat16_t>( __metal_atan(static_cast<float>(y_over_x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t atan2(bfloat16_t y, bfloat16_t x) { return static_cast<bfloat16_t>( __metal_atan2(static_cast<float>(y), static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t atanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_atanh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t ceil(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_ceil(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t cos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cos(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t cosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cosh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t cospi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cospi(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t divide(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_divide(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t exp(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t exp10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp10(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t exp2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp2(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fabs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fdim(bfloat16_t x, bfloat16_t y) { float t = static_cast<float>(x - y); return static_cast<bfloat16_t>(select(t, float(0), t < float(0) || x == y)); } METAL_FUNC bfloat16_t floor(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_floor(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fma(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fma( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z))); } METAL_FUNC bfloat16_t fmax(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmax3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmedian3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fmod(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmod(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t fract(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fract(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t frexp(bfloat16_t x, thread int& exp) { return static_cast<bfloat16_t>(__metal_frexp(static_cast<float>(x), &exp)); } METAL_FUNC bfloat16_t ldexp(bfloat16_t x, int k) { return static_cast<bfloat16_t>(__metal_ldexp(static_cast<float>(x), k, __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t log(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t log10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log10(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t log2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log2(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t max(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t max3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t median3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t min(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t min3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t nextafter(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_nextafter(static_cast<float>(x), static_cast<float>(y))); } METAL_FUNC bfloat16_t pow(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_pow(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t powr(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_powr(static_cast<float>(x), static_cast<float>(y), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t rint(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rint(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t round(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_round(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t rsqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rsqrt(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sin(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sinpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinpi(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t sqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sqrt(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t tan(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tan(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t tanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanh(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t tanpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanpi(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); } METAL_FUNC bfloat16_t trunc(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_trunc(static_cast<float>(x), __METAL_MAYBE_FAST_MATH__)); };\nnamespace fast {\nMETAL_FUNC bfloat16_t abs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t acos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acos(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t acosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acosh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t asin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asin(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t asinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asinh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t atan(bfloat16_t y_over_x) { return static_cast<bfloat16_t>( __metal_atan(static_cast<float>(y_over_x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t atan2(bfloat16_t y, bfloat16_t x) { return static_cast<bfloat16_t>( __metal_atan2(static_cast<float>(y), static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t atanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_atanh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t ceil(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_ceil(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t cos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cos(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t cosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cosh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t cospi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cospi(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t divide(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_divide(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t exp(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t exp10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp10(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t exp2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp2(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fabs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fdim(bfloat16_t x, bfloat16_t y) { float t = static_cast<float>(x - y); return static_cast<bfloat16_t>(select(t, float(0), t < float(0) || x == y)); } METAL_FUNC bfloat16_t floor(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_floor(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fma(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fma( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z))); } METAL_FUNC bfloat16_t fmax(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmax3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmedian3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmin3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fmod(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmod(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t fract(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fract(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t frexp(bfloat16_t x, thread int& exp) { return static_cast<bfloat16_t>(__metal_frexp(static_cast<float>(x), &exp)); } METAL_FUNC bfloat16_t ldexp(bfloat16_t x, int k) { return static_cast<bfloat16_t>(__metal_ldexp(static_cast<float>(x), k, __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t log(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t log10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log10(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t log2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log2(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t max(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t max3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t median3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t min(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t min3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t nextafter(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_nextafter(static_cast<float>(x), static_cast<float>(y))); } METAL_FUNC bfloat16_t pow(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_pow(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t powr(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_powr(static_cast<float>(x), static_cast<float>(y), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t rint(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rint(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t round(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_round(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t rsqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rsqrt(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sin(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sinpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinpi(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t sqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sqrt(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t tan(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tan(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t tanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanh(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t tanpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanpi(static_cast<float>(x), __METAL_FAST_MATH__)); } METAL_FUNC bfloat16_t trunc(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_trunc(static_cast<float>(x), __METAL_FAST_MATH__)); };\n}\nnamespace precise {\nMETAL_FUNC bfloat16_t abs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t acos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acos(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t acosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_acosh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t asin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asin(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t asinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_asinh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t atan(bfloat16_t y_over_x) { return static_cast<bfloat16_t>( __metal_atan(static_cast<float>(y_over_x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t atan2(bfloat16_t y, bfloat16_t x) { return static_cast<bfloat16_t>( __metal_atan2(static_cast<float>(y), static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t atanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_atanh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t ceil(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_ceil(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t cos(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cos(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t cosh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cosh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t cospi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_cospi(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t divide(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_divide(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t exp(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t exp10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp10(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t exp2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_exp2(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fabs(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fabs(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fdim(bfloat16_t x, bfloat16_t y) { float t = static_cast<float>(x - y); return static_cast<bfloat16_t>(select(t, float(0), t < float(0) || x == y)); } METAL_FUNC bfloat16_t floor(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_floor(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fma(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fma( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z))); } METAL_FUNC bfloat16_t fmax(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmax3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmedian3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmin(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmin3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fmod(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmod(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t fract(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_fract(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t frexp(bfloat16_t x, thread int& exp) { return static_cast<bfloat16_t>(__metal_frexp(static_cast<float>(x), &exp)); } METAL_FUNC bfloat16_t ldexp(bfloat16_t x, int k) { return static_cast<bfloat16_t>(__metal_ldexp(static_cast<float>(x), k, __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t log(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t log10(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log10(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t log2(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_log2(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t max(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmax(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t max3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmax3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t median3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmedian3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t min(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_fmin(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t min3(bfloat16_t x, bfloat16_t y, bfloat16_t z) { return static_cast<bfloat16_t>(__metal_fmin3( static_cast<float>(x), static_cast<float>(y), static_cast<float>(z), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t nextafter(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_nextafter(static_cast<float>(x), static_cast<float>(y))); } METAL_FUNC bfloat16_t pow(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_pow(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t powr(bfloat16_t x, bfloat16_t y) { return static_cast<bfloat16_t>( __metal_powr(static_cast<float>(x), static_cast<float>(y), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t rint(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rint(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t round(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_round(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t rsqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_rsqrt(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sin(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sin(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sinh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sinpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sinpi(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t sqrt(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_sqrt(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t tan(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tan(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t tanh(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanh(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t tanpi(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_tanpi(static_cast<float>(x), __METAL_PRECISE_MATH__)); } METAL_FUNC bfloat16_t trunc(bfloat16_t x) { return static_cast<bfloat16_t>(__metal_trunc(static_cast<float>(x), __METAL_PRECISE_MATH__)); };\n}\n}\nnamespace metal {\nMETAL_FUNC bfloat16_t simd_broadcast(bfloat16_t data, ushort broadcast_lane_id) { return uint16_to_bfloat16( __metal_simd_broadcast(bfloat16_to_uint16(data), broadcast_lane_id)); } METAL_FUNC bfloat16_t simd_shuffle(bfloat16_t data, ushort simd_lane_id) { return uint16_to_bfloat16( __metal_simd_shuffle(bfloat16_to_uint16(data), simd_lane_id)); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_down( bfloat16_t data, bfloat16_t filling_data, ushort delta, ushort modulo) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_down( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, modulo)); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_down( bfloat16_t data, bfloat16_t filling_data, ushort delta) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_down( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, __metal_get_simdgroup_size(ushort()))); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_up( bfloat16_t data, bfloat16_t filling_data, ushort delta, ushort modulo) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_up( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, modulo)); } METAL_FUNC bfloat16_t simd_shuffle_and_fill_up( bfloat16_t data, bfloat16_t filling_data, ushort delta) { return uint16_to_bfloat16(__metal_simd_shuffle_and_fill_up( bfloat16_to_uint16(data), bfloat16_to_uint16(filling_data), delta, __metal_get_simdgroup_size(ushort()))); } METAL_FUNC bfloat16_t simd_shuffle_down(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_down(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_rotate_down(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_rotate_down(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_rotate_up(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_rotate_up(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_up(bfloat16_t data, ushort delta) { return uint16_to_bfloat16( __metal_simd_shuffle_up(bfloat16_to_uint16(data), delta)); } METAL_FUNC bfloat16_t simd_shuffle_xor(bfloat16_t data, ushort mask) { return uint16_to_bfloat16( __metal_simd_shuffle_xor(bfloat16_to_uint16(data), mask)); };\nMETAL_FUNC bfloat16_t simd_max(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_max(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_min(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_min(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_exclusive_product(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_exclusive_product(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_exclusive_sum(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_exclusive_sum(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_inclusive_product(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_inclusive_product(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_prefix_inclusive_sum(bfloat16_t data) { return static_cast<bfloat16_t>( __metal_simd_prefix_inclusive_sum(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_product(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_product(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_sum(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_sum(static_cast<float>(data))); } METAL_FUNC bfloat16_t simd_xor(bfloat16_t data) { return static_cast<bfloat16_t>(__metal_simd_xor(static_cast<float>(data))); };\n}\nusing namespace metal;\nstruct complex64_t;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_to_complex64 =\n    !is_same_v<T, complex64_t> && is_convertible_v<T, float>;\ntemplate <typename T>\nstatic constexpr constant bool can_convert_from_complex64 =\n    !is_same_v<T, complex64_t> &&\n    (is_convertible_v<float, T> || is_convertible_v<bfloat16_t, T>);\nstruct complex64_t {\n  float real;\n  float imag;\n  constexpr complex64_t(float real, float imag) : real(real), imag(imag) {};\n  constexpr complex64_t() : real(0), imag(0) {};\n  constexpr complex64_t() threadgroup : real(0), imag(0) {};\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) thread : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) threadgroup : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) device : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_to_complex64<T>>::type>\n  constexpr complex64_t(T x) constant : real(x), imag(0) {}\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const thread {\n    return static_cast<T>(real);\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const threadgroup {\n    return static_cast<T>(real);\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const device {\n    return static_cast<T>(real);\n  }\n  template <\n      typename T,\n      typename = typename enable_if<can_convert_from_complex64<T>>::type>\n  constexpr operator T() const constant {\n    return static_cast<T>(real);\n  }\n};\nconstexpr complex64_t operator-(complex64_t x) {\n  return {-x.real, -x.imag};\n}\nconstexpr bool operator>=(complex64_t a, complex64_t b) {\n  return (a.real > b.real) || (a.real == b.real && a.imag >= b.imag);\n}\nconstexpr bool operator>(complex64_t a, complex64_t b) {\n  return (a.real > b.real) || (a.real == b.real && a.imag > b.imag);\n}\nconstexpr bool operator<=(complex64_t a, complex64_t b) {\n  return operator>=(b, a);\n}\nconstexpr bool operator<(complex64_t a, complex64_t b) {\n  return operator>(b, a);\n}\nconstexpr bool operator==(complex64_t a, complex64_t b) {\n  return a.real == b.real && a.imag == b.imag;\n}\nconstexpr complex64_t operator+(complex64_t a, complex64_t b) {\n  return {a.real + b.real, a.imag + b.imag};\n}\nconstexpr complex64_t operator-(complex64_t a, complex64_t b) {\n  return {a.real - b.real, a.imag - b.imag};\n}\nconstexpr complex64_t operator*(complex64_t a, complex64_t b) {\n  return {a.real * b.real - a.imag * b.imag, a.real * b.imag + a.imag * b.real};\n}\nconstexpr complex64_t operator/(complex64_t a, complex64_t b) {\n  auto denom = b.real * b.real + b.imag * b.imag;\n  auto x = a.real * b.real + a.imag * b.imag;\n  auto y = a.imag * b.real - a.real * b.imag;\n  return {x / denom, y / denom};\n}\nconstexpr complex64_t operator%(complex64_t a, complex64_t b) {\n  auto real = a.real - (b.real * static_cast<int64_t>(a.real / b.real));\n  auto imag = a.imag - (b.imag * static_cast<int64_t>(a.imag / b.imag));\n  if (real != 0 && (real < 0 != b.real < 0)) {\n    real += b.real;\n  }\n  if (imag != 0 && (imag < 0 != b.imag < 0)) {\n    imag += b.imag;\n  }\n  return {real, imag};\n}\nstatic constant constexpr int MAX_REDUCE_SPECIALIZED_DIMS = 4;\nstatic constant constexpr int REDUCE_N_READS = 4;\nstatic constant constexpr int REDUCE_N_WRITES = 4;\nstatic constant constexpr int SOFTMAX_N_READS = 4;\nstatic constant constexpr int RMS_N_READS = 4;\nstatic constant constexpr int RMS_LOOPED_LIMIT = 4096;\n\ntypedef half float16_t;\ntemplate <typename U>\nstruct Limits {\n  static const constant U max = metal::numeric_limits<U>::max();\n  static const constant U min = metal::numeric_limits<U>::min();\n  static const constant U finite_max = metal::numeric_limits<U>::max();\n  static const constant U finite_min = metal::numeric_limits<U>::min();\n};\ntemplate <> struct Limits<uint8_t> { static constexpr constant uint8_t max = metal::numeric_limits<uint8_t>::max(); static constexpr constant uint8_t min = metal::numeric_limits<uint8_t>::min(); static constexpr constant uint8_t finite_max = metal::numeric_limits<uint8_t>::max(); static constexpr constant uint8_t finite_min = metal::numeric_limits<uint8_t>::min(); };;\ntemplate <> struct Limits<uint16_t> { static constexpr constant uint16_t max = metal::numeric_limits<uint16_t>::max(); static constexpr constant uint16_t min = metal::numeric_limits<uint16_t>::min(); static constexpr constant uint16_t finite_max = metal::numeric_limits<uint16_t>::max(); static constexpr constant uint16_t finite_min = metal::numeric_limits<uint16_t>::min(); };;\ntemplate <> struct Limits<uint32_t> { static constexpr constant uint32_t max = metal::numeric_limits<uint32_t>::max(); static constexpr constant uint32_t min = metal::numeric_limits<uint32_t>::min(); static constexpr constant uint32_t finite_max = metal::numeric_limits<uint32_t>::max(); static constexpr constant uint32_t finite_min = metal::numeric_limits<uint32_t>::min(); };;\ntemplate <> struct Limits<uint64_t> { static constexpr constant uint64_t max = metal::numeric_limits<uint64_t>::max(); static constexpr constant uint64_t min = metal::numeric_limits<uint64_t>::min(); static constexpr constant uint64_t finite_max = metal::numeric_limits<uint64_t>::max(); static constexpr constant uint64_t finite_min = metal::numeric_limits<uint64_t>::min(); };;\ntemplate <> struct Limits<int8_t> { static constexpr constant int8_t max = metal::numeric_limits<int8_t>::max(); static constexpr constant int8_t min = metal::numeric_limits<int8_t>::min(); static constexpr constant int8_t finite_max = metal::numeric_limits<int8_t>::max(); static constexpr constant int8_t finite_min = metal::numeric_limits<int8_t>::min(); };;\ntemplate <> struct Limits<int16_t> { static constexpr constant int16_t max = metal::numeric_limits<int16_t>::max(); static constexpr constant int16_t min = metal::numeric_limits<int16_t>::min(); static constexpr constant int16_t finite_max = metal::numeric_limits<int16_t>::max(); static constexpr constant int16_t finite_min = metal::numeric_limits<int16_t>::min(); };;\ntemplate <> struct Limits<int32_t> { static constexpr constant int32_t max = metal::numeric_limits<int32_t>::max(); static constexpr constant int32_t min = metal::numeric_limits<int32_t>::min(); static constexpr constant int32_t finite_max = metal::numeric_limits<int32_t>::max(); static constexpr constant int32_t finite_min = metal::numeric_limits<int32_t>::min(); };;\ntemplate <> struct Limits<int64_t> { static constexpr constant int64_t max = metal::numeric_limits<int64_t>::max(); static constexpr constant int64_t min = metal::numeric_limits<int64_t>::min(); static constexpr constant int64_t finite_max = metal::numeric_limits<int64_t>::max(); static constexpr constant int64_t finite_min = metal::numeric_limits<int64_t>::min(); };;\ntemplate <> struct Limits<half> { static constexpr constant half max = metal::numeric_limits<half>::infinity(); static constexpr constant half min = -metal::numeric_limits<half>::infinity(); static constexpr constant half finite_max = metal::numeric_limits<half>::max(); static constexpr constant half finite_min = -metal::numeric_limits<half>::max(); };;\ntemplate <> struct Limits<float> { static constexpr constant float max = metal::numeric_limits<float>::infinity(); static constexpr constant float min = -metal::numeric_limits<float>::infinity(); static constexpr constant float finite_max = metal::numeric_limits<float>::max(); static constexpr constant float finite_min = -metal::numeric_limits<float>::max(); };;\ntemplate <> struct Limits<bfloat16_t> { static constexpr constant bfloat16_t max = metal::numeric_limits<bfloat16_t>::infinity(); static constexpr constant bfloat16_t min = -metal::numeric_limits<bfloat16_t>::infinity(); static constexpr constant bfloat16_t finite_max = metal::numeric_limits<bfloat16_t>::max(); static constexpr constant bfloat16_t finite_min = -metal::numeric_limits<bfloat16_t>::max(); };;\ntemplate <>\nstruct Limits<bool> {\n  static constexpr constant bool max = true;\n  static constexpr constant bool min = false;\n};\ntemplate <>\nstruct Limits<complex64_t> {\n  static constexpr constant complex64_t max = complex64_t(\n      metal::numeric_limits<float>::infinity(),\n      metal::numeric_limits<float>::infinity());\n  static constexpr constant complex64_t min = complex64_t(\n      -metal::numeric_limits<float>::infinity(),\n      -metal::numeric_limits<float>::infinity());\n};\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc(\n    uint elem,\n    constant const int* shape,\n    constant const StrideT* strides,\n    int ndim) {\n  IdxT loc = 0;\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    loc += (elem % shape[i]) * IdxT(strides[i]);\n    elem /= shape[i];\n  }\n  return loc;\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc(\n    StrideT elem,\n    constant const int* shape,\n    constant const StrideT* strides,\n    int ndim) {\n  IdxT loc = 0;\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    loc += (elem % shape[i]) * IdxT(strides[i]);\n    elem /= shape[i];\n  }\n  return loc;\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc(\n    uint3 elem,\n    constant const int* shape,\n    constant const StrideT* strides,\n    int ndim) {\n  IdxT loc =\n      elem.x * IdxT(strides[ndim - 1]) + elem.y * IdxT(strides[ndim - 2]);\n  for (int d = ndim - 3; d >= 0; --d) {\n    loc += (elem.z % shape[d]) * IdxT(strides[d]);\n    elem.z /= shape[d];\n  }\n  return loc;\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc_1(uint elem, constant const StrideT& stride) {\n  return elem * IdxT(stride);\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc_2(uint2 elem, constant const StrideT strides[2]) {\n  return elem.x * IdxT(strides[1]) + elem.y * IdxT(strides[0]);\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC IdxT elem_to_loc_3(uint3 elem, constant const StrideT strides[3]) {\n  return elem.x * IdxT(strides[2]) + elem.y * IdxT(strides[1]) +\n      elem.z * IdxT(strides[0]);\n}\ntemplate <typename StrideT, typename IdxT = StrideT>\nMETAL_FUNC vec<IdxT, 2> elem_to_loc_2_nd(\n    uint3 elem,\n    constant const int* shape,\n    constant const StrideT* a_strides,\n    constant const StrideT* b_strides,\n    int ndim) {\n  vec<IdxT, 2> loc = {\n      IdxT(\n          elem.x * IdxT(a_strides[ndim - 1]) +\n          IdxT(elem.y) * IdxT(a_strides[ndim - 2])),\n      IdxT(\n          elem.x * IdxT(b_strides[ndim - 1]) +\n          elem.y * IdxT(b_strides[ndim - 2]))};\n  for (int d = ndim - 3; d >= 0; --d) {\n    uint l = elem.z % shape[d];\n    loc.x += l * IdxT(a_strides[d]);\n    loc.y += l * IdxT(b_strides[d]);\n    elem.z /= shape[d];\n  }\n  return loc;\n}\ntemplate <typename IdxT = size_t>\nMETAL_FUNC vec<IdxT, 3> elem_to_loc_3_nd(\n    uint3 elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const size_t* c_strides,\n    int ndim) {\n  vec<IdxT, 3> loc = {\n      elem.x * IdxT(a_strides[ndim - 1]) + elem.y * IdxT(a_strides[ndim - 2]),\n      elem.x * IdxT(b_strides[ndim - 1]) + elem.y * IdxT(b_strides[ndim - 2]),\n      elem.x * IdxT(c_strides[ndim - 1]) + elem.y * IdxT(c_strides[ndim - 2])};\n  for (int d = ndim - 3; d >= 0; --d) {\n    uint l = elem.z % shape[d];\n    loc.x += l * IdxT(a_strides[d]);\n    loc.y += l * IdxT(b_strides[d]);\n    loc.z += l * IdxT(c_strides[d]);\n    elem.z /= shape[d];\n  }\n  return loc;\n}\ntemplate <int DIM, typename OffsetT = size_t, bool General = true>\nstruct LoopedElemToLoc {\n  int dim;\n  LoopedElemToLoc<DIM - 1, OffsetT, General> inner_looper;\n  OffsetT offset{0};\n  int index{0};\n  LoopedElemToLoc(int dim) : dim(dim), inner_looper(dim - 1) {}\n  void next(const constant int* shape, const constant size_t* strides) {\n    if (dim == 0) {\n      return;\n    }\n    index++;\n    offset += OffsetT(strides[dim - 1]);\n    if (index >= shape[dim - 1]) {\n      index = 0;\n      inner_looper.next(shape, strides);\n      offset = inner_looper.offset;\n    }\n  }\n  void next(int n, const constant int* shape, const constant size_t* strides) {\n    if (dim == 0) {\n      return;\n    }\n    index += n;\n    offset += n * OffsetT(strides[dim - 1]);\n    if (index >= shape[dim - 1]) {\n      int extra = index - shape[dim - 1];\n      if (extra >= shape[dim - 1]) {\n        inner_looper.next(1 + extra / shape[dim - 1], shape, strides);\n        extra = extra % shape[dim - 1];\n      } else {\n        inner_looper.next(shape, strides);\n      }\n      index = 0;\n      offset = inner_looper.offset;\n      if (extra > 0) {\n        next(extra, shape, strides);\n      }\n    }\n  }\n  OffsetT location() {\n    return offset;\n  }\n};\ntemplate <typename OffsetT>\nstruct LoopedElemToLoc<1, OffsetT, true> {\n  int dim;\n  OffsetT offset{0};\n  uint index{0};\n  LoopedElemToLoc(int dim) : dim(dim) {}\n  void next(const constant int* shape, const constant size_t* strides) {\n    index++;\n    if (dim > 1) {\n      offset = elem_to_loc<size_t, OffsetT>(index, shape, strides, dim);\n    } else {\n      offset += OffsetT(strides[0]);\n    }\n  }\n  void next(int n, const constant int* shape, const constant size_t* strides) {\n    index += n;\n    if (dim > 1) {\n      offset = elem_to_loc<size_t, OffsetT>(index, shape, strides, dim);\n    } else {\n      offset = index * OffsetT(strides[0]);\n    }\n  }\n  OffsetT location() {\n    return offset;\n  }\n};\ntemplate <typename OffsetT>\nstruct LoopedElemToLoc<1, OffsetT, false> {\n  OffsetT offset{0};\n  LoopedElemToLoc(int) {}\n  void next(const constant int*, const constant size_t* strides) {\n    offset += OffsetT(strides[0]);\n  }\n  void next(int n, const constant int*, const constant size_t* strides) {\n    offset += n * OffsetT(strides[0]);\n  }\n  OffsetT location() {\n    return offset;\n  }\n};\ntemplate <typename T, typename U>\ninline T ceildiv(T N, U M) {\n  return (N + M - 1) / M;\n}\ninline float log1p(float x) {\n  float xp1 = 1.0f + x;\n  if (xp1 == Limits<float>::max) {\n    return Limits<float>::max;\n  }\n  if (xp1 == 1.0f) {\n    return x;\n  }\n  return x * (metal::log(xp1) / (xp1 - 1.0f));\n}\ninline bfloat16_t log1p(bfloat16_t x) {\n  float xp1 = 1.0f + static_cast<float>(x);\n  if (xp1 == Limits<float>::max) {\n    return Limits<bfloat16_t>::max;\n  }\n  if (xp1 == 1.0f) {\n    return x;\n  }\n  return bfloat16_t(x * (metal::log(xp1) / (xp1 - 1.0f)));\n}\ninline uint64_t simd_shuffle_down(uint64_t data, uint16_t delta) {\n  return as_type<uint64_t>(\n      metal::simd_shuffle_down(as_type<uint2>(data), delta));\n}\ninline int64_t simd_shuffle_down(int64_t data, uint16_t delta) {\n  return as_type<int64_t>(\n      metal::simd_shuffle_down(as_type<uint2>(data), delta));\n}\ninline bool simd_shuffle_down(bool data, uint16_t delta) {\n  return simd_shuffle_down(static_cast<uint32_t>(data), delta);\n}\ninline complex64_t simd_shuffle_down(complex64_t data, uint16_t delta) {\n  return complex64_t(\n      simd_shuffle_down(data.real, delta), simd_shuffle_down(data.imag, delta));\n}\ninline uint64_t simd_shuffle_up(uint64_t data, uint16_t delta) {\n  return as_type<uint64_t>(metal::simd_shuffle_up(as_type<uint2>(data), delta));\n}\ninline int64_t simd_shuffle_up(int64_t data, uint16_t delta) {\n  return as_type<int64_t>(metal::simd_shuffle_up(as_type<uint2>(data), delta));\n}\ninline bool simd_shuffle_up(bool data, uint16_t delta) {\n  return simd_shuffle_up(static_cast<uint32_t>(data), delta);\n}\ninline complex64_t simd_shuffle_up(complex64_t data, uint16_t delta) {\n  return complex64_t(\n      simd_shuffle_up(data.real, delta), simd_shuffle_up(data.imag, delta));\n}\ninline uint64_t\nsimd_shuffle_and_fill_up(uint64_t data, uint64_t filling, uint16_t delta) {\n  return as_type<uint64_t>(metal::simd_shuffle_and_fill_up(\n      as_type<uint2>(data), as_type<uint2>(filling), delta));\n}\ninline int64_t\nsimd_shuffle_and_fill_up(int64_t data, int64_t filling, uint16_t delta) {\n  return as_type<int64_t>(metal::simd_shuffle_and_fill_up(\n      as_type<uint2>(data), as_type<uint2>(filling), delta));\n}\ninline bool simd_shuffle_and_fill_up(bool data, bool filling, uint16_t delta) {\n  return simd_shuffle_and_fill_up(\n      static_cast<uint32_t>(data), static_cast<uint32_t>(filling), delta);\n}\ninline complex64_t simd_shuffle_and_fill_up(\n    complex64_t data,\n    complex64_t filling,\n    uint16_t delta) {\n  return complex64_t(\n      simd_shuffle_and_fill_up(data.real, filling.real, delta),\n      simd_shuffle_and_fill_up(data.imag, filling.imag, delta));\n}\ninline uint64_t simd_shuffle(uint64_t data, uint16_t lane) {\n  return as_type<uint64_t>(metal::simd_shuffle(as_type<uint2>(data), lane));\n}\ninline int64_t simd_shuffle(int64_t data, uint16_t lane) {\n  return as_type<int64_t>(metal::simd_shuffle(as_type<uint2>(data), lane));\n}\ninline bool simd_shuffle(bool data, uint16_t lane) {\n  return simd_shuffle(static_cast<uint32_t>(data), lane);\n}\ninline complex64_t simd_shuffle(complex64_t data, uint16_t lane) {\n  return complex64_t(\n      simd_shuffle(data.real, lane), simd_shuffle(data.imag, lane));\n}\n"
+ "\nMETAL_FUNC ulong2 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n  }\n  return ulong2(loc_a, loc_b);\n}\nMETAL_FUNC ulong3 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const size_t* c_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  ulong loc_c{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n    loc_c += pos_in_dim * c_strides[i];\n  }\n  return ulong3(loc_a, loc_b, loc_c);\n}\n\ntemplate <int NDIM>\nstruct MLXConvParams {\n  const int N;\n  const int C;\n  const int O;\n  const int iS[NDIM];\n  const int wS[NDIM];\n  const int oS[NDIM];\n  const int str[NDIM];\n  const int pad[NDIM];\n  const int kdil[NDIM];\n  const int idil[NDIM];\n  const size_t in_strides[NDIM + 2];\n  const size_t wt_strides[NDIM + 2];\n  const size_t out_strides[NDIM + 2];\n  const int groups;\n  const bool flip;\n};\nnamespace mlx {\nnamespace steel {\nstruct ImplicitGemmConv2DParams {\n  const int M;\n  const int N;\n  const int K;\n  const int gemm_k_iterations;\n  const int inp_jump_w;\n  const int inp_jump_h;\n  const int inp_jump_c;\n  const int tiles_n;\n  const int tiles_m;\n  const int swizzle_log;\n};\nstruct Conv2DGeneralJumpParams {\n  const int f_wgt_jump_h;\n  const int f_wgt_jump_w;\n  const int f_out_jump_h;\n  const int f_out_jump_w;\n  const int adj_out_h;\n  const int adj_out_w;\n  const int adj_out_hw;\n  const int adj_implicit_m;\n};\nstruct Conv2DGeneralBaseInfo {\n  int weight_base;\n  int weight_size;\n};\n}\n}\nnamespace mlx {\nnamespace steel {\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderLargeFilter {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant ImplicitGemmConv2DParams* gemm_params;\n  short weight_h;\n  short weight_w;\n  const device T* src[n_rows];\n  int read_n[n_rows];\n  int read_ih[n_rows];\n  int read_iw[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderLargeFilter(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        gemm_params(gemm_params_),\n        weight_h(0),\n        weight_w(0) {\n    int out_n_pixels = params->oS[0] * params->oS[1];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / out_n_pixels;\n      int hw = offset_nhw % out_n_pixels;\n      int oh = hw / params->oS[1];\n      int ow = hw % params->oS[1];\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n      if (params->flip) {\n        ih += (params->wS[0] - 1) * params->kdil[0];\n        iw += (params->wS[1] - 1) * params->kdil[1];\n      }\n      src[i] = src_ + n * params->in_strides[0] + ih * params->in_strides[1] +\n          iw * params->in_strides[2] + bj;\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      int n = read_n[i];\n      int ih = read_ih[i] + weight_h * params->kdil[0];\n      int iw = read_iw[i] + weight_w * params->kdil[1];\n      if ((n < params->N) && (ih >= 0 && ih < params->iS[0]) &&\n          (iw >= 0 && iw < params->iS[1])) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = src[i][j];\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    if (++weight_w < params->wS[1]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_w;\n      }\n      return;\n    }\n    weight_w = 0;\n    if (++weight_h < params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_h;\n      }\n      return;\n    }\n    weight_h = 0;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; i++) {\n      src[i] += gemm_params->inp_jump_c;\n    }\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderSmallFilter {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  using mask_t = short;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant ImplicitGemmConv2DParams* gemm_params;\n  short weight_h;\n  short weight_w;\n  const device T* src[n_rows];\n  mask_t mask_h[n_rows];\n  mask_t mask_w[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderSmallFilter(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        gemm_params(gemm_params_),\n        weight_h(0),\n        weight_w(0) {\n    int out_n_pixels = params->oS[0] * params->oS[1];\n    int read_n[n_rows];\n    int read_ih[n_rows];\n    int read_iw[n_rows];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / out_n_pixels;\n      int hw = offset_nhw % out_n_pixels;\n      int oh = hw / params->oS[1];\n      int ow = hw % params->oS[1];\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n      if (params->flip) {\n        ih += (params->wS[0] - 1) * params->kdil[0];\n        iw += (params->wS[1] - 1) * params->kdil[1];\n      }\n      src[i] = src_ + n * params->in_strides[0] + ih * params->in_strides[1] +\n          iw * params->in_strides[2] + bj;\n    }\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      mask_h[i] = 0;\n      mask_w[i] = 0;\n    }\n    for (short kh = 0; kh < params->wS[0]; kh++) {\n      short flip_h = params->flip ? params->wS[0] - kh - 1 : kh;\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; ++i) {\n        int n = read_n[i];\n        int ih = read_ih[i] + flip_h * params->kdil[0];\n        bool in_bounds = n < params->N && ih >= 0 && ih < params->iS[0];\n        mask_h[i] |= (in_bounds << kh);\n      }\n    }\n    for (short kw = 0; kw < params->wS[1]; kw++) {\n      short flip_w = params->flip ? params->wS[1] - kw - 1 : kw;\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; ++i) {\n        int iw = read_iw[i] + flip_w * params->kdil[1];\n        bool in_bounds = iw >= 0 && iw < params->iS[1];\n        mask_w[i] |= (in_bounds << kw);\n      }\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n    mask_t h_mask = mask_t(1) << weight_h;\n    mask_t w_mask = mask_t(1) << weight_w;\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      if ((mask_h[i] & h_mask) && (mask_w[i] & w_mask)) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = src[i][j];\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    if (++weight_w < params->wS[1]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_w;\n      }\n      return;\n    }\n    weight_w = 0;\n    if (++weight_h < params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < n_rows; i++) {\n        src[i] += gemm_params->inp_jump_h;\n      }\n      return;\n    }\n    weight_h = 0;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; i++) {\n      src[i] += gemm_params->inp_jump_c;\n    }\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DWeightBlockLoader {\n  static constant constexpr const short BROWS = BN;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size =\n      (BN == 8) ? 1 : (tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4);\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const int src_ld;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  const constant MLXConvParams<2>* params;\n  int weight_hw;\n  const int read_n;\n  const bool do_read;\n  METAL_FUNC Conv2DWeightBlockLoader(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(params_->wt_strides[0]),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld + bj),\n        params(params_),\n        weight_hw(0),\n        read_n(offsets.y + bi),\n        do_read(read_n + n_rows * TROWS <= gemm_params_->N) {}\n  METAL_FUNC void load_unsafe() const {\n    if (BN != 8 || do_read) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BN; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = src[i * src_ld + j];\n        }\n      }\n    } else {\n      for (short i = 0; i < BN; i += TROWS) {\n        if ((read_n + i) < params->O) {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = src[i * src_ld + j];\n          }\n        } else {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    if (++weight_hw < (params->wS[1] * params->wS[0])) {\n      src += params->wt_strides[2];\n      return;\n    }\n    weight_hw = 0;\n    src += BK - (params->wS[1] * params->wS[0] - 1) * params->wt_strides[2];\n  }\n};\n}\n}\nnamespace mlx {\nnamespace steel {\ntemplate <short n_channels_>\nstruct ChannelHelper {\n  static constant constexpr const short n_channels = n_channels_;\n  static constant constexpr const short vec_size = n_channels_ <= 4 ? 4 : 8;\n  static constant constexpr const short excess = vec_size - n_channels_;\n};\ntemplate <>\nstruct ChannelHelper<1> {\n  static constant constexpr const short n_channels = 1;\n  static constant constexpr const short vec_size = 1;\n  static constant constexpr const short excess = 0;\n};\ntemplate <>\nstruct ChannelHelper<2> {\n  static constant constexpr const short n_channels = 2;\n  static constant constexpr const short vec_size = 2;\n  static constant constexpr const short excess = 0;\n};\ntemplate <>\nstruct ChannelHelper<3> {\n  static constant constexpr const short n_channels = 3;\n  static constant constexpr const short vec_size = 4;\n  static constant constexpr const short excess = 1;\n};\ntemplate <>\nstruct ChannelHelper<4> {\n  static constant constexpr const short n_channels = 4;\n  static constant constexpr const short vec_size = 4;\n  static constant constexpr const short excess = 0;\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short n_channels,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderSmallChannels {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = ChannelHelper<n_channels>::vec_size;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant ImplicitGemmConv2DParams* gemm_params;\n  short weight_hw;\n  const device T* src[n_rows];\n  int read_n[n_rows];\n  int read_ih[n_rows];\n  int read_iw[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderSmallChannels(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        gemm_params(gemm_params_),\n        weight_hw(thread_idx % TCOLS) {\n    int out_n_pixels = params->oS[0] * params->oS[1];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / out_n_pixels;\n      int hw = offset_nhw % out_n_pixels;\n      int oh = hw / params->oS[1];\n      int ow = hw % params->oS[1];\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      src[i] = src_ + n * params->in_strides[0] + ih * params->in_strides[1] +\n          iw * params->in_strides[2];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n    if (weight_hw >= params->wS[1] * params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n      return;\n    }\n    int wh = (weight_hw / params->wS[1]);\n    int ww = (weight_hw % params->wS[1]);\n    int flip_h = params->flip ? params->wS[0] - wh - 1 : wh;\n    int flip_w = params->flip ? params->wS[1] - ww - 1 : ww;\n    int weight_h = flip_h * params->kdil[0];\n    int weight_w = flip_w * params->kdil[1];\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      int n = read_n[i];\n      int ih = read_ih[i] + weight_h;\n      int iw = read_iw[i] + weight_w;\n      if ((n < params->N) && (ih >= 0 && ih < params->iS[0]) &&\n          (iw >= 0 && iw < params->iS[1])) {\n        const device T* curr_src = src[i] + weight_h * params->in_strides[1] +\n            weight_w * params->in_strides[2];\n#pragma clang loop unroll(full)\n        for (short j = 0; j < n_channels; ++j) {\n          dst[is * dst_ld + j] = curr_src[j];\n        }\n#pragma clang loop unroll(full)\n        for (short j = n_channels; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    weight_hw += TCOLS;\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short n_channels,\n    short tgp_padding = 0>\nstruct Conv2DWeightBlockLoaderSmallChannels {\n  static constant constexpr const short BROWS = BN;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = ChannelHelper<n_channels>::vec_size;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const int src_ld;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  const constant MLXConvParams<2>* params;\n  int weight_hw;\n  const int read_n;\n  const bool do_read;\n  METAL_FUNC Conv2DWeightBlockLoaderSmallChannels(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant ImplicitGemmConv2DParams* gemm_params_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(params_->wt_strides[0]),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld),\n        params(params_),\n        weight_hw(thread_idx % TCOLS),\n        read_n(offsets.y + bi),\n        do_read(read_n + BN <= gemm_params_->N) {}\n  METAL_FUNC void load_unsafe() const {\n    if (bi >= BROWS || bj >= BCOLS)\n      return;\n    if (read_n >= params->O || weight_hw >= params->wS[1] * params->wS[0]) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n      return;\n    }\n    const device T* curr_src = src + weight_hw * params->wt_strides[2];\n    if (BN != 8 || do_read) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < n_channels; j++) {\n          dst[i * dst_ld + j] = curr_src[i * src_ld + j];\n        }\n#pragma clang loop unroll(full)\n        for (short j = n_channels; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n    } else {\n      for (short i = 0; i < BROWS; i += TROWS) {\n        if (((read_n + i) < params->O)) {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < n_channels; j++) {\n            dst[i * dst_ld + j] = curr_src[i * src_ld + j];\n          }\n#pragma clang loop unroll(full)\n          for (short j = n_channels; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        } else {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    weight_hw += TCOLS;\n  }\n};\n}\n}\n\nnamespace mlx {\nnamespace steel {\ntemplate <typename OutT, typename InT>\nstruct TransformNone {\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT) {\n    return static_cast<OutT>(x);\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAdd {\n  TransformAdd(const float, const float) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT c) {\n    return static_cast<OutT>(x) + c;\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAxpby {\n  const float alpha;\n  const float beta;\n  TransformAxpby(const float alpha_, const float beta_)\n      : alpha(alpha_), beta(beta_) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  METAL_FUNC OutT apply(InT x, OutT c) const {\n    return static_cast<OutT>(x * alpha + (beta * c));\n  }\n};\ntemplate <typename T>\nstruct AccumHelper {\n  typedef float accum_type;\n};\nstruct BlockSwizzle {\n  static METAL_FUNC int2\n  swizzle(uint3 tid [[threadgroup_position_in_grid]], const int swizzle_log) {\n    const int tid_x = (tid.x) >> swizzle_log;\n    const int tid_y =\n        ((tid.y) << swizzle_log) + ((tid.x) & ((1 << swizzle_log) - 1));\n    return int2(tid_x, tid_y);\n  }\n};\n}\n}\n#pragma METAL internals : enable\nnamespace metal {\ntemplate <typename T>\nstruct is_empty : metal::bool_constant<__is_empty(T)> {};\ntemplate <typename... Ts>\nstruct make_void {\n  typedef void type;\n};\ntemplate <typename... Ts>\nusing void_t = typename make_void<Ts...>::type;\ntemplate <class T>\nstruct is_static : metal::bool_constant<is_empty<remove_cv_t<T>>::value> {};\ntemplate <typename T>\nstruct pointer_element {};\ntemplate <typename T>\nstruct pointer_element<thread T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<device T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<constant T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<threadgroup T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nusing pointer_element_t = typename pointer_element<remove_cv_t<T>>::type;\n}\n#pragma METAL internals : disable\n\n#pragma METAL internals : enable\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, T v>\nstruct integral_constant {\n  static constexpr constant T value = v;\n  using value_type = T;\n  using type = integral_constant;\n  METAL_FUNC constexpr operator value_type() const noexcept {\n    return value;\n  }\n};\ntemplate <bool B>\nusing bool_constant = integral_constant<bool, B>;\nusing true_type = bool_constant<true>;\nusing false_type = bool_constant<false>;\ntemplate <class T>\nstruct is_integral : bool_constant<metal::is_integral<T>::value> {};\ntemplate <class T, T v>\nstruct is_integral<integral_constant<T, v>>\n    : bool_constant<metal::is_integral<T>::value> {};\ntemplate <typename T>\nconstexpr constant bool is_integral_v = is_integral<T>::value;\ntemplate <int val>\nusing Int = integral_constant<int, val>;\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator+( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv + uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator-( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv - uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator*( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv * uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator/( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv / uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator==( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv == uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator!=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv != uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv < uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv > uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv <= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv >= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator&&( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv && uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator||( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv || uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T>\nMETAL_FUNC constexpr T sum(T x) {\n  return x;\n}\ntemplate <typename T, typename... Us>\nMETAL_FUNC constexpr auto sum(T x, Us... us) {\n  return x + sum(us...);\n}\n}\n}\n#pragma METAL internals : disable\n\nusing namespace metal;\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, int kFragRows_, int kFragCols_>\nstruct BaseMMAFrag {\n  static_assert(\n      kFragRows_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n  static_assert(\n      kFragCols_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n};\ntemplate <typename T>\nstruct BaseMMAFrag<T, 8, 8> {\n  static constant constexpr const int kFragRows = 8;\n  static constant constexpr const int kFragCols = 8;\n  static constant constexpr const int kElemsPerFrag = (kFragRows * kFragCols) / 32;\n  static constant constexpr const int kElemRows = 1;\n  static constant constexpr const int kElemCols = 2;\n  static_assert(\n      kElemRows * kElemCols == kElemsPerFrag,\n      \"MMAFrag shape is not consistent with MMAFrag size\");\n  typedef metal::simdgroup_matrix<T, kFragRows, kFragCols> mat_type;\n  typedef metal::vec<T, kElemsPerFrag> frag_type;\n  METAL_FUNC static constexpr short2 get_coord(ushort simd_lane_id\n                                               [[thread_index_in_simdgroup]]) {\n    const short qid = simd_lane_id / 4;\n    const short fm = (qid & 4) + ((simd_lane_id / 2) % 4);\n    const short fn = (qid & 2) * 2 + (simd_lane_id % 2) * 2;\n    return short2{fn, fm};\n  }\n  template <typename SrcPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  load(thread frag_type& dst, SrcPtrType src, StrX str_x, StrY str_y) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * kElemCols + j] = static_cast<T>(src[i * str_x + j * str_y]);\n      }\n    }\n  }\n  template <\n      typename SrcPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void load_safe(\n      thread frag_type& dst,\n      SrcPtrType src,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[i * kElemCols + j] =\n              static_cast<T>(src[(off_x + i) * str_x + (off_x + j) * str_y]);\n        } else {\n          dst[i * kElemCols + j] = T(0);\n        }\n      }\n    }\n  }\n  template <typename DstPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  store(const thread frag_type& src, DstPtrType dst, StrX str_x, StrY str_y) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * str_x + j * str_y] = static_cast<U>(src[i * kElemCols + j]);\n      }\n    }\n  }\n  template <\n      typename DstPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void store_safe(\n      const thread frag_type& src,\n      DstPtrType dst,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[(off_x + i) * str_x + (off_y + j) * str_y] =\n              static_cast<U>(src[i * kElemCols + j]);\n        }\n      }\n    }\n  }\n  METAL_FUNC static constexpr void mma(\n      thread frag_type& D,\n      thread frag_type& A,\n      thread frag_type& B,\n      thread frag_type& C) {\n    mat_type D_mat;\n    mat_type A_mat;\n    mat_type B_mat;\n    mat_type C_mat;\n    reinterpret_cast<thread frag_type&>(A_mat.thread_elements()) = A;\n    reinterpret_cast<thread frag_type&>(B_mat.thread_elements()) = B;\n    reinterpret_cast<thread frag_type&>(C_mat.thread_elements()) = C;\n    mma(D_mat, A_mat, B_mat, C_mat);\n    D = reinterpret_cast<thread frag_type&>(D_mat.thread_elements());\n  }\n  METAL_FUNC static constexpr void mma(\n      thread mat_type& D,\n      thread mat_type& A,\n      thread mat_type& B,\n      thread mat_type& C) {\n    simdgroup_multiply_accumulate(D, A, B, C);\n  }\n};\ntemplate <\n    typename T,\n    int kTileRows_,\n    int kTileCols_,\n    class MMAFrag_ = BaseMMAFrag<T, 8, 8>>\nstruct MMATile {\n  using MMAFrag_t = MMAFrag_;\n  using elem_type = T;\n  static constant constexpr const int kFragRows = MMAFrag_t::kFragRows;\n  static constant constexpr const int kFragCols = MMAFrag_t::kFragCols;\n  static constant constexpr const int kElemsPerFrag = MMAFrag_t::kElemsPerFrag;\n  static constant constexpr const int kTileRows = kTileRows_;\n  static constant constexpr const int kTileCols = kTileCols_;\n  static constant constexpr const int kRows = kTileRows * kFragRows;\n  static constant constexpr const int kCols = kTileCols * kFragCols;\n  static constant constexpr const int kNumFrags = kTileRows * kTileCols;\n  static constant constexpr const int kElemsPerTile = kNumFrags * kElemsPerFrag;\n  typedef typename MMAFrag_t::mat_type mat_type;\n  typedef typename MMAFrag_t::frag_type frag_type;\n  frag_type val_frags[kNumFrags] = {frag_type(0)};\n  METAL_FUNC MMATile() thread {}\n  METAL_FUNC constexpr void clear() {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kNumFrags; ++i) {\n      val_frags[i] = frag_type(0);\n    }\n  }\n  METAL_FUNC constexpr thread frag_type& frag_at(const short i, const short j) {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC constexpr const thread frag_type& frag_at(\n      const short i,\n      const short j) const {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC mat_type mat_at(const short i, const short j) {\n    mat_type val_mat;\n#pragma clang loop unroll(full)\n    for (short ii = 0; ii < kElemsPerFrag; ++ii) {\n      val_mat.thread_elements()[ii] = frag_at(i, j)[ii];\n    }\n    return val_mat;\n  }\n  METAL_FUNC thread elem_type* elems() {\n    return reinterpret_cast<thread elem_type*>(val_frags);\n  }\n  METAL_FUNC const thread elem_type* elems() const {\n    return reinterpret_cast<const thread elem_type*>(val_frags);\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void load(const threadgroup U* src) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(\n                src[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void store(threadgroup U* dst) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(\n                dst[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void load(const device U* src, const int ld) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(src[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void store(device U* dst, const int ld) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(dst[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  load_safe(const device U* src, const int ld, const short2 src_tile_dims) {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load_safe(\n            frag_at(i, j),\n            src,\n            ld,\n            Int<1>{},\n            src_tile_dims.y,\n            src_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  store_safe(device U* dst, const int ld, const short2 dst_tile_dims) const {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store_safe(\n            frag_at(i, j),\n            dst,\n            ld,\n            Int<1>{},\n            dst_tile_dims.y,\n            dst_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n};\ntemplate <typename T, typename U, int M, int N, int K>\nMETAL_FUNC void tile_matmad(\n    thread MMATile<T, M, N>& D,\n    thread MMATile<U, M, K>& A,\n    thread MMATile<U, K, N>& B,\n    thread MMATile<T, M, N>& C) {\n#pragma clang loop unroll(full)\n  for (short m = 0; m < M; ++m) {\n#pragma clang loop unroll(full)\n    for (short n = 0; n < N; ++n) {\n      short n_serp = (m % 2) ? (N - 1 - n) : n;\n#pragma clang loop unroll(full)\n      for (short k = 0; k < K; ++k) {\n        MMATile<T, M, N>::MMAFrag_t::mma(\n            D.frag_at(m, n_serp),\n            A.frag_at(m, k),\n            B.frag_at(k, n_serp),\n            C.frag_at(m, n_serp));\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    short lda_tgp,\n    short ldb_tgp,\n    typename AccumType = float,\n    typename Epilogue = TransformNone<U, AccumType>>\nstruct BlockMMA {\n  static constant constexpr const short kFragSize = 8;\n  using MMAFrag_acc_t = BaseMMAFrag<AccumType, kFragSize, kFragSize>;\n  static constant constexpr const short TM_stride = kFragSize * WM;\n  static constant constexpr const short TN_stride = kFragSize * WN;\n  static constant constexpr const short TM = BM / (kFragSize * WM);\n  static constant constexpr const short TN = BN / (kFragSize * WN);\n  static constant constexpr const short A_str_m = transpose_a ? 1 : lda_tgp;\n  static constant constexpr const short A_str_k = transpose_a ? lda_tgp : 1;\n  static constant constexpr const short B_str_k = transpose_b ? 1 : ldb_tgp;\n  static constant constexpr const short B_str_n = transpose_b ? ldb_tgp : 1;\n  static constant constexpr const short tile_stride_a = kFragSize * A_str_k;\n  static constant constexpr const short tile_stride_b = kFragSize * B_str_k;\n  MMATile<AccumType, TM, 1, MMAFrag_acc_t> Atile;\n  MMATile<AccumType, 1, TN, MMAFrag_acc_t> Btile;\n  MMATile<AccumType, TM, TN, MMAFrag_acc_t> Ctile;\n  short sm;\n  short sn;\n  short As_offset;\n  short Bs_offset;\n  METAL_FUNC BlockMMA(\n      ushort simd_group_id [[simdgroup_index_in_threadgroup]],\n      ushort simd_lane_id [[thread_index_in_simdgroup]]) {\n    short tm = kFragSize * (simd_group_id / WN);\n    short tn = kFragSize * (simd_group_id % WN);\n    short2 simd_coord = MMAFrag_acc_t::get_coord(simd_lane_id);\n    sm = simd_coord.y;\n    sn = simd_coord.x;\n    As_offset = (tm + sm) * A_str_m + (sn)*A_str_k;\n    Bs_offset = (sm)*B_str_k + (tn + sn) * B_str_n;\n    sm += tm;\n    sn += tn;\n  }\n  METAL_FUNC void mma(const threadgroup T* As, const threadgroup T* Bs) {\n    As += As_offset;\n    Bs += Bs_offset;\n#pragma clang loop unroll(full)\n    for (short kk = 0; kk < BK; kk += kFragSize) {\n      simdgroup_barrier(mem_flags::mem_none);\n      Atile.template load<T, WM, 1, A_str_m, A_str_k>(As);\n      simdgroup_barrier(mem_flags::mem_none);\n      Btile.template load<T, 1, WN, B_str_k, B_str_n>(Bs);\n      simdgroup_barrier(mem_flags::mem_none);\n      tile_matmad(Ctile, Atile, Btile, Ctile);\n      As += tile_stride_a;\n      Bs += tile_stride_b;\n    }\n  }\n  METAL_FUNC void store_result(device U* D, const int ldd) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    Ctile.template store<U, WM, WN>(D, ldd);\n  }\n  METAL_FUNC void\n  store_result_safe(device U* D, const int ldd, short2 dst_tile_dims) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    Ctile.template store_safe<U, WM, WN>(D, ldd, dst_tile_dims);\n  }\n  template <typename UnaryEpilogue>\n  METAL_FUNC void apply_epilogue(thread const UnaryEpilogue& epilogue_op) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = epilogue_op.apply(Ctile.elems()[i]);\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n#pragma clang loop unroll(full)\n        for (short k = 0; k < decltype(Ctile)::kElemsPerFrag; k++) {\n          accum[k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue_safe(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n        U c_elems[kelems] = {0};\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          if ((j * TN_stride + k) < dst_tile_dims.x) {\n            c_elems[k] = C[offset_c + k * fdc];\n          }\n        }\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          accum[k] = epilogue_op.apply(accum[k], c_elems[k]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread const auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          D[offset_d + k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result_safe(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (int i = 0; i < TM; i++) {\n      if (i * TM_stride < dst_tile_dims.y) {\n#pragma clang loop unroll(full)\n        for (int j = 0; j < TN; j++) {\n          thread const auto& accum = Ctile.frag_at(i, j);\n          int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n          int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n          for (short k = 0; k < kelems; k++) {\n            if ((j * TN_stride + k) < dst_tile_dims.x) {\n              D[offset_d + k] =\n                  epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n            }\n          }\n        }\n      }\n    }\n  }\n};\n}\n}\n\nusing namespace metal;\nusing namespace mlx::steel;\n"
+ "\n[[kernel]] void gather{0}_{3}_{6}_{7}(\n    const device {1}* src [[buffer(0)]],\n    device {1}* out [[buffer(1)]],\n    const constant int* src_shape [[buffer(2)]],\n    const constant size_t* src_strides [[buffer(3)]],\n    const constant size_t& src_ndim [[buffer(4)]],\n    const constant int* slice_sizes [[buffer(5)]],\n    const constant int* axes [[buffer(6)]],\n    const constant int* idx_shapes [[buffer(7)]],\n    const constant size_t* idx_strides [[buffer(8)]],\n    const constant bool* idx_contigs [[buffer(9)]],\n    const constant int& idx_ndim [[buffer(10)]],\n    {4}\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {{\n  Indices<{2}, {3}> idxs{{\n    {{ {5} }}, idx_shapes, idx_strides, idx_contigs, idx_ndim}};\n\n  return gather_impl<{1}, {2}, {3}, {6}, {7}>(\n      src,\n      out,\n      src_shape,\n      src_strides,\n      src_ndim,\n      slice_sizes,\n      axes,\n      idxs,\n      index,\n      grid_dim);\n}}\n"
+ "\n[[kernel]] void scatter{0}_{4}_updc_{7}_nwork{8}_{9}(\n    const device {1}* updates [[buffer(1)]],\n    device mlx_atomic<{1}>* out [[buffer(2)]],\n    const constant int* upd_shape [[buffer(3)]],\n    const constant size_t* upd_strides [[buffer(4)]],\n    const constant size_t& upd_ndim [[buffer(5)]],\n    const constant size_t& upd_size [[buffer(6)]],\n    const constant int* out_shape [[buffer(7)]],\n    const constant size_t* out_strides [[buffer(8)]],\n    const constant size_t& out_ndim [[buffer(9)]],\n    const constant int* axes [[buffer(10)]],\n    const constant int* idx_shapes [[buffer(11)]],\n    const constant size_t* idx_strides [[buffer(12)]],\n    const constant bool* idx_contigs [[buffer(13)]],\n    const constant int& idx_ndim [[buffer(14)]],\n    const constant size_t& idx_size [[buffer(15)]],\n    {5}\n    uint2 gid [[thread_position_in_grid]]) {{\n  Indices<{2}, {4}> idxs{{ {{ {6} }}, idx_shapes, idx_strides, idx_contigs, idx_ndim}};\n\n  return scatter_impl<{1}, {2}, {3}, {4}, {7}, {8}, {9}>(\n      updates,\n      out,\n      upd_shape,\n      upd_strides,\n      upd_ndim,\n      upd_size,\n      out_shape,\n      out_strides,\n      out_ndim,\n      axes,\n      idx_size,\n      idxs,\n      gid);\n}}\n"
+ "\nfloat erf(float a) {\n  float r, s, t, u;\n  t = metal::abs(a);\n  s = a * a;\n  if (t > 0.927734375f) {\n    r = metal::fma(\n        -1.72853470e-5f, t, 3.83197126e-4f);\n    u = metal::fma(\n        -3.88396438e-3f, t, 2.42546219e-2f);\n    r = metal::fma(r, s, u);\n    r = metal::fma(r, t, -1.06777877e-1f);\n    r = metal::fma(r, t, -6.34846687e-1f);\n    r = metal::fma(r, t, -1.28717512e-1f);\n    r = metal::fma(r, t, -t);\n    r = 1.0f - metal::exp(r);\n    r = metal::copysign(r, a);\n  } else {\n    r = -5.96761703e-4f;\n    r = metal::fma(r, s, 4.99119423e-3f);\n    r = metal::fma(r, s, -2.67681349e-2f);\n    r = metal::fma(r, s, 1.12819925e-1f);\n    r = metal::fma(r, s, -3.76125336e-1f);\n    r = metal::fma(r, s, 1.28379166e-1f);\n    r = metal::fma(r, a, a);\n  }\n  return r;\n}\nfloat erfinv(float a) {\n  auto t = metal::fma(a, 0.0f - a, 1.0f);\n  t = metal::log(t);\n  float p;\n  if (metal::abs(t) > 6.125f) {\n    p = 3.03697567e-10f;\n    p = metal::fma(p, t, 2.93243101e-8f);\n    p = metal::fma(p, t, 1.22150334e-6f);\n    p = metal::fma(p, t, 2.84108955e-5f);\n    p = metal::fma(p, t, 3.93552968e-4f);\n    p = metal::fma(p, t, 3.02698812e-3f);\n    p = metal::fma(p, t, 4.83185798e-3f);\n    p = metal::fma(p, t, -2.64646143e-1f);\n    p = metal::fma(p, t, 8.40016484e-1f);\n  } else {\n    p = 5.43877832e-9f;\n    p = metal::fma(p, t, 1.43285448e-7f);\n    p = metal::fma(p, t, 1.22774793e-6f);\n    p = metal::fma(p, t, 1.12963626e-7f);\n    p = metal::fma(p, t, -5.61530760e-5f);\n    p = metal::fma(p, t, -1.47697632e-4f);\n    p = metal::fma(p, t, 2.31468678e-3f);\n    p = metal::fma(p, t, 1.15392581e-2f);\n    p = metal::fma(p, t, -2.32015476e-1f);\n    p = metal::fma(p, t, 8.86226892e-1f);\n  }\n  return a * p;\n}\nfloat expm1f_scaled_unchecked(float a, float b) {\n  float f, j, r, s, t, u, v, x, y;\n  int i;\n  j = fma(1.442695f, a, 12582912.f);\n  j = j - 12582912.0f;\n  i = (int)j;\n  f = fma(j, -6.93145752e-1f, a);\n  s = f * f;\n  if (a == 0.0f)\n    s = a;\n  r = 1.97350979e-4f;\n  r = fma(r, f, 1.39309070e-3f);\n  r = fma(r, f, 8.33343994e-3f);\n  r = fma(r, f, 4.16668020e-2f);\n  r = fma(r, f, 1.66666716e-1f);\n  r = fma(r, f, 4.99999970e-1f);\n  u = (j == 1) ? (f + 0.5f) : f;\n  v = fma(r, s, u);\n  s = 0.5f * b;\n  t = ldexp(s, i);\n  y = t - s;\n  x = (t - y) - s;\n  r = fma(v, t, x) + y;\n  r = r + r;\n  if (j == 0)\n    r = v;\n  if (j == 1)\n    r = v + v;\n  return r;\n}\nfloat expm1f(float a) {\n  float r;\n  r = expm1f_scaled_unchecked(a, 1.0f);\n  if (abs(a - 1.0f) > 88.0f) {\n    r = pow(2, a);\n    r = fma(r, r, -1.0f);\n  }\n  return r;\n}\n\nnamespace {\nconstant float inf = metal::numeric_limits<float>::infinity();\n}\nstruct Abs {\n  template <typename T>\n  T operator()(T x) {\n    return metal::abs(x);\n  };\n  template <>\n  uint8_t operator()(uint8_t x) {\n    return x;\n  };\n  template <>\n  uint16_t operator()(uint16_t x) {\n    return x;\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x;\n  };\n  template <>\n  uint64_t operator()(uint64_t x) {\n    return x;\n  };\n  template <>\n  bool operator()(bool x) {\n    return x;\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {metal::precise::sqrt(x.real * x.real + x.imag * x.imag), 0};\n  };\n};\nstruct ArcCos {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::acos(x);\n  };\n};\nstruct ArcCosh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::acosh(x);\n  };\n};\nstruct ArcSin {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::asin(x);\n  };\n};\nstruct ArcSinh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::asinh(x);\n  };\n};\nstruct ArcTan {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::atan(x);\n  };\n};\nstruct ArcTanh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::atanh(x);\n  };\n};\nstruct Ceil {\n  template <typename T>\n  T operator()(T x) {\n    return metal::ceil(x);\n  };\n  template <>\n  int8_t operator()(int8_t x) {\n    return x;\n  };\n  template <>\n  int16_t operator()(int16_t x) {\n    return x;\n  };\n  template <>\n  int32_t operator()(int32_t x) {\n    return x;\n  };\n  template <>\n  int64_t operator()(int64_t x) {\n    return x;\n  };\n  template <>\n  uint8_t operator()(uint8_t x) {\n    return x;\n  };\n  template <>\n  uint16_t operator()(uint16_t x) {\n    return x;\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x;\n  };\n  template <>\n  uint64_t operator()(uint64_t x) {\n    return x;\n  };\n  template <>\n  bool operator()(bool x) {\n    return x;\n  };\n};\nstruct Cos {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::cos(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::cos(x.real) * metal::precise::cosh(x.imag),\n        -metal::precise::sin(x.real) * metal::precise::sinh(x.imag)};\n  };\n};\nstruct Cosh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::cosh(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::cosh(x.real) * metal::precise::cos(x.imag),\n        metal::precise::sinh(x.real) * metal::precise::sin(x.imag)};\n  };\n};\nstruct Conjugate {\n  complex64_t operator()(complex64_t x) {\n    return complex64_t{x.real, -x.imag};\n  }\n};\nstruct Erf {\n  template <typename T>\n  T operator()(T x) {\n    return static_cast<T>(erf(static_cast<float>(x)));\n  };\n};\nstruct ErfInv {\n  template <typename T>\n  T operator()(T x) {\n    return static_cast<T>(erfinv(static_cast<float>(x)));\n  };\n};\nstruct Exp {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::exp(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    auto m = metal::precise::exp(x.real);\n    return {m * metal::precise::cos(x.imag), m * metal::precise::sin(x.imag)};\n  }\n};\nstruct Expm1 {\n  template <typename T>\n  T operator()(T x) {\n    return static_cast<T>(expm1f(static_cast<float>(x)));\n  };\n};\nstruct Floor {\n  template <typename T>\n  T operator()(T x) {\n    return metal::floor(x);\n  };\n  template <>\n  int8_t operator()(int8_t x) {\n    return x;\n  };\n  template <>\n  int16_t operator()(int16_t x) {\n    return x;\n  };\n  template <>\n  int32_t operator()(int32_t x) {\n    return x;\n  };\n  template <>\n  int64_t operator()(int64_t x) {\n    return x;\n  };\n  template <>\n  uint8_t operator()(uint8_t x) {\n    return x;\n  };\n  template <>\n  uint16_t operator()(uint16_t x) {\n    return x;\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x;\n  };\n  template <>\n  uint64_t operator()(uint64_t x) {\n    return x;\n  };\n  template <>\n  bool operator()(bool x) {\n    return x;\n  };\n};\nstruct Imag {\n  template <typename T>\n  T operator()(T x) {\n    return x.imag;\n  };\n};\nstruct Log {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::log(x);\n  };\n};\nstruct Log2 {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::log2(x);\n  };\n};\nstruct Log10 {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::log10(x);\n  };\n};\nstruct Log1p {\n  template <typename T>\n  T operator()(T x) {\n    return log1p(x);\n  };\n};\nstruct LogicalNot {\n  template <typename T>\n  T operator()(T x) {\n    return !x;\n  };\n};\nstruct Negative {\n  template <typename T>\n  T operator()(T x) {\n    return -x;\n  };\n};\nstruct Real {\n  template <typename T>\n  T operator()(T x) {\n    return x.real;\n  };\n};\nstruct Round {\n  template <typename T>\n  T operator()(T x) {\n    return metal::rint(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {metal::rint(x.real), metal::rint(x.imag)};\n  };\n};\nstruct Sigmoid {\n  template <typename T>\n  T operator()(T x) {\n    auto y = 1 / (1 + metal::exp(-metal::abs(x)));\n    return (x < 0) ? 1 - y : y;\n  }\n};\nstruct Sign {\n  template <typename T>\n  T operator()(T x) {\n    return (x > T(0)) - (x < T(0));\n  };\n  template <>\n  uint32_t operator()(uint32_t x) {\n    return x != 0;\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    if (x == complex64_t(0)) {\n      return x;\n    }\n    return x /\n        (complex64_t)metal::precise::sqrt(x.real * x.real + x.imag * x.imag);\n  };\n};\nstruct Sin {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::sin(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::sin(x.real) * metal::precise::cosh(x.imag),\n        metal::precise::cos(x.real) * metal::precise::sinh(x.imag)};\n  };\n};\nstruct Sinh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::sinh(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    return {\n        metal::precise::sinh(x.real) * metal::precise::cos(x.imag),\n        metal::precise::cosh(x.real) * metal::precise::sin(x.imag)};\n  };\n};\nstruct Square {\n  template <typename T>\n  T operator()(T x) {\n    return x * x;\n  };\n};\nstruct Sqrt {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::sqrt(x);\n  };\n};\nstruct Rsqrt {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::rsqrt(x);\n  };\n};\nstruct Tan {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::tan(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    float tan_a = metal::precise::tan(x.real);\n    float tanh_b = metal::precise::tanh(x.imag);\n    float t1 = tan_a * tanh_b;\n    float denom = 1. + t1 * t1;\n    return {(tan_a - tanh_b * t1) / denom, (tanh_b + tan_a * t1) / denom};\n  };\n};\nstruct Tanh {\n  template <typename T>\n  T operator()(T x) {\n    return metal::precise::tanh(x);\n  };\n  template <>\n  complex64_t operator()(complex64_t x) {\n    float tanh_a = metal::precise::tanh(x.real);\n    float tan_b = metal::precise::tan(x.imag);\n    float t1 = tanh_a * tan_b;\n    float denom = 1. + t1 * t1;\n    return {(tanh_a + tan_b * t1) / denom, (tan_b - tanh_a * t1) / denom};\n  };\n};\n"
+ "\nnamespace mlx {\nnamespace steel {\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DInputBlockLoaderGeneral {\n  static constant constexpr const short BROWS = BM;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size = tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4;\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const constant MLXConvParams<2>* params;\n  const constant Conv2DGeneralJumpParams* jump_params;\n  const short base_wh;\n  const short base_ww;\n  short weight_h;\n  short weight_w;\n  const device T* src[n_rows];\n  int read_n[n_rows];\n  int read_ih[n_rows];\n  int read_iw[n_rows];\n  METAL_FUNC Conv2DInputBlockLoaderGeneral(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int4 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant Conv2DGeneralJumpParams* jump_params_,\n      const short base_wh_,\n      const short base_ww_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        params(params_),\n        jump_params(jump_params_),\n        base_wh(base_wh_),\n        base_ww(base_ww_),\n        weight_h(base_wh_),\n        weight_w(base_ww_) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; ++i) {\n      int offset_nhw = offsets.y + bi + i * TROWS;\n      int n = offset_nhw / jump_params->adj_out_hw;\n      int hw = offset_nhw % jump_params->adj_out_hw;\n      int oh =\n          (hw / jump_params->adj_out_w) * jump_params->f_out_jump_h + offsets.z;\n      int ow =\n          (hw % jump_params->adj_out_w) * jump_params->f_out_jump_w + offsets.w;\n      int ih = oh * params->str[0] - params->pad[0];\n      int iw = ow * params->str[1] - params->pad[1];\n      read_n[i] = n;\n      read_ih[i] = ih;\n      read_iw[i] = iw;\n      src[i] = src_ + n * params->in_strides[0] + bj;\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n#pragma clang loop unroll(full)\n    for (short i = 0, is = 0; i < n_rows; ++i, is += TROWS) {\n      int n = read_n[i];\n      int h_flip = params->flip ? params->wS[0] - weight_h - 1 : weight_h;\n      int w_flip = params->flip ? params->wS[1] - weight_w - 1 : weight_w;\n      int ih_dil = read_ih[i] + h_flip * params->kdil[0];\n      int iw_dil = read_iw[i] + w_flip * params->kdil[1];\n      int ih = ih_dil / params->idil[0];\n      int iw = iw_dil / params->idil[1];\n      size_t offset = ih * params->in_strides[1] + iw * params->in_strides[2];\n      if ((n < params->N) && (ih_dil >= 0 && ih < params->iS[0]) &&\n          (iw_dil >= 0 && iw < params->iS[1])) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = (src[i])[offset + j];\n        }\n      }\n      else {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; ++j) {\n          dst[is * dst_ld + j] = T(0);\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    weight_w += jump_params->f_wgt_jump_w;\n    if (weight_w < params->wS[1]) {\n      return;\n    }\n    weight_w = base_ww;\n    weight_h += jump_params->f_wgt_jump_h;\n    if (weight_h < params->wS[0]) {\n      return;\n    }\n    weight_h = base_wh;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < n_rows; i++) {\n      src[i] += BK;\n    }\n  }\n};\ntemplate <\n    typename T,\n    short BM,\n    short BN,\n    short BK,\n    short tgp_size,\n    short tgp_padding = 0>\nstruct Conv2DWeightBlockLoaderGeneral {\n  static constant constexpr const short BROWS = BN;\n  static constant constexpr const short BCOLS = BK;\n  static constant constexpr const short dst_ld = BCOLS + tgp_padding;\n  static constant constexpr const short vec_size =\n      (BN == 8) ? 1 : (tgp_size / (BROWS * BCOLS) >= 8 ? 8 : 4);\n  static constant constexpr const short TCOLS = BCOLS / vec_size;\n  static constant constexpr const short TROWS = tgp_size / TCOLS;\n  static constant constexpr const short n_rows = BROWS / TROWS;\n  const int src_ld;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  const constant MLXConvParams<2>* params;\n  const constant Conv2DGeneralJumpParams* jump_params;\n  const short base_wh;\n  const short base_ww;\n  short weight_h;\n  short weight_w;\n  const int start_row;\n  METAL_FUNC Conv2DWeightBlockLoaderGeneral(\n      const device T* src_,\n      threadgroup T* dst_,\n      const int2 offsets,\n      const constant MLXConvParams<2>* params_,\n      const constant Conv2DGeneralJumpParams* jump_params_,\n      const short base_wh_,\n      const short base_ww_,\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(params_->wt_strides[0]),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld + bj),\n        params(params_),\n        jump_params(jump_params_),\n        base_wh(base_wh_),\n        base_ww(base_ww_),\n        weight_h(base_wh_),\n        weight_w(base_ww_),\n        start_row(offsets.y + bi) {}\n  METAL_FUNC void load_unsafe() const {\n    const device T* curr_src = src + weight_h * params->wt_strides[1] +\n        weight_w * params->wt_strides[2];\n    if ((start_row + BN <= params->O)) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BN; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = curr_src[i * src_ld + j];\n        }\n      }\n    } else {\n      for (short i = 0; i < BN; i += TROWS) {\n        if ((start_row + i) < params->O) {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = curr_src[i * src_ld + j];\n          }\n        } else {\n#pragma clang loop unroll(full)\n          for (short j = 0; j < vec_size; j++) {\n            dst[i * dst_ld + j] = T(0);\n          }\n        }\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    weight_w += jump_params->f_wgt_jump_w;\n    if (weight_w < params->wS[1]) {\n      return;\n    }\n    weight_w = base_ww;\n    weight_h += jump_params->f_wgt_jump_h;\n    if (weight_h < params->wS[0]) {\n      return;\n    }\n    weight_h = base_wh;\n    src += BK;\n  }\n};\n}\n}\n\ntemplate <\n    typename T,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    typename AccumType = float,\n    typename Epilogue = TransformNone<T, AccumType>>\n[[kernel, max_total_threads_per_threadgroup(WM* WN * 32)]] void\nimplicit_gemm_conv_2d_general(\n    const device T* A [[buffer(0)]],\n    const device T* B [[buffer(1)]],\n    device T* C [[buffer(2)]],\n    const constant MLXConvParams<2>* params [[buffer(3)]],\n    const constant ImplicitGemmConv2DParams* gemm_params [[buffer(4)]],\n    const constant Conv2DGeneralJumpParams* jump_params [[buffer(5)]],\n    const constant Conv2DGeneralBaseInfo* base_h [[buffer(6)]],\n    const constant Conv2DGeneralBaseInfo* base_w [[buffer(7)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_gid [[simdgroup_index_in_threadgroup]],\n    uint simd_lid [[thread_index_in_simdgroup]]) {\n  (void)lid;\n  constexpr bool transpose_a = false;\n  constexpr bool transpose_b = true;\n  constexpr short tgp_padding_a = 16 / sizeof(T);\n  constexpr short tgp_padding_b = 16 / sizeof(T);\n  constexpr short shape_a_cols = (transpose_a ? BM : BK) + tgp_padding_a;\n  constexpr short shape_b_cols = (transpose_b ? BK : BN) + tgp_padding_b;\n  constexpr short shape_a_rows = (transpose_a ? BK : BM);\n  constexpr short shape_b_rows = (transpose_b ? BN : BK);\n  constexpr short tgp_mem_size_a = shape_a_cols * shape_a_rows;\n  constexpr short tgp_mem_size_b = shape_b_cols * shape_b_rows;\n  constexpr short tgp_size = WM * WN * 32;\n  using loader_a_t =\n      Conv2DInputBlockLoaderGeneral<T, BM, BN, BK, tgp_size, tgp_padding_a>;\n  using loader_b_t =\n      Conv2DWeightBlockLoaderGeneral<T, BM, BN, BK, tgp_size, tgp_padding_b>;\n  using mma_t = BlockMMA<\n      T,\n      T,\n      BM,\n      BN,\n      BK,\n      WM,\n      WN,\n      transpose_a,\n      transpose_b,\n      shape_a_cols,\n      shape_b_cols>;\n  threadgroup T As[tgp_mem_size_a];\n  threadgroup T Bs[tgp_mem_size_b];\n  const int tid_y = ((tid.y) << gemm_params->swizzle_log) +\n      ((tid.x) & ((1 << gemm_params->swizzle_log) - 1));\n  const int tid_x = (tid.x) >> gemm_params->swizzle_log;\n  if (gemm_params->tiles_n <= tid_x || gemm_params->tiles_m <= tid_y) {\n    return;\n  }\n  const int tid_z = tid.z;\n  const int base_oh = tid_z / jump_params->f_out_jump_w;\n  const int base_ow = tid_z % jump_params->f_out_jump_w;\n  const int base_wh = base_h[base_oh].weight_base;\n  const int base_ww = base_w[base_ow].weight_base;\n  const int base_wh_size = base_h[base_oh].weight_size;\n  const int base_ww_size = base_w[base_ow].weight_size;\n  const int c_row = tid_y * BM;\n  const int c_col = tid_x * BN;\n  const int K = gemm_params->K;\n  B += c_col * K;\n  const int4 offsets_a(0, c_row, base_oh, base_ow);\n  const int2 offsets_b(0, c_col);\n  loader_a_t loader_a(\n      A,\n      As,\n      offsets_a,\n      params,\n      jump_params,\n      base_wh,\n      base_ww,\n      simd_gid,\n      simd_lid);\n  loader_b_t loader_b(\n      B,\n      Bs,\n      offsets_b,\n      params,\n      jump_params,\n      base_wh,\n      base_ww,\n      simd_gid,\n      simd_lid);\n  mma_t mma_op(simd_gid, simd_lid);\n  int gemm_k_iterations =\n      base_wh_size * base_ww_size * gemm_params->gemm_k_iterations;\n  for (int k = 0; k < gemm_k_iterations; k++) {\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    loader_a.load_unsafe();\n    loader_b.load_unsafe();\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    mma_op.mma(As, Bs);\n    loader_a.next();\n    loader_b.next();\n  }\n  threadgroup_barrier(mem_flags::mem_none);\n  {\n    int offset_m = c_row + mma_op.sm;\n    int offset_n = c_col + mma_op.sn;\n    C += offset_n;\n    if (offset_n >= gemm_params->N)\n      return;\n    short diff = gemm_params->N - offset_n;\n#pragma clang loop unroll(full)\n    for (int i = 0; i < mma_t::TM; i++) {\n      int cm = offset_m + i * mma_t::TM_stride;\n      int n = cm / jump_params->adj_out_hw;\n      int hw = cm % jump_params->adj_out_hw;\n      int oh =\n          (hw / jump_params->adj_out_w) * jump_params->f_out_jump_h + base_oh;\n      int ow =\n          (hw % jump_params->adj_out_w) * jump_params->f_out_jump_w + base_ow;\n      if (n < params->N && oh < params->oS[0] && ow < params->oS[1]) {\n        int offset_cm = n * params->out_strides[0] +\n            oh * params->out_strides[1] + ow * params->out_strides[2];\n#pragma clang loop unroll(full)\n        for (int j = 0; j < mma_t::TN; j++) {\n          thread const auto& accum = mma_op.Ctile.frag_at(i, j);\n          int offset = offset_cm + (j * mma_t::TN_stride);\n          constexpr short kelems = decltype(mma_op.Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n          for (short k = 0; k < kelems; k++) {\n            if ((j * mma_t::TN_stride + k) < diff) {\n              C[offset + k] = Epilogue::apply(accum[k]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"
+ "\nnamespace mlx {\nnamespace steel {\ntemplate <\n    typename T,\n    short BROWS,\n    short BCOLS,\n    short dst_ld,\n    short reduction_dim,\n    short tgp_size,\n    short alignment = 1,\n    short n_reads = (BCOLS * BROWS) / (tgp_size),\n    short TCOLS = BCOLS / n_reads,\n    short TROWS = tgp_size / TCOLS>\nstruct BlockLoader {\n  static constant constexpr const short n_rows = (BROWS + TROWS - 1) / TROWS;\n  static constant constexpr const short vec_size = n_reads;\n  const int src_ld;\n  const int tile_stride;\n  const short thread_idx;\n  const short bi;\n  const short bj;\n  threadgroup T* dst;\n  const device T* src;\n  struct alignas(alignment * sizeof(T)) ReadVector {\n    uint8_t v[sizeof(T) * vec_size];\n  };\n  METAL_FUNC BlockLoader(\n      const device T* src_,\n      const int src_ld_,\n      threadgroup T* dst_,\n      ushort simd_group_id [[simdgroup_index_in_threadgroup]],\n      ushort simd_lane_id [[thread_index_in_simdgroup]])\n      : src_ld(src_ld_),\n        tile_stride(reduction_dim ? BCOLS : BROWS * src_ld),\n        thread_idx(simd_group_id * 32 + simd_lane_id),\n        bi(thread_idx / TCOLS),\n        bj(vec_size * (thread_idx % TCOLS)),\n        dst(dst_ + bi * dst_ld + bj),\n        src(src_ + bi * src_ld + bj) {}\n  template <typename UnaryOp>\n  METAL_FUNC void apply_inplace_op(thread const UnaryOp& op) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        dst[i * dst_ld + j] = op.apply(dst[i * dst_ld + j]);\n      }\n    }\n  }\n  METAL_FUNC void load_unsafe() const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < BROWS; i += TROWS) {\n      *((threadgroup ReadVector*)(&dst[i * dst_ld])) =\n          *((const device ReadVector*)(&src[i * src_ld]));\n    }\n  }\n  METAL_FUNC void load_safe(short2 src_tile_dim) const {\n    src_tile_dim = src_tile_dim - short2(bj, bi);\n    if (src_tile_dim.x <= 0 || src_tile_dim.y <= 0) {\n#pragma clang loop unroll(full)\n      for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n        for (short j = 0; j < vec_size; j++) {\n          dst[i * dst_ld + j] = T(0);\n        }\n      }\n      return;\n    }\n    bool tmp_idx[vec_size];\n    T tmp_val[vec_size];\n#pragma clang loop unroll(full)\n    for (short i = 0; i < BROWS; i += TROWS) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        tmp_idx[j] = (i < src_tile_dim.y) && (j < src_tile_dim.x);\n      }\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        tmp_val[j] = src[(tmp_idx[j] ? i * src_ld + j : 0)];\n      }\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        tmp_val[j] = tmp_idx[j] ? tmp_val[j] : T(0);\n      }\n#pragma clang loop unroll(full)\n      for (short j = 0; j < vec_size; j++) {\n        dst[i * dst_ld + j] = tmp_val[j];\n      }\n    }\n  }\n  METAL_FUNC void next() {\n    src += tile_stride;\n  }\n};\n}\n}\nMETAL_FUNC ulong2 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n  }\n  return ulong2(loc_a, loc_b);\n}\nMETAL_FUNC ulong3 elem_to_loc_broadcast(\n    uint elem,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const size_t* c_strides,\n    int ndim) {\n  ulong loc_a{0};\n  ulong loc_b{0};\n  ulong loc_c{0};\n  for (int i = ndim - 1; i >= 0 && elem > 0; --i) {\n    int pos_in_dim = (elem % shape[i]);\n    elem /= shape[i];\n    loc_a += pos_in_dim * a_strides[i];\n    loc_b += pos_in_dim * b_strides[i];\n    loc_c += pos_in_dim * c_strides[i];\n  }\n  return ulong3(loc_a, loc_b, loc_c);\n}\nnamespace mlx {\nnamespace steel {\ntemplate <typename OutT, typename InT>\nstruct TransformNone {\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT) {\n    return static_cast<OutT>(x);\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAdd {\n  TransformAdd(const float, const float) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  static METAL_FUNC OutT apply(InT x, OutT c) {\n    return static_cast<OutT>(x) + c;\n  }\n};\ntemplate <typename OutT, typename InT>\nstruct TransformAxpby {\n  const float alpha;\n  const float beta;\n  TransformAxpby(const float alpha_, const float beta_)\n      : alpha(alpha_), beta(beta_) {}\n  static METAL_FUNC OutT apply(InT x) {\n    return static_cast<OutT>(x);\n  }\n  METAL_FUNC OutT apply(InT x, OutT c) const {\n    return static_cast<OutT>(x * alpha + (beta * c));\n  }\n};\ntemplate <typename T>\nstruct AccumHelper {\n  typedef float accum_type;\n};\nstruct BlockSwizzle {\n  static METAL_FUNC int2\n  swizzle(uint3 tid [[threadgroup_position_in_grid]], const int swizzle_log) {\n    const int tid_x = (tid.x) >> swizzle_log;\n    const int tid_y =\n        ((tid.y) << swizzle_log) + ((tid.x) & ((1 << swizzle_log) - 1));\n    return int2(tid_x, tid_y);\n  }\n};\n}\n}\n#pragma METAL internals : enable\nnamespace metal {\ntemplate <typename T>\nstruct is_empty : metal::bool_constant<__is_empty(T)> {};\ntemplate <typename... Ts>\nstruct make_void {\n  typedef void type;\n};\ntemplate <typename... Ts>\nusing void_t = typename make_void<Ts...>::type;\ntemplate <class T>\nstruct is_static : metal::bool_constant<is_empty<remove_cv_t<T>>::value> {};\ntemplate <typename T>\nstruct pointer_element {};\ntemplate <typename T>\nstruct pointer_element<thread T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<device T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<constant T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nstruct pointer_element<threadgroup T*> {\n  using type = remove_cv_t<T>;\n};\ntemplate <typename T>\nusing pointer_element_t = typename pointer_element<remove_cv_t<T>>::type;\n}\n#pragma METAL internals : disable\n\n#pragma METAL internals : enable\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, T v>\nstruct integral_constant {\n  static constexpr constant T value = v;\n  using value_type = T;\n  using type = integral_constant;\n  METAL_FUNC constexpr operator value_type() const noexcept {\n    return value;\n  }\n};\ntemplate <bool B>\nusing bool_constant = integral_constant<bool, B>;\nusing true_type = bool_constant<true>;\nusing false_type = bool_constant<false>;\ntemplate <class T>\nstruct is_integral : bool_constant<metal::is_integral<T>::value> {};\ntemplate <class T, T v>\nstruct is_integral<integral_constant<T, v>>\n    : bool_constant<metal::is_integral<T>::value> {};\ntemplate <typename T>\nconstexpr constant bool is_integral_v = is_integral<T>::value;\ntemplate <int val>\nusing Int = integral_constant<int, val>;\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator+( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv + uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator-( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv - uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator*( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv * uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator/( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv / uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator==( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv == uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator!=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv != uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv < uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv > uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator<=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv <= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator>=( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv >= uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator&&( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv && uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T, T tv, typename U, U uv> METAL_FUNC constexpr auto operator||( integral_constant<T, tv>, integral_constant<U, uv>) { constexpr auto res = tv || uv; return integral_constant<decltype(res), res>{}; };\ntemplate <typename T>\nMETAL_FUNC constexpr T sum(T x) {\n  return x;\n}\ntemplate <typename T, typename... Us>\nMETAL_FUNC constexpr auto sum(T x, Us... us) {\n  return x + sum(us...);\n}\n}\n}\n#pragma METAL internals : disable\n\nusing namespace metal;\nnamespace mlx {\nnamespace steel {\ntemplate <typename T, int kFragRows_, int kFragCols_>\nstruct BaseMMAFrag {\n  static_assert(\n      kFragRows_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n  static_assert(\n      kFragCols_ == 8,\n      \"Only 8 x 8 fragment matrices are currently supported\");\n};\ntemplate <typename T>\nstruct BaseMMAFrag<T, 8, 8> {\n  static constant constexpr const int kFragRows = 8;\n  static constant constexpr const int kFragCols = 8;\n  static constant constexpr const int kElemsPerFrag = (kFragRows * kFragCols) / 32;\n  static constant constexpr const int kElemRows = 1;\n  static constant constexpr const int kElemCols = 2;\n  static_assert(\n      kElemRows * kElemCols == kElemsPerFrag,\n      \"MMAFrag shape is not consistent with MMAFrag size\");\n  typedef metal::simdgroup_matrix<T, kFragRows, kFragCols> mat_type;\n  typedef metal::vec<T, kElemsPerFrag> frag_type;\n  METAL_FUNC static constexpr short2 get_coord(ushort simd_lane_id\n                                               [[thread_index_in_simdgroup]]) {\n    const short qid = simd_lane_id / 4;\n    const short fm = (qid & 4) + ((simd_lane_id / 2) % 4);\n    const short fn = (qid & 2) * 2 + (simd_lane_id % 2) * 2;\n    return short2{fn, fm};\n  }\n  template <typename SrcPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  load(thread frag_type& dst, SrcPtrType src, StrX str_x, StrY str_y) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * kElemCols + j] = static_cast<T>(src[i * str_x + j * str_y]);\n      }\n    }\n  }\n  template <\n      typename SrcPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void load_safe(\n      thread frag_type& dst,\n      SrcPtrType src,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[i * kElemCols + j] =\n              static_cast<T>(src[(off_x + i) * str_x + (off_x + j) * str_y]);\n        } else {\n          dst[i * kElemCols + j] = T(0);\n        }\n      }\n    }\n  }\n  template <typename DstPtrType, typename StrX, typename StrY>\n  METAL_FUNC static constexpr void\n  store(const thread frag_type& src, DstPtrType dst, StrX str_x, StrY str_y) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        dst[i * str_x + j * str_y] = static_cast<U>(src[i * kElemCols + j]);\n      }\n    }\n  }\n  template <\n      typename DstPtrType,\n      typename StrX,\n      typename StrY,\n      typename LimX,\n      typename LimY,\n      typename OffX,\n      typename OffY>\n  METAL_FUNC static constexpr void store_safe(\n      const thread frag_type& src,\n      DstPtrType dst,\n      StrX str_x,\n      StrY str_y,\n      LimX lim_x,\n      LimY lim_y,\n      OffX off_x = Int<0>{},\n      OffY off_y = Int<0>{}) {\n    using U = pointer_element_t<DstPtrType>;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kElemRows; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kElemCols; j++) {\n        if ((off_x + i) < lim_x && (off_y + j) < lim_y) {\n          dst[(off_x + i) * str_x + (off_y + j) * str_y] =\n              static_cast<U>(src[i * kElemCols + j]);\n        }\n      }\n    }\n  }\n  METAL_FUNC static constexpr void mma(\n      thread frag_type& D,\n      thread frag_type& A,\n      thread frag_type& B,\n      thread frag_type& C) {\n    mat_type D_mat;\n    mat_type A_mat;\n    mat_type B_mat;\n    mat_type C_mat;\n    reinterpret_cast<thread frag_type&>(A_mat.thread_elements()) = A;\n    reinterpret_cast<thread frag_type&>(B_mat.thread_elements()) = B;\n    reinterpret_cast<thread frag_type&>(C_mat.thread_elements()) = C;\n    mma(D_mat, A_mat, B_mat, C_mat);\n    D = reinterpret_cast<thread frag_type&>(D_mat.thread_elements());\n  }\n  METAL_FUNC static constexpr void mma(\n      thread mat_type& D,\n      thread mat_type& A,\n      thread mat_type& B,\n      thread mat_type& C) {\n    simdgroup_multiply_accumulate(D, A, B, C);\n  }\n};\ntemplate <\n    typename T,\n    int kTileRows_,\n    int kTileCols_,\n    class MMAFrag_ = BaseMMAFrag<T, 8, 8>>\nstruct MMATile {\n  using MMAFrag_t = MMAFrag_;\n  using elem_type = T;\n  static constant constexpr const int kFragRows = MMAFrag_t::kFragRows;\n  static constant constexpr const int kFragCols = MMAFrag_t::kFragCols;\n  static constant constexpr const int kElemsPerFrag = MMAFrag_t::kElemsPerFrag;\n  static constant constexpr const int kTileRows = kTileRows_;\n  static constant constexpr const int kTileCols = kTileCols_;\n  static constant constexpr const int kRows = kTileRows * kFragRows;\n  static constant constexpr const int kCols = kTileCols * kFragCols;\n  static constant constexpr const int kNumFrags = kTileRows * kTileCols;\n  static constant constexpr const int kElemsPerTile = kNumFrags * kElemsPerFrag;\n  typedef typename MMAFrag_t::mat_type mat_type;\n  typedef typename MMAFrag_t::frag_type frag_type;\n  frag_type val_frags[kNumFrags] = {frag_type(0)};\n  METAL_FUNC MMATile() thread {}\n  METAL_FUNC constexpr void clear() {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kNumFrags; ++i) {\n      val_frags[i] = frag_type(0);\n    }\n  }\n  METAL_FUNC constexpr thread frag_type& frag_at(const short i, const short j) {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC constexpr const thread frag_type& frag_at(\n      const short i,\n      const short j) const {\n    return val_frags[i * kTileCols + j];\n  }\n  METAL_FUNC mat_type mat_at(const short i, const short j) {\n    mat_type val_mat;\n#pragma clang loop unroll(full)\n    for (short ii = 0; ii < kElemsPerFrag; ++ii) {\n      val_mat.thread_elements()[ii] = frag_at(i, j)[ii];\n    }\n    return val_mat;\n  }\n  METAL_FUNC thread elem_type* elems() {\n    return reinterpret_cast<thread elem_type*>(val_frags);\n  }\n  METAL_FUNC const thread elem_type* elems() const {\n    return reinterpret_cast<const thread elem_type*>(val_frags);\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void load(const threadgroup U* src) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(\n                src[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y, int str_x, int str_y>\n  METAL_FUNC void store(threadgroup U* dst) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(\n                dst[(i * kFragRows) * w_x * str_x +\n                    (j * kFragCols) * w_y * str_y]),\n            Int<str_x>{},\n            Int<str_y>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void load(const device U* src, const int ld) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load(\n            frag_at(i, j),\n            &(src[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void store(device U* dst, const int ld) const {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store(\n            frag_at(i, j),\n            &(dst[(i * kFragRows) * w_x * ld + (j * kFragCols) * w_y]),\n            ld,\n            Int<1>{});\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  load_safe(const device U* src, const int ld, const short2 src_tile_dims) {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::load_safe(\n            frag_at(i, j),\n            src,\n            ld,\n            Int<1>{},\n            src_tile_dims.y,\n            src_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n  template <typename U, int w_x, int w_y>\n  METAL_FUNC void\n  store_safe(device U* dst, const int ld, const short2 dst_tile_dims) const {\n#pragma clang loop unroll(full)\n    for (int i = 0; i < kTileRows; ++i) {\n#pragma clang loop unroll(full)\n      for (int j = 0; j < kTileCols; ++j) {\n        MMAFrag_t::store_safe(\n            frag_at(i, j),\n            dst,\n            ld,\n            Int<1>{},\n            dst_tile_dims.y,\n            dst_tile_dims.x,\n            (i * kFragRows) * w_x,\n            (j * kFragCols) * w_y);\n      }\n    }\n  }\n};\ntemplate <typename T, typename U, int M, int N, int K>\nMETAL_FUNC void tile_matmad(\n    thread MMATile<T, M, N>& D,\n    thread MMATile<U, M, K>& A,\n    thread MMATile<U, K, N>& B,\n    thread MMATile<T, M, N>& C) {\n#pragma clang loop unroll(full)\n  for (short m = 0; m < M; ++m) {\n#pragma clang loop unroll(full)\n    for (short n = 0; n < N; ++n) {\n      short n_serp = (m % 2) ? (N - 1 - n) : n;\n#pragma clang loop unroll(full)\n      for (short k = 0; k < K; ++k) {\n        MMATile<T, M, N>::MMAFrag_t::mma(\n            D.frag_at(m, n_serp),\n            A.frag_at(m, k),\n            B.frag_at(k, n_serp),\n            C.frag_at(m, n_serp));\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    short lda_tgp,\n    short ldb_tgp,\n    typename AccumType = float,\n    typename Epilogue = TransformNone<U, AccumType>>\nstruct BlockMMA {\n  static constant constexpr const short kFragSize = 8;\n  using MMAFrag_acc_t = BaseMMAFrag<AccumType, kFragSize, kFragSize>;\n  static constant constexpr const short TM_stride = kFragSize * WM;\n  static constant constexpr const short TN_stride = kFragSize * WN;\n  static constant constexpr const short TM = BM / (kFragSize * WM);\n  static constant constexpr const short TN = BN / (kFragSize * WN);\n  static constant constexpr const short A_str_m = transpose_a ? 1 : lda_tgp;\n  static constant constexpr const short A_str_k = transpose_a ? lda_tgp : 1;\n  static constant constexpr const short B_str_k = transpose_b ? 1 : ldb_tgp;\n  static constant constexpr const short B_str_n = transpose_b ? ldb_tgp : 1;\n  static constant constexpr const short tile_stride_a = kFragSize * A_str_k;\n  static constant constexpr const short tile_stride_b = kFragSize * B_str_k;\n  MMATile<AccumType, TM, 1, MMAFrag_acc_t> Atile;\n  MMATile<AccumType, 1, TN, MMAFrag_acc_t> Btile;\n  MMATile<AccumType, TM, TN, MMAFrag_acc_t> Ctile;\n  short sm;\n  short sn;\n  short As_offset;\n  short Bs_offset;\n  METAL_FUNC BlockMMA(\n      ushort simd_group_id [[simdgroup_index_in_threadgroup]],\n      ushort simd_lane_id [[thread_index_in_simdgroup]]) {\n    short tm = kFragSize * (simd_group_id / WN);\n    short tn = kFragSize * (simd_group_id % WN);\n    short2 simd_coord = MMAFrag_acc_t::get_coord(simd_lane_id);\n    sm = simd_coord.y;\n    sn = simd_coord.x;\n    As_offset = (tm + sm) * A_str_m + (sn)*A_str_k;\n    Bs_offset = (sm)*B_str_k + (tn + sn) * B_str_n;\n    sm += tm;\n    sn += tn;\n  }\n  METAL_FUNC void mma(const threadgroup T* As, const threadgroup T* Bs) {\n    As += As_offset;\n    Bs += Bs_offset;\n#pragma clang loop unroll(full)\n    for (short kk = 0; kk < BK; kk += kFragSize) {\n      simdgroup_barrier(mem_flags::mem_none);\n      Atile.template load<T, WM, 1, A_str_m, A_str_k>(As);\n      simdgroup_barrier(mem_flags::mem_none);\n      Btile.template load<T, 1, WN, B_str_k, B_str_n>(Bs);\n      simdgroup_barrier(mem_flags::mem_none);\n      tile_matmad(Ctile, Atile, Btile, Ctile);\n      As += tile_stride_a;\n      Bs += tile_stride_b;\n    }\n  }\n  METAL_FUNC void store_result(device U* D, const int ldd) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    Ctile.template store<U, WM, WN>(D, ldd);\n  }\n  METAL_FUNC void\n  store_result_safe(device U* D, const int ldd, short2 dst_tile_dims) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = Epilogue::apply(Ctile.elems()[i]);\n    }\n    D += sm * ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    Ctile.template store_safe<U, WM, WN>(D, ldd, dst_tile_dims);\n  }\n  template <typename UnaryEpilogue>\n  METAL_FUNC void apply_epilogue(thread const UnaryEpilogue& epilogue_op) {\n#pragma clang loop unroll(full)\n    for (short i = 0; i < decltype(Ctile)::kElemsPerTile; i++) {\n      Ctile.elems()[i] = epilogue_op.apply(Ctile.elems()[i]);\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n#pragma clang loop unroll(full)\n        for (short k = 0; k < decltype(Ctile)::kElemsPerFrag; k++) {\n          accum[k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  template <typename BinaryEpilogue>\n  METAL_FUNC void apply_epilogue_safe(\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const BinaryEpilogue& epilogue_op) {\n    C += (sm)*ldc + (sn)*fdc;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n        U c_elems[kelems] = {0};\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          if ((j * TN_stride + k) < dst_tile_dims.x) {\n            c_elems[k] = C[offset_c + k * fdc];\n          }\n        }\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          accum[k] = epilogue_op.apply(accum[k], c_elems[k]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < TM; i++) {\n#pragma clang loop unroll(full)\n      for (short j = 0; j < TN; j++) {\n        thread const auto& accum = Ctile.frag_at(i, j);\n        int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n        int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n        for (short k = 0; k < kelems; k++) {\n          D[offset_d + k] = epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n        }\n      }\n    }\n  }\n  METAL_FUNC void store_result_safe(\n      device U* D,\n      const int ldd,\n      const device U* C,\n      const int ldc,\n      const int fdc,\n      short2 dst_tile_dims,\n      thread const Epilogue& epilogue_op) const {\n    C += (sm)*ldc + (sn)*fdc;\n    D += (sm)*ldd + sn;\n    dst_tile_dims -= short2(sn, sm);\n    if (dst_tile_dims.x <= 0 || dst_tile_dims.y <= 0)\n      return;\n    constexpr short kelems = decltype(Ctile)::kElemsPerFrag;\n#pragma clang loop unroll(full)\n    for (int i = 0; i < TM; i++) {\n      if (i * TM_stride < dst_tile_dims.y) {\n#pragma clang loop unroll(full)\n        for (int j = 0; j < TN; j++) {\n          thread const auto& accum = Ctile.frag_at(i, j);\n          int offset_c = (i * TM_stride) * ldc + (j * TN_stride) * fdc;\n          int offset_d = (i * TM_stride) * ldd + (j * TN_stride);\n#pragma clang loop unroll(full)\n          for (short k = 0; k < kelems; k++) {\n            if ((j * TN_stride + k) < dst_tile_dims.x) {\n              D[offset_d + k] =\n                  epilogue_op.apply(accum[k], C[offset_c + k * fdc]);\n            }\n          }\n        }\n      }\n    }\n  }\n};\n}\n}\nnamespace mlx {\nnamespace steel {\nstruct GEMMParams {\n  const int M;\n  const int N;\n  const int K;\n  const int lda;\n  const int ldb;\n  const int ldd;\n  const int tiles_n;\n  const int tiles_m;\n  const size_t batch_stride_a;\n  const size_t batch_stride_b;\n  const size_t batch_stride_d;\n  const int swizzle_log;\n  const int gemm_k_iterations_aligned;\n  const int batch_ndim;\n};\nstruct GEMMSpiltKParams {\n  const int M;\n  const int N;\n  const int K;\n  const int lda;\n  const int ldb;\n  const int ldc;\n  const int tiles_n;\n  const int tiles_m;\n  const int split_k_partitions;\n  const int split_k_partition_stride;\n  const int split_k_partition_size;\n  const int gemm_k_iterations_aligned;\n};\nstruct GEMMAddMMParams {\n  const int ldc;\n  const int fdc;\n  const size_t batch_stride_c;\n  const float alpha;\n  const float beta;\n};\n}\n}\nusing namespace metal;\nnamespace mlx {\nnamespace steel {\ntemplate <bool M_aligned, bool N_aligned, bool K_aligned>\nstruct LoopAlignment {};\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    bool MN_aligned,\n    bool K_aligned,\n    typename AccumType = typename AccumHelper<T>::accum_type,\n    typename Epilogue = TransformNone<U, AccumType>>\nstruct GEMMKernel {\n  static constant constexpr const short tgp_padding_a = 16 / sizeof(T);\n  static constant constexpr const short tgp_padding_b = 16 / sizeof(T);\n  static constant constexpr const short tgp_mem_size_a =\n      transpose_a ? BK * (BM + tgp_padding_a) : BM * (BK + tgp_padding_a);\n  static constant constexpr const short tgp_mem_size_b =\n      transpose_b ? BN * (BK + tgp_padding_b) : BK * (BN + tgp_padding_b);\n  static constant constexpr const short tgp_mem_size = tgp_mem_size_a + tgp_mem_size_b;\n  static constant constexpr const short tgp_size = WM * WN * 32;\n  using loader_a_t = BlockLoader<\n      T,\n      transpose_a ? BK : BM,\n      transpose_a ? BM : BK,\n      transpose_a ? BM + tgp_padding_a : BK + tgp_padding_a,\n      !transpose_a,\n      tgp_size>;\n  using loader_b_t = BlockLoader<\n      T,\n      transpose_b ? BN : BK,\n      transpose_b ? BK : BN,\n      transpose_b ? BK + tgp_padding_b : BN + tgp_padding_b,\n      transpose_b,\n      tgp_size>;\n  using mma_t = BlockMMA<\n      T,\n      U,\n      BM,\n      BN,\n      BK,\n      WM,\n      WN,\n      transpose_a,\n      transpose_b,\n      transpose_a ? BM + tgp_padding_a : BK + tgp_padding_a,\n      transpose_b ? BK + tgp_padding_b : BN + tgp_padding_b,\n      AccumType,\n      Epilogue>;\n  template <bool M_aligned, bool N_aligned, bool K_aligned_>\n  static METAL_FUNC void gemm_loop(\n      threadgroup T* As [[threadgroup(0)]],\n      threadgroup T* Bs [[threadgroup(1)]],\n      const int gemm_k_iterations,\n      thread loader_a_t& loader_a,\n      thread loader_b_t& loader_b,\n      thread mma_t& mma_op,\n      thread const short& tgp_bm,\n      thread const short& tgp_bn,\n      thread const short& lbk,\n      LoopAlignment<M_aligned, N_aligned, K_aligned_> l = {}) {\n    (void)l;\n    short2 tile_dims_A = transpose_a ? short2(tgp_bm, BK) : short2(BK, tgp_bm);\n    short2 tile_dims_B = transpose_b ? short2(BK, tgp_bn) : short2(tgp_bn, BK);\n    for (int k = 0; k < gemm_k_iterations; k++) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      if (M_aligned) {\n        loader_a.load_unsafe();\n      } else {\n        loader_a.load_safe(tile_dims_A);\n      }\n      if (N_aligned) {\n        loader_b.load_unsafe();\n      } else {\n        loader_b.load_safe(tile_dims_B);\n      }\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      mma_op.mma(As, Bs);\n      loader_a.next();\n      loader_b.next();\n    }\n    if (!K_aligned_) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      short2 tile_dims_A_last =\n          transpose_a ? short2(tgp_bm, lbk) : short2(lbk, tgp_bm);\n      short2 tile_dims_B_last =\n          transpose_b ? short2(lbk, tgp_bn) : short2(tgp_bn, lbk);\n      loader_a.load_safe(tile_dims_A_last);\n      loader_b.load_safe(tile_dims_B_last);\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      mma_op.mma(As, Bs);\n    }\n  }\n  static METAL_FUNC void run(\n      const device T* A [[buffer(0)]],\n      const device T* B [[buffer(1)]],\n      device U* D [[buffer(2)]],\n      const constant GEMMParams* params [[buffer(3)]],\n      threadgroup T* As [[threadgroup(0)]],\n      threadgroup T* Bs [[threadgroup(1)]],\n      uint simd_lane_id [[thread_index_in_simdgroup]],\n      uint simd_group_id [[simdgroup_index_in_threadgroup]],\n      uint3 tid [[threadgroup_position_in_grid]],\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    (void)lid;\n    const int tid_y = ((tid.y) << params->swizzle_log) +\n        ((tid.x) & ((1 << params->swizzle_log) - 1));\n    const int tid_x = (tid.x) >> params->swizzle_log;\n    if (params->tiles_n <= tid_x || params->tiles_m <= tid_y) {\n      return;\n    }\n    threadgroup_barrier(mem_flags::mem_none);\n    const int c_row = tid_y * BM;\n    const int c_col = tid_x * BN;\n    const size_t c_row_long = size_t(c_row);\n    const size_t c_col_long = size_t(c_col);\n    A += transpose_a ? c_row_long : c_row_long * params->lda;\n    B += transpose_b ? c_col_long * params->ldb : c_col_long;\n    D += c_row_long * params->ldd + c_col_long;\n    thread loader_a_t loader_a(A, params->lda, As, simd_group_id, simd_lane_id);\n    thread loader_b_t loader_b(B, params->ldb, Bs, simd_group_id, simd_lane_id);\n    thread mma_t mma_op(simd_group_id, simd_lane_id);\n    int gemm_k_iterations = params->gemm_k_iterations_aligned;\n    if (MN_aligned) {\n      for (int k = 0; k < gemm_k_iterations; k++) {\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        loader_a.load_unsafe();\n        loader_b.load_unsafe();\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        mma_op.mma(As, Bs);\n        loader_a.next();\n        loader_b.next();\n      }\n      threadgroup_barrier(mem_flags::mem_none);\n      if (!K_aligned) {\n        int lbk = params->K - params->gemm_k_iterations_aligned * BK;\n        short2 tile_dims_A = transpose_a ? short2(BM, lbk) : short2(lbk, BM);\n        short2 tile_dims_B = transpose_b ? short2(lbk, BN) : short2(BN, lbk);\n        loader_a.load_safe(tile_dims_A);\n        loader_b.load_safe(tile_dims_B);\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        mma_op.mma(As, Bs);\n      }\n      mma_op.store_result(D, params->ldd);\n      return;\n    }\n    else {\n      short tgp_bm = min(BM, params->M - c_row);\n      short tgp_bn = min(BN, params->N - c_col);\n      short leftover_bk = params->K - params->gemm_k_iterations_aligned * BK;\n      if (tgp_bm == BM && tgp_bn == BN) {\n        gemm_loop<true, true, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result(D, params->ldd);\n        return;\n      } else if (tgp_bn == BN) {\n        gemm_loop<false, true, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n        return;\n      } else if (tgp_bm == BM) {\n        gemm_loop<true, false, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n        return;\n      } else {\n        gemm_loop<false, false, K_aligned>(\n            As,\n            Bs,\n            gemm_k_iterations,\n            loader_a,\n            loader_b,\n            mma_op,\n            tgp_bm,\n            tgp_bn,\n            leftover_bk);\n        mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n        return;\n      }\n    }\n  }\n};\n}\n}\n"
+ "\nstruct Add {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x + y;\n  }\n};\nstruct FloorDivide {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x / y;\n  }\n  template <>\n  float operator()(float x, float y) {\n    return trunc(x / y);\n  }\n  template <>\n  half operator()(half x, half y) {\n    return trunc(x / y);\n  }\n  template <>\n  bfloat16_t operator()(bfloat16_t x, bfloat16_t y) {\n    return trunc(x / y);\n  }\n};\nstruct Divide {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x / y;\n  }\n};\nstruct Remainder {\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T> & !metal::is_signed_v<T>, T>\n  operator()(T x, T y) {\n    return x % y;\n  }\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T> & metal::is_signed_v<T>, T>\n  operator()(T x, T y) {\n    auto r = x % y;\n    if (r != 0 && (r < 0 != y < 0)) {\n      r += y;\n    }\n    return r;\n  }\n  template <typename T>\n  metal::enable_if_t<!metal::is_integral_v<T>, T> operator()(T x, T y) {\n    T r = fmod(x, y);\n    if (r != 0 && (r < 0 != y < 0)) {\n      r += y;\n    }\n    return r;\n  }\n  template <>\n  complex64_t operator()(complex64_t x, complex64_t y) {\n    return x % y;\n  }\n};\nstruct Equal {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x == y;\n  }\n};\nstruct NaNEqual {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x == y || (metal::isnan(x) && metal::isnan(y));\n  }\n  template <>\n  bool operator()(complex64_t x, complex64_t y) {\n    return x == y ||\n        (metal::isnan(x.real) && metal::isnan(y.real) && metal::isnan(x.imag) &&\n         metal::isnan(y.imag)) ||\n        (x.real == y.real && metal::isnan(x.imag) && metal::isnan(y.imag)) ||\n        (metal::isnan(x.real) && metal::isnan(y.real) && x.imag == y.imag);\n  }\n};\nstruct Greater {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x > y;\n  }\n};\nstruct GreaterEqual {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x >= y;\n  }\n};\nstruct Less {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x < y;\n  }\n};\nstruct LessEqual {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x <= y;\n  }\n};\nstruct LogAddExp {\n  template <typename T>\n  T operator()(T x, T y) {\n    if (metal::isnan(x) || metal::isnan(y)) {\n      return metal::numeric_limits<T>::quiet_NaN();\n    }\n    constexpr T inf = metal::numeric_limits<T>::infinity();\n    T maxval = metal::max(x, y);\n    T minval = metal::min(x, y);\n    return (minval == -inf || maxval == inf)\n        ? maxval\n        : (maxval + log1p(metal::exp(minval - maxval)));\n  };\n};\nstruct Maximum {\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T>, T> operator()(T x, T y) {\n    return metal::max(x, y);\n  }\n  template <typename T>\n  metal::enable_if_t<!metal::is_integral_v<T>, T> operator()(T x, T y) {\n    if (metal::isnan(x)) {\n      return x;\n    }\n    return x > y ? x : y;\n  }\n  template <>\n  complex64_t operator()(complex64_t x, complex64_t y) {\n    if (metal::isnan(x.real) || metal::isnan(x.imag)) {\n      return x;\n    }\n    return x > y ? x : y;\n  }\n};\nstruct Minimum {\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T>, T> operator()(T x, T y) {\n    return metal::min(x, y);\n  }\n  template <typename T>\n  metal::enable_if_t<!metal::is_integral_v<T>, T> operator()(T x, T y) {\n    if (metal::isnan(x)) {\n      return x;\n    }\n    return x < y ? x : y;\n  }\n  template <>\n  complex64_t operator()(complex64_t x, complex64_t y) {\n    if (metal::isnan(x.real) || metal::isnan(x.imag)) {\n      return x;\n    }\n    return x < y ? x : y;\n  }\n};\nstruct Multiply {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x * y;\n  }\n};\nstruct NotEqual {\n  template <typename T>\n  bool operator()(T x, T y) {\n    return x != y;\n  }\n  template <>\n  bool operator()(complex64_t x, complex64_t y) {\n    return x.real != y.real || x.imag != y.imag;\n  }\n};\nstruct Power {\n  template <typename T>\n  metal::enable_if_t<!metal::is_integral_v<T>, T> operator()(T base, T exp) {\n    return metal::pow(base, exp);\n  }\n  template <typename T>\n  metal::enable_if_t<metal::is_integral_v<T>, T> operator()(T base, T exp) {\n    T res = 1;\n    while (exp) {\n      if (exp & 1) {\n        res *= base;\n      }\n      exp >>= 1;\n      base *= base;\n    }\n    return res;\n  }\n  template <>\n  complex64_t operator()(complex64_t x, complex64_t y) {\n    auto x_theta = metal::atan2(x.imag, x.real);\n    auto x_ln_r = 0.5 * metal::log(x.real * x.real + x.imag * x.imag);\n    auto mag = metal::exp(y.real * x_ln_r - y.imag * x_theta);\n    auto phase = y.imag * x_ln_r + y.real * x_theta;\n    return {mag * metal::cos(phase), mag * metal::sin(phase)};\n  }\n};\nstruct Subtract {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x - y;\n  }\n};\nstruct LogicalAnd {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x && y;\n  };\n};\nstruct LogicalOr {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x || y;\n  };\n};\nstruct BitwiseAnd {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x & y;\n  };\n};\nstruct BitwiseOr {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x | y;\n  };\n};\nstruct BitwiseXor {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x ^ y;\n  };\n};\nstruct LeftShift {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x << y;\n  };\n};\nstruct RightShift {\n  template <typename T>\n  T operator()(T x, T y) {\n    return x >> y;\n  };\n};\nstruct ArcTan2 {\n  template <typename T>\n  T operator()(T y, T x) {\n    return metal::precise::atan2(y, x);\n  }\n};\nstruct DivMod {\n  template <typename T>\n  metal::array<T, 2> operator()(T x, T y) {\n    return {FloorDivide{}(x, y), Remainder{}(x, y)};\n  };\n};\n"
+ "\nstruct Select {\n  template <typename T>\n  T operator()(bool condition, T x, T y) {\n    return condition ? x : y;\n  }\n};\n"
+ "\ntemplate <typename IdxT, int NIDX>\nstruct Indices {\n  const array<const device IdxT*, NIDX> buffers;\n  const constant int* shapes;\n  const constant size_t* strides;\n  const constant bool* row_contiguous;\n  const int ndim;\n};\ntemplate <typename IdxT>\nMETAL_FUNC size_t offset_neg_idx(IdxT idx, int size) {\n  if (is_unsigned_v<IdxT>) {\n    return idx;\n  } else {\n    return (idx < 0) ? idx + size : idx;\n  }\n}\n\ntemplate <\n    typename T,\n    typename IdxT,\n    typename Op,\n    int NIDX,\n    bool UPD_ROW_CONTIG,\n    int NWORK,\n    typename LocT>\nMETAL_FUNC void scatter_impl(\n    const device T* updates,\n    device mlx_atomic<T>* out,\n    const constant int* upd_shape,\n    const constant size_t* upd_strides,\n    const constant size_t& upd_ndim,\n    const constant size_t& upd_size,\n    const constant int* out_shape,\n    const constant size_t* out_strides,\n    const constant size_t& out_ndim,\n    const constant int* axes,\n    const constant size_t& idx_size,\n    const thread Indices<IdxT, NIDX>& indices,\n    uint2 gid [[thread_position_in_grid]]) {\n  Op op;\n  auto ind_idx = gid.y * NWORK;\n  LocT out_offset = 0;\n  if (upd_size > 1) {\n    out_offset = elem_to_loc<size_t, LocT>(\n        gid.x, upd_shape + indices.ndim, out_strides, out_ndim);\n  }\n  for (int j = 0; j < NWORK && ind_idx < idx_size; ++j, ind_idx++) {\n    LocT out_idx = out_offset;\n    for (int i = 0; i < NIDX; ++i) {\n      auto idx_loc = indices.row_contiguous[i]\n          ? ind_idx\n          : elem_to_loc<size_t, LocT>(\n                ind_idx,\n                &indices.shapes[indices.ndim * i],\n                &indices.strides[indices.ndim * i],\n                indices.ndim);\n      auto ax = axes[i];\n      auto idx_val = offset_neg_idx(indices.buffers[i][idx_loc], out_shape[ax]);\n      out_idx +=\n          static_cast<LocT>(idx_val) * static_cast<LocT>(out_strides[ax]);\n    }\n    auto upd_idx = ind_idx * static_cast<LocT>(upd_size) + gid.x;\n    if constexpr (!UPD_ROW_CONTIG) {\n      upd_idx =\n          elem_to_loc<size_t, LocT>(upd_idx, upd_shape, upd_strides, upd_ndim);\n    }\n    op.atomic_update(out, updates[upd_idx], out_idx);\n  }\n}\n"
+ "\ntemplate <typename IdxT, int NIDX>\nstruct Indices {\n  const array<const device IdxT*, NIDX> buffers;\n  const constant int* shapes;\n  const constant size_t* strides;\n  const constant bool* row_contiguous;\n  const int ndim;\n};\ntemplate <typename IdxT>\nMETAL_FUNC size_t offset_neg_idx(IdxT idx, int size) {\n  if (is_unsigned_v<IdxT>) {\n    return idx;\n  } else {\n    return (idx < 0) ? idx + size : idx;\n  }\n}\n\ntemplate <typename T, typename IdxT, int NIDX, int IDX_NDIM, typename LocT>\nMETAL_FUNC void gather_impl(\n    const device T* src [[buffer(0)]],\n    device T* out [[buffer(1)]],\n    const constant int* src_shape [[buffer(2)]],\n    const constant size_t* src_strides [[buffer(3)]],\n    const constant size_t& src_ndim [[buffer(4)]],\n    const constant int* slice_sizes [[buffer(5)]],\n    const constant int* axes [[buffer(6)]],\n    const thread Indices<IdxT, NIDX>& indices,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  LocT src_idx = 0;\n  for (int i = 0; i < NIDX; ++i) {\n    LocT idx_loc;\n    if (IDX_NDIM == 0) {\n      idx_loc = 0;\n    } else if (IDX_NDIM == 1) {\n      idx_loc = index.x * static_cast<LocT>(indices.strides[indices.ndim * i]);\n    } else {\n      idx_loc = index.x * static_cast<LocT>(indices.strides[indices.ndim * i]);\n      idx_loc += indices.row_contiguous[i]\n          ? index.y\n          : elem_to_loc<size_t, LocT>(\n                index.y,\n                &indices.shapes[indices.ndim * i + 1],\n                &indices.strides[indices.ndim * i + 1],\n                indices.ndim - 1);\n    }\n    auto ax = axes[i];\n    auto idx_val = offset_neg_idx(indices.buffers[i][idx_loc], src_shape[ax]);\n    src_idx += static_cast<LocT>(idx_val) * static_cast<LocT>(src_strides[ax]);\n  }\n  auto src_offset =\n      elem_to_loc<size_t, LocT>(index.z, slice_sizes, src_strides, src_ndim);\n  LocT out_idx = index.z;\n  if (IDX_NDIM == 1) {\n    out_idx += static_cast<LocT>(grid_dim.z) * index.x;\n  } else if (IDX_NDIM >= 2) {\n    out_idx += grid_dim.z * (index.x * static_cast<LocT>(grid_dim.y) + index.y);\n  }\n  out[out_idx] = src[src_offset + src_idx];\n}\n"
+ "\ntemplate <typename T, typename Op>\n[[kernel]] void ternary_v(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    uint index [[thread_position_in_grid]]) {\n  d[index] = Op()(a[index], b[index], c[index]);\n}\ntemplate <typename T, typename Op>\n[[kernel]] void ternary_v2(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  d[offset] = Op()(a[offset], b[offset], c[offset]);\n}\ntemplate <typename T, typename Op>\n[[kernel]] void ternary_g_nd1(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const size_t& a_strides,\n    constant const size_t& b_strides,\n    constant const size_t& c_strides,\n    uint index [[thread_position_in_grid]]) {\n  auto a_idx = elem_to_loc_1<size_t, uint>(index, a_strides);\n  auto b_idx = elem_to_loc_1<size_t, uint>(index, b_strides);\n  auto c_idx = elem_to_loc_1<size_t, uint>(index, c_strides);\n  d[index] = Op()(a[a_idx], b[b_idx], c[c_idx]);\n}\ntemplate <typename T, typename Op, typename IdxT = size_t>\n[[kernel]] void ternary_g_nd2(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const size_t a_strides[2],\n    constant const size_t b_strides[2],\n    constant const size_t c_strides[2],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_2<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_2<size_t, IdxT>(index, b_strides);\n  auto c_idx = elem_to_loc_2<size_t, IdxT>(index, c_strides);\n  IdxT out_idx = index.x + IdxT(grid_dim.x) * index.y;\n  d[out_idx] = Op()(a[a_idx], b[b_idx], c[c_idx]);\n}\ntemplate <typename T, typename Op, typename IdxT = size_t>\n[[kernel]] void ternary_g_nd3(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const size_t a_strides[3],\n    constant const size_t b_strides[3],\n    constant const size_t c_strides[3],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_3<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_3<size_t, IdxT>(index, b_strides);\n  auto c_idx = elem_to_loc_3<size_t, IdxT>(index, c_strides);\n  IdxT out_idx = index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n  d[out_idx] = Op()(a[a_idx], b[b_idx], c[c_idx]);\n}\ntemplate <typename T, typename Op, int N = 1, typename IdxT = size_t>\n[[kernel]] void ternary_g(\n    device const bool* a,\n    device const T* b,\n    device const T* c,\n    device T* d,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const size_t* c_strides,\n    constant const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc_3_nd<IdxT>(\n      {N * index.x, index.y, index.z},\n      shape,\n      a_strides,\n      b_strides,\n      c_strides,\n      ndim);\n  auto xshape = shape[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  IdxT a_xstride = a_strides[ndim - 1];\n  IdxT b_xstride = b_strides[ndim - 1];\n  IdxT c_xstride = c_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    d[out_idx++] = Op()(a[idx.x], b[idx.y], c[idx.z]);\n    idx.x += a_xstride;\n    idx.y += b_xstride;\n    idx.z += c_xstride;\n  }\n}\n"
+ "\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_ss(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[0], b[0]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[0], b[index]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[index], b[0]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint index [[thread_position_in_grid]]) {\n  auto out = Op()(a[index], b[index]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  auto out = Op()(a[0], b[offset]);\n  c[offset] = out[0];\n  d[offset] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  auto out = Op()(a[offset], b[0]);\n  c[offset] = out[0];\n  d[offset] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  auto out = Op()(a[offset], b[offset]);\n  c[offset] = out[0];\n  d[offset] = out[1];\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_g_nd1(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const size_t& a_stride,\n    constant const size_t& b_stride,\n    uint index [[thread_position_in_grid]]) {\n  auto a_idx = elem_to_loc_1<size_t, uint>(index, a_stride);\n  auto b_idx = elem_to_loc_1<size_t, uint>(index, b_stride);\n  auto out = Op()(a[a_idx], b[b_idx]);\n  c[index] = out[0];\n  d[index] = out[1];\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = size_t>\n[[kernel]] void binary_g_nd2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const size_t a_strides[2],\n    constant const size_t b_strides[2],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_2<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_2<size_t, IdxT>(index, b_strides);\n  IdxT out_idx = index.x + IdxT(grid_dim.x) * index.y;\n  auto out = Op()(a[a_idx], b[b_idx]);\n  c[out_idx] = out[0];\n  d[out_idx] = out[1];\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = size_t>\n[[kernel]] void binary_g_nd3(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const size_t a_strides[3],\n    constant const size_t b_strides[3],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_3<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_3<size_t, IdxT>(index, b_strides);\n  IdxT out_idx = index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n  auto out = Op()(a[a_idx], b[b_idx]);\n  c[out_idx] = out[0];\n  d[out_idx] = out[1];\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N = 1,\n    typename IdxT = size_t>\n[[kernel]] void binary_g(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    device U* d,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc_2_nd<size_t, IdxT>(\n      {N * index.x, index.y, index.z}, shape, a_strides, b_strides, ndim);\n  auto xshape = shape[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  IdxT a_xstride = a_strides[ndim - 1];\n  IdxT b_xstride = b_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    auto out = Op()(a[idx.x], b[idx.y]);\n    c[out_idx] = out[0];\n    d[out_idx++] = out[1];\n    idx.x += a_xstride;\n    idx.y += b_xstride;\n  }\n}\n"
+ "\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_ss(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[0], b[0]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[0], b[index]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[index], b[0]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint index [[thread_position_in_grid]]) {\n  c[index] = Op()(a[index], b[index]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_sv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  c[offset] = Op()(a[0], b[offset]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vs2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  c[offset] = Op()(a[offset], b[0]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_vv2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  c[offset] = Op()(a[offset], b[offset]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void binary_g_nd1(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const size_t& a_stride,\n    constant const size_t& b_stride,\n    uint index [[thread_position_in_grid]]) {\n  auto a_idx = elem_to_loc_1<size_t, uint>(index, a_stride);\n  auto b_idx = elem_to_loc_1<size_t, uint>(index, b_stride);\n  c[index] = Op()(a[a_idx], b[b_idx]);\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = size_t>\n[[kernel]] void binary_g_nd2(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const size_t a_strides[2],\n    constant const size_t b_strides[2],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_2<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_2<size_t, IdxT>(index, b_strides);\n  IdxT out_idx = index.x + IdxT(grid_dim.x) * index.y;\n  c[out_idx] = Op()(a[a_idx], b[b_idx]);\n}\ntemplate <typename T, typename U, typename Op, typename IdxT = size_t>\n[[kernel]] void binary_g_nd3(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const size_t a_strides[3],\n    constant const size_t b_strides[3],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto a_idx = elem_to_loc_3<size_t, IdxT>(index, a_strides);\n  auto b_idx = elem_to_loc_3<size_t, IdxT>(index, b_strides);\n  IdxT out_idx = index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n  c[out_idx] = Op()(a[a_idx], b[b_idx]);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N = 1,\n    typename IdxT = size_t>\n[[kernel]] void binary_g(\n    device const T* a,\n    device const T* b,\n    device U* c,\n    constant const int* shape,\n    constant const size_t* a_strides,\n    constant const size_t* b_strides,\n    constant const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc_2_nd<size_t, IdxT>(\n      {N * index.x, index.y, index.z}, shape, a_strides, b_strides, ndim);\n  auto xshape = shape[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  IdxT a_xstride = a_strides[ndim - 1];\n  IdxT b_xstride = b_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    c[out_idx++] = Op()(a[idx.x], b[idx.y]);\n    idx.x += a_xstride;\n    idx.y += b_xstride;\n  }\n}\n"
+ "\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void unary_v(\n    device const T* in,\n    device U* out,\n    uint index [[thread_position_in_grid]]) {\n  out[index] = Op()(in[index]);\n}\ntemplate <typename T, typename U, typename Op>\n[[kernel]] void unary_v2(\n    device const T* in,\n    device U* out,\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  out[offset] = Op()(in[offset]);\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N = 1,\n    typename IdxT = size_t>\n[[kernel]] void unary_g(\n    device const T* in,\n    device U* out,\n    constant const int* in_shape,\n    constant const size_t* in_strides,\n    device const int& ndim,\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto idx = elem_to_loc<size_t, IdxT>(\n      {N * index.x, index.y, index.z}, in_shape, in_strides, ndim);\n  auto xshape = in_shape[ndim - 1];\n  IdxT xstride = in_strides[ndim - 1];\n  IdxT out_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    out[out_idx++] = Op()(in[idx]);\n    idx += xstride;\n  }\n}\n"
+ "\ntemplate <typename T, typename U>\n[[kernel]] void copy_s(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint index [[thread_position_in_grid]]) {\n  dst[index] = static_cast<U>(src[0]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_v(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint index [[thread_position_in_grid]]) {\n  dst[index] = static_cast<U>(src[index]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_s2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  dst[offset] = static_cast<U>(src[0]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_v2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  size_t offset = index.x + grid_dim.x * size_t(index.y);\n  dst[offset] = static_cast<U>(src[offset]);\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_g_nd1(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t& src_stride [[buffer(3)]],\n    uint index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_1<int64_t, int>(index, src_stride);\n  dst[index] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_g_nd2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    uint2 index [[thread_position_in_grid]],\n    uint2 grid_dim [[threads_per_grid]]) {\n  auto src_idx = elem_to_loc_2<int64_t, IdxT>(index, src_strides);\n  IdxT dst_idx = index.x + IdxT(grid_dim.x) * index.y;\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_g_nd3(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto src_idx = elem_to_loc_3<int64_t, IdxT>(index, src_strides);\n  IdxT dst_idx =\n      index.x + IdxT(grid_dim.x) * (index.y + IdxT(grid_dim.y) * index.z);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, int N = 1, typename IdxT = int64_t>\n[[kernel]] void copy_g(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int* src_shape [[buffer(2)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int& ndim [[buffer(5)]],\n    uint3 index [[thread_position_in_grid]],\n    uint3 grid_dim [[threads_per_grid]]) {\n  auto src_idx = elem_to_loc<int64_t, IdxT>(\n      {N * index.x, index.y, index.z}, src_shape, src_strides, ndim);\n  if (N == 1) {\n    IdxT dst_idx =\n        index.x + grid_dim.x * (index.y + IdxT(grid_dim.y) * index.z);\n    dst[dst_idx] = static_cast<U>(src[src_idx]);\n    return;\n  }\n  auto xshape = src_shape[ndim - 1];\n  IdxT dst_idx = N * index.x + xshape * (index.y + IdxT(grid_dim.y) * index.z);\n  auto src_xstride = src_strides[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    dst[dst_idx + i] = static_cast<U>(src[src_idx]);\n    src_idx += src_xstride;\n  }\n}\ntemplate <typename T, typename U>\n[[kernel]] void copy_gg_nd1(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t& src_stride [[buffer(3)]],\n    constant const int64_t& dst_stride [[buffer(4)]],\n    uint index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_1<int64_t, int>(index, src_stride);\n  auto dst_idx = elem_to_loc_1<int64_t, int>(index, dst_stride);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_nd2(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    uint2 index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_2<int64_t, IdxT>(index, src_strides);\n  auto dst_idx = elem_to_loc_2<int64_t, IdxT>(index, dst_strides);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, typename IdxT = int64_t>\n[[kernel]] void copy_gg_nd3(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    uint3 index [[thread_position_in_grid]]) {\n  auto src_idx = elem_to_loc_3<int64_t, IdxT>(index, src_strides);\n  auto dst_idx = elem_to_loc_3<int64_t, IdxT>(index, dst_strides);\n  dst[dst_idx] = static_cast<U>(src[src_idx]);\n}\ntemplate <typename T, typename U, int N = 1, typename IdxT = int64_t>\n[[kernel]] void copy_gg(\n    device const T* src [[buffer(0)]],\n    device U* dst [[buffer(1)]],\n    constant const int* src_shape [[buffer(2)]],\n    constant const int64_t* src_strides [[buffer(3)]],\n    constant const int64_t* dst_strides [[buffer(4)]],\n    constant const int& ndim [[buffer(5)]],\n    uint3 index [[thread_position_in_grid]]) {\n  auto idx = elem_to_loc_2_nd<int64_t, IdxT>(\n      {N * index.x, index.y, index.z},\n      src_shape,\n      src_strides,\n      dst_strides,\n      ndim);\n  if (N == 1) {\n    dst[idx.y] = static_cast<U>(src[idx.x]);\n    return;\n  }\n  IdxT src_xstride = src_strides[ndim - 1];\n  IdxT dst_xstride = dst_strides[ndim - 1];\n  auto xshape = src_shape[ndim - 1];\n  for (int i = 0; i < N && (int(N * index.x) + i) < xshape; ++i) {\n    dst[idx.y] = static_cast<U>(src[idx.x]);\n    idx.x += src_xstride;\n    idx.y += dst_xstride;\n  }\n}\n"
+ "\ntemplate <typename U>\nstruct CumSum {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_scan(T val) { return simd_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_scan(T val) { for (int i = 1; i <= 16; i *= 2) { val = operator()(val, simd_shuffle_and_fill_up(val, init, i)); } return val; }\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_exclusive_scan(T val) { return simd_exclusive_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_exclusive_scan(T val) { val = simd_scan(val); return simd_shuffle_and_fill_up(val, init, 1); }\n  static constexpr constant U init = static_cast<U>(0);\n  template <typename T>\n  U operator()(U a, T b) {\n    return a + b;\n  }\n  U simd_scan_impl(U x) {\n    return simd_prefix_inclusive_sum(x);\n  }\n  U simd_exclusive_scan_impl(U x) {\n    return simd_prefix_exclusive_sum(x);\n  }\n};\ntemplate <typename U>\nstruct CumProd {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_scan(T val) { return simd_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_scan(T val) { for (int i = 1; i <= 16; i *= 2) { val = operator()(val, simd_shuffle_and_fill_up(val, init, i)); } return val; }\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_exclusive_scan(T val) { return simd_exclusive_scan_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_exclusive_scan(T val) { val = simd_scan(val); return simd_shuffle_and_fill_up(val, init, 1); }\n  static constexpr constant U init = static_cast<U>(1.0f);\n  template <typename T>\n  U operator()(U a, T b) {\n    return a * b;\n  }\n  U simd_scan_impl(U x) {\n    return simd_prefix_inclusive_product(x);\n  }\n  U simd_exclusive_scan_impl(U x) {\n    return simd_prefix_exclusive_product(x);\n  }\n};\ntemplate <>\nstruct CumProd<bool> {\n  static constexpr constant bool init = true;\n  template <typename T>\n  bool operator()(bool a, T b) {\n    return a & static_cast<bool>(b);\n  }\n  bool simd_scan(bool x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      bool other = simd_shuffle_and_fill_up(x, init, i);\n      x &= other;\n    }\n    return x;\n  }\n  bool simd_exclusive_scan(bool x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename U>\nstruct CumMax {\n  static constexpr constant U init = Limits<U>::min;\n  template <typename T>\n  U operator()(U a, T b) {\n    return (a >= b) ? a : b;\n  }\n  U simd_scan(U x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      U other = simd_shuffle_and_fill_up(x, init, i);\n      x = (x >= other) ? x : other;\n    }\n    return x;\n  }\n  U simd_exclusive_scan(U x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename U>\nstruct CumMin {\n  static constexpr constant U init = Limits<U>::max;\n  template <typename T>\n  U operator()(U a, T b) {\n    return (a <= b) ? a : b;\n  }\n  U simd_scan(U x) {\n    for (int i = 1; i <= 16; i *= 2) {\n      U other = simd_shuffle_and_fill_up(x, init, i);\n      x = (x <= other) ? x : other;\n    }\n    return x;\n  }\n  U simd_exclusive_scan(U x) {\n    x = simd_scan(x);\n    return simd_shuffle_and_fill_up(x, init, 1);\n  }\n};\ntemplate <typename T, typename U, int N_READS, bool reverse>\ninline void load_unsafe(U values[N_READS], const device T* input) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      values[N_READS - i - 1] = input[i];\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      values[i] = input[i];\n    }\n  }\n}\ntemplate <typename T, typename U, int N_READS, bool reverse>\ninline void load_safe(\n    U values[N_READS],\n    const device T* input,\n    int start,\n    int total,\n    U init) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      values[N_READS - i - 1] =\n          (start + N_READS - i - 1 < total) ? input[i] : init;\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      values[i] = (start + i < total) ? input[i] : init;\n    }\n  }\n}\ntemplate <typename U, int N_READS, bool reverse>\ninline void write_unsafe(U values[N_READS], device U* out) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      out[i] = values[N_READS - i - 1];\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      out[i] = values[i];\n    }\n  }\n}\ntemplate <typename U, int N_READS, bool reverse>\ninline void write_safe(U values[N_READS], device U* out, int start, int total) {\n  if (reverse) {\n    for (int i = 0; i < N_READS; i++) {\n      if (start + N_READS - i - 1 < total) {\n        out[i] = values[N_READS - i - 1];\n      }\n    }\n  } else {\n    for (int i = 0; i < N_READS; i++) {\n      if (start + i < total) {\n        out[i] = values[i];\n      }\n    }\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS,\n    bool inclusive,\n    bool reverse>\n[[kernel]] void contiguous_scan(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& axis_size [[buffer(2)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 lsize [[threads_per_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  constexpr int simd_size = 32;\n  Op op;\n  size_t offset = (gid.y + gsize.y * size_t(gid.z)) * axis_size;\n  in += offset;\n  out += offset;\n  uint simd_groups = lsize.x / simd_size;\n  U prefix = Op::init;\n  U values[N_READS];\n  threadgroup U simdgroup_sums[32];\n  for (uint r = 0; r < ceildiv(axis_size, N_READS * lsize.x); r++) {\n    uint offset = r * lsize.x * N_READS + lid.x * N_READS;\n    if (reverse) {\n      if ((offset + N_READS) < axis_size) {\n        load_unsafe<T, U, N_READS, reverse>(\n            values, in + axis_size - offset - N_READS);\n      } else {\n        load_safe<T, U, N_READS, reverse>(\n            values,\n            in + axis_size - offset - N_READS,\n            offset,\n            axis_size,\n            Op::init);\n      }\n    } else {\n      if ((offset + N_READS) < axis_size) {\n        load_unsafe<T, U, N_READS, reverse>(values, in + offset);\n      } else {\n        load_safe<T, U, N_READS, reverse>(\n            values, in + offset, offset, axis_size, Op::init);\n      }\n    }\n    for (int i = 1; i < N_READS; i++) {\n      values[i] = op(values[i], values[i - 1]);\n    }\n    U prev_thread = op.simd_exclusive_scan(values[N_READS - 1]);\n    if (simd_lane_id == simd_size - 1) {\n      simdgroup_sums[simd_group_id] = op(prev_thread, values[N_READS - 1]);\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (simd_group_id == 0) {\n      U prev_simdgroup = op.simd_exclusive_scan(simdgroup_sums[simd_lane_id]);\n      simdgroup_sums[simd_lane_id] = prev_simdgroup;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < N_READS; i++) {\n      values[i] = op(values[i], prefix);\n      values[i] = op(values[i], simdgroup_sums[simd_group_id]);\n      values[i] = op(values[i], prev_thread);\n    }\n    if (reverse) {\n      if (inclusive) {\n        if ((offset + N_READS) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(\n              values, out + axis_size - offset - N_READS);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values, out + axis_size - offset - N_READS, offset, axis_size);\n        }\n      } else {\n        if (lid.x == 0 && offset == 0) {\n          out[axis_size - 1] = Op::init;\n        }\n        if ((offset + N_READS + 1) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(\n              values, out + axis_size - offset - 1 - N_READS);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values,\n              out + axis_size - offset - 1 - N_READS,\n              offset + 1,\n              axis_size);\n        }\n      }\n    } else {\n      if (inclusive) {\n        if ((offset + N_READS) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(values, out + offset);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values, out + offset, offset, axis_size);\n        }\n      } else {\n        if (lid.x == 0 && offset == 0) {\n          out[0] = Op::init;\n        }\n        if ((offset + N_READS + 1) < axis_size) {\n          write_unsafe<U, N_READS, reverse>(values, out + offset + 1);\n        } else {\n          write_safe<U, N_READS, reverse>(\n              values, out + offset + 1, offset + 1, axis_size);\n        }\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (simd_group_id == simd_groups - 1 && simd_lane_id == simd_size - 1) {\n      simdgroup_sums[0] = values[N_READS - 1];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    prefix = simdgroup_sums[0];\n  }\n}\ntemplate <\n    typename T,\n    typename U,\n    typename Op,\n    int N_READS,\n    bool inclusive,\n    bool reverse>\n[[kernel]] void strided_scan(\n    const device T* in [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant size_t& axis_size [[buffer(2)]],\n    const constant size_t& stride [[buffer(3)]],\n    const constant size_t& stride_blocks [[buffer(4)]],\n    uint3 gid [[threadgroup_position_in_grid]],\n    uint3 gsize [[threadgroups_per_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]]) {\n  constexpr int simd_size = 32;\n  constexpr int BM = 32;\n  constexpr int BN = 32;\n  constexpr int BN_pad = 32 + 16 / sizeof(U);\n  constexpr int n_simds = BN / N_READS;\n  constexpr int n_scans = BN / n_simds;\n  Op op;\n  threadgroup U read_buffer[BM * BN_pad];\n  U values[n_scans];\n  U prefix[n_scans];\n  for (int i = 0; i < n_scans; i++) {\n    prefix[i] = Op::init;\n  }\n  size_t full_gid = gid.y + gsize.y * size_t(gid.z);\n  size_t offset = full_gid / stride_blocks * axis_size * stride;\n  size_t global_index_x = full_gid % stride_blocks * BN;\n  uint read_offset_y = (lid.x * N_READS) / BN;\n  uint read_offset_x = (lid.x * N_READS) % BN;\n  uint scan_offset_y = simd_lane_id;\n  uint scan_offset_x = simd_group_id * n_scans;\n  uint stride_limit = stride - global_index_x;\n  in += offset + global_index_x + read_offset_x;\n  out += offset + global_index_x + read_offset_x;\n  threadgroup U* read_into =\n      read_buffer + read_offset_y * BN_pad + read_offset_x;\n  threadgroup U* read_from =\n      read_buffer + scan_offset_y * BN_pad + scan_offset_x;\n  for (uint j = 0; j < axis_size; j += BM) {\n    uint index_y = j + read_offset_y;\n    uint check_index_y = index_y;\n    if (reverse) {\n      index_y = axis_size - 1 - index_y;\n    }\n    if (check_index_y < axis_size && (read_offset_x + N_READS) < stride_limit) {\n      for (int i = 0; i < N_READS; i++) {\n        read_into[i] = in[index_y * stride + i];\n      }\n    } else {\n      for (int i = 0; i < N_READS; i++) {\n        if (check_index_y < axis_size && (read_offset_x + i) < stride_limit) {\n          read_into[i] = in[index_y * stride + i];\n        } else {\n          read_into[i] = Op::init;\n        }\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < n_scans; i++) {\n      values[i] = read_from[i];\n    }\n    simdgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < n_scans; i++) {\n      values[i] = op.simd_scan(values[i]);\n      values[i] = op(values[i], prefix[i]);\n      prefix[i] = simd_shuffle(values[i], simd_size - 1);\n    }\n    for (int i = 0; i < n_scans; i++) {\n      read_from[i] = values[i];\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    if (!inclusive) {\n      if (check_index_y == 0) {\n        if ((read_offset_x + N_READS) < stride_limit) {\n          for (int i = 0; i < N_READS; i++) {\n            out[index_y * stride + i] = Op::init;\n          }\n        } else {\n          for (int i = 0; i < N_READS; i++) {\n            if ((read_offset_x + i) < stride_limit) {\n              out[index_y * stride + i] = Op::init;\n            }\n          }\n        }\n      }\n      if (reverse) {\n        index_y -= 1;\n        check_index_y += 1;\n      } else {\n        index_y += 1;\n        check_index_y += 1;\n      }\n    }\n    if (check_index_y < axis_size && (read_offset_x + N_READS) < stride_limit) {\n      for (int i = 0; i < N_READS; i++) {\n        out[index_y * stride + i] = read_into[i];\n      }\n    } else {\n      for (int i = 0; i < N_READS; i++) {\n        if (check_index_y < axis_size && (read_offset_x + i) < stride_limit) {\n          out[index_y * stride + i] = read_into[i];\n        }\n      }\n    }\n  }\n}\n"
+ "\ntemplate [[host_name(\"{0}\")]] [[kernel]] decltype({1}) {1};\n"
+ "\ntemplate [[host_name(\"{0}\")]] [[kernel]] void arange<{1}>(\n    constant const {1}& start,\n    constant const {1}& step,\n    device {1}* out,\n    uint index [[thread_position_in_grid]]);\n"
+ "\ntemplate [[host_name(\"{name}\")]]\n[[kernel]] void gemm<{itype}, {bm}, {bn}, {bk}, {wm}, {wn}, {trans_a}, {trans_b}, float>(\n    const device {itype} *A [[buffer(0)]],\n    const device {itype} *B [[buffer(1)]],\n    const device {itype} *C [[buffer(2), function_constant(use_out_source)]],\n    device {itype} *D [[buffer(3)]],\n    const constant GEMMParams* params [[buffer(4)]],\n    const constant GEMMAddMMParams* addmm_params [[buffer(5), function_constant(use_out_source)]],\n    const constant int* batch_shape [[buffer(6)]],\n    const constant size_t* batch_strides [[buffer(7)]],\n    const constant uint32_t* lhs_indices [[buffer(10), function_constant(do_gather)]],\n    const constant uint32_t* rhs_indices [[buffer(11), function_constant(do_gather)]],\n    const constant uint32_t* C_indices [[buffer(12), function_constant(gather_bias)]],\n    const constant int* operand_shape [[buffer(13), function_constant(do_gather)]],\n    const constant size_t* operand_strides [[buffer(14), function_constant(do_gather)]],\n    const constant packed_int3& operand_batch_ndim [[buffer(15), function_constant(do_gather)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]);\n"
+ "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\n    implicit_gemm_conv_2d_general<{itype}, {bm}, {bn}, {bk}, {wm}, {wn}>(\n        const device {itype}* A [[buffer(0)]],\n        const device {itype}* B [[buffer(1)]],\n        device {itype}* C [[buffer(2)]],\n        const constant MLXConvParams<2>* params [[buffer(3)]],\n        const constant ImplicitGemmConv2DParams* gemm_params [[buffer(4)]],\n        const constant Conv2DGeneralJumpParams* jump_params [[buffer(5)]],\n        const constant Conv2DGeneralBaseInfo* base_h [[buffer(6)]],\n        const constant Conv2DGeneralBaseInfo* base_w [[buffer(7)]],\n        uint3 tid [[threadgroup_position_in_grid]],\n        uint3 lid [[thread_position_in_threadgroup]],\n        uint simd_gid [[simdgroup_index_in_threadgroup]],\n        uint simd_lid [[thread_index_in_simdgroup]]);\n"
+ "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\ngemm_splitk<\n    {itype},\n    {otype},\n    {bm},\n    {bn},\n    {bk},\n    {wm},\n    {wn},\n    {trans_a},\n    {trans_b},\n    {mn_aligned},\n    {k_aligned}>(\n    const device {itype}* A [[buffer(0)]],\n    const device {itype}* B [[buffer(1)]],\n    device {otype}* C [[buffer(2)]],\n    const constant GEMMSpiltKParams* params [[buffer(3)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]);\n"
+ "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\ngemm_splitk_accum<{atype}, {otype}>(\n    const device {atype}* C_split [[buffer(0)]],\n    device {otype}* D [[buffer(1)]],\n    const constant int& k_partitions [[buffer(2)]],\n    const constant int& partition_stride [[buffer(3)]],\n    const constant int& ldd [[buffer(4)]],\n    uint2 gid [[thread_position_in_grid]]);\n"
+ "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\ngemm_splitk_accum_axpby<{atype}, {otype}>(\n    const device {atype}* C_split [[buffer(0)]],\n    device {otype}* D [[buffer(1)]],\n    const constant int& k_partitions [[buffer(2)]],\n    const constant int& partition_stride [[buffer(3)]],\n    const constant int& ldd [[buffer(4)]],\n    const device {otype}* C [[buffer(5)]],\n    const constant int& ldc [[buffer(6)]],\n    const constant int& fdc [[buffer(7)]],\n    const constant float& alpha [[buffer(8)]],\n    const constant float& beta [[buffer(9)]],\n    uint2 gid [[thread_position_in_grid]]);\n"
+ "\ntemplate [[host_name(\"{name}\")]] [[kernel]] void\nimplicit_gemm_conv_2d<{itype}, {bm}, {bn}, {bk}, {wm}, {wn}, {n_channels}, {small_filter}>(\n    const device {itype}* A [[buffer(0)]],\n    const device {itype}* B [[buffer(1)]],\n    device {itype}* C [[buffer(2)]],\n    const constant MLXConvParams<2>* params [[buffer(3)]],\n    const constant ImplicitGemmConv2DParams* gemm_params [[buffer(4)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_gid [[simdgroup_index_in_threadgroup]],\n    uint simd_lid [[thread_index_in_simdgroup]]);\n"
+ "\nusing namespace metal;\n#pragma METAL internals : enable\ntemplate <typename T>\nconstexpr constant bool is_metal_atomic = _disjunction<\n    is_same<T, int>,\n    is_same<T, uint>,\n    is_same<T, ulong>,\n    is_same<T, float>>::value;\n#pragma METAL internals : disable\ntemplate <typename T, typename = void>\nstruct mlx_atomic {\n  atomic<uint> val;\n};\ntemplate <typename T>\nstruct mlx_atomic<T, enable_if_t<is_metal_atomic<T>>> {\n  atomic<T> val;\n};\ntemplate <typename T, enable_if_t<is_metal_atomic<T>, bool> = true>\nMETAL_FUNC T\nmlx_atomic_load_explicit(device mlx_atomic<T>* object, size_t offset) {\n  return atomic_load_explicit(&(object[offset].val), memory_order_relaxed);\n}\ntemplate <typename T, enable_if_t<is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void\nmlx_atomic_store_explicit(device mlx_atomic<T>* object, T val, size_t offset) {\n  atomic_store_explicit(&(object[offset].val), val, memory_order_relaxed);\n}\ntemplate <typename T, enable_if_t<is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_and_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  atomic_fetch_and_explicit(&(object[offset].val), val, memory_order_relaxed);\n}\ntemplate <typename T, enable_if_t<is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_or_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  atomic_fetch_or_explicit(&(object[offset].val), val, memory_order_relaxed);\n}\ntemplate <typename T, enable_if_t<is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_min_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  atomic_fetch_min_explicit(&(object[offset].val), val, memory_order_relaxed);\n}\ntemplate <typename T, enable_if_t<is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_max_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  atomic_fetch_max_explicit(&(object[offset].val), val, memory_order_relaxed);\n}\ntemplate <typename T, enable_if_t<is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_add_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  atomic_fetch_add_explicit(&(object[offset].val), val, memory_order_relaxed);\n}\ntemplate <typename T, enable_if_t<is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_mul_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  T expected = mlx_atomic_load_explicit(object, offset);\n  while (!mlx_atomic_compare_exchange_weak_explicit(\n      object, &expected, val * expected, offset)) {\n  }\n}\ntemplate <typename T, enable_if_t<is_metal_atomic<T>, bool> = true>\nMETAL_FUNC bool mlx_atomic_compare_exchange_weak_explicit(\n    device mlx_atomic<T>* object,\n    thread T* expected,\n    T val,\n    size_t offset) {\n  return atomic_compare_exchange_weak_explicit(\n      &(object[offset].val),\n      expected,\n      val,\n      memory_order_relaxed,\n      memory_order_relaxed);\n}\ntemplate <>\nMETAL_FUNC void mlx_atomic_fetch_min_explicit<float>(\n    device mlx_atomic<float>* object,\n    float val,\n    size_t offset) {\n  float expected = mlx_atomic_load_explicit(object, offset);\n  while (val < expected) {\n    if (mlx_atomic_compare_exchange_weak_explicit(\n            object, &expected, val, offset)) {\n      return;\n    }\n  }\n}\ntemplate <>\nMETAL_FUNC void mlx_atomic_fetch_max_explicit<float>(\n    device mlx_atomic<float>* object,\n    float val,\n    size_t offset) {\n  float expected = mlx_atomic_load_explicit(object, offset);\n  while (val > expected) {\n    if (mlx_atomic_compare_exchange_weak_explicit(\n            object, &expected, val, offset)) {\n      return;\n    }\n  }\n}\nnamespace {\ntemplate <typename T>\nconstexpr constant uint packing_size = sizeof(uint) / sizeof(T);\ntemplate <typename T>\nunion uint_or_packed {\n  T val[packing_size<T>];\n  uint bits;\n};\ntemplate <typename T, typename Op>\nstruct mlx_atomic_update_helper {\n  uint operator()(uint_or_packed<T> init, T update, size_t elem_offset) {\n    Op op;\n    init.val[elem_offset] = op(update, init.val[elem_offset]);\n    return init.bits;\n  }\n};\ntemplate <typename T, typename Op>\nMETAL_FUNC void mlx_atomic_update_and_store(\n    device mlx_atomic<T>* object,\n    T update,\n    size_t offset) {\n  size_t pack_offset = offset / packing_size<T>;\n  size_t elem_offset = offset % packing_size<T>;\n  mlx_atomic_update_helper<T, Op> helper;\n  uint_or_packed<T> expected;\n  expected.bits =\n      atomic_load_explicit(&(object[pack_offset].val), memory_order_relaxed);\n  while (Op::condition(update, expected.val[elem_offset]) &&\n         !mlx_atomic_compare_exchange_weak_explicit(\n             object,\n             &(expected.bits),\n             helper(expected, update, elem_offset),\n             pack_offset)) {\n  }\n}\ntemplate <typename T>\nstruct __None {\n  static bool condition(T a, T b) {\n#pragma unused(a)\n#pragma unused(b)\n    return true;\n  }\n  T operator()(T a, T b) {\n#pragma unused(b)\n    return a;\n  }\n};\ntemplate <typename T>\nstruct __Add {\n  static bool condition(T a, T b) {\n#pragma unused(a)\n#pragma unused(b)\n    return true;\n  }\n  T operator()(T a, T b) {\n    return a + b;\n  }\n};\ntemplate <typename T>\nstruct __Mul {\n  static bool condition(T a, T b) {\n#pragma unused(a)\n    return b != 0;\n  }\n  T operator()(T a, T b) {\n    return a * b;\n  }\n};\ntemplate <typename T>\nstruct __Max {\n  static bool condition(T a, T b) {\n    return a > b;\n  }\n  T operator()(T a, T b) {\n    return max(a, b);\n  }\n};\ntemplate <typename T>\nstruct __Min {\n  static bool condition(T a, T b) {\n    return a < b;\n  }\n  T operator()(T a, T b) {\n    return min(a, b);\n  }\n};\n}\ntemplate <typename T, enable_if_t<!is_metal_atomic<T>, bool> = true>\nMETAL_FUNC T\nmlx_atomic_load_explicit(device mlx_atomic<T>* object, size_t offset) {\n  size_t pack_offset = offset / sizeof(T);\n  size_t elem_offset = offset % sizeof(T);\n  uint_or_packed<T> packed_val;\n  packed_val.bits =\n      atomic_load_explicit(&(object[pack_offset].val), memory_order_relaxed);\n  return packed_val.val[elem_offset];\n}\ntemplate <typename T, enable_if_t<!is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void\nmlx_atomic_store_explicit(device mlx_atomic<T>* object, T val, size_t offset) {\n  mlx_atomic_update_and_store<T, __None<T>>(object, val, offset);\n}\ntemplate <typename T, enable_if_t<!is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_and_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  size_t pack_offset = offset / packing_size<T>;\n  size_t elem_offset = offset % packing_size<T>;\n  uint_or_packed<T> identity;\n  identity.bits = 4294967295U;\n  identity.val[elem_offset] = val;\n  atomic_fetch_and_explicit(\n      &(object[pack_offset].val), identity.bits, memory_order_relaxed);\n}\ntemplate <typename T, enable_if_t<!is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_or_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  size_t pack_offset = offset / packing_size<T>;\n  size_t elem_offset = offset % packing_size<T>;\n  uint_or_packed<T> identity;\n  identity.bits = 0;\n  identity.val[elem_offset] = val;\n  atomic_fetch_or_explicit(\n      &(object[pack_offset].val), identity.bits, memory_order_relaxed);\n}\ntemplate <typename T, enable_if_t<!is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_min_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  mlx_atomic_update_and_store<T, __Min<T>>(object, val, offset);\n}\ntemplate <typename T, enable_if_t<!is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_max_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  mlx_atomic_update_and_store<T, __Max<T>>(object, val, offset);\n}\ntemplate <typename T, enable_if_t<!is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_add_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  mlx_atomic_update_and_store<T, __Add<T>>(object, val, offset);\n}\ntemplate <typename T, enable_if_t<!is_metal_atomic<T>, bool> = true>\nMETAL_FUNC void mlx_atomic_fetch_mul_explicit(\n    device mlx_atomic<T>* object,\n    T val,\n    size_t offset) {\n  mlx_atomic_update_and_store<T, __Mul<T>>(object, val, offset);\n}\ntemplate <typename T, enable_if_t<!is_metal_atomic<T>, bool> = true>\nMETAL_FUNC bool mlx_atomic_compare_exchange_weak_explicit(\n    device mlx_atomic<T>* object,\n    thread uint* expected,\n    uint val,\n    size_t offset) {\n  return atomic_compare_exchange_weak_explicit(\n      &(object[offset].val),\n      expected,\n      val,\n      memory_order_relaxed,\n      memory_order_relaxed);\n}\nstatic constant constexpr const uint8_t simd_size = 32;\nunion bool4_or_uint {\n  bool4 b;\n  unsigned int i;\n};\nstruct None {\n  template <typename T>\n  void atomic_update(device mlx_atomic<T>* out, T val, size_t offset = 0) {\n    mlx_atomic_store_explicit(out, val, offset);\n  }\n};\ntemplate <typename U = bool>\nstruct And {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_reduce(T val) { return simd_reduce_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_reduce(T val) { for (short i = simd_size / 2; i > 0; i /= 2) { val = operator()(val, simd_shuffle_down(val, i)); } return val; }\n  bool simd_reduce_impl(bool val) {\n    return simd_all(val);\n  }\n  static constexpr constant bool init = true;\n  void atomic_update(\n      device mlx_atomic<unsigned int>* out,\n      bool val,\n      int elem_idx,\n      size_t offset = 0) {\n    if (!val) {\n      bool4_or_uint update;\n      update.b = {true, true, true, true};\n      update.b[elem_idx] = false;\n      mlx_atomic_fetch_and_explicit(out, update.i, offset);\n    }\n  }\n  void\n  atomic_update(device mlx_atomic<bool>* out, bool val, size_t offset = 0) {\n    if (!val) {\n      mlx_atomic_store_explicit(out, val, offset);\n    }\n  }\n  void update(device bool* out, bool val) {\n    *out &= val;\n  }\n  bool operator()(bool a, bool b) {\n    return a && b;\n  }\n};\ntemplate <typename U = bool>\nstruct Or {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_reduce(T val) { return simd_reduce_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_reduce(T val) { for (short i = simd_size / 2; i > 0; i /= 2) { val = operator()(val, simd_shuffle_down(val, i)); } return val; }\n  bool simd_reduce_impl(bool val) {\n    return simd_any(val);\n  }\n  static constexpr constant bool init = false;\n  void atomic_update(\n      device mlx_atomic<unsigned int>* out,\n      bool val,\n      int elem_idx,\n      size_t offset = 0) {\n    if (val) {\n      bool4_or_uint update;\n      update.b = {false, false, false, false};\n      update.b[elem_idx] = true;\n      mlx_atomic_fetch_or_explicit(out, update.i, offset);\n    }\n  }\n  void\n  atomic_update(device mlx_atomic<bool>* out, bool val, size_t offset = 0) {\n    if (val) {\n      mlx_atomic_store_explicit(out, val, offset);\n    }\n  }\n  void update(device bool* out, bool val) {\n    *out |= val;\n  }\n  bool operator()(bool a, bool b) {\n    return a || b;\n  }\n};\ntemplate <typename U>\nstruct Sum {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_reduce(T val) { return simd_reduce_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_reduce(T val) { for (short i = simd_size / 2; i > 0; i /= 2) { val = operator()(val, simd_shuffle_down(val, i)); } return val; }\n  template <typename T>\n  T simd_reduce_impl(T val) {\n    return simd_sum(val);\n  }\n  static constexpr constant U init = U(0);\n  template <typename T>\n  void atomic_update(device mlx_atomic<T>* out, T val, size_t offset = 0) {\n    mlx_atomic_fetch_add_explicit(out, val, offset);\n  }\n  U operator()(U a, U b) {\n    return a + b;\n  }\n};\ntemplate <typename U>\nstruct Prod {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_reduce(T val) { return simd_reduce_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_reduce(T val) { for (short i = simd_size / 2; i > 0; i /= 2) { val = operator()(val, simd_shuffle_down(val, i)); } return val; }\n  template <typename T>\n  T simd_reduce_impl(T val) {\n    return simd_product(val);\n  }\n  static constexpr constant U init = U(1);\n  template <typename T>\n  void atomic_update(device mlx_atomic<T>* out, T val, size_t offset = 0) {\n    mlx_atomic_fetch_mul_explicit(out, val, offset);\n  }\n  U operator()(U a, U b) {\n    return a * b;\n  }\n};\ntemplate <typename U>\nstruct Min {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_reduce(T val) { return simd_reduce_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_reduce(T val) { for (short i = simd_size / 2; i > 0; i /= 2) { val = operator()(val, simd_shuffle_down(val, i)); } return val; }\n  template <typename T>\n  T simd_reduce_impl(T val) {\n    return simd_min(val);\n  }\n  static constexpr constant U init = Limits<U>::max;\n  template <typename T>\n  void atomic_update(device mlx_atomic<T>* out, T val, size_t offset = 0) {\n    mlx_atomic_fetch_min_explicit(out, val, offset);\n  }\n  U operator()(U a, U b) {\n    return a < b ? a : b;\n  }\n};\ntemplate <typename U>\nstruct Max {\n  template <typename T, metal::enable_if_t<sizeof(T) < 8, bool> = true> T simd_reduce(T val) { return simd_reduce_impl(val); } template <typename T, metal::enable_if_t<sizeof(T) == 8, bool> = true> T simd_reduce(T val) { for (short i = simd_size / 2; i > 0; i /= 2) { val = operator()(val, simd_shuffle_down(val, i)); } return val; }\n  template <typename T>\n  T simd_reduce_impl(T val) {\n    return simd_max(val);\n  }\n  static constexpr constant U init = Limits<U>::min;\n  template <typename T>\n  void atomic_update(device mlx_atomic<T>* out, T val, size_t offset = 0) {\n    mlx_atomic_fetch_max_explicit(out, val, offset);\n  }\n  U operator()(U a, U b) {\n    return a > b ? a : b;\n  }\n};\n"
+ "\nusing namespace metal;\ntemplate <\n    typename T,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    int N_CHANNELS = 0,\n    bool SMALL_FILTER = false>\n[[kernel, max_total_threads_per_threadgroup(WM* WN * 32)]] void\nimplicit_gemm_conv_2d(\n    const device T* A [[buffer(0)]],\n    const device T* B [[buffer(1)]],\n    device T* C [[buffer(2)]],\n    const constant MLXConvParams<2>* params [[buffer(3)]],\n    const constant ImplicitGemmConv2DParams* gemm_params [[buffer(4)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint simd_gid [[simdgroup_index_in_threadgroup]],\n    uint simd_lid [[thread_index_in_simdgroup]]) {\n  using namespace mlx::steel;\n  (void)lid;\n  constexpr bool transpose_a = false;\n  constexpr bool transpose_b = true;\n  constexpr short tgp_padding_a = 16 / sizeof(T);\n  constexpr short tgp_padding_b = 16 / sizeof(T);\n  constexpr short shape_a_cols = (transpose_a ? BM : BK) + tgp_padding_a;\n  constexpr short shape_b_cols = (transpose_b ? BK : BN) + tgp_padding_b;\n  constexpr short shape_a_rows = (transpose_a ? BK : BM);\n  constexpr short shape_b_rows = (transpose_b ? BN : BK);\n  constexpr short tgp_mem_size_a = shape_a_cols * shape_a_rows;\n  constexpr short tgp_mem_size_b = shape_b_cols * shape_b_rows;\n  constexpr short tgp_size = WM * WN * 32;\n  using loader_a_t = typename metal::conditional_t<\n      N_CHANNELS != 0 && N_CHANNELS <= 4,\n      Conv2DInputBlockLoaderSmallChannels<\n          T,\n          BM,\n          BN,\n          BK,\n          tgp_size,\n          N_CHANNELS,\n          tgp_padding_a>,\n      typename metal::conditional_t<\n          SMALL_FILTER,\n          Conv2DInputBlockLoaderSmallFilter<\n              T,\n              BM,\n              BN,\n              BK,\n              tgp_size,\n              tgp_padding_a>,\n          Conv2DInputBlockLoaderLargeFilter<\n              T,\n              BM,\n              BN,\n              BK,\n              tgp_size,\n              tgp_padding_a>>>;\n  using loader_b_t = typename metal::conditional_t<\n      N_CHANNELS != 0 && N_CHANNELS <= 4,\n      Conv2DWeightBlockLoaderSmallChannels<\n          T,\n          BM,\n          BN,\n          BK,\n          tgp_size,\n          N_CHANNELS,\n          tgp_padding_b>,\n      Conv2DWeightBlockLoader<T, BM, BN, BK, tgp_size, tgp_padding_b>>;\n  using mma_t = BlockMMA<\n      T,\n      T,\n      BM,\n      BN,\n      BK,\n      WM,\n      WN,\n      transpose_a,\n      transpose_b,\n      shape_a_cols,\n      shape_b_cols>;\n  threadgroup T As[tgp_mem_size_a];\n  threadgroup T Bs[tgp_mem_size_b];\n  const int tid_y = ((tid.y) << gemm_params->swizzle_log) +\n      ((tid.x) & ((1 << gemm_params->swizzle_log) - 1));\n  const int tid_x = (tid.x) >> gemm_params->swizzle_log;\n  if (gemm_params->tiles_n <= tid_x || gemm_params->tiles_m <= tid_y) {\n    return;\n  }\n  const int c_row = tid_y * BM;\n  const int c_col = tid_x * BN;\n  const int K = gemm_params->K;\n  const int N = gemm_params->N;\n  const int C_per_group = params->C / params->groups;\n  A += tid.z * C_per_group;\n  B += tid.z * N * K;\n  C += tid.z * N;\n  B += c_col * K;\n  C += c_row * (N * params->groups) + c_col;\n  const int2 offsets_a(0, c_row);\n  const int2 offsets_b(0, c_col);\n  loader_a_t loader_a(\n      A, As, offsets_a, params, gemm_params, simd_gid, simd_lid);\n  loader_b_t loader_b(\n      B, Bs, offsets_b, params, gemm_params, simd_gid, simd_lid);\n  mma_t mma_op(simd_gid, simd_lid);\n  int gemm_k_iterations = gemm_params->gemm_k_iterations;\n  for (int k = 0; k < gemm_k_iterations; k++) {\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    loader_a.load_unsafe();\n    loader_b.load_unsafe();\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    mma_op.mma(As, Bs);\n    loader_a.next();\n    loader_b.next();\n  }\n  threadgroup_barrier(mem_flags::mem_none);\n  short tgp_bm = min(BM, gemm_params->M - c_row);\n  short tgp_bn = min(BN, gemm_params->N - c_col);\n  const int ldc = N * params->groups;\n  mma_op.store_result_safe(C, ldc, short2(tgp_bn, tgp_bm));\n}\n"
+ "\nusing namespace metal;\ntemplate <typename T>\nMETAL_FUNC void thread_swap(thread T& a, thread T& b) {\n  T w = a;\n  a = b;\n  b = w;\n}\ntemplate <typename T>\nstruct LessThan {\n  static constexpr constant T init = Limits<T>::max;\n  METAL_FUNC bool operator()(T a, T b) {\n    return a < b;\n  }\n};\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short N_PER_THREAD,\n    typename CompareOp>\nstruct ThreadSort {\n  static METAL_FUNC void sort(\n      thread val_t (&vals)[N_PER_THREAD],\n      thread idx_t (&idxs)[N_PER_THREAD]) {\n    CompareOp op;\n#pragma clang loop unroll(full)\n    for (short i = 0; i < N_PER_THREAD; ++i) {\n#pragma clang loop unroll(full)\n      for (short j = i & 1; j < N_PER_THREAD - 1; j += 2) {\n        if (op(vals[j + 1], vals[j])) {\n          thread_swap(vals[j + 1], vals[j]);\n          thread_swap(idxs[j + 1], idxs[j]);\n        }\n      }\n    }\n  }\n};\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp>\nstruct BlockMergeSort {\n  using thread_sort_t =\n      ThreadSort<val_t, idx_t, ARG_SORT, N_PER_THREAD, CompareOp>;\n  static METAL_FUNC int merge_partition(\n      const threadgroup val_t* As,\n      const threadgroup val_t* Bs,\n      short A_sz,\n      short B_sz,\n      short sort_md) {\n    CompareOp op;\n    short A_st = max(0, sort_md - B_sz);\n    short A_ed = min(sort_md, A_sz);\n    while (A_st < A_ed) {\n      short md = A_st + (A_ed - A_st) / 2;\n      auto a = As[md];\n      auto b = Bs[sort_md - 1 - md];\n      if (op(b, a)) {\n        A_ed = md;\n      } else {\n        A_st = md + 1;\n      }\n    }\n    return A_ed;\n  }\n  static METAL_FUNC void merge_step(\n      const threadgroup val_t* As,\n      const threadgroup val_t* Bs,\n      const threadgroup idx_t* As_idx,\n      const threadgroup idx_t* Bs_idx,\n      short A_sz,\n      short B_sz,\n      thread val_t (&vals)[N_PER_THREAD],\n      thread idx_t (&idxs)[N_PER_THREAD]) {\n    CompareOp op;\n    short a_idx = 0;\n    short b_idx = 0;\n    for (int i = 0; i < N_PER_THREAD; ++i) {\n      auto a = As[a_idx];\n      auto b = Bs[b_idx];\n      bool pred = (b_idx < B_sz) && (a_idx >= A_sz || op(b, a));\n      vals[i] = pred ? b : a;\n      idxs[i] = pred ? Bs_idx[b_idx] : As_idx[a_idx];\n      b_idx += short(pred);\n      a_idx += short(!pred);\n    }\n  }\n  static METAL_FUNC void sort(\n      threadgroup val_t* tgp_vals [[threadgroup(0)]],\n      threadgroup idx_t* tgp_idxs [[threadgroup(1)]],\n      int size_sorted_axis,\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    int idx = lid.x * N_PER_THREAD;\n    thread val_t thread_vals[N_PER_THREAD];\n    thread idx_t thread_idxs[N_PER_THREAD];\n    for (int i = 0; i < N_PER_THREAD; ++i) {\n      thread_vals[i] = tgp_vals[idx + i];\n      if (ARG_SORT) {\n        thread_idxs[i] = tgp_idxs[idx + i];\n      }\n    }\n    if (idx < size_sorted_axis) {\n      thread_sort_t::sort(thread_vals, thread_idxs);\n    }\n    for (int merge_threads = 2; merge_threads <= BLOCK_THREADS;\n         merge_threads *= 2) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      for (int i = 0; i < N_PER_THREAD; ++i) {\n        tgp_vals[idx + i] = thread_vals[i];\n        if (ARG_SORT) {\n          tgp_idxs[idx + i] = thread_idxs[i];\n        }\n      }\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      int merge_group = lid.x / merge_threads;\n      int merge_lane = lid.x % merge_threads;\n      int sort_sz = N_PER_THREAD * merge_threads;\n      int sort_st = N_PER_THREAD * merge_threads * merge_group;\n      int A_st = sort_st;\n      int A_ed = sort_st + sort_sz / 2;\n      int B_st = sort_st + sort_sz / 2;\n      int B_ed = sort_st + sort_sz;\n      const threadgroup val_t* As = tgp_vals + A_st;\n      const threadgroup val_t* Bs = tgp_vals + B_st;\n      int A_sz = A_ed - A_st;\n      int B_sz = B_ed - B_st;\n      int sort_md = N_PER_THREAD * merge_lane;\n      int partition = merge_partition(As, Bs, A_sz, B_sz, sort_md);\n      As += partition;\n      Bs += sort_md - partition;\n      A_sz -= partition;\n      B_sz -= sort_md - partition;\n      const threadgroup idx_t* As_idx =\n          ARG_SORT ? tgp_idxs + A_st + partition : nullptr;\n      const threadgroup idx_t* Bs_idx =\n          ARG_SORT ? tgp_idxs + B_st + sort_md - partition : nullptr;\n      merge_step(As, Bs, As_idx, Bs_idx, A_sz, B_sz, thread_vals, thread_idxs);\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = 0; i < N_PER_THREAD; ++i) {\n      tgp_vals[idx + i] = thread_vals[i];\n      if (ARG_SORT) {\n        tgp_idxs[idx + i] = thread_idxs[i];\n      }\n    }\n  }\n};\ntemplate <\n    typename T,\n    typename U,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp = LessThan<T>>\nstruct KernelMergeSort {\n  using val_t = T;\n  using idx_t = uint;\n  using block_merge_sort_t = BlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD,\n      CompareOp>;\n  static constant constexpr const short N_PER_BLOCK = BLOCK_THREADS * N_PER_THREAD;\n  static METAL_FUNC void block_sort(\n      const device T* inp,\n      device U* out,\n      const constant int& size_sorted_axis,\n      const constant int& in_stride_sorted_axis,\n      const constant int& out_stride_sorted_axis,\n      const constant int& in_stride_segment_axis,\n      const constant int& out_stride_segment_axis,\n      threadgroup val_t* tgp_vals,\n      threadgroup idx_t* tgp_idxs,\n      uint3 tid [[threadgroup_position_in_grid]],\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    inp += tid.y * in_stride_segment_axis;\n    out += tid.y * out_stride_segment_axis;\n    for (short i = lid.x; i < N_PER_BLOCK; i += BLOCK_THREADS) {\n      tgp_vals[i] = i < size_sorted_axis ? inp[i * in_stride_sorted_axis]\n                                         : val_t(CompareOp::init);\n      if (ARG_SORT) {\n        tgp_idxs[i] = i;\n      }\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    block_merge_sort_t::sort(tgp_vals, tgp_idxs, size_sorted_axis, lid);\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = lid.x; i < size_sorted_axis; i += BLOCK_THREADS) {\n      if (ARG_SORT) {\n        out[i * out_stride_sorted_axis] = tgp_idxs[i];\n      } else {\n        out[i * out_stride_sorted_axis] = tgp_vals[i];\n      }\n    }\n  }\n};\ntemplate <\n    typename T,\n    typename U,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void block_sort(\n    const device T* inp [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant int& size_sorted_axis [[buffer(2)]],\n    const constant int& in_stride_sorted_axis [[buffer(3)]],\n    const constant int& out_stride_sorted_axis [[buffer(4)]],\n    const constant int& in_stride_segment_axis [[buffer(5)]],\n    const constant int& out_stride_segment_axis [[buffer(6)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel =\n      KernelMergeSort<T, U, ARG_SORT, BLOCK_THREADS, N_PER_THREAD>;\n  using val_t = typename sort_kernel::val_t;\n  using idx_t = typename sort_kernel::idx_t;\n  if (ARG_SORT) {\n    threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n    threadgroup idx_t tgp_idxs[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        in_stride_segment_axis,\n        out_stride_segment_axis,\n        tgp_vals,\n        tgp_idxs,\n        tid,\n        lid);\n  } else {\n    threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        in_stride_segment_axis,\n        out_stride_segment_axis,\n        tgp_vals,\n        nullptr,\n        tid,\n        lid);\n  }\n}\nconstant constexpr const int zero_helper = 0;\ntemplate <\n    typename T,\n    typename U,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void block_sort_nc(\n    const device T* inp [[buffer(0)]],\n    device U* out [[buffer(1)]],\n    const constant int& size_sorted_axis [[buffer(2)]],\n    const constant int& in_stride_sorted_axis [[buffer(3)]],\n    const constant int& out_stride_sorted_axis [[buffer(4)]],\n    const constant int& nc_dim [[buffer(5)]],\n    const constant int* nc_shape [[buffer(6)]],\n    const constant size_t* in_nc_strides [[buffer(7)]],\n    const constant size_t* out_nc_strides [[buffer(8)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel =\n      KernelMergeSort<T, U, ARG_SORT, BLOCK_THREADS, N_PER_THREAD>;\n  using val_t = typename sort_kernel::val_t;\n  using idx_t = typename sort_kernel::idx_t;\n  auto in_block_idx = elem_to_loc(tid.y, nc_shape, in_nc_strides, nc_dim);\n  auto out_block_idx = elem_to_loc(tid.y, nc_shape, out_nc_strides, nc_dim);\n  inp += in_block_idx;\n  out += out_block_idx;\n  if (ARG_SORT) {\n    threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n    threadgroup idx_t tgp_idxs[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        zero_helper,\n        zero_helper,\n        tgp_vals,\n        tgp_idxs,\n        tid,\n        lid);\n  } else {\n    threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n    sort_kernel::block_sort(\n        inp,\n        out,\n        size_sorted_axis,\n        in_stride_sorted_axis,\n        out_stride_sorted_axis,\n        zero_helper,\n        zero_helper,\n        tgp_vals,\n        nullptr,\n        tid,\n        lid);\n  }\n}\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp = LessThan<val_t>>\nstruct KernelMultiBlockMergeSort {\n  using block_merge_sort_t = BlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD,\n      CompareOp>;\n  static constant constexpr const short N_PER_BLOCK = BLOCK_THREADS * N_PER_THREAD;\n  static METAL_FUNC void block_sort(\n      const device val_t* inp,\n      device val_t* out_vals,\n      device idx_t* out_idxs,\n      const constant int& size_sorted_axis,\n      const constant int& stride_sorted_axis,\n      threadgroup val_t* tgp_vals,\n      threadgroup idx_t* tgp_idxs,\n      uint3 tid [[threadgroup_position_in_grid]],\n      uint3 lid [[thread_position_in_threadgroup]]) {\n    int base_idx = tid.x * N_PER_BLOCK;\n    for (short i = lid.x; i < N_PER_BLOCK; i += BLOCK_THREADS) {\n      int idx = base_idx + i;\n      tgp_vals[i] = idx < size_sorted_axis ? inp[idx * stride_sorted_axis]\n                                           : val_t(CompareOp::init);\n      tgp_idxs[i] = idx;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    block_merge_sort_t::sort(tgp_vals, tgp_idxs, size_sorted_axis, lid);\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (int i = lid.x; i < N_PER_BLOCK; i += BLOCK_THREADS) {\n      int idx = base_idx + i;\n      if (idx < size_sorted_axis) {\n        out_vals[idx] = tgp_vals[i];\n        out_idxs[idx] = tgp_idxs[i];\n      }\n    }\n  }\n  static METAL_FUNC int merge_partition(\n      const device val_t* As,\n      const device val_t* Bs,\n      int A_sz,\n      int B_sz,\n      int sort_md) {\n    CompareOp op;\n    int A_st = max(0, sort_md - B_sz);\n    int A_ed = min(sort_md, A_sz);\n    while (A_st < A_ed) {\n      int md = A_st + (A_ed - A_st) / 2;\n      auto a = As[md];\n      auto b = Bs[sort_md - 1 - md];\n      if (op(b, a)) {\n        A_ed = md;\n      } else {\n        A_st = md + 1;\n      }\n    }\n    return A_ed;\n  }\n};\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void mb_block_sort(\n    const device val_t* inp [[buffer(0)]],\n    device val_t* out_vals [[buffer(1)]],\n    device idx_t* out_idxs [[buffer(2)]],\n    const constant int& size_sorted_axis [[buffer(3)]],\n    const constant int& stride_sorted_axis [[buffer(4)]],\n    const constant int& nc_dim [[buffer(5)]],\n    const constant int* nc_shape [[buffer(6)]],\n    const constant size_t* nc_strides [[buffer(7)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel = KernelMultiBlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD>;\n  auto block_idx = elem_to_loc(tid.y, nc_shape, nc_strides, nc_dim);\n  inp += block_idx;\n  out_vals += tid.y * size_sorted_axis;\n  out_idxs += tid.y * size_sorted_axis;\n  threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n  threadgroup idx_t tgp_idxs[sort_kernel::N_PER_BLOCK];\n  sort_kernel::block_sort(\n      inp,\n      out_vals,\n      out_idxs,\n      size_sorted_axis,\n      stride_sorted_axis,\n      tgp_vals,\n      tgp_idxs,\n      tid,\n      lid);\n}\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD>\n[[kernel]] void mb_block_partition(\n    device idx_t* block_partitions [[buffer(0)]],\n    const device val_t* dev_vals [[buffer(1)]],\n    const device idx_t* dev_idxs [[buffer(2)]],\n    const constant int& size_sorted_axis [[buffer(3)]],\n    const constant int& merge_tiles [[buffer(4)]],\n    const constant int& n_blocks [[buffer(5)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]],\n    uint3 tgp_dims [[threads_per_threadgroup]]) {\n  using sort_kernel = KernelMultiBlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD>;\n  block_partitions += tid.y * tgp_dims.x;\n  dev_vals += tid.y * size_sorted_axis;\n  dev_idxs += tid.y * size_sorted_axis;\n  for (int i = lid.x; i <= n_blocks; i += tgp_dims.x) {\n    int merge_group = i / merge_tiles;\n    int merge_lane = i % merge_tiles;\n    int sort_sz = sort_kernel::N_PER_BLOCK * merge_tiles;\n    int sort_st = sort_kernel::N_PER_BLOCK * merge_tiles * merge_group;\n    int A_st = min(size_sorted_axis, sort_st);\n    int A_ed = min(size_sorted_axis, sort_st + sort_sz / 2);\n    int B_st = A_ed;\n    int B_ed = min(size_sorted_axis, B_st + sort_sz / 2);\n    int partition_at = min(B_ed - A_st, sort_kernel::N_PER_BLOCK * merge_lane);\n    int partition = sort_kernel::merge_partition(\n        dev_vals + A_st,\n        dev_vals + B_st,\n        A_ed - A_st,\n        B_ed - B_st,\n        partition_at);\n    block_partitions[i] = A_st + partition;\n  }\n}\ntemplate <\n    typename val_t,\n    typename idx_t,\n    bool ARG_SORT,\n    short BLOCK_THREADS,\n    short N_PER_THREAD,\n    typename CompareOp = LessThan<val_t>>\n[[kernel, max_total_threads_per_threadgroup(BLOCK_THREADS)]] void\nmb_block_merge(\n    const device idx_t* block_partitions [[buffer(0)]],\n    const device val_t* dev_vals_in [[buffer(1)]],\n    const device idx_t* dev_idxs_in [[buffer(2)]],\n    device val_t* dev_vals_out [[buffer(3)]],\n    device idx_t* dev_idxs_out [[buffer(4)]],\n    const constant int& size_sorted_axis [[buffer(5)]],\n    const constant int& merge_tiles [[buffer(6)]],\n    const constant int& num_tiles [[buffer(7)]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  using sort_kernel = KernelMultiBlockMergeSort<\n      val_t,\n      idx_t,\n      ARG_SORT,\n      BLOCK_THREADS,\n      N_PER_THREAD,\n      CompareOp>;\n  using block_sort_t = typename sort_kernel::block_merge_sort_t;\n  block_partitions += tid.y * (num_tiles + 1);\n  dev_vals_in += tid.y * size_sorted_axis;\n  dev_idxs_in += tid.y * size_sorted_axis;\n  dev_vals_out += tid.y * size_sorted_axis;\n  dev_idxs_out += tid.y * size_sorted_axis;\n  int block_idx = tid.x;\n  int merge_group = block_idx / merge_tiles;\n  int sort_st = sort_kernel::N_PER_BLOCK * merge_tiles * merge_group;\n  int sort_sz = sort_kernel::N_PER_BLOCK * merge_tiles;\n  int sort_md = sort_kernel::N_PER_BLOCK * block_idx - sort_st;\n  int A_st = block_partitions[block_idx + 0];\n  int A_ed = block_partitions[block_idx + 1];\n  int B_st = min(size_sorted_axis, 2 * sort_st + sort_sz / 2 + sort_md - A_st);\n  int B_ed = min(\n      size_sorted_axis,\n      2 * sort_st + sort_sz / 2 + sort_md + sort_kernel::N_PER_BLOCK - A_ed);\n  if ((block_idx % merge_tiles) == merge_tiles - 1) {\n    A_ed = min(size_sorted_axis, sort_st + sort_sz / 2);\n    B_ed = min(size_sorted_axis, sort_st + sort_sz);\n  }\n  int A_sz = A_ed - A_st;\n  int B_sz = B_ed - B_st;\n  thread val_t thread_vals[N_PER_THREAD];\n  thread idx_t thread_idxs[N_PER_THREAD];\n  for (int i = 0; i < N_PER_THREAD; i++) {\n    int idx = BLOCK_THREADS * i + lid.x;\n    if (idx < (A_sz + B_sz)) {\n      thread_vals[i] = (idx < A_sz) ? dev_vals_in[A_st + idx]\n                                    : dev_vals_in[B_st + idx - A_sz];\n      thread_idxs[i] = (idx < A_sz) ? dev_idxs_in[A_st + idx]\n                                    : dev_idxs_in[B_st + idx - A_sz];\n    } else {\n      thread_vals[i] = CompareOp::init;\n      thread_idxs[i] = 0;\n    }\n  }\n  threadgroup val_t tgp_vals[sort_kernel::N_PER_BLOCK];\n  threadgroup idx_t tgp_idxs[sort_kernel::N_PER_BLOCK];\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  for (int i = 0; i < N_PER_THREAD; i++) {\n    int idx = BLOCK_THREADS * i + lid.x;\n    tgp_vals[idx] = thread_vals[i];\n    tgp_idxs[idx] = thread_idxs[i];\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  int sort_md_local = min(A_sz + B_sz, N_PER_THREAD * int(lid.x));\n  int A_st_local = block_sort_t::merge_partition(\n      tgp_vals, tgp_vals + A_sz, A_sz, B_sz, sort_md_local);\n  int A_ed_local = A_sz;\n  int B_st_local = sort_md_local - A_st_local;\n  int B_ed_local = B_sz;\n  int A_sz_local = A_ed_local - A_st_local;\n  int B_sz_local = B_ed_local - B_st_local;\n  block_sort_t::merge_step(\n      tgp_vals + A_st_local,\n      tgp_vals + A_ed_local + B_st_local,\n      tgp_idxs + A_st_local,\n      tgp_idxs + A_ed_local + B_st_local,\n      A_sz_local,\n      B_sz_local,\n      thread_vals,\n      thread_idxs);\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  for (int i = 0; i < N_PER_THREAD; ++i) {\n    int idx = lid.x * N_PER_THREAD;\n    tgp_vals[idx + i] = thread_vals[i];\n    tgp_idxs[idx + i] = thread_idxs[i];\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  int base_idx = tid.x * sort_kernel::N_PER_BLOCK;\n  for (int i = lid.x; i < sort_kernel::N_PER_BLOCK; i += BLOCK_THREADS) {\n    int idx = base_idx + i;\n    if (idx < size_sorted_axis) {\n      dev_vals_out[idx] = tgp_vals[i];\n      dev_idxs_out[idx] = tgp_idxs[i];\n    }\n  }\n}\n"
+ "\nusing namespace mlx::steel;\nconstant bool has_batch [[function_constant(10)]];\nconstant bool use_out_source [[function_constant(100)]];\nconstant bool do_axpby [[function_constant(110)]];\nconstant bool align_M [[function_constant(200)]];\nconstant bool align_N [[function_constant(201)]];\nconstant bool align_K [[function_constant(202)]];\nconstant bool do_gather [[function_constant(300)]];\nconstant bool gather_bias = do_gather && use_out_source;\ntemplate <\n    typename T,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    typename AccumType = float>\n[[kernel, max_total_threads_per_threadgroup(WM* WN * 32)]] void gemm(\n    const device T* A [[buffer(0)]],\n    const device T* B [[buffer(1)]],\n    const device T* C [[buffer(2), function_constant(use_out_source)]],\n    device T* D [[buffer(3)]],\n    const constant GEMMParams* params [[buffer(4)]],\n    const constant GEMMAddMMParams* addmm_params [[buffer(5), function_constant(use_out_source)]],\n    const constant int* batch_shape [[buffer(6)]],\n    const constant size_t* batch_strides [[buffer(7)]],\n    const constant uint32_t* lhs_indices [[buffer(10), function_constant(do_gather)]],\n    const constant uint32_t* rhs_indices [[buffer(11), function_constant(do_gather)]],\n    const constant uint32_t* C_indices [[buffer(12), function_constant(gather_bias)]],\n    const constant int* operand_shape [[buffer(13), function_constant(do_gather)]],\n    const constant size_t* operand_strides [[buffer(14), function_constant(do_gather)]],\n    const constant packed_int3& operand_batch_ndim [[buffer(15), function_constant(do_gather)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  (void)lid;\n  using gemm_kernel = GEMMKernel<\n      T,\n      T,\n      BM,\n      BN,\n      BK,\n      WM,\n      WN,\n      transpose_a,\n      transpose_b,\n      true,\n      true,\n      AccumType>;\n  using loader_a_t = typename gemm_kernel::loader_a_t;\n  using loader_b_t = typename gemm_kernel::loader_b_t;\n  using mma_t = typename gemm_kernel::mma_t;\n  const int tid_y = ((tid.y) << params->swizzle_log) +\n      ((tid.x) & ((1 << params->swizzle_log) - 1));\n  const int tid_x = (tid.x) >> params->swizzle_log;\n  if (params->tiles_n <= tid_x || params->tiles_m <= tid_y) {\n    return;\n  }\n  if (do_gather) {\n    uint32_t indx_A, indx_B, indx_C;\n    if (has_batch) {\n      const constant size_t* indx_A_bstrides = batch_strides;\n      const constant size_t* indx_B_bstrides =\n          batch_strides + params->batch_ndim;\n      ulong2 indx_offsets = elem_to_loc_broadcast(\n          tid.z,\n          batch_shape,\n          indx_A_bstrides,\n          indx_B_bstrides,\n          params->batch_ndim);\n      indx_A = lhs_indices[indx_offsets.x];\n      indx_B = rhs_indices[indx_offsets.y];\n      if (use_out_source) {\n        const constant size_t* indx_C_bstrides =\n            indx_B_bstrides + params->batch_ndim;\n        auto indx_offset_C = elem_to_loc(\n            tid.z, batch_shape, indx_C_bstrides, params->batch_ndim);\n        indx_C = C_indices[indx_offset_C];\n      }\n    } else {\n      indx_A = lhs_indices[params->batch_stride_a * tid.z];\n      indx_B = rhs_indices[params->batch_stride_b * tid.z];\n      if (use_out_source) {\n        indx_C = C_indices[addmm_params->batch_stride_c * tid.z];\n      }\n    }\n    int batch_ndim_A = operand_batch_ndim.x;\n    const constant int* batch_shape_A = operand_shape;\n    const constant size_t* batch_strides_A = operand_strides;\n    A += elem_to_loc(indx_A, batch_shape_A, batch_strides_A, batch_ndim_A);\n    int batch_ndim_B = operand_batch_ndim.y;\n    const constant int* batch_shape_B = batch_shape_A + batch_ndim_A;\n    const constant size_t* batch_strides_B = batch_strides_A + batch_ndim_A;\n    B += elem_to_loc(indx_B, batch_shape_B, batch_strides_B, batch_ndim_B);\n    if (use_out_source) {\n      int batch_ndim_C = operand_batch_ndim.z;\n      const constant int* batch_shape_C = batch_shape_B + batch_ndim_B;\n      const constant size_t* batch_strides_C = batch_strides_B + batch_ndim_B;\n      C += elem_to_loc(indx_C, batch_shape_C, batch_strides_C, batch_ndim_C);\n    }\n  }\n  else {\n    if (has_batch) {\n      const constant size_t* A_bstrides = batch_strides;\n      const constant size_t* B_bstrides = batch_strides + params->batch_ndim;\n      ulong2 batch_offsets = elem_to_loc_broadcast(\n          tid.z, batch_shape, A_bstrides, B_bstrides, params->batch_ndim);\n      A += batch_offsets.x;\n      B += batch_offsets.y;\n      if (use_out_source) {\n        const constant size_t* C_bstrides = B_bstrides + params->batch_ndim;\n        C += elem_to_loc(tid.z, batch_shape, C_bstrides, params->batch_ndim);\n      }\n    } else {\n      A += params->batch_stride_a * tid.z;\n      B += params->batch_stride_b * tid.z;\n      if (use_out_source) {\n        C += addmm_params->batch_stride_c * tid.z;\n      }\n    }\n  }\n  D += params->batch_stride_d * tid.z;\n  threadgroup T As[gemm_kernel::tgp_mem_size_a];\n  threadgroup T Bs[gemm_kernel::tgp_mem_size_b];\n  threadgroup_barrier(mem_flags::mem_none);\n  const int c_row = tid_y * BM;\n  const int c_col = tid_x * BN;\n  const size_t c_row_long = size_t(c_row);\n  const size_t c_col_long = size_t(c_col);\n  A += transpose_a ? c_row_long : c_row_long * params->lda;\n  B += transpose_b ? c_col_long * params->ldb : c_col_long;\n  D += c_row_long * params->ldd + c_col_long;\n  if (use_out_source) {\n    C += c_row_long * addmm_params->ldc + c_col_long * addmm_params->fdc;\n  }\n  thread mma_t mma_op(simd_group_id, simd_lane_id);\n  thread loader_a_t loader_a(A, params->lda, As, simd_group_id, simd_lane_id);\n  thread loader_b_t loader_b(B, params->ldb, Bs, simd_group_id, simd_lane_id);\n  const short tgp_bm = align_M ? BM : short(min(BM, params->M - c_row));\n  const short tgp_bn = align_N ? BN : short(min(BN, params->N - c_col));\n  int gemm_k_iterations = params->gemm_k_iterations_aligned;\n  if (!align_K) {\n    const int k_last = params->gemm_k_iterations_aligned * BK;\n    const int k_remain = params->K - k_last;\n    const size_t k_jump_a =\n        transpose_a ? params->lda * size_t(k_last) : size_t(k_last);\n    const size_t k_jump_b =\n        transpose_b ? size_t(k_last) : params->ldb * size_t(k_last);\n    loader_a.src += k_jump_a;\n    loader_b.src += k_jump_b;\n    const short2 tile_dims_A =\n        transpose_a ? short2(tgp_bm, k_remain) : short2(k_remain, tgp_bm);\n    const short2 tile_dims_B =\n        transpose_b ? short2(k_remain, tgp_bn) : short2(tgp_bn, k_remain);\n    loader_a.load_safe(tile_dims_A);\n    loader_b.load_safe(tile_dims_B);\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    mma_op.mma(As, Bs);\n    loader_a.src -= k_jump_a;\n    loader_b.src -= k_jump_b;\n  }\n  const TransformAdd<AccumType, AccumType> epilogue_op_add(\n      addmm_params->alpha, addmm_params->beta);\n  const TransformAxpby<AccumType, AccumType> epilogue_op_axpby(\n      addmm_params->alpha, addmm_params->beta);\n  if (align_M && align_N) {\n    for (int k = 0; k < gemm_k_iterations; k++) {\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      loader_a.load_unsafe();\n      loader_b.load_unsafe();\n      threadgroup_barrier(mem_flags::mem_threadgroup);\n      mma_op.mma(As, Bs);\n      loader_a.next();\n      loader_b.next();\n    }\n    threadgroup_barrier(mem_flags::mem_none);\n    if (use_out_source) {\n      if (do_axpby) {\n        mma_op.apply_epilogue(\n            C, addmm_params->ldc, addmm_params->fdc, epilogue_op_axpby);\n      } else {\n        mma_op.apply_epilogue(\n            C, addmm_params->ldc, addmm_params->fdc, epilogue_op_add);\n      }\n    }\n    return mma_op.store_result(D, params->ldd);\n  }\n  else {\n    const int leftover_bk = 0;\n    if ((align_M || tgp_bm == BM) && (align_N || tgp_bn == BN)) {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<true, true, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue(\n              C, addmm_params->ldc, addmm_params->fdc, epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue(\n              C, addmm_params->ldc, addmm_params->fdc, epilogue_op_add);\n        }\n      }\n      return mma_op.store_result(D, params->ldd);\n    } else if (align_N || tgp_bn == BN) {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<false, true, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_add);\n        }\n      }\n      return mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n    } else if (align_M || tgp_bm == BM) {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<true, false, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_add);\n        }\n      }\n      return mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n    } else {\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iterations,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<false, false, true>{});\n      if (use_out_source) {\n        if (do_axpby) {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_axpby);\n        } else {\n          mma_op.apply_epilogue_safe(\n              C,\n              addmm_params->ldc,\n              addmm_params->fdc,\n              short2(tgp_bn, tgp_bm),\n              epilogue_op_add);\n        }\n      }\n      return mma_op.store_result_safe(D, params->ldd, short2(tgp_bn, tgp_bm));\n    }\n  }\n}\n"
+ "\nusing namespace mlx::steel;\ntemplate <\n    typename T,\n    typename U,\n    int BM,\n    int BN,\n    int BK,\n    int WM,\n    int WN,\n    bool transpose_a,\n    bool transpose_b,\n    bool MN_aligned,\n    bool K_aligned>\n[[kernel, max_total_threads_per_threadgroup(WM* WN * 32)]] void gemm_splitk(\n    const device T* A [[buffer(0)]],\n    const device T* B [[buffer(1)]],\n    device U* C [[buffer(2)]],\n    const constant GEMMSpiltKParams* params [[buffer(3)]],\n    uint simd_lane_id [[thread_index_in_simdgroup]],\n    uint simd_group_id [[simdgroup_index_in_threadgroup]],\n    uint3 tid [[threadgroup_position_in_grid]],\n    uint3 lid [[thread_position_in_threadgroup]]) {\n  (void)lid;\n  using gemm_kernel = GEMMKernel<\n      T,\n      U,\n      BM,\n      BN,\n      BK,\n      WM,\n      WN,\n      transpose_a,\n      transpose_b,\n      MN_aligned,\n      K_aligned>;\n  using loader_a_t = typename gemm_kernel::loader_a_t;\n  using loader_b_t = typename gemm_kernel::loader_b_t;\n  using mma_t = typename gemm_kernel::mma_t;\n  threadgroup T As[gemm_kernel::tgp_mem_size_a];\n  threadgroup T Bs[gemm_kernel::tgp_mem_size_b];\n  const int tid_x = tid.x;\n  const int tid_y = tid.y;\n  const int tid_z = tid.z;\n  if (params->tiles_n <= tid_x || params->tiles_m <= tid_y) {\n    return;\n  }\n  const int c_row = tid_y * BM;\n  const int c_col = tid_x * BN;\n  const int k_start = params->split_k_partition_size * tid_z;\n  const size_t c_row_long = size_t(c_row);\n  const size_t c_col_long = size_t(c_col);\n  const size_t k_start_long = size_t(k_start);\n  A += transpose_a ? (c_row_long + k_start_long * params->lda)\n                   : (k_start_long + c_row_long * params->lda);\n  B += transpose_b ? (k_start_long + c_col_long * params->ldb)\n                   : (c_col_long + k_start_long * params->ldb);\n  C += (size_t(params->split_k_partition_stride) * tid_z) +\n      (c_row_long * params->ldc + c_col_long);\n  thread loader_a_t loader_a(A, params->lda, As, simd_group_id, simd_lane_id);\n  thread loader_b_t loader_b(B, params->ldb, Bs, simd_group_id, simd_lane_id);\n  thread mma_t mma_op(simd_group_id, simd_lane_id);\n  int gemm_k_iterations = params->gemm_k_iterations_aligned;\n  short tgp_bm = min(BM, params->M - c_row);\n  short tgp_bn = min(BN, params->N - c_col);\n  short leftover_bk = params->K % BK;\n  if (MN_aligned || (tgp_bm == BM && tgp_bn == BN)) {\n    gemm_kernel::gemm_loop(\n        As,\n        Bs,\n        gemm_k_iterations,\n        loader_a,\n        loader_b,\n        mma_op,\n        tgp_bm,\n        tgp_bn,\n        leftover_bk,\n        LoopAlignment<true, true, true>{});\n  } else if (tgp_bn == BN) {\n    gemm_kernel::gemm_loop(\n        As,\n        Bs,\n        gemm_k_iterations,\n        loader_a,\n        loader_b,\n        mma_op,\n        tgp_bm,\n        tgp_bn,\n        leftover_bk,\n        LoopAlignment<false, true, true>{});\n  } else if (tgp_bm == BM) {\n    gemm_kernel::gemm_loop(\n        As,\n        Bs,\n        gemm_k_iterations,\n        loader_a,\n        loader_b,\n        mma_op,\n        tgp_bm,\n        tgp_bn,\n        leftover_bk,\n        LoopAlignment<true, false, true>{});\n  } else {\n    gemm_kernel::gemm_loop(\n        As,\n        Bs,\n        gemm_k_iterations,\n        loader_a,\n        loader_b,\n        mma_op,\n        tgp_bm,\n        tgp_bn,\n        leftover_bk,\n        LoopAlignment<false, false, true>{});\n  }\n  threadgroup_barrier(mem_flags::mem_threadgroup);\n  if ((tid_z + 1) == (params->split_k_partitions)) {\n    int gemm_k_iter_remaining =\n        (params->K - (k_start + params->split_k_partition_size)) / BK;\n    if (!K_aligned || gemm_k_iter_remaining > 0)\n      gemm_kernel::gemm_loop(\n          As,\n          Bs,\n          gemm_k_iter_remaining,\n          loader_a,\n          loader_b,\n          mma_op,\n          tgp_bm,\n          tgp_bn,\n          leftover_bk,\n          LoopAlignment<false, false, K_aligned>{});\n  }\n  if (MN_aligned || (tgp_bm == BM && tgp_bn == BN)) {\n    mma_op.store_result(C, params->ldc);\n  } else {\n    mma_op.store_result_safe(C, params->ldc, short2(tgp_bn, tgp_bm));\n  }\n}\ntemplate <\n    typename AccT,\n    typename OutT,\n    typename Epilogue = TransformNone<OutT, AccT>>\n[[kernel]] void gemm_splitk_accum(\n    const device AccT* C_split [[buffer(0)]],\n    device OutT* D [[buffer(1)]],\n    const constant int& k_partitions [[buffer(2)]],\n    const constant int& partition_stride [[buffer(3)]],\n    const constant int& ldd [[buffer(4)]],\n    uint2 gid [[thread_position_in_grid]]) {\n  D += gid.x + gid.y * size_t(ldd);\n  C_split += gid.x + gid.y * size_t(ldd);\n  size_t offset = 0;\n  AccT out = 0;\n  for (int i = 0; i < k_partitions; i++) {\n    out += C_split[offset];\n    offset += partition_stride;\n  }\n  D[0] = Epilogue::apply(out);\n}\ntemplate <\n    typename AccT,\n    typename OutT,\n    typename Epilogue = TransformAxpby<OutT, AccT>>\n[[kernel]] void gemm_splitk_accum_axpby(\n    const device AccT* C_split [[buffer(0)]],\n    device OutT* D [[buffer(1)]],\n    const constant int& k_partitions [[buffer(2)]],\n    const constant int& partition_stride [[buffer(3)]],\n    const constant int& ldd [[buffer(4)]],\n    const device OutT* C [[buffer(5)]],\n    const constant int& ldc [[buffer(6)]],\n    const constant int& fdc [[buffer(7)]],\n    const constant float& alpha [[buffer(8)]],\n    const constant float& beta [[buffer(9)]],\n    uint2 gid [[thread_position_in_grid]]) {\n  C += gid.x * size_t(fdc) + gid.y * size_t(ldc);\n  D += gid.x + gid.y * size_t(ldd);\n  C_split += gid.x + gid.y * size_t(ldd);\n  size_t offset = 0;\n  AccT out = 0;\n  for (int i = 0; i < k_partitions; i++) {\n    out += C_split[offset];\n    offset += partition_stride;\n  }\n  Epilogue op(alpha, beta);\n  D[0] = op.apply(out, *C);\n}\n"
+ "                asset inputs:file = @"
+ "                asset unconnected:displacement = @"
+ "                color3f inputs:diffuseColor = (0.5, 0.5, 0.5)\n"
+ "                color3f inputs:diffuseColor.connect = </"
+ "                float inputs:metallic.connect = </"
+ "                float inputs:occlusion.connect = </"
+ "                float inputs:roughness = 0.9\n"
+ "                float inputs:roughness.connect = </"
+ "                float outputs:r\n"
+ "                float2 inputs:st.connect = </"
+ "                float2 outputs:result\n"
+ "                float3 outputs:rgb\n"
+ "                float4 inputs:bias = (-1, -1, -1, -1)\n"
+ "                float4 inputs:scale = (2, 2, 2, 2)\n"
+ "                int[] indices = ["
+ "                interpolation = \"faceVarying\"\n"
+ "                normal3f inputs:normal.connect = </"
+ "                prepend apiSchemas = [\"MaterialBindingAPI\"]\n"
+ "                rel material:binding = </"
+ "                token inputs:sourceColorSpace = \"raw\"\n"
+ "                token inputs:varname.connect = </"
+ "                token outputs:surface\n"
+ "                uniform token elementType = \"face\"\n"
+ "                uniform token familyName = \"materialBind\"\n"
+ "                uniform token info:id = \"UsdPreviewSurface\"\n"
+ "                uniform token info:id = \"UsdPrimvarReader_float2\"\n"
+ "                uniform token info:id = \"UsdUVTexture\"\n"
+ "            )\n"
+ "            def GeomSubset \""
+ "            def Shader \"diffuseColor_texture\"\n"
+ "            def Shader \"metallic_texture\"\n"
+ "            def Shader \"normal_texture\"\n"
+ "            def Shader \"occlusion_texture\"\n"
+ "            def Shader \"roughness_texture\"\n"
+ "            def Shader \"surfaceShader\"\n"
+ "            def Shader \"uvReader_st\"\n"
+ "            float3[] extent = [("
+ "            int preferredIblVersion = 2\n"
+ "            int[] faceVertexCounts = ["
+ "            int[] faceVertexIndices = ["
+ "            int[] primvars:st:indices = ["
+ "            normal3f[] normals = ["
+ "            point3f[] points = ["
+ "            prepend apiSchemas = [\"MaterialBindingAPI\"]\n"
+ "            rel material:binding = </"
+ "            texCoord2f[] primvars:st = ["
+ "            token inputs:frame:stPrimvarName = \"st\"\n"
+ "            token outputs:surface.connect = </"
+ "            uniform token subdivisionScheme = \"none\"\n"
+ "            uniform token subsetFamily:materialBind:familyType = \"nonOverlapping\"\n"
+ "            {\n"
+ "            }\n"
+ "        )\n"
+ "        def Material \""
+ "        def Mesh \"Mesh\" (\n"
+ "        dictionary Apple = {\n"
+ "        int[] creatorVer = ["
+ "        string creator = \""
+ "        string creator = \"CorePhotogrammetry Bake\"\n"
+ "        string name = \""
+ "        {\n"
+ "        }\n"
+ "    assetInfo = {\n"
+ "    customLayerData = {\n"
+ "    def Scope \"Geometry\"\n"
+ "    def Scope \"Materials\"\n"
+ "    defaultPrim = \""
+ "    kind = \"component\"\n"
+ "    metersPerUnit = 1\n"
+ "    upAxis = \"Y\"\n"
+ "    {\n"
+ "    }\n"
+ " ADC interval optimization is disabled."
+ " Convolution currently only supports floating point types"
+ " Expected an array with "
+ " Got error at axis "
+ " Got input dilation "
+ " Got kernel dilation "
+ " Got padding "
+ " Got strides "
+ " If you are using grad your function must return a scalar."
+ " Max"
+ " Min"
+ " Prod"
+ " Sum"
+ " ["
+ " and "
+ " and weight array to match but got shapes -"
+ " and weight: "
+ " and weights of shape "
+ " are too large for array with shape "
+ " attempting "
+ " axes for array with "
+ " bytes which is greater than"
+ " bytes."
+ " cannot be broadcast."
+ " cannot be smaller than weight spatial dimensions."
+ " cannot infer output shapes."
+ " dimensions does not match the sum of the array ("
+ " dimensions following the format [C_out, ..., C_in]."
+ " dimensions following the format [N, ..., C_in]."
+ " dimensions for "
+ " dimensions."
+ " does not match array of dimension "
+ " does not match cotangent shape "
+ " error "
+ " f_dc_"
+ " f_rest_"
+ " faces\n"
+ " for "
+ " for array with "
+ " for array with shape "
+ " for axis "
+ " for function"
+ " for input with shape "
+ " for output array with "
+ " for shape "
+ " for the input or updates."
+ " from an array with "
+ " from broadcasted shape "
+ " groups."
+ " in "
+ " index arrays for input with "
+ " index arrays not yet supported."
+ " input channels but got "
+ " input channels instead."
+ " input: "
+ " inputs."
+ " into shape "
+ " is not valid for broadcasted index shape "
+ " is out of bounds for array with "
+ " must match second to last dimension of"
+ " not supported for arange."
+ " of groups. Got "
+ " of groups. Got input with shape "
+ " opacity\n"
+ " or <"
+ " output channels and "
+ " passed to split"
+ " rot_"
+ " scale_"
+ " second input with shape "
+ " splits along axis "
+ " the maximum allowed buffer size of "
+ " were provided which results"
+ " which is not equal to 1."
+ " with ML depth map upscaled to ["
+ " with depth map upscaled to ["
+ " with mask upscaled to ["
+ " with size "
+ " x "
+ "\" (\n"
+ "#usda 1.0\n"
+ "("
+ "(\n"
+ ")\n"
+ ") and indices ("
+ ") dimensions."
+ ") does not match number of cotangents ("
+ ") for array with "
+ ") passed to concatenate"
+ ") passed to pad"
+ ") passed to split"
+ "), ("
+ ")."
+ ")]\n"
+ ", B "
+ ", and weight of shape "
+ ", expected to have "
+ ", padding high "
+ ", padding low "
+ ", which is not a real floating point type."
+ ". Last error message: "
+ ". Padding sizes must be non-negative"
+ ".inputs:frame:stPrimvarName>\n"
+ ".metallib"
+ "/AppleInternal/Library/BuildRoots/4~B1yyugALbG9fePd-qn-lmE9OgDk8l8TRtiRU3Fs/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS26.0.Internal.sdk/usr/local/include/usd/pxr/base/tf/refPtr.h"
+ "/AppleInternal/Library/BuildRoots/4~B1yyugALbG9fePd-qn-lmE9OgDk8l8TRtiRU3Fs/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS26.0.Internal.sdk/usr/local/include/usd/pxr/usd/sdf/declareHandles.h"
+ "/AppleInternal/Library/BuildRoots/4~B1yyugALbG9fePd-qn-lmE9OgDk8l8TRtiRU3Fs/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS26.0.Internal.sdk/usr/local/include/usd/pxr/usd/usd/object.h"
+ "/Library/Caches/com.apple.xbs/Binaries/CorePhotogrammetry/install/TempContent/Objects/3rd-party/MLXWrapper/mlx/mlx/backend/metal/kernels//mlx.metallib"
+ "/Materials/"
+ "/diffuseColor_texture.outputs:rgb>\n"
+ "/metallic_texture.outputs:r>\n"
+ "/normal_texture.outputs:rgb>\n"
+ "/occlusion_texture.outputs:r>\n"
+ "/roughness_texture.outputs:r>\n"
+ "/surfaceShader.outputs:surface>\n"
+ "/uvReader_st.outputs:result>\n"
+ "0000000000"
+ "00010203040506070809101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899"
+ "0123456789abcdef"
+ "3"
+ ">"
+ ">\n"
+ ">."
+ "@\n"
+ "A "
+ "Abs"
+ "Abs has no CPU implementation."
+ "AccumulateL1Loss"
+ "AccumulateL1Loss\n"
+ "AccumulateL1LossForward has no eval_cpu implementation."
+ "AccumulateL1LossForward has no jvp implementation."
+ "AccumulateL1LossForward has no vmap implementation."
+ "Adam"
+ "Adam forward\n"
+ "AdamFWD has no eval_cpu implementation."
+ "AdamFWD has no jvp implementation."
+ "AdamFWD has no vjp implementation."
+ "AdamFWD has no vmap implementation."
+ "Add"
+ "Add has no CPU implementation."
+ "All arrays must have the same shape"
+ "And"
+ "ApplyAdditiveLMDampingExplicitSFM_Huber_lossFactorGraph_16363852131482720456"
+ "ApplyAdditiveLMDampingExplicitSFM_Huber_lossFactorGraph_16363852131482720456_func"
+ "ApplyAdditiveLMDampingExplicitSFM_L2_lossFactorGraph_16363852131482720456"
+ "ApplyAdditiveLMDampingExplicitSFM_L2_lossFactorGraph_16363852131482720456_func"
+ "ApplyAdditiveLMDampingToDiagonalsSFM_Huber_lossFactorGraph_14266438070451934278"
+ "ApplyAdditiveLMDampingToDiagonalsSFM_Huber_lossFactorGraph_14266438070451934278_func"
+ "ApplyAdditiveLMDampingToDiagonalsSFM_L2_lossFactorGraph_14266438070451934278"
+ "ApplyAdditiveLMDampingToDiagonalsSFM_L2_lossFactorGraph_14266438070451934278_func"
+ "ApplyMultiplicativeLMDampingToDiagonalsSFM_Huber_lossFactorGraph_14266438070451934278"
+ "ApplyMultiplicativeLMDampingToDiagonalsSFM_Huber_lossFactorGraph_14266438070451934278_func"
+ "ApplyMultiplicativeLMDampingToDiagonalsSFM_L2_lossFactorGraph_14266438070451934278"
+ "ApplyMultiplicativeLMDampingToDiagonalsSFM_L2_lossFactorGraph_14266438070451934278_func"
+ "ApplyMultiplicativedLMDampingExplicitSFM_Huber_lossFactorGraph_16363852131482720456"
+ "ApplyMultiplicativedLMDampingExplicitSFM_Huber_lossFactorGraph_16363852131482720456_func"
+ "ApplyMultiplicativedLMDampingExplicitSFM_L2_lossFactorGraph_16363852131482720456"
+ "ApplyMultiplicativedLMDampingExplicitSFM_L2_lossFactorGraph_16363852131482720456_func"
+ "Arange"
+ "Arange has no CPU implementation."
+ "ArcCos"
+ "ArcCos has no CPU implementation."
+ "ArcTan2"
+ "ArcTan2 has no CPU implementation."
+ "ArgSort"
+ "ArgSort has no CPU implementation."
+ "Array split does not result in sub arrays with equal size:"
+ "AsStrided"
+ "AsStrided has no CPU implementation."
+ "AsStrided must be used with row contiguous arrays only."
+ "AsType"
+ "AsType has no CPU implementation."
+ "At most one of a_min and a_max may be None"
+ "AtomicI32ToF32"
+ "Attempting to allocate "
+ "Axis "
+ "BilateralGridSample3DBWD\n"
+ "BilateralGridSample3DBWD has no eval_cpu implementation."
+ "BilateralGridSample3DBWD has no jvp implementation."
+ "BilateralGridSample3DBWD has no vjp implementation."
+ "BilateralGridSample3DBWD has no vmap implementation."
+ "BilateralGridSample3DFWD\n"
+ "BilateralGridSample3DFWD has no eval_cpu implementation."
+ "BilateralGridSample3DFWD has no jvp implementation."
+ "BilateralGridSample3DFWD has no vmap implementation."
+ "BlockMatrix"
+ "BlockMatrix$1"
+ "BlockMatrix$10"
+ "BlockMatrix$11"
+ "BlockMatrix$12"
+ "BlockMatrix$13"
+ "BlockMatrix$14"
+ "BlockMatrix$15"
+ "BlockMatrix$16"
+ "BlockMatrix$17"
+ "BlockMatrix$18"
+ "BlockMatrix$19"
+ "BlockMatrix$2"
+ "BlockMatrix$20"
+ "BlockMatrix$21"
+ "BlockMatrix$22"
+ "BlockMatrix$23"
+ "BlockMatrix$24"
+ "BlockMatrix$3"
+ "BlockMatrix$4"
+ "BlockMatrix$5"
+ "BlockMatrix$6"
+ "BlockMatrix$7"
+ "BlockMatrix$8"
+ "BlockMatrix$9"
+ "BlockMatrixCounts"
+ "BlockMatrixCounts$1"
+ "BlockMatrixCounts$10"
+ "BlockMatrixCounts$11"
+ "BlockMatrixCounts$12"
+ "BlockMatrixCounts$13"
+ "BlockMatrixCounts$14"
+ "BlockMatrixCounts$15"
+ "BlockMatrixCounts$16"
+ "BlockMatrixCounts$17"
+ "BlockMatrixCounts$18"
+ "BlockMatrixCounts$19"
+ "BlockMatrixCounts$2"
+ "BlockMatrixCounts$20"
+ "BlockMatrixCounts$21"
+ "BlockMatrixCounts$22"
+ "BlockMatrixCounts$23"
+ "BlockMatrixCounts$24"
+ "BlockMatrixCounts$3"
+ "BlockMatrixCounts$4"
+ "BlockMatrixCounts$5"
+ "BlockMatrixCounts$6"
+ "BlockMatrixCounts$7"
+ "BlockMatrixCounts$8"
+ "BlockMatrixCounts$9"
+ "BlockOffsets"
+ "BlockOffsets$1"
+ "BlockOffsets$2"
+ "BlockOffsets$3"
+ "BlockOffsets$4"
+ "BlockOffsets$5"
+ "BlockOffsets$6"
+ "BlockOffsets$7"
+ "BlockOffsets$8"
+ "BlockOffsets$9"
+ "Broadcast"
+ "Broadcast has no CPU implementation."
+ "Buffer$17"
+ "Buffer$18"
+ "Buffer$19"
+ "Buffer$20"
+ "Buffer$21"
+ "Buffer$22"
+ "Buffer$23"
+ "Buffer$24"
+ "Buffer$25"
+ "Buffer$26"
+ "Buffer$27"
+ "Buffer$28"
+ "Buffer$31"
+ "Buffer$32"
+ "Buffer$33"
+ "Buffer$34"
+ "Buffer$35"
+ "Buffer$36"
+ "Buffer$37"
+ "Buffer$38"
+ "Buffer$40"
+ "Buffer$41"
+ "Buffer$42"
+ "Buffer$43"
+ "Buffer$44"
+ "Buffer$45"
+ "Buffer$46"
+ "Buffer$47"
+ "CPGGaussian3DGetColorRestDimension"
+ "CPGGaussianSplattingSessionRun"
+ "CalculateContributions"
+ "Cannot broadcast array of shape "
+ "Cannot enqueue work after stream is stopped."
+ "Ceil"
+ "Ceil has no CPU implementation."
+ "Concatenate"
+ "Concatenate has no CPU implementation."
+ "Contiguous"
+ "Contiguous has no CPU implementation."
+ "Convolution"
+ "Convolution has no CPU implementation."
+ "Copy"
+ "Copy has no CPU implementation."
+ "Cos"
+ "Cos has no CPU implementation."
+ "Create submesh failed."
+ "Create subtask failed."
+ "Cum"
+ "D convolution."
+ "DPSMVS.estimating_man"
+ "DPSMVS.loading_man"
+ "DPSMVS.loading_man.LoadingImage"
+ "DPSMVS.loading_man.ResizeConvert"
+ "DPSMVS.prepare.images"
+ "DPSMVS.prepare.metadata"
+ "DPSMVS.saving_man"
+ "DPSMVS.saving_man.resize.DepthConf"
+ "DPSMVS.saving_man.resize.Mask"
+ "DPSMVS.squeeze.images"
+ "Data size and provided shape mismatch in array construction."
+ "DenseFeatureMatcherModel_OC"
+ "Divide"
+ "Divide has no CPU implementation."
+ "Duplicate axes detected in reduction."
+ "DuplicateWithKeys"
+ "Epsilonbundleadjustgc_Huber_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_Huber_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian_18417033517928820306"
+ "Epsilonbundleadjustgc_Huber_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian_18417033517928820306_func"
+ "Epsilonbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878857649563246"
+ "Epsilonbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878857649563246_func"
+ "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian"
+ "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian_18221862684647205804"
+ "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian_18221862684647205804_func"
+ "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstraintLeastSquaresHuberisotropic_gaussian"
+ "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstraintLeastSquaresHuberisotropic_gaussian_18221862684647205804"
+ "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstraintLeastSquaresHuberisotropic_gaussian_18221862684647205804_func"
+ "Epsilonbundleadjustgc_Huber_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_Huber_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878857633941635"
+ "Epsilonbundleadjustgc_Huber_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878857633941635_func"
+ "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian"
+ "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian_16001595286909791879"
+ "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian_16001595286909791879_func"
+ "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstraintLeastSquaresHuberisotropic_gaussian"
+ "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstraintLeastSquaresHuberisotropic_gaussian_16001595286909791879"
+ "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstraintLeastSquaresHuberisotropic_gaussian_16001595286909791879_func"
+ "Epsilonbundleadjustgc_Huber_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_Huber_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878856524932754"
+ "Epsilonbundleadjustgc_Huber_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878856524932754_func"
+ "Epsilonbundleadjustgc_L2_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_L2_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian_18417033517928820306"
+ "Epsilonbundleadjustgc_L2_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian_18417033517928820306_func"
+ "Epsilonbundleadjustgc_L2_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_L2_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878857649563246"
+ "Epsilonbundleadjustgc_L2_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878857649563246_func"
+ "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian_8399906543938503264"
+ "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian_8399906543938503264_func"
+ "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstraintLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstraintLeastSquaresL2isotropic_gaussian_8399906543938503264"
+ "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstraintLeastSquaresL2isotropic_gaussian_8399906543938503264_func"
+ "Epsilonbundleadjustgc_L2_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_L2_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878857633941635"
+ "Epsilonbundleadjustgc_L2_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878857633941635_func"
+ "Epsilonbundleadjustgc_L2_KannalaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_L2_KannalaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian_18172086786342403070"
+ "Epsilonbundleadjustgc_L2_KannalaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian_18172086786342403070_func"
+ "Epsilonbundleadjustgc_L2_KannalaReprojectionConstraintLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_L2_KannalaReprojectionConstraintLeastSquaresL2isotropic_gaussian_18172086786342403070"
+ "Epsilonbundleadjustgc_L2_KannalaReprojectionConstraintLeastSquaresL2isotropic_gaussian_18172086786342403070_func"
+ "Epsilonbundleadjustgc_L2_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "Epsilonbundleadjustgc_L2_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878856524932754"
+ "Epsilonbundleadjustgc_L2_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian_14254878856524932754_func"
+ "Equal"
+ "Equal has no CPU implementation."
+ "ErfInv"
+ "ErfInv has no CPU implementation."
+ "EspressoStreamPool: Failed to create e5rt stream operation with options.  Return code: "
+ "EspressoStreamPool: Failed to create e5rt stream.  Return code: "
+ "EvaluateHpSFM_Huber_lossFactorGraph_467343712571576471"
+ "EvaluateHpSFM_Huber_lossFactorGraph_467343712571576471_func"
+ "EvaluateHpSFM_L2_lossFactorGraph_467343712571576471"
+ "EvaluateHpSFM_L2_lossFactorGraph_467343712571576471_func"
+ "Exp"
+ "Exp has no CPU implementation."
+ "ExtractDiagJtJSFM_Huber_lossFactorGraph_10211267900542196221"
+ "ExtractDiagJtJSFM_Huber_lossFactorGraph_10211267900542196221_func"
+ "ExtractDiagJtJSFM_L2_lossFactorGraph_10211267900542196221"
+ "ExtractDiagJtJSFM_L2_lossFactorGraph_10211267900542196221_func"
+ "Fail to write usdz"
+ "Failed to create ANST model"
+ "Failed to create ANST model, error: %@"
+ "Failed to find metallib"
+ "Failed to load device"
+ "Failed to load device library from <"
+ "Failed to resize pixel buffer"
+ "Failed to run semantic mask detection on image, error: %@"
+ "Floor"
+ "Floor has no CPU implementation."
+ "Full"
+ "Full has no CPU implementation."
+ "GESS"
+ "Gather"
+ "Gather has no CPU implementation."
+ "GaussianSplattingSession: "
+ "Gess_Kernels.metallib"
+ "Given groups="
+ "Greater"
+ "Greater has no CPU implementation."
+ "GreaterEqual"
+ "GreaterEqual has no CPU implementation."
+ "GridSample3DBackward"
+ "GridSample3DForward"
+ "Group"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable-delta"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable0-delta"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable0WithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable0WithOffsets_11766776421010059282"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable0WithOffsets_11766776421010059282_func"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable0_11766776421010059282"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable0_11766776421010059282_func"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariableWithOffsets_9017226649725815957"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariableWithOffsets_9017226649725815957_func"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable_11766776421010059282"
+ "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable_11766776421010059282_func"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable-delta"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable0-delta"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable0WithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable0WithOffsets_11766055196898389796"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable0WithOffsets_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable0_11766055196898389796"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable0_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariableWithOffsets_9019890729246572383"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariableWithOffsets_9019890729246572383_func"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable_11766055196898389796"
+ "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable-delta"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable0-delta"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable0WithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable0WithOffsets_11766055197652885987"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable0WithOffsets_11766055197652885987_func"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable0_11766055197652885987"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable0_11766055197652885987_func"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariableWithOffsets_9019890728685578306"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariableWithOffsets_9019890728685578306_func"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable_11766055197652885987"
+ "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable_11766055197652885987_func"
+ "GroupUpdatebundleadjustgc_Huber_PixelConstant-delta"
+ "GroupUpdatebundleadjustgc_Huber_PixelConstantWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_PixelConstantWithOffsets_11766055199608425866"
+ "GroupUpdatebundleadjustgc_Huber_PixelConstantWithOffsets_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_Huber_PixelConstant_11766055199608425866"
+ "GroupUpdatebundleadjustgc_Huber_PixelConstant_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_Huber_PointConstant-delta"
+ "GroupUpdatebundleadjustgc_Huber_PointConstantWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_PointConstantWithOffsets_11766055196898389796"
+ "GroupUpdatebundleadjustgc_Huber_PointConstantWithOffsets_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_Huber_PointConstant_11766055196898389796"
+ "GroupUpdatebundleadjustgc_Huber_PointConstant_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_Huber_PointVariable-delta"
+ "GroupUpdatebundleadjustgc_Huber_PointVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_PointVariableWithOffsets_9019890729246572383"
+ "GroupUpdatebundleadjustgc_Huber_PointVariableWithOffsets_9019890729246572383_func"
+ "GroupUpdatebundleadjustgc_Huber_PointVariable_11766055196898389796"
+ "GroupUpdatebundleadjustgc_Huber_PointVariable_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_Huber_PoseGroupVariable-delta"
+ "GroupUpdatebundleadjustgc_Huber_PoseGroupVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_PoseGroupVariableWithOffsets_7590855739712434305"
+ "GroupUpdatebundleadjustgc_Huber_PoseGroupVariableWithOffsets_7590855739712434305_func"
+ "GroupUpdatebundleadjustgc_Huber_PoseGroupVariable_15130723012933083569"
+ "GroupUpdatebundleadjustgc_Huber_PoseGroupVariable_15130723012933083569_func"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable-delta"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable0-delta"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable0WithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable0WithOffsets_11766055199608425866"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable0WithOffsets_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable0_11766055199608425866"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable0_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariableWithOffsets_9019890729462173443"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariableWithOffsets_9019890729462173443_func"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable_11766055199608425866"
+ "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_Huber_RotationVariable-delta"
+ "GroupUpdatebundleadjustgc_Huber_RotationVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_RotationVariableWithOffsets_8937303712768260735"
+ "GroupUpdatebundleadjustgc_Huber_RotationVariableWithOffsets_8937303712768260735_func"
+ "GroupUpdatebundleadjustgc_Huber_RotationVariable_11740727864984060672"
+ "GroupUpdatebundleadjustgc_Huber_RotationVariable_11740727864984060672_func"
+ "GroupUpdatebundleadjustgc_Huber_TranslationVariable-delta"
+ "GroupUpdatebundleadjustgc_Huber_TranslationVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_Huber_TranslationVariableWithOffsets_9019890729246572383"
+ "GroupUpdatebundleadjustgc_Huber_TranslationVariableWithOffsets_9019890729246572383_func"
+ "GroupUpdatebundleadjustgc_Huber_TranslationVariable_11766055196898389796"
+ "GroupUpdatebundleadjustgc_Huber_TranslationVariable_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariable-delta"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariable0-delta"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariable0WithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariable0WithOffsets_11766776421010059282"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariable0WithOffsets_11766776421010059282_func"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariable0_11766776421010059282"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariable0_11766776421010059282_func"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariableWithOffsets_9017226649725815957"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariableWithOffsets_9017226649725815957_func"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariable_11766776421010059282"
+ "GroupUpdatebundleadjustgc_L2_FocalLengthVariable_11766776421010059282_func"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable-delta"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable0-delta"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable0WithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable0WithOffsets_11766055196898389796"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable0WithOffsets_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable0_11766055196898389796"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable0_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariableWithOffsets_9019890729246572383"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariableWithOffsets_9019890729246572383_func"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable_11766055196898389796"
+ "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable-delta"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable0-delta"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable0WithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable0WithOffsets_11766055197652885987"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable0WithOffsets_11766055197652885987_func"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable0_11766055197652885987"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable0_11766055197652885987_func"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariableWithOffsets_9019890728685578306"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariableWithOffsets_9019890728685578306_func"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable_11766055197652885987"
+ "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable_11766055197652885987_func"
+ "GroupUpdatebundleadjustgc_L2_PixelConstant-delta"
+ "GroupUpdatebundleadjustgc_L2_PixelConstantWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_PixelConstantWithOffsets_11766055199608425866"
+ "GroupUpdatebundleadjustgc_L2_PixelConstantWithOffsets_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_L2_PixelConstant_11766055199608425866"
+ "GroupUpdatebundleadjustgc_L2_PixelConstant_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_L2_PointConstant-delta"
+ "GroupUpdatebundleadjustgc_L2_PointConstantWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_PointConstantWithOffsets_11766055196898389796"
+ "GroupUpdatebundleadjustgc_L2_PointConstantWithOffsets_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_L2_PointConstant_11766055196898389796"
+ "GroupUpdatebundleadjustgc_L2_PointConstant_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_L2_PointVariable-delta"
+ "GroupUpdatebundleadjustgc_L2_PointVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_PointVariableWithOffsets_9019890729246572383"
+ "GroupUpdatebundleadjustgc_L2_PointVariableWithOffsets_9019890729246572383_func"
+ "GroupUpdatebundleadjustgc_L2_PointVariable_11766055196898389796"
+ "GroupUpdatebundleadjustgc_L2_PointVariable_11766055196898389796_func"
+ "GroupUpdatebundleadjustgc_L2_PoseGroupVariable-delta"
+ "GroupUpdatebundleadjustgc_L2_PoseGroupVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_PoseGroupVariableWithOffsets_7590855739712434305"
+ "GroupUpdatebundleadjustgc_L2_PoseGroupVariableWithOffsets_7590855739712434305_func"
+ "GroupUpdatebundleadjustgc_L2_PoseGroupVariable_15130723012933083569"
+ "GroupUpdatebundleadjustgc_L2_PoseGroupVariable_15130723012933083569_func"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable-delta"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable0-delta"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable0WithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable0WithOffsets_11766055199608425866"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable0WithOffsets_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable0_11766055199608425866"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable0_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariableWithOffsets_9019890729462173443"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariableWithOffsets_9019890729462173443_func"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable_11766055199608425866"
+ "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable_11766055199608425866_func"
+ "GroupUpdatebundleadjustgc_L2_RotationVariable-delta"
+ "GroupUpdatebundleadjustgc_L2_RotationVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_RotationVariableWithOffsets_8937303712768260735"
+ "GroupUpdatebundleadjustgc_L2_RotationVariableWithOffsets_8937303712768260735_func"
+ "GroupUpdatebundleadjustgc_L2_RotationVariable_11740727864984060672"
+ "GroupUpdatebundleadjustgc_L2_RotationVariable_11740727864984060672_func"
+ "GroupUpdatebundleadjustgc_L2_TranslationVariable-delta"
+ "GroupUpdatebundleadjustgc_L2_TranslationVariableWithOffsets-delta"
+ "GroupUpdatebundleadjustgc_L2_TranslationVariableWithOffsets_9019890729246572383"
+ "GroupUpdatebundleadjustgc_L2_TranslationVariableWithOffsets_9019890729246572383_func"
+ "GroupUpdatebundleadjustgc_L2_TranslationVariable_11766055196898389796"
+ "GroupUpdatebundleadjustgc_L2_TranslationVariable_11766055196898389796_func"
+ "Group_"
+ "INF"
+ "IdentifyTileRanges"
+ "Invalid SemanticMaskExtractor"
+ "Invalid axis "
+ "Invalid axis ("
+ "Invalid high padding size ("
+ "Invalid low padding size ("
+ "Invalid number of padding sizes passed to pad "
+ "Invalid padding mode ("
+ "IsotropicNoise"
+ "IsotropicNoise$1"
+ "IsotropicNoise$2"
+ "IsotropicNoise$3"
+ "IsotropicNoise$4"
+ "IsotropicNoise$5"
+ "IsotropicNoise$6"
+ "IsotropicNoise$7"
+ "JVP is not implemented for cumulative prod/min/max"
+ "LRGB2SRGB"
+ "LRGB2SRGB\n"
+ "LRGB2SRGBForward has no eval_cpu implementation."
+ "LRGB2SRGBForward has no jvp implementation."
+ "LRGB2SRGBForward has no vmap implementation."
+ "LearnedMVSModel/"
+ "Less"
+ "Less has no CPU implementation."
+ "LessEqual"
+ "LessEqual has no CPU implementation."
+ "Load mtl file failed"
+ "Load mtl file failed to open file stream"
+ "Loaded image "
+ "Log"
+ "Log has no CPU implementation."
+ "Log10"
+ "Log2"
+ "LogicalAnd"
+ "LogicalAnd has no CPU implementation."
+ "LogicalNot"
+ "LogicalNot has no CPU implementation."
+ "LogicalOr"
+ "LogicalOr has no CPU implementation."
+ "MLX_BFS_MAX_WIDTH"
+ "MLX_MAX_OPS_PER_BUFFER"
+ "MTLAccelerationStructure"
+ "MTLAccelerationStructureBoundingBoxGeometryDescriptor"
+ "MTLAccelerationStructureCommandEncoder"
+ "MTLAccelerationStructureCurveGeometryDescriptor"
+ "MTLAccelerationStructureDescriptor"
+ "MTLAccelerationStructureGeometryDescriptor"
+ "MTLAccelerationStructureMotionBoundingBoxGeometryDescriptor"
+ "MTLAccelerationStructureMotionCurveGeometryDescriptor"
+ "MTLAccelerationStructureMotionTriangleGeometryDescriptor"
+ "MTLAccelerationStructurePassDescriptor"
+ "MTLAccelerationStructurePassSampleBufferAttachmentDescriptor"
+ "MTLAccelerationStructurePassSampleBufferAttachmentDescriptorArray"
+ "MTLAccelerationStructureTriangleGeometryDescriptor"
+ "MTLAllocation"
+ "MTLArchitecture"
+ "MTLArgument"
+ "MTLArgumentDescriptor"
+ "MTLArgumentEncoder"
+ "MTLArrayType"
+ "MTLAttribute"
+ "MTLAttributeDescriptor"
+ "MTLAttributeDescriptorArray"
+ "MTLBinaryArchive"
+ "MTLBinaryArchiveDescriptor"
+ "MTLBinding"
+ "MTLBlitCommandEncoder"
+ "MTLBlitPassDescriptor"
+ "MTLBlitPassSampleBufferAttachmentDescriptor"
+ "MTLBlitPassSampleBufferAttachmentDescriptorArray"
+ "MTLBuffer"
+ "MTLBufferBinding"
+ "MTLBufferLayoutDescriptor"
+ "MTLBufferLayoutDescriptorArray"
+ "MTLCaptureDescriptor"
+ "MTLCaptureManager"
+ "MTLCommandBuffer"
+ "MTLCommandBufferDescriptor"
+ "MTLCommandBufferEncoderInfo"
+ "MTLCommandEncoder"
+ "MTLCommandQueue"
+ "MTLCommandQueueDescriptor"
+ "MTLCompileOptions"
+ "MTLComputeCommandEncoder"
+ "MTLComputePassDescriptor"
+ "MTLComputePassSampleBufferAttachmentDescriptor"
+ "MTLComputePassSampleBufferAttachmentDescriptorArray"
+ "MTLComputePipelineDescriptor"
+ "MTLComputePipelineReflection"
+ "MTLComputePipelineState"
+ "MTLCounter"
+ "MTLCounterSampleBuffer"
+ "MTLCounterSampleBufferDescriptor"
+ "MTLCounterSet"
+ "MTLDepthStencilDescriptor"
+ "MTLDepthStencilState"
+ "MTLDevice"
+ "MTLDrawable"
+ "MTLDynamicLibrary"
+ "MTLEvent"
+ "MTLFence"
+ "MTLFunction"
+ "MTLFunctionConstant"
+ "MTLFunctionConstantValues"
+ "MTLFunctionDescriptor"
+ "MTLFunctionHandle"
+ "MTLFunctionLog"
+ "MTLFunctionLogDebugLocation"
+ "MTLFunctionStitchingAttribute"
+ "MTLFunctionStitchingAttributeAlwaysInline"
+ "MTLFunctionStitchingFunctionNode"
+ "MTLFunctionStitchingGraph"
+ "MTLFunctionStitchingInputNode"
+ "MTLFunctionStitchingNode"
+ "MTLHeap"
+ "MTLHeapDescriptor"
+ "MTLIOCommandBuffer"
+ "MTLIOCommandQueue"
+ "MTLIOCommandQueueDescriptor"
+ "MTLIOFileHandle"
+ "MTLIOScratchBuffer"
+ "MTLIOScratchBufferAllocator"
+ "MTLIndirectCommandBuffer"
+ "MTLIndirectCommandBufferDescriptor"
+ "MTLIndirectComputeCommand"
+ "MTLIndirectInstanceAccelerationStructureDescriptor"
+ "MTLIndirectRenderCommand"
+ "MTLInstanceAccelerationStructureDescriptor"
+ "MTLIntersectionFunctionDescriptor"
+ "MTLIntersectionFunctionTable"
+ "MTLIntersectionFunctionTableDescriptor"
+ "MTLLibrary"
+ "MTLLinkedFunctions"
+ "MTLLogContainer"
+ "MTLLogState"
+ "MTLLogStateDescriptor"
+ "MTLMeshRenderPipelineDescriptor"
+ "MTLMotionKeyframeData"
+ "MTLObjectPayloadBinding"
+ "MTLParallelRenderCommandEncoder"
+ "MTLPipelineBufferDescriptor"
+ "MTLPipelineBufferDescriptorArray"
+ "MTLPointerType"
+ "MTLPrimitiveAccelerationStructureDescriptor"
+ "MTLRasterizationRateLayerArray"
+ "MTLRasterizationRateLayerDescriptor"
+ "MTLRasterizationRateMap"
+ "MTLRasterizationRateMapDescriptor"
+ "MTLRasterizationRateSampleArray"
+ "MTLRenderCommandEncoder"
+ "MTLRenderPassAttachmentDescriptor"
+ "MTLRenderPassColorAttachmentDescriptor"
+ "MTLRenderPassColorAttachmentDescriptorArray"
+ "MTLRenderPassDepthAttachmentDescriptor"
+ "MTLRenderPassDescriptor"
+ "MTLRenderPassSampleBufferAttachmentDescriptor"
+ "MTLRenderPassSampleBufferAttachmentDescriptorArray"
+ "MTLRenderPassStencilAttachmentDescriptor"
+ "MTLRenderPipelineColorAttachmentDescriptor"
+ "MTLRenderPipelineColorAttachmentDescriptorArray"
+ "MTLRenderPipelineDescriptor"
+ "MTLRenderPipelineFunctionsDescriptor"
+ "MTLRenderPipelineReflection"
+ "MTLRenderPipelineState"
+ "MTLResidencySet"
+ "MTLResidencySetDescriptor"
+ "MTLResource"
+ "MTLResourceStateCommandEncoder"
+ "MTLResourceStatePassDescriptor"
+ "MTLResourceStatePassSampleBufferAttachmentDescriptor"
+ "MTLResourceStatePassSampleBufferAttachmentDescriptorArray"
+ "MTLSamplerDescriptor"
+ "MTLSamplerState"
+ "MTLSharedEvent"
+ "MTLSharedEventHandle"
+ "MTLSharedEventListener"
+ "MTLSharedTextureHandle"
+ "MTLStageInputOutputDescriptor"
+ "MTLStencilDescriptor"
+ "MTLStitchedLibraryDescriptor"
+ "MTLStructMember"
+ "MTLStructType"
+ "MTLTexture"
+ "MTLTextureBinding"
+ "MTLTextureDescriptor"
+ "MTLTextureReferenceType"
+ "MTLThreadgroupBinding"
+ "MTLTileRenderPipelineColorAttachmentDescriptor"
+ "MTLTileRenderPipelineColorAttachmentDescriptorArray"
+ "MTLTileRenderPipelineDescriptor"
+ "MTLType"
+ "MTLVertexAttribute"
+ "MTLVertexAttributeDescriptor"
+ "MTLVertexAttributeDescriptorArray"
+ "MTLVertexBufferLayoutDescriptor"
+ "MTLVertexBufferLayoutDescriptorArray"
+ "MTLVertexDescriptor"
+ "MTLVisibleFunctionTable"
+ "MTLVisibleFunctionTableDescriptor"
+ "MaterializeJacobiansbundleadjustgc_Huber_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_Huber_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian_2486289328186530707"
+ "MaterializeJacobiansbundleadjustgc_Huber_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian_2486289328186530707_func"
+ "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442297367643623"
+ "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442297367643623_func"
+ "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian_7712258836057557342"
+ "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian_7712258836057557342_func"
+ "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstraintLeastSquaresHuberisotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstraintLeastSquaresHuberisotropic_gaussian_16295420538273410461"
+ "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstraintLeastSquaresHuberisotropic_gaussian_16295420538273410461_func"
+ "MaterializeJacobiansbundleadjustgc_Huber_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_Huber_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442296638915524"
+ "MaterializeJacobiansbundleadjustgc_Huber_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442296638915524_func"
+ "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian_10605774952435220749"
+ "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraintLeastSquaresHuberisotropic_gaussian_10605774952435220749_func"
+ "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstraintLeastSquaresHuberisotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstraintLeastSquaresHuberisotropic_gaussian_279555616407098832"
+ "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstraintLeastSquaresHuberisotropic_gaussian_279555616407098832_func"
+ "MaterializeJacobiansbundleadjustgc_Huber_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_Huber_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442297267523507"
+ "MaterializeJacobiansbundleadjustgc_Huber_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442297267523507_func"
+ "MaterializeJacobiansbundleadjustgc_L2_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_L2_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian_2486289328186530707"
+ "MaterializeJacobiansbundleadjustgc_L2_FocalLengthPriorConstraintNameLeastSquaresL2isotropic_gaussian_2486289328186530707_func"
+ "MaterializeJacobiansbundleadjustgc_L2_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_L2_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442297367643623"
+ "MaterializeJacobiansbundleadjustgc_L2_HeikkilaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442297367643623_func"
+ "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian_6016989436463011119"
+ "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian_6016989436463011119_func"
+ "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstraintLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstraintLeastSquaresL2isotropic_gaussian_7365192789340267386"
+ "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstraintLeastSquaresL2isotropic_gaussian_7365192789340267386_func"
+ "MaterializeJacobiansbundleadjustgc_L2_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_L2_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442296638915524"
+ "MaterializeJacobiansbundleadjustgc_L2_KannalaDistortionPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442296638915524_func"
+ "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian_16710380821692038399"
+ "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstantPointConstraintLeastSquaresL2isotropic_gaussian_16710380821692038399_func"
+ "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstraintLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstraintLeastSquaresL2isotropic_gaussian_9322427598522339507"
+ "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstraintLeastSquaresL2isotropic_gaussian_9322427598522339507_func"
+ "MaterializeJacobiansbundleadjustgc_L2_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian"
+ "MaterializeJacobiansbundleadjustgc_L2_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442297267523507"
+ "MaterializeJacobiansbundleadjustgc_L2_PrincipalPointPriorConstraintNameLeastSquaresL2isotropic_gaussian_16737442297267523507_func"
+ "Matmul"
+ "Matmul has no CPU implementation."
+ "Matrix dimensions (%dx%d) do not match x vector dimensions %dx%d\n"
+ "Matrix dimensions (%dx%d) do not match y vector dimensions %dx%d\n"
+ "Max"
+ "Max<{0}>"
+ "Maximum"
+ "Maximum has no CPU implementation."
+ "Metal GPU is not available."
+ "Min"
+ "Min<{0}>"
+ "Minimum"
+ "Minimum has no CPU implementation."
+ "Multiply"
+ "Multiply has no CPU implementation."
+ "NAN"
+ "NSArray"
+ "NSAutoreleasePool"
+ "NSBundle"
+ "NSCondition"
+ "NSDate"
+ "NSDictionary"
+ "NSError"
+ "NSNotificationCenter"
+ "NSNumber"
+ "NSObject"
+ "NSProcessInfo"
+ "NSSet"
+ "NSString"
+ "NSURL"
+ "NSValue"
+ "NaNEqual"
+ "Negative"
+ "Negative has no CPU implementation."
+ "No arrays provided for stacking"
+ "Non-manifold vertex."
+ "NormSqbundleadjustgc_Huber_FocalLengthVariable0_13504948212066289804"
+ "NormSqbundleadjustgc_Huber_FocalLengthVariable0_13504948212066289804_func"
+ "NormSqbundleadjustgc_Huber_FocalLengthVariable_13504948212066289804"
+ "NormSqbundleadjustgc_Huber_FocalLengthVariable_13504948212066289804_func"
+ "NormSqbundleadjustgc_Huber_HeikkilaDistortionVariable0_11766761173800193807"
+ "NormSqbundleadjustgc_Huber_HeikkilaDistortionVariable0_11766761173800193807_func"
+ "NormSqbundleadjustgc_Huber_HeikkilaDistortionVariable_11766761173800193807"
+ "NormSqbundleadjustgc_Huber_HeikkilaDistortionVariable_11766761173800193807_func"
+ "NormSqbundleadjustgc_Huber_KannalaDistortionVariable0_11766761173988408095"
+ "NormSqbundleadjustgc_Huber_KannalaDistortionVariable0_11766761173988408095_func"
+ "NormSqbundleadjustgc_Huber_KannalaDistortionVariable_11766761173988408095"
+ "NormSqbundleadjustgc_Huber_KannalaDistortionVariable_11766761173988408095_func"
+ "NormSqbundleadjustgc_Huber_PixelConstant_11766761171506639238"
+ "NormSqbundleadjustgc_Huber_PixelConstant_11766761171506639238_func"
+ "NormSqbundleadjustgc_Huber_PointConstant_11766761173800193807"
+ "NormSqbundleadjustgc_Huber_PointConstant_11766761173800193807_func"
+ "NormSqbundleadjustgc_Huber_PointVariable_11766761173800193807"
+ "NormSqbundleadjustgc_Huber_PointVariable_11766761173800193807_func"
+ "NormSqbundleadjustgc_Huber_PrincipalPointVariable0_11766761171506639238"
+ "NormSqbundleadjustgc_Huber_PrincipalPointVariable0_11766761171506639238_func"
+ "NormSqbundleadjustgc_Huber_PrincipalPointVariable_11766761171506639238"
+ "NormSqbundleadjustgc_Huber_PrincipalPointVariable_11766761171506639238_func"
+ "NormSqbundleadjustgc_Huber_RotationVariable_7748915237511836784"
+ "NormSqbundleadjustgc_Huber_RotationVariable_7748915237511836784_func"
+ "NormSqbundleadjustgc_Huber_TranslationVariable_11766761173800193807"
+ "NormSqbundleadjustgc_Huber_TranslationVariable_11766761173800193807_func"
+ "NormSqbundleadjustgc_L2_FocalLengthVariable0_13504948212066289804"
+ "NormSqbundleadjustgc_L2_FocalLengthVariable0_13504948212066289804_func"
+ "NormSqbundleadjustgc_L2_FocalLengthVariable_13504948212066289804"
+ "NormSqbundleadjustgc_L2_FocalLengthVariable_13504948212066289804_func"
+ "NormSqbundleadjustgc_L2_HeikkilaDistortionVariable0_11766761173800193807"
+ "NormSqbundleadjustgc_L2_HeikkilaDistortionVariable0_11766761173800193807_func"
+ "NormSqbundleadjustgc_L2_HeikkilaDistortionVariable_11766761173800193807"
+ "NormSqbundleadjustgc_L2_HeikkilaDistortionVariable_11766761173800193807_func"
+ "NormSqbundleadjustgc_L2_KannalaDistortionVariable0_11766761173988408095"
+ "NormSqbundleadjustgc_L2_KannalaDistortionVariable0_11766761173988408095_func"
+ "NormSqbundleadjustgc_L2_KannalaDistortionVariable_11766761173988408095"
+ "NormSqbundleadjustgc_L2_KannalaDistortionVariable_11766761173988408095_func"
+ "NormSqbundleadjustgc_L2_PixelConstant_11766761171506639238"
+ "NormSqbundleadjustgc_L2_PixelConstant_11766761171506639238_func"
+ "NormSqbundleadjustgc_L2_PointConstant_11766761173800193807"
+ "NormSqbundleadjustgc_L2_PointConstant_11766761173800193807_func"
+ "NormSqbundleadjustgc_L2_PointVariable_11766761173800193807"
+ "NormSqbundleadjustgc_L2_PointVariable_11766761173800193807_func"
+ "NormSqbundleadjustgc_L2_PrincipalPointVariable0_11766761171506639238"
+ "NormSqbundleadjustgc_L2_PrincipalPointVariable0_11766761171506639238_func"
+ "NormSqbundleadjustgc_L2_PrincipalPointVariable_11766761171506639238"
+ "NormSqbundleadjustgc_L2_PrincipalPointVariable_11766761171506639238_func"
+ "NormSqbundleadjustgc_L2_RotationVariable_7748915237511836784"
+ "NormSqbundleadjustgc_L2_RotationVariable_7748915237511836784_func"
+ "NormSqbundleadjustgc_L2_TranslationVariable_11766761173800193807"
+ "NormSqbundleadjustgc_L2_TranslationVariable_11766761173800193807_func"
+ "NotEqual"
+ "NotEqual has no CPU implementation."
+ "Number of dimensions must be positive."
+ "NumberOfElements"
+ "NumberOfElements has no CPU implementation."
+ "Offsetsbundleadjustgc_Huber_FocalLengthVariableVariableGroup"
+ "Offsetsbundleadjustgc_Huber_HeikkilaDistortionVariableVariableGroup"
+ "Offsetsbundleadjustgc_Huber_KannalaDistortionVariableVariableGroup"
+ "Offsetsbundleadjustgc_Huber_PointVariableVariableGroup"
+ "Offsetsbundleadjustgc_Huber_PoseGroupVariable"
+ "Offsetsbundleadjustgc_Huber_PrincipalPointVariableVariableGroup"
+ "Offsetsbundleadjustgc_Huber_RotationVariableVariableGroup"
+ "Offsetsbundleadjustgc_Huber_TranslationVariableVariableGroup"
+ "Offsetsbundleadjustgc_L2_FocalLengthVariableVariableGroup"
+ "Offsetsbundleadjustgc_L2_HeikkilaDistortionVariableVariableGroup"
+ "Offsetsbundleadjustgc_L2_KannalaDistortionVariableVariableGroup"
+ "Offsetsbundleadjustgc_L2_PointVariableVariableGroup"
+ "Offsetsbundleadjustgc_L2_PoseGroupVariable"
+ "Offsetsbundleadjustgc_L2_PrincipalPointVariableVariableGroup"
+ "Offsetsbundleadjustgc_L2_RotationVariableVariableGroup"
+ "Offsetsbundleadjustgc_L2_TranslationVariableVariableGroup"
+ "Operation canceled"
+ "Or"
+ "Pad"
+ "Pad has no CPU implementation."
+ "Pad vmap is NYI."
+ "Photogrammetry_GaussianSplatting_Kernels.metallib"
+ "Power"
+ "Power has no CPU implementation."
+ "PreprocessBWD"
+ "PreprocessForward"
+ "Prod"
+ "Prod<{0}>"
+ "PruneGaussianPairings"
+ "RMSprop"
+ "RMSprop forward\n"
+ "RMSpropFWD has no eval_cpu implementation."
+ "RMSpropFWD has no jvp implementation."
+ "RMSpropFWD has no vjp implementation."
+ "RMSpropFWD has no vmap implementation."
+ "RandomBits"
+ "RandomBits has no CPU implementation."
+ "Rasterize Gaussians backward\n"
+ "Rasterize Gaussians forward1\n"
+ "Rasterize Gaussians forward2\n"
+ "RasterizeGaussiansBWD has no eval_cpu implementation."
+ "RasterizeGaussiansBWD has no jvp implementation."
+ "RasterizeGaussiansBWD has no vjp implementation."
+ "RasterizeGaussiansBWD has no vmap implementation."
+ "RasterizeGaussiansFWD1 has no eval_cpu implementation."
+ "RasterizeGaussiansFWD1 has no jvp implementation."
+ "RasterizeGaussiansFWD1 has no vjp implementation."
+ "RasterizeGaussiansFWD1 has no vmap implementation."
+ "RasterizeGaussiansFWD2 has no eval_cpu implementation."
+ "RasterizeGaussiansFWD2 has no jvp implementation."
+ "RasterizeGaussiansFWD2 has no vmap implementation."
+ "Reconstructing 3D Gaussian"
+ "Reduce has no CPU implementation."
+ "Reduce type VJP not yet implemented."
+ "RenderBackward"
+ "RenderForward"
+ "Request detail=%d"
+ "Reshape"
+ "Reshape has no CPU implementation."
+ "Resources"
+ "Rsqrt"
+ "SFM_Huber_lossFactorGraphBlockAndFactorReordering"
+ "SFM_Huber_lossFactorGraphBlockAndFactorReordering_1890073621647191326"
+ "SFM_Huber_lossFactorGraphBlockAndFactorReordering_1890073621647191326_func"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_Huber_FocalLengthVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_Huber_HeikkilaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_Huber_KannalaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_Huber_PoseGroupVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_Huber_PrincipalPointVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_Huber_FocalLengthVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_Huber_HeikkilaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_Huber_KannalaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_Huber_PoseGroupVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_Huber_PrincipalPointVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_M_bundleadjustgc_Huber_FocalLengthVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_M_bundleadjustgc_Huber_HeikkilaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_M_bundleadjustgc_Huber_KannalaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_M_bundleadjustgc_Huber_PoseGroupVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_M_bundleadjustgc_Huber_PrincipalPointVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_be_bundleadjustgc_Huber_PointVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_Huber_FocalLengthVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_Huber_HeikkilaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_Huber_KannalaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_Huber_PoseGroupVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_Huber_PrincipalPointVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_deltae_bundleadjustgc_Huber_PointVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_Huber_FocalLengthVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_Huber_HeikkilaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_Huber_KannalaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_Huber_PoseGroupVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_Huber_PrincipalPointVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_iHee_bundleadjustgc_Huber_PointVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_p_bundleadjustgc_Huber_FocalLengthVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_p_bundleadjustgc_Huber_HeikkilaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_p_bundleadjustgc_Huber_KannalaDistortionVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_p_bundleadjustgc_Huber_PoseGroupVariable"
+ "SFM_Huber_lossFactorGraphLinearizationPoint_p_bundleadjustgc_Huber_PrincipalPointVariable"
+ "SFM_L2_lossFactorGraphBlockAndFactorReordering"
+ "SFM_L2_lossFactorGraphBlockAndFactorReordering_1890073621647191326"
+ "SFM_L2_lossFactorGraphBlockAndFactorReordering_1890073621647191326_func"
+ "SFM_L2_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_L2_FocalLengthVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_L2_HeikkilaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_L2_KannalaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_L2_PoseGroupVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_JtJp_bundleadjustgc_L2_PrincipalPointVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_L2_FocalLengthVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_L2_HeikkilaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_L2_KannalaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_L2_PoseGroupVariable$1"
+ "SFM_L2_lossFactorGraphLinearizationPoint_Jte_bundleadjustgc_L2_PrincipalPointVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_M_bundleadjustgc_L2_FocalLengthVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_M_bundleadjustgc_L2_HeikkilaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_M_bundleadjustgc_L2_KannalaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_M_bundleadjustgc_L2_PoseGroupVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_M_bundleadjustgc_L2_PrincipalPointVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_be_bundleadjustgc_L2_PointVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_L2_FocalLengthVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_L2_HeikkilaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_L2_KannalaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_L2_PoseGroupVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_blockM_bundleadjustgc_L2_PrincipalPointVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_deltae_bundleadjustgc_L2_PointVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_L2_FocalLengthVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_L2_HeikkilaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_L2_KannalaDistortionVariable$1"
+ "SFM_L2_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_L2_PoseGroupVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_deltar_bundleadjustgc_L2_PrincipalPointVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_iHee_bundleadjustgc_L2_PointVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_p_bundleadjustgc_L2_FocalLengthVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_p_bundleadjustgc_L2_HeikkilaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_p_bundleadjustgc_L2_KannalaDistortionVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_p_bundleadjustgc_L2_PoseGroupVariable"
+ "SFM_L2_lossFactorGraphLinearizationPoint_p_bundleadjustgc_L2_PrincipalPointVariable"
+ "SO3FromLogbundleadjustgc_Huber_RotationVariable_11740727864984060672"
+ "SO3FromLogbundleadjustgc_Huber_RotationVariable_11740727864984060672_func"
+ "SO3FromLogbundleadjustgc_L2_RotationVariable_11740727864984060672"
+ "SO3FromLogbundleadjustgc_L2_RotationVariable_11740727864984060672_func"
+ "SRGB2LRGB"
+ "SRGB2LRGB\n"
+ "SRGB2LRGBForward has no eval_cpu implementation."
+ "SRGB2LRGBForward has no jvp implementation."
+ "SRGB2LRGBForward has no vmap implementation."
+ "SSIM backward\n"
+ "SSIM forward\n"
+ "SSIMBWD has no eval_cpu implementation."
+ "SSIMBWD has no jvp implementation."
+ "SSIMBWD has no vjp implementation."
+ "SSIMBWD has no vmap implementation."
+ "SSIMBackward"
+ "SSIMFWD has no eval_cpu implementation."
+ "SSIMFWD has no jvp implementation."
+ "SSIMFWD has no vmap implementation."
+ "SSIMForward"
+ "Scalar$19"
+ "Scalar$20"
+ "Scalar$21"
+ "Scalar$22"
+ "Scalar$23"
+ "Scan has no CPU implementation."
+ "Scatter"
+ "Scatter has no CPU implementation."
+ "SchurBackSubstitute1SFM_Huber_lossFactorGraph_13530503230912187044"
+ "SchurBackSubstitute1SFM_Huber_lossFactorGraph_13530503230912187044_func"
+ "SchurBackSubstitute1SFM_L2_lossFactorGraph_10939424283405806394"
+ "SchurBackSubstitute1SFM_L2_lossFactorGraph_10939424283405806394_func"
+ "SchurBackSubstitute2SFM_Huber_lossFactorGraph_7749251354072786925"
+ "SchurBackSubstitute2SFM_Huber_lossFactorGraph_7749251354072786925_func"
+ "SchurBackSubstitute2SFM_L2_lossFactorGraph_7749251354072786925"
+ "SchurBackSubstitute2SFM_L2_lossFactorGraph_7749251354072786925_func"
+ "SchurInvertEliminatedAdditiveDampingSFM_Huber_lossFactorGraph_13668474302907867620"
+ "SchurInvertEliminatedAdditiveDampingSFM_Huber_lossFactorGraph_13668474302907867620_func"
+ "SchurInvertEliminatedAdditiveDampingSFM_L2_lossFactorGraph_13668474302907867620"
+ "SchurInvertEliminatedAdditiveDampingSFM_L2_lossFactorGraph_13668474302907867620_func"
+ "SchurInvertEliminatedMultiplicativeDampingSFM_Huber_lossFactorGraph_13668474302907867620"
+ "SchurInvertEliminatedMultiplicativeDampingSFM_Huber_lossFactorGraph_13668474302907867620_func"
+ "SchurInvertEliminatedMultiplicativeDampingSFM_L2_lossFactorGraph_13668474302907867620"
+ "SchurInvertEliminatedMultiplicativeDampingSFM_L2_lossFactorGraph_13668474302907867620_func"
+ "SchurInvertEliminatedSFM_Huber_lossFactorGraph_13668474302907867620"
+ "SchurInvertEliminatedSFM_Huber_lossFactorGraph_13668474302907867620_func"
+ "SchurInvertEliminatedSFM_L2_lossFactorGraph_13668474302907867620"
+ "SchurInvertEliminatedSFM_L2_lossFactorGraph_13668474302907867620_func"
+ "SchurLinearizeSFM_Huber_lossFactorGraph_9148011906146170221"
+ "SchurLinearizeSFM_Huber_lossFactorGraph_9148011906146170221_func"
+ "SchurLinearizeSFM_L2_lossFactorGraph_13053972866459577333"
+ "SchurLinearizeSFM_L2_lossFactorGraph_13053972866459577333_func"
+ "SchurOuterProductSFM_Huber_lossFactorGraph_6526576586284478757"
+ "SchurOuterProductSFM_Huber_lossFactorGraph_6526576586284478757_func"
+ "SchurOuterProductSFM_L2_lossFactorGraph_7533801155325510748"
+ "SchurOuterProductSFM_L2_lossFactorGraph_7533801155325510748_func"
+ "Select"
+ "Select has no CPU implementation."
+ "Shapes "
+ "Sigmoid"
+ "Sigmoid has no CPU implementation."
+ "Sign"
+ "Sign has no CPU implementation."
+ "Sin"
+ "Sin has no CPU implementation."
+ "Slice"
+ "Slice has no CPU implementation."
+ "SliceUpdate"
+ "SliceUpdate has no CPU implementation."
+ "Sort"
+ "Sort has no CPU implementation."
+ "Split"
+ "Split has no CPU implementation."
+ "Sqrt"
+ "Sqrt has no CPU implementation."
+ "Square"
+ "Square has no CPU implementation."
+ "Start handling request #%zu, identifier=%s"
+ "StopGradient"
+ "StopGradient has no CPU implementation."
+ "Subtract"
+ "Subtract has no CPU implementation."
+ "Sum"
+ "Sum<{0}>"
+ "SymmetrizeDiagJtJSFM_Huber_lossFactorGraph_7798007793919822096"
+ "SymmetrizeDiagJtJSFM_Huber_lossFactorGraph_7798007793919822096_func"
+ "SymmetrizeDiagJtJSFM_L2_lossFactorGraph_7798007793919822096"
+ "SymmetrizeDiagJtJSFM_L2_lossFactorGraph_7798007793919822096_func"
+ "Synchronize"
+ "Texture"
+ "Texture_"
+ "Transpose"
+ "Transpose has no CPU implementation."
+ "URL"
+ "URLForAuxiliaryExecutable:"
+ "Uexpected grid vertices."
+ "Unable create execution stream operation with precompiled compute operation with options: "
+ "Unable to check if new compile required: "
+ "Unable to compile: "
+ "Unable to create compiler options: "
+ "Unable to create e5rt compiler: "
+ "Unable to create precompiled compute op options with program function: "
+ "Unable to find group attribute when writing USDA file!"
+ "Unable to retain program library function : "
+ "Unable to safely factor shape."
+ "Unable to set allocate intermediate buffers options for precompiled_compute_op: "
+ "Unable to set compiler options: "
+ "Unable to write USDA file \""
+ "Unexpected transition."
+ "Unhandled exception during CPGFeatureMatcherSession creation"
+ "Unhandled exception during CPGGaussianSplattingSession creation"
+ "Unknown exception during CPGFeatureMatcherSession creation"
+ "Unknown exception during CPGGaussianSplattingSession creation"
+ "Unsupported compilation type "
+ "UpdateGaussianStatistics"
+ "UpdateGaussianStatistics\n"
+ "UpdateGaussianStatisticsForward has no eval_cpu implementation."
+ "UpdateGaussianStatisticsForward has no jvp implementation."
+ "UpdateGaussianStatisticsForward has no vjp implementation."
+ "UpdateGaussianStatisticsForward has no vmap implementation."
+ "Updatebundleadjustgc_Huber_FocalLengthVariable-delta"
+ "Updatebundleadjustgc_Huber_FocalLengthVariable0-delta"
+ "Updatebundleadjustgc_Huber_FocalLengthVariable0_13505215581398723922"
+ "Updatebundleadjustgc_Huber_FocalLengthVariable0_13505215581398723922_func"
+ "Updatebundleadjustgc_Huber_FocalLengthVariable_13505215581398723922"
+ "Updatebundleadjustgc_Huber_FocalLengthVariable_13505215581398723922_func"
+ "Updatebundleadjustgc_Huber_HeikkilaDistortionVariable-delta"
+ "Updatebundleadjustgc_Huber_HeikkilaDistortionVariable0-delta"
+ "Updatebundleadjustgc_Huber_HeikkilaDistortionVariable0_11766055196898389796"
+ "Updatebundleadjustgc_Huber_HeikkilaDistortionVariable0_11766055196898389796_func"
+ "Updatebundleadjustgc_Huber_HeikkilaDistortionVariable_11766055196898389796"
+ "Updatebundleadjustgc_Huber_HeikkilaDistortionVariable_11766055196898389796_func"
+ "Updatebundleadjustgc_Huber_KannalaDistortionVariable-delta"
+ "Updatebundleadjustgc_Huber_KannalaDistortionVariable0-delta"
+ "Updatebundleadjustgc_Huber_KannalaDistortionVariable0_11766055197652885987"
+ "Updatebundleadjustgc_Huber_KannalaDistortionVariable0_11766055197652885987_func"
+ "Updatebundleadjustgc_Huber_KannalaDistortionVariable_11766055197652885987"
+ "Updatebundleadjustgc_Huber_KannalaDistortionVariable_11766055197652885987_func"
+ "Updatebundleadjustgc_Huber_PixelConstant-delta"
+ "Updatebundleadjustgc_Huber_PixelConstant_11766055199608425866"
+ "Updatebundleadjustgc_Huber_PixelConstant_11766055199608425866_func"
+ "Updatebundleadjustgc_Huber_PointConstant-delta"
+ "Updatebundleadjustgc_Huber_PointConstant_11766055196898389796"
+ "Updatebundleadjustgc_Huber_PointConstant_11766055196898389796_func"
+ "Updatebundleadjustgc_Huber_PointVariable-delta"
+ "Updatebundleadjustgc_Huber_PointVariable_11766055196898389796"
+ "Updatebundleadjustgc_Huber_PointVariable_11766055196898389796_func"
+ "Updatebundleadjustgc_Huber_PrincipalPointVariable-delta"
+ "Updatebundleadjustgc_Huber_PrincipalPointVariable0-delta"
+ "Updatebundleadjustgc_Huber_PrincipalPointVariable0_11766055199608425866"
+ "Updatebundleadjustgc_Huber_PrincipalPointVariable0_11766055199608425866_func"
+ "Updatebundleadjustgc_Huber_PrincipalPointVariable_11766055199608425866"
+ "Updatebundleadjustgc_Huber_PrincipalPointVariable_11766055199608425866_func"
+ "Updatebundleadjustgc_Huber_RotationVariable-delta"
+ "Updatebundleadjustgc_Huber_RotationVariable_11740727864984060672"
+ "Updatebundleadjustgc_Huber_RotationVariable_11740727864984060672_func"
+ "Updatebundleadjustgc_Huber_TranslationVariable-delta"
+ "Updatebundleadjustgc_Huber_TranslationVariable_11766055196898389796"
+ "Updatebundleadjustgc_Huber_TranslationVariable_11766055196898389796_func"
+ "Updatebundleadjustgc_L2_FocalLengthVariable-delta"
+ "Updatebundleadjustgc_L2_FocalLengthVariable0-delta"
+ "Updatebundleadjustgc_L2_FocalLengthVariable0_13505215581398723922"
+ "Updatebundleadjustgc_L2_FocalLengthVariable0_13505215581398723922_func"
+ "Updatebundleadjustgc_L2_FocalLengthVariable_13505215581398723922"
+ "Updatebundleadjustgc_L2_FocalLengthVariable_13505215581398723922_func"
+ "Updatebundleadjustgc_L2_HeikkilaDistortionVariable-delta"
+ "Updatebundleadjustgc_L2_HeikkilaDistortionVariable0-delta"
+ "Updatebundleadjustgc_L2_HeikkilaDistortionVariable0_11766055196898389796"
+ "Updatebundleadjustgc_L2_HeikkilaDistortionVariable0_11766055196898389796_func"
+ "Updatebundleadjustgc_L2_HeikkilaDistortionVariable_11766055196898389796"
+ "Updatebundleadjustgc_L2_HeikkilaDistortionVariable_11766055196898389796_func"
+ "Updatebundleadjustgc_L2_KannalaDistortionVariable-delta"
+ "Updatebundleadjustgc_L2_KannalaDistortionVariable0-delta"
+ "Updatebundleadjustgc_L2_KannalaDistortionVariable0_11766055197652885987"
+ "Updatebundleadjustgc_L2_KannalaDistortionVariable0_11766055197652885987_func"
+ "Updatebundleadjustgc_L2_KannalaDistortionVariable_11766055197652885987"
+ "Updatebundleadjustgc_L2_KannalaDistortionVariable_11766055197652885987_func"
+ "Updatebundleadjustgc_L2_PixelConstant-delta"
+ "Updatebundleadjustgc_L2_PixelConstant_11766055199608425866"
+ "Updatebundleadjustgc_L2_PixelConstant_11766055199608425866_func"
+ "Updatebundleadjustgc_L2_PointConstant-delta"
+ "Updatebundleadjustgc_L2_PointConstant_11766055196898389796"
+ "Updatebundleadjustgc_L2_PointConstant_11766055196898389796_func"
+ "Updatebundleadjustgc_L2_PointVariable-delta"
+ "Updatebundleadjustgc_L2_PointVariable_11766055196898389796"
+ "Updatebundleadjustgc_L2_PointVariable_11766055196898389796_func"
+ "Updatebundleadjustgc_L2_PrincipalPointVariable-delta"
+ "Updatebundleadjustgc_L2_PrincipalPointVariable0-delta"
+ "Updatebundleadjustgc_L2_PrincipalPointVariable0_11766055199608425866"
+ "Updatebundleadjustgc_L2_PrincipalPointVariable0_11766055199608425866_func"
+ "Updatebundleadjustgc_L2_PrincipalPointVariable_11766055199608425866"
+ "Updatebundleadjustgc_L2_PrincipalPointVariable_11766055199608425866_func"
+ "Updatebundleadjustgc_L2_RotationVariable-delta"
+ "Updatebundleadjustgc_L2_RotationVariable_11740727864984060672"
+ "Updatebundleadjustgc_L2_RotationVariable_11740727864984060672_func"
+ "Updatebundleadjustgc_L2_TranslationVariable-delta"
+ "Updatebundleadjustgc_L2_TranslationVariable_11766055196898389796"
+ "Updatebundleadjustgc_L2_TranslationVariable_11766055196898389796_func"
+ "UpsampleMultiplyBWD\n"
+ "UpsampleMultiplyBWD has no eval_cpu implementation."
+ "UpsampleMultiplyBWD has no jvp implementation."
+ "UpsampleMultiplyBWD has no vjp implementation."
+ "UpsampleMultiplyBWD has no vmap implementation."
+ "UpsampleMultiplyBackward"
+ "UpsampleMultiplyFWD\n"
+ "UpsampleMultiplyFWD has no eval_cpu implementation."
+ "UpsampleMultiplyFWD has no jvp implementation."
+ "UpsampleMultiplyFWD has no vmap implementation."
+ "UpsampleMultiplyForward"
+ "Usda file doesn't exist or is empty"
+ "VJP is not implemented for cumulative min/max"
+ "View %zu"
+ "View %zu, SrcView %zu"
+ "[Arange::eval_gpu] Does not support bool"
+ "[Arange::eval_gpu] Does not support complex64"
+ "[Convolution::eval_gpu] Only supports 1D, 2D or 3D convolutions."
+ "[Device::set_residency_set] Can only be set once."
+ "[Event::wait] Timed out"
+ "[Gather::eval_gpu] Gathering with more than "
+ "[Gather] Boolean indices not supported."
+ "[METAL] Command buffer execution failed: "
+ "[Metal::binary] Must use 1024 sized block"
+ "[Metal::copy] Must use 1024 sized block"
+ "[Metal::ternary] Must use 1024 sized block"
+ "[Metal::unary] Must use 1024 sized block"
+ "[Primitive::jvp] Not implemented for "
+ "[Primitive::output_shapes] "
+ "[Primitive::vjp] Not implemented for "
+ "[Primitive::vmap] Not implemented for "
+ "[Scatter::eval_gpu] Does not support "
+ "[Scatter::eval_gpu] Gathering with more than "
+ "[Scatter::eval_gpu] Invalid number of threads"
+ "[arange] Cannot compute length."
+ "[arange] Maximum size exceeded."
+ "[argsort] Received invalid axis "
+ "[astype] Type of cotangents does not match primal output type."
+ "[async_eval] Not allowed inside a graph transformation."
+ "[bits] Bit width must be in {1, 2, 4} but got "
+ "[bits] Expected key shape (2) but received "
+ "[bits] Expected key type uint32 but received "
+ "[concatenate] All the input array dimensions must match exactly "
+ "[concatenate] All the input arrays must have the same number of "
+ "[concatenate] Invalid axis ("
+ "[concatenate] No arrays provided for concatenation"
+ "[conv] Can only handle groups != 1 in 1D or 2D convolutions."
+ "[conv] Expect the input channels in the input"
+ "[conv] If groups > 1, the output channels must be divisible by the number"
+ "[conv] Input dilation sizes must be positive."
+ "[conv] Invalid input array with "
+ "[conv] Invalid input array with type "
+ "[conv] Invalid input dilation "
+ "[conv] Invalid kernel dilation "
+ "[conv] Invalid padding "
+ "[conv] Invalid strides "
+ "[conv] Invalid weight array with "
+ "[conv] Kernel dilation sizes must be positive."
+ "[conv] Only works for inputs with 1-3 spatial dimensions. The inputs must be in the format [N, ..., C_in]"
+ "[conv] Padding sizes must be non-negative."
+ "[conv] Spatial dimensions of input after padding "
+ "[conv] Stride sizes must be positive."
+ "[conv] The input channels must be divisible by the number"
+ "[cumprod] Axis "
+ "[cumsum] Axis "
+ "[default_stream] Cannot get gpu stream without gpu backend."
+ "[eval] Attempting to eval an array during function transformations like compile or vmap is not allowed."
+ "[eval] Attempting to eval an array without a primitive.\nIf you are compiling a function, make sure all the inputs and outputs are captured:\nhttps://ml-explore.github.io/mlx/build/html/usage/compile.html#pure-functions.\nIf you are not using compile, this may be a bug. Please file an issue here:\nhttps://github.com/ml-explore/mlx/issues."
+ "[expand_dims] Invalid axis "
+ "[expand_dims] Received duplicate axes."
+ "[flatten] Invalid start_axis "
+ "[flatten] start_axis must be less than or equal to end_axis"
+ "[floor] Not supported for complex64."
+ "[full] Negative dimensions not allowed."
+ "[gather] Axes don't match array dimensions."
+ "[gather] Cannot calculate JVP with respect to indices."
+ "[gather] Got indices with invalid dtype. Indices must be integral."
+ "[gather] Got slice_sizes with size "
+ "[gather] Number of index arrays does not match number of axes."
+ "[gather] Repeat axes not allowed in gather."
+ "[gather] Slice sizes must be in [0, a.shape(i)]. Got "
+ "[gather] Too many index arrays. Got "
+ "[grad] Invalid argument number for function with "
+ "[grad] Must specify at least one argument."
+ "[grad] Repeat argument number not allowed in grad."
+ "[linalg::norm] Received too many axes for norm."
+ "[malloc] Unable to allocate "
+ "[malloc_or_wait] Unable to allocate "
+ "[matmul] Does not yet support non-floating point types."
+ "[matmul] Got 0 dimension input. Inputs must have at least one dimension."
+ "[matmul] Got matrices with incorrectly broadcasted shapes: "
+ "[matmul] Last dimension of first input with shape "
+ "[matmul] Only real floating point types are supported but "
+ "[max] Cannot max reduce zero size array."
+ "[mean] axis "
+ "[metal::Device] Failed to make new command queue."
+ "[metal::Device] Unable to build metal library from source\n"
+ "[metal::Device] Unable to construct residency set.\n"
+ "[metal::Device] Unable to create new command buffer"
+ "[metal::Device] Unable to load function "
+ "[metal::Device] Unable to load kernel "
+ "[min] Cannot min reduce zero size array."
+ "[moveaxis] Invalid axis "
+ "[negative] Not supported for bool, use logical_not instead."
+ "[number_of_elements] Can't get the shape for axis "
+ "[randint] randint only accepts integer dtypes and bool."
+ "[repeat] Number of repeats cannot be negative"
+ "[reshape] Cannot infer the shape of an empty array"
+ "[reshape] Cannot reshape array of size "
+ "[reshape] Reshape can only infer one dimension."
+ "[scatter] Axes don't match array dimensions."
+ "[scatter] Boolean indices not supported."
+ "[scatter] Cannot calculate VJP with respect to indices."
+ "[scatter] GPU scatter does not yet support "
+ "[scatter] Got indices with invalid dtype. Indices must be integral."
+ "[scatter] JVP not yet implemented"
+ "[scatter] Number of index arrays does not match number of axes."
+ "[scatter] Repeat axes not allowed in scatter."
+ "[scatter] Too many index arrays. Got "
+ "[scatter] Update shape "
+ "[scatter] Updates with "
+ "[scatter] Updates with shape "
+ "[scatter] VJP not implemented for scatter_prod"
+ "[set_default_device] Cannot set gpu device without gpu backend."
+ "[slice] Invalid number of indices or strides for "
+ "[sort] Received invalid axis "
+ "[squeeze] Cannot squeeze axis "
+ "[squeeze] Invalid axes "
+ "[squeeze] Invalid axis "
+ "[squeeze] Received duplicate axes."
+ "[swapaxes] Invalid axis "
+ "[take] Cannot do a non-empty take from an array with zero elements."
+ "[take] Received invalid axis "
+ "[take_along_axis] Indices of dimension "
+ "[take_along_axis] Received invalid axis "
+ "[transpose] Invalid axis ("
+ "[transpose] Recived "
+ "[transpose] Repeat axes not allowed."
+ "[uniform] Can only generate uniform numbers with real floating point type."
+ "[uniform] Cannot generate random values of shape "
+ "[uniform] Unsupported type."
+ "[vjp] Number of outputs to compute gradients for ("
+ "[vjp] Output shape "
+ "] (\n"
+ "_1_32_32_reduce_"
+ "_K_"
+ "_MN_"
+ "_align_K_"
+ "_align_M_"
+ "_align_N_"
+ "_axpby0"
+ "_bc"
+ "_bk"
+ "_block_sort_"
+ "_bm"
+ "_bn"
+ "_bo"
+ "_channel_"
+ "_confidence"
+ "_copy"
+ "_do_axpby_"
+ "_do_gather_"
+ "_filter_"
+ "_has_batch_"
+ "_large"
+ "_learned"
+ "_lpfuse"
+ "_nc"
+ "_reduce_"
+ "_sm"
+ "_sn"
+ "_tm"
+ "_tn"
+ "_use_out_source_"
+ "_wm"
+ "_wn"
+ "accelerationStructureCommandEncoderWithDescriptor:"
+ "accelerationStructurePassDescriptor"
+ "access"
+ "activeProcessorCount"
+ "addAllocation:"
+ "addAllocations:count:"
+ "addBarrier"
+ "addComputePipelineFunctionsWithDescriptor:error:"
+ "addDebugMarker:range:"
+ "addFunctionWithDescriptor:library:error:"
+ "addLibraryWithDescriptor:error:"
+ "addLogHandler:"
+ "addMeshRenderPipelineFunctionsWithDescriptor:error:"
+ "addObject:"
+ "addObserverForName:object:queue:usingBlock:"
+ "addPresentedHandler:"
+ "addRenderPipelineFunctionsWithDescriptor:error:"
+ "addResidencySet:"
+ "addResidencySets:count:"
+ "addScheduledHandler:"
+ "addTileRenderPipelineFunctionsWithDescriptor:error:"
+ "aligned"
+ "alignment"
+ "allAllocations"
+ "allBundles"
+ "allFrameworks"
+ "allObjects"
+ "all_reduce"
+ "alloc"
+ "allocationCount"
+ "allowDuplicateIntersectionFunctionInvocation"
+ "allowGPUOptimizedContents"
+ "allowReferencingUndefinedSymbols"
+ "alphaBlendOperation"
+ "and"
+ "and the concatenation axis is "
+ "appStoreReceiptURL"
+ "arange"
+ "architecture"
+ "areProgrammableSamplePositionsSupported"
+ "areRasterOrderGroupsSupported"
+ "arg"
+ "argument not found"
+ "argumentBuffersSupport"
+ "argumentDescriptor"
+ "argumentIndex"
+ "argumentIndexStride"
+ "arguments"
+ "array with dimension "
+ "arrayType"
+ "arrayWithObject:"
+ "attributeIndex"
+ "attributeType"
+ "attributes"
+ "attributesOfItemAtPath:error:"
+ "atype"
+ "automaticTerminationSupportEnabled"
+ "autorelease"
+ "backFaceStencil"
+ "beginActivityWithOptions:reason:"
+ "bfloat16"
+ "bfloat16_t"
+ "binaryArchives"
+ "binaryFunctions"
+ "binary_g_nd1"
+ "binary_g_nd2"
+ "binary_g_nd3"
+ "binary_ss"
+ "binary_sv"
+ "binary_sv2"
+ "binary_vs"
+ "binary_vs2"
+ "binary_vv"
+ "binary_vv2"
+ "bindings"
+ "bk"
+ "blitCommandEncoderWithDescriptor:"
+ "blitPassDescriptor"
+ "block_sort"
+ "block_sort_nc"
+ "bm"
+ "bn"
+ "bool"
+ "bool_"
+ "borderColor"
+ "boundingBoxBuffer"
+ "boundingBoxBufferOffset"
+ "boundingBoxBuffers"
+ "boundingBoxCount"
+ "boundingBoxStride"
+ "broadcast"
+ "buffer"
+ "bufferAlignment"
+ "bufferBytesPerRow"
+ "bufferDataSize"
+ "bufferDataType"
+ "bufferIndex"
+ "bufferOffset"
+ "bufferPointerType"
+ "bufferSize"
+ "bufferStructType"
+ "buffers"
+ "builtInPlugInsPath"
+ "builtInPlugInsURL"
+ "bundleIdentifier"
+ "bundleURL"
+ "bundleWithURL:"
+ "bundleadjustgc_Huber_FocalLengthPriorConstraintName_foOffsets_arg0"
+ "bundleadjustgc_Huber_FocalLengthPriorConstraintName_sortedToStandard_arg0"
+ "bundleadjustgc_Huber_FocalLengthPriorConstraintName_standardToSorted_arg0"
+ "bundleadjustgc_Huber_FocalLengthVariable_buffer"
+ "bundleadjustgc_Huber_FocalLengthVariable_buffer_cloned"
+ "bundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName_foOffsets_arg0"
+ "bundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName_sortedToStandard_arg0"
+ "bundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName_standardToSorted_arg0"
+ "bundleadjustgc_Huber_HeikkilaDistortionVariable_buffer"
+ "bundleadjustgc_Huber_HeikkilaDistortionVariable_buffer_cloned"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_foOffsets_arg0"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_foOffsets_arg1"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_foOffsets_arg4"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_foOffsets_arg5"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_sortedToStandard_arg0"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_sortedToStandard_arg1"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_sortedToStandard_arg4"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_sortedToStandard_arg5"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_standardToSorted_arg0"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_standardToSorted_arg1"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_standardToSorted_arg4"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_standardToSorted_arg5"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_foOffsets_arg0"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_foOffsets_arg1"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_foOffsets_arg3"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_foOffsets_arg4"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_foOffsets_arg5"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_sortedToStandard_arg0"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_sortedToStandard_arg1"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_sortedToStandard_arg3"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_sortedToStandard_arg4"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_sortedToStandard_arg5"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_standardToSorted_arg0"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_standardToSorted_arg1"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_standardToSorted_arg3"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_standardToSorted_arg4"
+ "bundleadjustgc_Huber_HeikkilaReprojectionConstraint_standardToSorted_arg5"
+ "bundleadjustgc_Huber_KannalaDistortionPriorConstraintName_foOffsets_arg0"
+ "bundleadjustgc_Huber_KannalaDistortionPriorConstraintName_sortedToStandard_arg0"
+ "bundleadjustgc_Huber_KannalaDistortionPriorConstraintName_standardToSorted_arg0"
+ "bundleadjustgc_Huber_KannalaDistortionVariable_buffer"
+ "bundleadjustgc_Huber_KannalaDistortionVariable_buffer_cloned"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_foOffsets_arg0"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_foOffsets_arg1"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_foOffsets_arg4"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_foOffsets_arg5"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_sortedToStandard_arg0"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_sortedToStandard_arg1"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_sortedToStandard_arg4"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_sortedToStandard_arg5"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_standardToSorted_arg0"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_standardToSorted_arg1"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_standardToSorted_arg4"
+ "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_standardToSorted_arg5"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_foOffsets_arg0"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_foOffsets_arg1"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_foOffsets_arg3"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_foOffsets_arg4"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_foOffsets_arg5"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_sortedToStandard_arg0"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_sortedToStandard_arg1"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_sortedToStandard_arg3"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_sortedToStandard_arg4"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_sortedToStandard_arg5"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_standardToSorted_arg0"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_standardToSorted_arg1"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_standardToSorted_arg3"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_standardToSorted_arg4"
+ "bundleadjustgc_Huber_KannalaReprojectionConstraint_standardToSorted_arg5"
+ "bundleadjustgc_Huber_PixelConstant_buffer"
+ "bundleadjustgc_Huber_PointConstant_buffer"
+ "bundleadjustgc_Huber_PointVariable_buffer"
+ "bundleadjustgc_Huber_PrincipalPointPriorConstraintName_foOffsets_arg0"
+ "bundleadjustgc_Huber_PrincipalPointPriorConstraintName_sortedToStandard_arg0"
+ "bundleadjustgc_Huber_PrincipalPointPriorConstraintName_standardToSorted_arg0"
+ "bundleadjustgc_Huber_PrincipalPointVariable_buffer"
+ "bundleadjustgc_Huber_PrincipalPointVariable_buffer_cloned"
+ "bundleadjustgc_Huber_RotationVariable_buffer"
+ "bundleadjustgc_Huber_RotationVariablerectify_1979711527383196702"
+ "bundleadjustgc_Huber_RotationVariablerectify_1979711527383196702_func"
+ "bundleadjustgc_Huber_TranslationVariable_buffer"
+ "bundleadjustgc_L2_FocalLengthPriorConstraintName_foOffsets_arg0"
+ "bundleadjustgc_L2_FocalLengthPriorConstraintName_sortedToStandard_arg0"
+ "bundleadjustgc_L2_FocalLengthPriorConstraintName_standardToSorted_arg0"
+ "bundleadjustgc_L2_FocalLengthVariable_buffer"
+ "bundleadjustgc_L2_FocalLengthVariable_buffer_cloned"
+ "bundleadjustgc_L2_HeikkilaDistortionPriorConstraintName_foOffsets_arg0"
+ "bundleadjustgc_L2_HeikkilaDistortionPriorConstraintName_sortedToStandard_arg0"
+ "bundleadjustgc_L2_HeikkilaDistortionPriorConstraintName_standardToSorted_arg0"
+ "bundleadjustgc_L2_HeikkilaDistortionVariable_buffer"
+ "bundleadjustgc_L2_HeikkilaDistortionVariable_buffer_cloned"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_foOffsets_arg0"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_foOffsets_arg1"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_foOffsets_arg4"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_foOffsets_arg5"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_sortedToStandard_arg0"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_sortedToStandard_arg1"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_sortedToStandard_arg4"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_sortedToStandard_arg5"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_standardToSorted_arg0"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_standardToSorted_arg1"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_standardToSorted_arg4"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_standardToSorted_arg5"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_foOffsets_arg0"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_foOffsets_arg1"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_foOffsets_arg3"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_foOffsets_arg4"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_foOffsets_arg5"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_sortedToStandard_arg0"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_sortedToStandard_arg1"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_sortedToStandard_arg3"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_sortedToStandard_arg4"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_sortedToStandard_arg5"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_standardToSorted_arg0"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_standardToSorted_arg1"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_standardToSorted_arg3"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_standardToSorted_arg4"
+ "bundleadjustgc_L2_HeikkilaReprojectionConstraint_standardToSorted_arg5"
+ "bundleadjustgc_L2_KannalaDistortionPriorConstraintName_foOffsets_arg0"
+ "bundleadjustgc_L2_KannalaDistortionPriorConstraintName_sortedToStandard_arg0"
+ "bundleadjustgc_L2_KannalaDistortionPriorConstraintName_standardToSorted_arg0"
+ "bundleadjustgc_L2_KannalaDistortionVariable_buffer"
+ "bundleadjustgc_L2_KannalaDistortionVariable_buffer_cloned"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_foOffsets_arg0"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_foOffsets_arg1"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_foOffsets_arg4"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_foOffsets_arg5"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_sortedToStandard_arg0"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_sortedToStandard_arg1"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_sortedToStandard_arg4"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_sortedToStandard_arg5"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_standardToSorted_arg0"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_standardToSorted_arg1"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_standardToSorted_arg4"
+ "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_standardToSorted_arg5"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_foOffsets_arg0"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_foOffsets_arg1"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_foOffsets_arg3"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_foOffsets_arg4"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_foOffsets_arg5"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_sortedToStandard_arg0"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_sortedToStandard_arg1"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_sortedToStandard_arg3"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_sortedToStandard_arg4"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_sortedToStandard_arg5"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_standardToSorted_arg0"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_standardToSorted_arg1"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_standardToSorted_arg3"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_standardToSorted_arg4"
+ "bundleadjustgc_L2_KannalaReprojectionConstraint_standardToSorted_arg5"
+ "bundleadjustgc_L2_PixelConstant_buffer"
+ "bundleadjustgc_L2_PointConstant_buffer"
+ "bundleadjustgc_L2_PointVariable_buffer"
+ "bundleadjustgc_L2_PrincipalPointPriorConstraintName_foOffsets_arg0"
+ "bundleadjustgc_L2_PrincipalPointPriorConstraintName_sortedToStandard_arg0"
+ "bundleadjustgc_L2_PrincipalPointPriorConstraintName_standardToSorted_arg0"
+ "bundleadjustgc_L2_PrincipalPointVariable_buffer"
+ "bundleadjustgc_L2_PrincipalPointVariable_buffer_cloned"
+ "bundleadjustgc_L2_RotationVariable_buffer"
+ "bundleadjustgc_L2_RotationVariablerectify_1979711527383196702"
+ "bundleadjustgc_L2_RotationVariablerectify_1979711527383196702_func"
+ "bundleadjustgc_L2_TranslationVariable_buffer"
+ "c"
+ "cStringUsingEncoding:"
+ "c_"
+ "cannot switch from automatic to manual argument indexing"
+ "cannot switch from manual to automatic argument indexing"
+ "captureObject"
+ "carg_"
+ "caseInsensitiveCompare:"
+ "charValue"
+ "characterAtIndex:"
+ "clearBarrier"
+ "clearColor"
+ "clearDepth"
+ "clearStencil"
+ "col_reduce_2pass"
+ "col_reduce_longcolumn"
+ "col_reduce_looped"
+ "col_reduce_small"
+ "color_diffuse"
+ "color_rest"
+ "column"
+ "com.apple.corepg.MVSOutOfCoreConsole.DPSMVS"
+ "com.apple.corepg.semseg"
+ "com.gcd.task4"
+ "commandBufferWithUnretainedReferences"
+ "commandQueue"
+ "commandTypes"
+ "compare:"
+ "compareFunction"
+ "compileSymbolVisibility"
+ "complex64"
+ "complex64_t"
+ "compressionType"
+ "computeCommandEncoderWithDescriptor:"
+ "computeCommandEncoderWithDispatchType:"
+ "computeFunction"
+ "computePassDescriptor"
+ "concurrentDispatchThreadgroups:threadsPerThreadgroup:"
+ "concurrentDispatchThreads:threadsPerThreadgroup:"
+ "const device {0} *idx{1} [[buffer({2})]],"
+ "constantBlockAlignment"
+ "constantDataAtIndex:"
+ "constantValues"
+ "containsAllocation:"
+ "contig_"
+ "contiguous_scan"
+ "controlDependencies"
+ "controlPointBuffer"
+ "controlPointBufferOffset"
+ "controlPointBuffers"
+ "controlPointCount"
+ "controlPointFormat"
+ "controlPointStride"
+ "convertSparsePixelRegions:toTileRegions:withTileSize:alignmentMode:numRegions:"
+ "convertSparseTileRegions:toPixelRegions:withTileSize:numRegions:"
+ "convertToUSDZ:writeToURL:"
+ "copy"
+ "copyAccelerationStructure:toAccelerationStructure:"
+ "copyFromBuffer:sourceOffset:sourceBytesPerRow:sourceBytesPerImage:sourceSize:toTexture:destinationSlice:destinationLevel:destinationOrigin:options:"
+ "copyFromBuffer:sourceOffset:toBuffer:destinationOffset:size:"
+ "copyFromTexture:sourceSlice:sourceLevel:sourceOrigin:sourceSize:toBuffer:destinationOffset:destinationBytesPerRow:destinationBytesPerImage:"
+ "copyFromTexture:sourceSlice:sourceLevel:sourceOrigin:sourceSize:toBuffer:destinationOffset:destinationBytesPerRow:destinationBytesPerImage:options:"
+ "copyFromTexture:sourceSlice:sourceLevel:sourceOrigin:sourceSize:toTexture:destinationSlice:destinationLevel:destinationOrigin:"
+ "copyFromTexture:sourceSlice:sourceLevel:toTexture:destinationSlice:destinationLevel:sliceCount:levelCount:"
+ "copyIndirectCommandBuffer:sourceRange:destination:destinationIndex:"
+ "copyParameterDataToBuffer:offset:"
+ "copyStatusToBuffer:offset:"
+ "copy_g_nd1"
+ "copy_g_nd2"
+ "copy_g_nd3"
+ "copy_gg_nd1"
+ "copy_gg_nd2"
+ "copy_gg_nd3"
+ "counterSet"
+ "counterSets"
+ "counters"
+ "cpuCacheMode"
+ "currentAllocatedSize"
+ "curveBasis"
+ "curveEndCaps"
+ "curveType"
+ "data"
+ "dataSize"
+ "dataType"
+ "dataWithBytes:length:"
+ "dateWithTimeIntervalSinceNow:"
+ "dealloc"
+ "debugDescription"
+ "debugSignposts"
+ "def Xform \""
+ "defaultCaptureScope"
+ "defaultCenter"
+ "defaultRasterSampleCount"
+ "depth"
+ "depthAttachmentPixelFormat"
+ "depthCompareFunction"
+ "depthFailureOperation"
+ "depthPlane"
+ "depthResolveFilter"
+ "depthStencilPassOperation"
+ "depth_plane_aux_bdkhw"
+ "depth_plane_ref_bd1hw"
+ "description"
+ "descriptionWithLocale:"
+ "destination"
+ "destinationAlphaBlendFactor"
+ "destinationRGBBlendFactor"
+ "detail"
+ "device"
+ "dictionary"
+ "dictionaryWithObject:forKey:"
+ "didModifyRange:"
+ "dimensions. However, got arrays with dimensions "
+ "disableAutomaticTermination:"
+ "disableSuddenTermination"
+ "dispatchQueue"
+ "dispatchThreadgroupsWithIndirectBuffer:indirectBufferOffset:threadsPerThreadgroup:"
+ "dispatchThreadsPerTile:"
+ "dispatchType"
+ "domain"
+ "doubleValue"
+ "drain"
+ "drawIndexedPatches:patchIndexBuffer:patchIndexBufferOffset:controlPointIndexBuffer:controlPointIndexBufferOffset:indirectBuffer:indirectBufferOffset:"
+ "drawIndexedPatches:patchStart:patchCount:patchIndexBuffer:patchIndexBufferOffset:controlPointIndexBuffer:controlPointIndexBufferOffset:instanceCount:baseInstance:"
+ "drawIndexedPatches:patchStart:patchCount:patchIndexBuffer:patchIndexBufferOffset:controlPointIndexBuffer:controlPointIndexBufferOffset:instanceCount:baseInstance:tessellationFactorBuffer:tessellationFactorBufferOffset:tessellationFactorBufferInstanceStride:"
+ "drawIndexedPrimitives:indexCount:indexType:indexBuffer:indexBufferOffset:instanceCount:"
+ "drawIndexedPrimitives:indexCount:indexType:indexBuffer:indexBufferOffset:instanceCount:baseVertex:baseInstance:"
+ "drawIndexedPrimitives:indexType:indexBuffer:indexBufferOffset:indirectBuffer:indirectBufferOffset:"
+ "drawMeshThreadgroups:threadsPerObjectThreadgroup:threadsPerMeshThreadgroup:"
+ "drawMeshThreadgroupsWithIndirectBuffer:indirectBufferOffset:threadsPerObjectThreadgroup:threadsPerMeshThreadgroup:"
+ "drawMeshThreads:threadsPerObjectThreadgroup:threadsPerMeshThreadgroup:"
+ "drawPatches:patchIndexBuffer:patchIndexBufferOffset:indirectBuffer:indirectBufferOffset:"
+ "drawPatches:patchStart:patchCount:patchIndexBuffer:patchIndexBufferOffset:instanceCount:baseInstance:"
+ "drawPatches:patchStart:patchCount:patchIndexBuffer:patchIndexBufferOffset:instanceCount:baseInstance:tessellationFactorBuffer:tessellationFactorBufferOffset:tessellationFactorBufferInstanceStride:"
+ "drawPrimitives:indirectBuffer:indirectBufferOffset:"
+ "drawPrimitives:vertexStart:vertexCount:"
+ "drawPrimitives:vertexStart:vertexCount:instanceCount:"
+ "drawPrimitives:vertexStart:vertexCount:instanceCount:baseInstance:"
+ "drawableID"
+ "element color_space 1\n"
+ "element extrinsic 16 \n"
+ "element image_size 2\n"
+ "element intrinsic 9\n"
+ "element sorting_mode 1\n"
+ "element splatting_mode 1\n"
+ "element version 3\n"
+ "elementArrayType"
+ "elementIsArgumentBuffer"
+ "elementPointerType"
+ "elementStructType"
+ "elementTextureReferenceType"
+ "elementType"
+ "enableAutomaticTermination:"
+ "enableLogging"
+ "enableSuddenTermination"
+ "encodeSignalEvent:value:"
+ "encodeWaitForEvent:value:"
+ "encodedLength"
+ "encoderLabel"
+ "endActivity:"
+ "endOfEncoderSampleIndex"
+ "endOfFragmentSampleIndex"
+ "endOfVertexSampleIndex"
+ "endResidency"
+ "enqueue"
+ "enqueueBarrier"
+ "environment"
+ "errorOptions"
+ "errorState"
+ "errorWithDomain:code:userInfo:"
+ "eval_images"
+ "except for the concatenation axis. However, the provided shapes are "
+ "exclusive_"
+ "executablePath"
+ "executableURL"
+ "executeCommandsInBuffer:indirectBuffer:indirectBufferOffset:"
+ "executeCommandsInBuffer:withRange:"
+ "f:original_id_before_split"
+ "f_rest_"
+ "fastMathEnabled"
+ "fileExistsAtPath:"
+ "fileSize"
+ "fileSystemRepresentation"
+ "fillBuffer:range:value:"
+ "finalization.ply"
+ "firstMipmapInTail"
+ "float16"
+ "float16_t"
+ "float32"
+ "for "
+ "format"
+ "format specifier requires numeric argument"
+ "fragmentAdditionalBinaryFunctions"
+ "fragmentArguments"
+ "fragmentBindings"
+ "fragmentBuffers"
+ "fragmentFunction"
+ "fragmentLinkedFunctions"
+ "fragmentPreloadedLibraries"
+ "frontFaceStencil"
+ "fullUserName"
+ "function"
+ "functionConstantsDictionary"
+ "functionCount"
+ "functionDescriptor"
+ "functionGraphs"
+ "functionHandleWithFunction:"
+ "functionHandleWithFunction:stage:"
+ "functionType"
+ "functions"
+ "g1"
+ "g1_"
+ "g2_"
+ "g2large"
+ "g2large_"
+ "g3_"
+ "g3large"
+ "g3large_"
+ "gather{0}{1}_{2}_{3}_{4}"
+ "gaussian3d is empty without any features"
+ "gaussian_splatting.ply"
+ "gemv_"
+ "gemv_t_"
+ "geometryDescriptors"
+ "getBytes:bytesPerRow:bytesPerImage:fromRegion:mipmapLevel:slice:"
+ "getDefaultSamplePositions:count:"
+ "getSamplePositions:count:"
+ "getTextureAccessCounters:region:mipLevel:slice:resetCounters:countersBuffer:countersBufferOffset:"
+ "getValue:size:"
+ "gg"
+ "gg1_"
+ "gg2_"
+ "gg2large_"
+ "gg3_"
+ "gg3large_"
+ "ggn2_"
+ "ggn4large_"
+ "globallyUniqueString"
+ "gn"
+ "gn1_"
+ "gn2_"
+ "gn4large_"
+ "gpuAddress"
+ "gpuResourceID"
+ "groups"
+ "hasPerformanceProfile:"
+ "hasUnifiedMemory"
+ "hash"
+ "hazardTrackingMode"
+ "heap"
+ "heapAccelerationStructureSizeAndAlignWithDescriptor:"
+ "heapAccelerationStructureSizeAndAlignWithSize:"
+ "heapBufferSizeAndAlignWithLength:options:"
+ "heapOffset"
+ "heapTextureSizeAndAlignWithDescriptor:"
+ "horizontal"
+ "horizontalSampleStorage"
+ "hostName"
+ "idujyxxwsi"
+ "idx{0}"
+ "imageblockMemoryLengthForDimensions:"
+ "imageblockSampleLength"
+ "images_small"
+ "implicit_gemm_conv_2d_"
+ "implicit_gemm_conv_2d_general_"
+ "inclusive_"
+ "index"
+ "indexBuffer"
+ "indexBufferIndex"
+ "indexBufferOffset"
+ "indexType"
+ "indirectComputeCommandAtIndex:"
+ "indirectRenderCommandAtIndex:"
+ "inf"
+ "infoDictionary"
+ "inheritBuffers"
+ "inheritPipelineState"
+ "init"
+ "initWithArgumentIndex:"
+ "initWithBool:"
+ "initWithBytes:objCType:"
+ "initWithBytesNoCopy:length:encoding:freeWhenDone:"
+ "initWithCString:encoding:"
+ "initWithChar:"
+ "initWithCoder:"
+ "initWithConfiguration:"
+ "initWithDispatchQueue:"
+ "initWithDomain:code:userInfo:"
+ "initWithDouble:"
+ "initWithFloat:"
+ "initWithFunctionName:nodes:outputNode:attributes:"
+ "initWithInt:"
+ "initWithLong:"
+ "initWithLongLong:"
+ "initWithName:arguments:controlDependencies:"
+ "initWithObjects:count:"
+ "initWithObjects:forKeys:count:"
+ "initWithPath:"
+ "initWithSampleCount:"
+ "initWithSampleCount:horizontal:vertical:"
+ "initWithShort:"
+ "initWithString:"
+ "initWithURL:"
+ "initWithUnsignedChar:"
+ "initWithUnsignedInt:"
+ "initWithUnsignedLong:"
+ "initWithUnsignedLongLong:"
+ "initWithUnsignedShort:"
+ "initWithVersion:"
+ "init_reduce"
+ "initialCapacity"
+ "inner_rectangle"
+ "inputPrimitiveTopology"
+ "insertDebugCaptureBoundary"
+ "insertDebugSignpost:"
+ "insertLibraries"
+ "installName"
+ "instanceCount"
+ "instanceCountBuffer"
+ "instanceCountBufferOffset"
+ "instanceDescriptorBuffer"
+ "instanceDescriptorBufferOffset"
+ "instanceDescriptorStride"
+ "instanceDescriptorType"
+ "instanceTransformationMatrixLayout"
+ "instancedAccelerationStructures"
+ "int16"
+ "int32"
+ "int64"
+ "int8"
+ "intValue"
+ "integerValue"
+ "intersectionFunctionTableDescriptor"
+ "intersectionFunctionTableOffset"
+ "invalid fill character '{'"
+ "invalid format specifier"
+ "invalid format specifier for char"
+ "invalid format string"
+ "invalid precision"
+ "iosurface"
+ "iosurfacePlane"
+ "isActive"
+ "isAliasable"
+ "isAlphaToCoverageEnabled"
+ "isAlphaToOneEnabled"
+ "isArgument"
+ "isBlendingEnabled"
+ "isCapturing"
+ "isDepth24Stencil8PixelFormatSupported"
+ "isDepthTexture"
+ "isDepthWriteEnabled"
+ "isDeviceCertifiedFor:"
+ "isEqual:"
+ "isEqualToNumber:"
+ "isEqualToString:"
+ "isEqualToValue:"
+ "isFramebufferOnly"
+ "isHeadless"
+ "isLoaded"
+ "isLowPower"
+ "isLowPowerModeEnabled"
+ "isMacCatalystApp"
+ "isOperatingSystemAtLeastVersion:"
+ "isPatchControlPointData"
+ "isPatchData"
+ "isRasterizationEnabled"
+ "isRemovable"
+ "isShareable"
+ "isSparse"
+ "isTessellationFactorScaleEnabled"
+ "isUsed"
+ "isiOSAppOnMac"
+ "item can only be called on arrays of size 1."
+ "item() const can only be called on evaled arrays"
+ "itype"
+ "k_aligned"
+ "kernelEndTime"
+ "kernelStartTime"
+ "keyEnumerator"
+ "label"
+ "languageVersion"
+ "large"
+ "layerAtIndex:"
+ "layerCount"
+ "layers"
+ "layouts"
+ "lengthOfBytesUsingEncoding:"
+ "level"
+ "libraries"
+ "libraryType"
+ "line"
+ "linkedFunctions"
+ "lmvs_point_cloud.bin"
+ "loadAction"
+ "loadAndReturnError:"
+ "loadBuffer:offset:size:sourceHandle:sourceHandleOffset:"
+ "loadBytes:size:sourceHandle:sourceHandleOffset:"
+ "loadTexture:slice:level:size:sourceBytesPerRow:sourceBytesPerImage:destinationOrigin:sourceHandle:sourceHandleOffset:"
+ "localizedFailureReason"
+ "localizedInfoDictionary"
+ "localizedRecoveryOptions"
+ "localizedRecoverySuggestion"
+ "localizedStringForKey:value:table:"
+ "location"
+ "locationNumber"
+ "lock"
+ "lodAverage"
+ "lodMaxClamp"
+ "lodMinClamp"
+ "logState"
+ "longValue"
+ "magFilter"
+ "mainBundle"
+ "makeAliasable"
+ "mapPhysicalToScreenCoordinates:forLayer:"
+ "mapScreenToPhysicalCoordinates:forLayer:"
+ "maskForSemanticCategory:"
+ "mask_bdkhw"
+ "mathFloatingPointFunctions"
+ "mathMode"
+ "max"
+ "maxAnisotropy"
+ "maxArgumentBufferSamplerCount"
+ "maxAvailableSizeWithAlignment:"
+ "maxBufferLength"
+ "maxCallStackDepth"
+ "maxCommandBufferCount"
+ "maxCommandsInFlight"
+ "maxFragmentBufferBindCount"
+ "maxFragmentCallStackDepth"
+ "maxInstanceCount"
+ "maxKernelBufferBindCount"
+ "maxKernelThreadgroupMemoryBindCount"
+ "maxMeshBufferBindCount"
+ "maxMotionTransformCount"
+ "maxObjectBufferBindCount"
+ "maxObjectThreadgroupMemoryBindCount"
+ "maxSampleCount"
+ "maxTessellationFactor"
+ "maxThreadgroupMemoryLength"
+ "maxThreadsPerThreadgroup"
+ "maxTotalThreadgroupsPerMeshGrid"
+ "maxTotalThreadsPerMeshThreadgroup"
+ "maxTotalThreadsPerObjectThreadgroup"
+ "maxTransferRate"
+ "maxVertexAmplificationCount"
+ "maxVertexBufferBindCount"
+ "maxVertexCallStackDepth"
+ "max_buffer_length"
+ "max_recommended_working_set_size"
+ "maximumConcurrentCompilationTaskCount"
+ "maximumLengthOfBytesUsingEncoding:"
+ "mb_block_merge"
+ "mb_block_partition"
+ "mb_block_sort"
+ "memberByName:"
+ "members"
+ "memoryBarrierWithResources:count:"
+ "memoryBarrierWithResources:count:afterStages:beforeStages:"
+ "memoryBarrierWithScope:"
+ "memoryBarrierWithScope:afterStages:beforeStages:"
+ "memory_size"
+ "merge_"
+ "merge_mbsort_"
+ "meshBindings"
+ "meshBuffers"
+ "meshFunction"
+ "meshLinkedFunctions"
+ "meshThreadExecutionWidth"
+ "meshThreadgroupSizeIsMultipleOfThreadExecutionWidth"
+ "methodSignatureForSelector:"
+ "min"
+ "minFilter"
+ "minimumTextureBufferAlignmentForPixelFormat:"
+ "mipFilter"
+ "missing '}' in format string"
+ "mn_aligned"
+ "model.mil"
+ "motionEndBorderMode"
+ "motionEndTime"
+ "motionKeyframeCount"
+ "motionStartBorderMode"
+ "motionStartTime"
+ "motionTransformBuffer"
+ "motionTransformBufferOffset"
+ "motionTransformCount"
+ "motionTransformCountBuffer"
+ "motionTransformCountBufferOffset"
+ "motionTransformStride"
+ "motionTransformType"
+ "moveTextureMappingsFromTexture:sourceSlice:sourceLevel:sourceOrigin:sourceSize:toTexture:destinationSlice:destinationLevel:destinationOrigin:"
+ "mutability"
+ "mutableBytes"
+ "n"
+ "n_channels"
+ "naive_unfold_nd_"
+ "naive_unfold_transpose_nd_"
+ "nan"
+ "nc"
+ "negative precision"
+ "negative width"
+ "newAccelerationStructureWithDescriptor:"
+ "newAccelerationStructureWithDescriptor:offset:"
+ "newAccelerationStructureWithSize:offset:"
+ "newArgumentEncoderForBufferAtIndex:"
+ "newArgumentEncoderWithArguments:"
+ "newArgumentEncoderWithBufferBinding:"
+ "newArgumentEncoderWithBufferIndex:"
+ "newArgumentEncoderWithBufferIndex:reflection:"
+ "newBinaryArchiveWithDescriptor:error:"
+ "newBufferWithLength:options:offset:"
+ "newCaptureScopeWithCommandQueue:"
+ "newCommandQueueWithDescriptor:"
+ "newCommandQueueWithMaxCommandBufferCount:"
+ "newComputePipelineStateWithAdditionalBinaryFunctions:error:"
+ "newComputePipelineStateWithDescriptor:options:completionHandler:"
+ "newComputePipelineStateWithFunction:completionHandler:"
+ "newComputePipelineStateWithFunction:options:completionHandler:"
+ "newComputePipelineStateWithFunction:options:reflection:error:"
+ "newCounterSampleBufferWithDescriptor:error:"
+ "newDefaultLibrary"
+ "newDefaultLibraryWithBundle:error:"
+ "newDynamicLibrary:error:"
+ "newDynamicLibraryWithURL:error:"
+ "newEvent"
+ "newFence"
+ "newFunctionWithDescriptor:completionHandler:"
+ "newFunctionWithDescriptor:error:"
+ "newFunctionWithName:constantValues:completionHandler:"
+ "newFunctionWithName:constantValues:error:"
+ "newHeapWithDescriptor:"
+ "newIOCommandQueueWithDescriptor:error:"
+ "newIOFileHandleWithURL:compressionMethod:error:"
+ "newIOFileHandleWithURL:error:"
+ "newIOHandleWithURL:compressionMethod:error:"
+ "newIOHandleWithURL:error:"
+ "newIndirectCommandBufferWithDescriptor:maxCommandCount:options:"
+ "newIntersectionFunctionTableWithDescriptor:"
+ "newIntersectionFunctionTableWithDescriptor:stage:"
+ "newIntersectionFunctionWithDescriptor:completionHandler:"
+ "newIntersectionFunctionWithDescriptor:error:"
+ "newLibraryWithData:error:"
+ "newLibraryWithFile:error:"
+ "newLibraryWithSource:options:completionHandler:"
+ "newLibraryWithStitchedDescriptor:completionHandler:"
+ "newLibraryWithStitchedDescriptor:error:"
+ "newLogStateWithDescriptor:error:"
+ "newRasterizationRateMapWithDescriptor:"
+ "newRemoteBufferViewForDevice:"
+ "newRemoteTextureViewForDevice:"
+ "newRenderPipelineStateWithAdditionalBinaryFunctions:error:"
+ "newRenderPipelineStateWithDescriptor:completionHandler:"
+ "newRenderPipelineStateWithDescriptor:options:completionHandler:"
+ "newRenderPipelineStateWithDescriptor:options:reflection:error:"
+ "newRenderPipelineStateWithMeshDescriptor:options:completionHandler:"
+ "newRenderPipelineStateWithMeshDescriptor:options:reflection:error:"
+ "newRenderPipelineStateWithTileDescriptor:options:completionHandler:"
+ "newRenderPipelineStateWithTileDescriptor:options:reflection:error:"
+ "newResidencySetWithDescriptor:error:"
+ "newSamplerStateWithDescriptor:"
+ "newScratchBufferWithMinimumSize:"
+ "newSharedEvent"
+ "newSharedEventHandle"
+ "newSharedEventWithHandle:"
+ "newSharedTextureHandle"
+ "newSharedTextureWithDescriptor:"
+ "newSharedTextureWithHandle:"
+ "newTextureViewWithPixelFormat:"
+ "newTextureViewWithPixelFormat:textureType:levels:slices:swizzle:"
+ "newTextureWithDescriptor:offset:"
+ "newVisibleFunctionTableWithDescriptor:"
+ "newVisibleFunctionTableWithDescriptor:stage:"
+ "nextObject"
+ "nodes"
+ "normalizedCoordinates"
+ "notifyListener:atValue:block:"
+ "number is too big"
+ "numberWithBool:"
+ "numberWithChar:"
+ "numberWithDouble:"
+ "numberWithInt:"
+ "numberWithLong:"
+ "numberWithLongLong:"
+ "numberWithShort:"
+ "numberWithUnsignedChar:"
+ "numberWithUnsignedInt:"
+ "numberWithUnsignedLong:"
+ "numberWithUnsignedLongLong:"
+ "numberWithUnsignedShort:"
+ "obj loading failed with invalid mesh attributes"
+ "objCType"
+ "objectBindings"
+ "objectBuffers"
+ "objectEnumerator"
+ "objectForInfoDictionaryKey:"
+ "objectFunction"
+ "objectLinkedFunctions"
+ "objectPayloadAlignment"
+ "objectPayloadDataSize"
+ "objectThreadExecutionWidth"
+ "objectThreadgroupSizeIsMultipleOfThreadExecutionWidth"
+ "offset"
+ "opacity"
+ "opaque"
+ "operatingSystem"
+ "operatingSystemVersion"
+ "operatingSystemVersionString"
+ "optimizationLevel"
+ "optimizeContentsForCPUAccess:"
+ "optimizeContentsForCPUAccess:slice:level:"
+ "optimizeContentsForGPUAccess:slice:level:"
+ "optimizeIndirectCommandBuffer:withRange:"
+ "options"
+ "or"
+ "otype"
+ "outputNode"
+ "outputURL"
+ "parallelRenderCommandEncoderWithDescriptor:"
+ "parameterBufferSizeAndAlign"
+ "parentRelativeLevel"
+ "parentRelativeSlice"
+ "parentTexture"
+ "partition_"
+ "partition_mbsort_"
+ "patchControlPointCount"
+ "patchType"
+ "pathForAuxiliaryExecutable:"
+ "payloadMemoryLength"
+ "peerCount"
+ "peerGroupID"
+ "peerIndex"
+ "performActivityWithOptions:reason:usingBlock:"
+ "performExpiringActivityWithReason:usingBlock:"
+ "physicalGranularity"
+ "physicalSizeForLayer:"
+ "pmvs_point_cloud.bin"
+ "pointerType"
+ "pointerValue"
+ "popDebugGroup"
+ "pose_RT_measure_bkhw"
+ "pose_R_measure_bkhw"
+ "pose_T_measure_bkhw"
+ "precision is not integer"
+ "preflightAndReturnError:"
+ "preloadedLibraries"
+ "prepareWithError:"
+ "preprocessorMacros"
+ "present"
+ "presentAfterMinimumDuration:"
+ "presentAtTime:"
+ "presentDrawable:"
+ "presentDrawable:afterMinimumDuration:"
+ "presentDrawable:atTime:"
+ "presentedTime"
+ "preserveInvariance"
+ "primitiveDataBuffer"
+ "primitiveDataBufferOffset"
+ "primitiveDataElementSize"
+ "primitiveDataStride"
+ "priority"
+ "privateFrameworksPath"
+ "privateFrameworksURL"
+ "privateFunctions"
+ "processIdentifier"
+ "processName"
+ "processorCount"
+ "prod"
+ "property float"
+ "property float extrinsic\n"
+ "property float intrinsic\n"
+ "property uchar color_space\n"
+ "property uchar sorting_mode\n"
+ "property uchar splatting_mode\n"
+ "property uchar version\n"
+ "property uint image_size\n"
+ "pushDebugGroup:"
+ "rAddressMode"
+ "radiusBuffer"
+ "radiusBufferOffset"
+ "radiusBuffers"
+ "radiusFormat"
+ "radiusStride"
+ "rangeOfString:options:"
+ "rasterSampleCount"
+ "rasterizationRateMap"
+ "rasterizationRateMapDescriptorWithScreenSize:"
+ "rasterizationRateMapDescriptorWithScreenSize:layer:"
+ "rasterizationRateMapDescriptorWithScreenSize:layerCount:layers:"
+ "ray_angles_bdkhw"
+ "ray_vecs_bdchw"
+ "rbits"
+ "rbitsc"
+ "readMask"
+ "readWriteTextureSupport"
+ "recommendedMaxWorkingSetSize"
+ "refitAccelerationStructure:descriptor:destination:scratchBuffer:scratchBufferOffset:"
+ "refitAccelerationStructure:descriptor:destination:scratchBuffer:scratchBufferOffset:options:"
+ "registryID"
+ "release"
+ "remoteStorageBuffer"
+ "remoteStorageTexture"
+ "removeAllAllocations"
+ "removeAllDebugMarkers"
+ "removeAllocation:"
+ "removeAllocations:count:"
+ "removeObserver:"
+ "removeResidencySet:"
+ "removeResidencySets:count:"
+ "renderCommandEncoder"
+ "renderPassDescriptor"
+ "renderTargetArrayLength"
+ "renderTargetHeight"
+ "renderTargetWidth"
+ "render_appearance"
+ "requestResidency"
+ "required"
+ "reset"
+ "resetCommandsInBuffer:withRange:"
+ "resetTextureAccessCounters:region:mipLevel:slice:"
+ "resetWithRange:"
+ "resolveCounterRange:"
+ "resolveCounters:inRange:destinationBuffer:destinationOffset:"
+ "resolveDepthPlane"
+ "resolveLevel"
+ "resolveSlice"
+ "resolveTexture"
+ "resourceOptions"
+ "resourceStateCommandEncoder"
+ "resourceStateCommandEncoderWithDescriptor:"
+ "resourceStatePassDescriptor"
+ "resourceURL"
+ "respondsToSelector:"
+ "resultForPixelBuffer:orientation:error:"
+ "retain"
+ "retainCount"
+ "retainedReferences"
+ "reverse_"
+ "rgbBlendOperation"
+ "rgb_aux_bk3hw"
+ "rgb_ref_b3hw"
+ "rootResource"
+ "rot_0"
+ "rot_1"
+ "rot_2"
+ "rot_3"
+ "row_reduce_looped"
+ "row_reduce_simple"
+ "row_reduce_small"
+ "s2"
+ "sAddressMode"
+ "s_"
+ "sampleBuffer"
+ "sampleBufferAttachments"
+ "sampleCount"
+ "sampleCountersInBuffer:atSampleIndex:withBarrier:"
+ "sampleTimestamps:gpuTimestamp:"
+ "scale_0"
+ "scale_1"
+ "scale_2"
+ "scan_"
+ "scatter{0}{1}_{2}_{3}_{4}_nwork{5}_{6}"
+ "scratchBufferAllocator"
+ "screenSize"
+ "segmentControlPointCount"
+ "segmentCount"
+ "serializeToURL:error:"
+ "setAccelerationStructure:atIndex:"
+ "setAccess:"
+ "setAllowDuplicateIntersectionFunctionInvocation:"
+ "setAllowGPUOptimizedContents:"
+ "setAllowReferencingUndefinedSymbols:"
+ "setAlphaBlendOperation:"
+ "setAlphaToCoverageEnabled:"
+ "setAlphaToOneEnabled:"
+ "setArgumentBuffer:offset:"
+ "setArgumentBuffer:startOffset:arrayElement:"
+ "setArgumentIndex:"
+ "setArguments:"
+ "setAttributes:"
+ "setAutomaticTerminationSupportEnabled:"
+ "setBackFaceStencil:"
+ "setBarrier"
+ "setBinaryArchives:"
+ "setBinaryFunctions:"
+ "setBlendColorRed:green:blue:alpha:"
+ "setBlendingEnabled:"
+ "setBorderColor:"
+ "setBoundingBoxBuffer:"
+ "setBoundingBoxBufferOffset:"
+ "setBoundingBoxBuffers:"
+ "setBoundingBoxCount:"
+ "setBoundingBoxStride:"
+ "setBuffer:"
+ "setBuffer:offset:attributeStride:atIndex:"
+ "setBufferIndex:"
+ "setBufferOffset:atIndex:"
+ "setBufferOffset:attributeStride:atIndex:"
+ "setBufferSize:"
+ "setBuffers:offsets:attributeStrides:withRange:"
+ "setBuffers:offsets:withRange:"
+ "setBytes:length:attributeStride:atIndex:"
+ "setClearStencil:"
+ "setColorStoreAction:atIndex:"
+ "setColorStoreActionOptions:atIndex:"
+ "setCommandTypes:"
+ "setCompareFunction:"
+ "setCompileSymbolVisibility:"
+ "setCompressionType:"
+ "setComputePipelineState:atIndex:"
+ "setComputePipelineStates:withRange:"
+ "setConstantBlockAlignment:"
+ "setConstantValue:type:atIndex:"
+ "setConstantValue:type:withName:"
+ "setConstantValues:"
+ "setConstantValues:type:withRange:"
+ "setControlDependencies:"
+ "setControlPointBuffer:"
+ "setControlPointBufferOffset:"
+ "setControlPointBuffers:"
+ "setControlPointCount:"
+ "setControlPointFormat:"
+ "setControlPointStride:"
+ "setCounterSet:"
+ "setCpuCacheMode:"
+ "setCurveBasis:"
+ "setCurveEndCaps:"
+ "setCurveType:"
+ "setDataType:"
+ "setDefaultCaptureScope:"
+ "setDepth:"
+ "setDepthAttachment:"
+ "setDepthBias:slopeScale:clamp:"
+ "setDepthClipMode:"
+ "setDepthFailureOperation:"
+ "setDepthPlane:"
+ "setDepthResolveFilter:"
+ "setDepthStencilPassOperation:"
+ "setDepthStoreAction:"
+ "setDepthStoreActionOptions:"
+ "setDestination:"
+ "setDestinationAlphaBlendFactor:"
+ "setDestinationRGBBlendFactor:"
+ "setDispatchType:"
+ "setEnableLogging:"
+ "setEnableSegmentation:"
+ "setEndOfEncoderSampleIndex:"
+ "setEndOfFragmentSampleIndex:"
+ "setEndOfVertexSampleIndex:"
+ "setFastMathEnabled:"
+ "setFormat:"
+ "setFragmentAccelerationStructure:atBufferIndex:"
+ "setFragmentAdditionalBinaryFunctions:"
+ "setFragmentBufferOffset:atIndex:"
+ "setFragmentBuffers:offsets:withRange:"
+ "setFragmentIntersectionFunctionTable:atBufferIndex:"
+ "setFragmentIntersectionFunctionTables:withBufferRange:"
+ "setFragmentLinkedFunctions:"
+ "setFragmentPreloadedLibraries:"
+ "setFragmentSamplerState:atIndex:"
+ "setFragmentSamplerState:lodMinClamp:lodMaxClamp:atIndex:"
+ "setFragmentSamplerStates:lodMinClamps:lodMaxClamps:withRange:"
+ "setFragmentSamplerStates:withRange:"
+ "setFragmentTextures:withRange:"
+ "setFragmentVisibleFunctionTable:atBufferIndex:"
+ "setFragmentVisibleFunctionTables:withBufferRange:"
+ "setFrontFaceStencil:"
+ "setFunction:atIndex:"
+ "setFunctionCount:"
+ "setFunctionGraphs:"
+ "setFunctionName:"
+ "setFunctions:"
+ "setFunctions:withRange:"
+ "setGroups:"
+ "setHazardTrackingMode:"
+ "setImageblockSampleLength:"
+ "setImageblockWidth:height:"
+ "setIndex:"
+ "setIndexBufferIndex:"
+ "setIndirectCommandBuffer:atIndex:"
+ "setIndirectCommandBuffers:withRange:"
+ "setInheritBuffers:"
+ "setInheritPipelineState:"
+ "setInitialCapacity:"
+ "setInputPrimitiveTopology:"
+ "setInsertLibraries:"
+ "setInstallName:"
+ "setInstanceCount:"
+ "setInstanceCountBuffer:"
+ "setInstanceCountBufferOffset:"
+ "setInstanceDescriptorBuffer:"
+ "setInstanceDescriptorBufferOffset:"
+ "setInstanceDescriptorStride:"
+ "setInstanceDescriptorType:"
+ "setInstanceTransformationMatrixLayout:"
+ "setInstancedAccelerationStructures:"
+ "setIntersectionFunctionTable:atBufferIndex:"
+ "setIntersectionFunctionTable:atIndex:"
+ "setIntersectionFunctionTableOffset:"
+ "setIntersectionFunctionTables:withBufferRange:"
+ "setIntersectionFunctionTables:withRange:"
+ "setKernelBuffer:offset:atIndex:"
+ "setKernelBuffer:offset:attributeStride:atIndex:"
+ "setLayer:atIndex:"
+ "setLevel:"
+ "setLibraries:"
+ "setLibraryType:"
+ "setLinkedFunctions:"
+ "setLodAverage:"
+ "setLodMaxClamp:"
+ "setLodMinClamp:"
+ "setLogState:"
+ "setMagFilter:"
+ "setMathFloatingPointFunctions:"
+ "setMathMode:"
+ "setMaxAnisotropy:"
+ "setMaxCallStackDepth:"
+ "setMaxCommandBufferCount:"
+ "setMaxCommandsInFlight:"
+ "setMaxFragmentBufferBindCount:"
+ "setMaxFragmentCallStackDepth:"
+ "setMaxInstanceCount:"
+ "setMaxKernelBufferBindCount:"
+ "setMaxKernelThreadgroupMemoryBindCount:"
+ "setMaxMeshBufferBindCount:"
+ "setMaxMotionTransformCount:"
+ "setMaxObjectBufferBindCount:"
+ "setMaxObjectThreadgroupMemoryBindCount:"
+ "setMaxTessellationFactor:"
+ "setMaxTotalThreadgroupsPerMeshGrid:"
+ "setMaxTotalThreadsPerMeshThreadgroup:"
+ "setMaxTotalThreadsPerObjectThreadgroup:"
+ "setMaxTotalThreadsPerThreadgroup:"
+ "setMaxVertexAmplificationCount:"
+ "setMaxVertexBufferBindCount:"
+ "setMaxVertexCallStackDepth:"
+ "setMeshBuffer:offset:atIndex:"
+ "setMeshBufferOffset:atIndex:"
+ "setMeshBuffers:offsets:withRange:"
+ "setMeshBytes:length:atIndex:"
+ "setMeshFunction:"
+ "setMeshLinkedFunctions:"
+ "setMeshSamplerState:atIndex:"
+ "setMeshSamplerState:lodMinClamp:lodMaxClamp:atIndex:"
+ "setMeshSamplerStates:lodMinClamps:lodMaxClamps:withRange:"
+ "setMeshSamplerStates:withRange:"
+ "setMeshTexture:atIndex:"
+ "setMeshTextures:withRange:"
+ "setMeshThreadgroupSizeIsMultipleOfThreadExecutionWidth:"
+ "setMinFilter:"
+ "setMipFilter:"
+ "setMotionEndBorderMode:"
+ "setMotionEndTime:"
+ "setMotionKeyframeCount:"
+ "setMotionStartBorderMode:"
+ "setMotionStartTime:"
+ "setMotionTransformBuffer:"
+ "setMotionTransformBufferOffset:"
+ "setMotionTransformCount:"
+ "setMotionTransformCountBuffer:"
+ "setMotionTransformCountBufferOffset:"
+ "setMotionTransformStride:"
+ "setMotionTransformType:"
+ "setMutability:"
+ "setName:"
+ "setNetworkResolution:"
+ "setNodes:"
+ "setNormalizedCoordinates:"
+ "setObject:atIndexedSubscript:"
+ "setObjectBuffer:offset:atIndex:"
+ "setObjectBufferOffset:atIndex:"
+ "setObjectBuffers:offsets:withRange:"
+ "setObjectBytes:length:atIndex:"
+ "setObjectFunction:"
+ "setObjectLinkedFunctions:"
+ "setObjectSamplerState:atIndex:"
+ "setObjectSamplerState:lodMinClamp:lodMaxClamp:atIndex:"
+ "setObjectSamplerStates:lodMinClamps:lodMaxClamps:withRange:"
+ "setObjectSamplerStates:withRange:"
+ "setObjectTexture:atIndex:"
+ "setObjectTextures:withRange:"
+ "setObjectThreadgroupMemoryLength:atIndex:"
+ "setObjectThreadgroupSizeIsMultipleOfThreadExecutionWidth:"
+ "setOffset:"
+ "setOpaque:"
+ "setOpaqueCurveIntersectionFunctionWithSignature:atIndex:"
+ "setOpaqueCurveIntersectionFunctionWithSignature:withRange:"
+ "setOpaqueTriangleIntersectionFunctionWithSignature:atIndex:"
+ "setOpaqueTriangleIntersectionFunctionWithSignature:withRange:"
+ "setOptimizationLevel:"
+ "setOptions:"
+ "setOutputNode:"
+ "setOutputURL:"
+ "setOwnerWithIdentity:"
+ "setPayloadMemoryLength:"
+ "setPreloadedLibraries:"
+ "setPreserveInvariance:"
+ "setPrimitiveDataBuffer:"
+ "setPrimitiveDataBufferOffset:"
+ "setPrimitiveDataElementSize:"
+ "setPrimitiveDataStride:"
+ "setPriority:"
+ "setPrivateFunctions:"
+ "setProcessName:"
+ "setRAddressMode:"
+ "setRadiusBuffer:"
+ "setRadiusBufferOffset:"
+ "setRadiusBuffers:"
+ "setRadiusFormat:"
+ "setRadiusStride:"
+ "setRasterSampleCount:"
+ "setRasterizationEnabled:"
+ "setRasterizationRateMap:"
+ "setReadMask:"
+ "setRenderPipelineState:atIndex:"
+ "setRenderPipelineStates:withRange:"
+ "setResolveDepthPlane:"
+ "setResolveLevel:"
+ "setResolveSlice:"
+ "setResolveTexture:"
+ "setResourceOptions:"
+ "setRetainedReferences:"
+ "setRgbBlendOperation:"
+ "setRunningFrameRate:"
+ "setSAddressMode:"
+ "setSampleBuffer:"
+ "setSampleCount:"
+ "setSamplePositions:count:"
+ "setSamplerState:atIndex:"
+ "setSamplerState:lodMinClamp:lodMaxClamp:atIndex:"
+ "setSamplerStates:lodMinClamps:lodMaxClamps:withRange:"
+ "setSamplerStates:withRange:"
+ "setScissorRect:"
+ "setScissorRects:count:"
+ "setScratchBufferAllocator:"
+ "setScreenSize:"
+ "setSegmentControlPointCount:"
+ "setSegmentCount:"
+ "setShaderValidation:"
+ "setShouldMaximizeConcurrentCompilation:"
+ "setSignaledValue:"
+ "setSize:"
+ "setSlice:"
+ "setSourceAlphaBlendFactor:"
+ "setSourceRGBBlendFactor:"
+ "setSparsePageSize:"
+ "setSpecializedName:"
+ "setStageInRegion:"
+ "setStageInRegionWithIndirectBuffer:indirectBufferOffset:"
+ "setStageInputDescriptor:"
+ "setStartOfEncoderSampleIndex:"
+ "setStartOfFragmentSampleIndex:"
+ "setStartOfVertexSampleIndex:"
+ "setStencilAttachment:"
+ "setStencilAttachmentPixelFormat:"
+ "setStencilCompareFunction:"
+ "setStencilFailureOperation:"
+ "setStencilFrontReferenceValue:backReferenceValue:"
+ "setStencilReferenceValue:"
+ "setStencilResolveFilter:"
+ "setStencilStoreAction:"
+ "setStencilStoreActionOptions:"
+ "setStepFunction:"
+ "setStepRate:"
+ "setStoreActionOptions:"
+ "setStride:"
+ "setSupportAddingBinaryFunctions:"
+ "setSupportAddingFragmentBinaryFunctions:"
+ "setSupportAddingVertexBinaryFunctions:"
+ "setSupportArgumentBuffers:"
+ "setSupportDynamicAttributeStride:"
+ "setSupportIndirectCommandBuffers:"
+ "setSupportRayTracing:"
+ "setSwizzle:"
+ "setTAddressMode:"
+ "setTessellationControlPointIndexType:"
+ "setTessellationFactorBuffer:offset:instanceStride:"
+ "setTessellationFactorFormat:"
+ "setTessellationFactorScale:"
+ "setTessellationFactorScaleEnabled:"
+ "setTessellationFactorStepFunction:"
+ "setTessellationOutputWindingOrder:"
+ "setTessellationPartitionMode:"
+ "setTextures:withRange:"
+ "setThreadgroupMemoryLength:"
+ "setThreadgroupMemoryLength:atIndex:"
+ "setThreadgroupMemoryLength:offset:atIndex:"
+ "setThreadgroupSizeMatchesTileSize:"
+ "setTileAccelerationStructure:atBufferIndex:"
+ "setTileAdditionalBinaryFunctions:"
+ "setTileBuffer:offset:atIndex:"
+ "setTileBufferOffset:atIndex:"
+ "setTileBuffers:offsets:withRange:"
+ "setTileBytes:length:atIndex:"
+ "setTileFunction:"
+ "setTileHeight:"
+ "setTileIntersectionFunctionTable:atBufferIndex:"
+ "setTileIntersectionFunctionTables:withBufferRange:"
+ "setTileSamplerState:atIndex:"
+ "setTileSamplerState:lodMinClamp:lodMaxClamp:atIndex:"
+ "setTileSamplerStates:lodMinClamps:lodMaxClamps:withRange:"
+ "setTileSamplerStates:withRange:"
+ "setTileTexture:atIndex:"
+ "setTileTextures:withRange:"
+ "setTileVisibleFunctionTable:atBufferIndex:"
+ "setTileVisibleFunctionTables:withBufferRange:"
+ "setTileWidth:"
+ "setTransformationMatrixBuffer:"
+ "setTransformationMatrixBufferOffset:"
+ "setTransformationMatrixLayout:"
+ "setTriangleFillMode:"
+ "setType:"
+ "setUrl:"
+ "setVertexAccelerationStructure:atBufferIndex:"
+ "setVertexAdditionalBinaryFunctions:"
+ "setVertexAmplificationCount:viewMappings:"
+ "setVertexBuffer:offset:attributeStride:atIndex:"
+ "setVertexBufferOffset:atIndex:"
+ "setVertexBufferOffset:attributeStride:atIndex:"
+ "setVertexBuffers:"
+ "setVertexBuffers:offsets:attributeStrides:withRange:"
+ "setVertexBuffers:offsets:withRange:"
+ "setVertexBytes:length:attributeStride:atIndex:"
+ "setVertexDescriptor:"
+ "setVertexFormat:"
+ "setVertexIntersectionFunctionTable:atBufferIndex:"
+ "setVertexIntersectionFunctionTables:withBufferRange:"
+ "setVertexLinkedFunctions:"
+ "setVertexPreloadedLibraries:"
+ "setVertexSamplerState:atIndex:"
+ "setVertexSamplerState:lodMinClamp:lodMaxClamp:atIndex:"
+ "setVertexSamplerStates:lodMinClamps:lodMaxClamps:withRange:"
+ "setVertexSamplerStates:withRange:"
+ "setVertexTexture:atIndex:"
+ "setVertexTextures:withRange:"
+ "setVertexVisibleFunctionTable:atBufferIndex:"
+ "setVertexVisibleFunctionTables:withBufferRange:"
+ "setViewport:"
+ "setViewports:count:"
+ "setVisibilityResultBuffer:"
+ "setVisibilityResultMode:offset:"
+ "setVisibleFunctionTable:atBufferIndex:"
+ "setVisibleFunctionTable:atIndex:"
+ "setVisibleFunctionTables:withBufferRange:"
+ "setVisibleFunctionTables:withRange:"
+ "setWriteMask:"
+ "sfm_scene_scaled.json"
+ "shaderValidation"
+ "sharedFrameworksPath"
+ "sharedFrameworksURL"
+ "sharedSupportPath"
+ "sharedSupportURL"
+ "shortValue"
+ "shouldMaximizeConcurrentCompilation"
+ "showPools"
+ "signal"
+ "signalEvent:value:"
+ "signaledValue"
+ "size"
+ "size_t"
+ "slice"
+ "small_filter"
+ "sort_"
+ "sort_mbsort_"
+ "sourceAlphaBlendFactor"
+ "sourceRGBBlendFactor"
+ "sparsePageSize"
+ "sparseTileSizeInBytes"
+ "sparseTileSizeInBytesForSparsePageSize:"
+ "sparseTileSizeWithTextureType:pixelFormat:sampleCount:"
+ "sparseTileSizeWithTextureType:pixelFormat:sampleCount:sparsePageSize:"
+ "specializedName"
+ "ss"
+ "stageInputAttributes"
+ "stageInputDescriptor"
+ "stageInputOutputDescriptor"
+ "startCaptureWithCommandQueue:"
+ "startCaptureWithDevice:"
+ "startCaptureWithScope:"
+ "startOfEncoderSampleIndex"
+ "startOfFragmentSampleIndex"
+ "startOfVertexSampleIndex"
+ "staticThreadgroupMemoryLength"
+ "status"
+ "steel_gemm_fused_"
+ "steel_gemm_splitk_"
+ "steel_gemm_splitk_accum_"
+ "stencilAttachment"
+ "stencilAttachmentPixelFormat"
+ "stencilCompareFunction"
+ "stencilFailureOperation"
+ "stencilResolveFilter"
+ "stepFunction"
+ "stepRate"
+ "stopCapture"
+ "storeAction"
+ "storeActionOptions"
+ "stride"
+ "strided_"
+ "strided_scan"
+ "string pointer is null"
+ "stringByAppendingString:"
+ "stringValue"
+ "stringWithString:"
+ "structType"
+ "sum"
+ "supportAddingBinaryFunctions"
+ "supportAddingFragmentBinaryFunctions"
+ "supportAddingVertexBinaryFunctions"
+ "supportArgumentBuffers"
+ "supportDynamicAttributeStride"
+ "supportIndirectCommandBuffers"
+ "supportRayTracing"
+ "supports32BitFloatFiltering"
+ "supports32BitMSAA"
+ "supportsBCTextureCompression"
+ "supportsCounterSampling:"
+ "supportsDestination:"
+ "supportsDynamicLibraries"
+ "supportsFamily:"
+ "supportsFeatureSet:"
+ "supportsFunctionPointers"
+ "supportsFunctionPointersFromRender"
+ "supportsPrimitiveMotionBlur"
+ "supportsPullModelInterpolation"
+ "supportsQueryTextureLOD"
+ "supportsRasterizationRateMapWithLayerCount:"
+ "supportsRaytracingFromRender"
+ "supportsRenderDynamicLibraries"
+ "supportsShaderBarycentricCoordinates"
+ "supportsTextureSampleCount:"
+ "supportsVertexAmplificationCount:"
+ "sv"
+ "sv2"
+ "swizzle"
+ "synchronizeResource:"
+ "synchronizeTexture:slice:level:"
+ "systemUptime"
+ "t"
+ "tAddressMode"
+ "tailSizeInBytes"
+ "ternary_g"
+ "ternary_g_nd1"
+ "ternary_g_nd2"
+ "ternary_g_nd3"
+ "ternary_v"
+ "ternary_v2"
+ "tessellationControlPointIndexType"
+ "tessellationFactorFormat"
+ "tessellationFactorStepFunction"
+ "tessellationOutputWindingOrder"
+ "tessellationPartitionMode"
+ "textureBarrier"
+ "textureBufferDescriptorWithPixelFormat:width:resourceOptions:usage:"
+ "textureCubeDescriptorWithPixelFormat:size:mipmapped:"
+ "textureDataType"
+ "textureReferenceType"
+ "thermalState"
+ "threadGroupSizeIsMultipleOfThreadExecutionWidth"
+ "threadgroupMemoryAlignment"
+ "threadgroupMemoryDataSize"
+ "threadgroupMemoryLength"
+ "threadgroupSizeMatchesTileSize"
+ "tileAdditionalBinaryFunctions"
+ "tileArguments"
+ "tileBindings"
+ "tileBuffers"
+ "tileFunction"
+ "tileHeight"
+ "tileWidth"
+ "trans_a"
+ "trans_b"
+ "transformationMatrixBuffer"
+ "transformationMatrixBufferOffset"
+ "transformationMatrixLayout"
+ "transformer"
+ "triangleCount"
+ "tryCancel"
+ "uint"
+ "uint16"
+ "uint64"
+ "uint8"
+ "uncertainty"
+ "unknown failure in obj loading"
+ "unknown format specifier"
+ "unload"
+ "unlock"
+ "unmatched '}' in format string"
+ "unsignedCharValue"
+ "unsignedIntValue"
+ "unsignedIntegerValue"
+ "unsignedLongValue"
+ "unsignedShortValue"
+ "updateFence:"
+ "updateFence:afterStages:"
+ "updateTextureMapping:mode:indirectBuffer:indirectBufferOffset:"
+ "updateTextureMapping:mode:region:mipLevel:slice:"
+ "updateTextureMappings:mode:regions:mipLevels:slices:numRegions:"
+ "updc_false"
+ "updc_true"
+ "url"
+ "useHeap:"
+ "useHeap:stages:"
+ "useHeaps:count:"
+ "useHeaps:count:stages:"
+ "useResidencySet:"
+ "useResidencySets:count:"
+ "useResource:usage:stages:"
+ "useResources:count:usage:"
+ "useResources:count:usage:stages:"
+ "usedSize"
+ "userInfo"
+ "userName"
+ "uv_coords_bChw2"
+ "v"
+ "v16@?0^v8"
+ "v2"
+ "v24@?0^v8^v16"
+ "v2_"
+ "v:is_on_boundary_loop_fill_hole"
+ "v:remesh_fixed"
+ "v_"
+ "valueWithBytes:objCType:"
+ "valueWithPointer:"
+ "version"
+ "vertexAdditionalBinaryFunctions"
+ "vertexArguments"
+ "vertexAttributes"
+ "vertexBindings"
+ "vertexBuffer"
+ "vertexBufferOffset"
+ "vertexBuffers"
+ "vertexDescriptor"
+ "vertexFormat"
+ "vertexFunction"
+ "vertexLinkedFunctions"
+ "vertexPreloadedLibraries"
+ "vertexStride"
+ "vertical"
+ "verticalSampleStorage"
+ "visibilityResultBuffer"
+ "visibleFunctionTableDescriptor"
+ "vs"
+ "vs2"
+ "vv"
+ "vv2"
+ "wait"
+ "waitForEvent:value:"
+ "waitForFence:"
+ "waitForFence:beforeStages:"
+ "waitUntilDate:"
+ "waitUntilScheduled"
+ "waitUntilSignaledValue:timeoutMS:"
+ "wc4dgpnatv"
+ "width is not integer"
+ "winograd_conv_2d_input_transform_"
+ "winograd_conv_2d_output_transform_"
+ "winograd_conv_2d_weight_transform_"
+ "with axes of size "
+ "wm"
+ "wn"
+ "writeCompactedAccelerationStructureSize:toBuffer:offset:sizeDataType:"
+ "writeMask"
+ "}\n"
- "\":"
- "\": "
- "\"bytes\": ["
- "\"subtype\": "
- "%.2X"
- ",\n"
- ". Espresso E5RT Code "
- ". Supported functions are: "
- "/AppleInternal/Library/BuildRoots/24f0c819-1ca3-11f0-bc1f-ae4c1e672297/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS18.5.Internal.sdk/usr/local/include/usd/pxr/base/tf/refPtr.h"
- "/AppleInternal/Library/BuildRoots/24f0c819-1ca3-11f0-bc1f-ae4c1e672297/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS18.5.Internal.sdk/usr/local/include/usd/pxr/usd/sdf/declareHandles.h"
- "/AppleInternal/Library/BuildRoots/24f0c819-1ca3-11f0-bc1f-ae4c1e672297/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS18.5.Internal.sdk/usr/local/include/usd/pxr/usd/usd/object.h"
- "000000"
- "1"
- ": 0x"
- "<discarded>"
- "ApplyAdditiveLMDampingExplicitSFM_Huber_lossFactorGraph_func"
- "ApplyAdditiveLMDampingExplicitSFM_L2_lossFactorGraph_func"
- "ApplyAdditiveLMDampingToDiagonalsSFM_Huber_lossFactorGraph_func"
- "ApplyAdditiveLMDampingToDiagonalsSFM_L2_lossFactorGraph_func"
- "ApplyMultiplicativeLMDampingToDiagonalsSFM_Huber_lossFactorGraph_func"
- "ApplyMultiplicativeLMDampingToDiagonalsSFM_L2_lossFactorGraph_func"
- "ApplyMultiplicativedLMDampingExplicitSFM_Huber_lossFactorGraph_func"
- "ApplyMultiplicativedLMDampingExplicitSFM_L2_lossFactorGraph_func"
- "ComputerVision_Tess_Kernels.metallib"
- "ERROR_GPU_OUT_OF_MEMORY"
- "ERROR_INVALID_METAL_LIBRARY"
- "ERROR_NONE"
- "ERROR_NO_AVAILABLE_METAL_LIB"
- "ERROR_OUT_OF_MEMORY"
- "ERROR_UNEXPECTED_LOW_INTERSECT_RATE"
- "Epsilonbundleadjustgc_Huber_FocalLengthPriorConstraintName"
- "Epsilonbundleadjustgc_Huber_FocalLengthPriorConstraintName_func"
- "Epsilonbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName"
- "Epsilonbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName_func"
- "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint"
- "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_func"
- "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstraint"
- "Epsilonbundleadjustgc_Huber_HeikkilaReprojectionConstraint_func"
- "Epsilonbundleadjustgc_Huber_KannalaDistortionPriorConstraintName"
- "Epsilonbundleadjustgc_Huber_KannalaDistortionPriorConstraintName_func"
- "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint"
- "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_func"
- "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstraint"
- "Epsilonbundleadjustgc_Huber_KannalaReprojectionConstraint_func"
- "Epsilonbundleadjustgc_Huber_PrincipalPointPriorConstraintName"
- "Epsilonbundleadjustgc_Huber_PrincipalPointPriorConstraintName_func"
- "Epsilonbundleadjustgc_L2_FocalLengthPriorConstraintName"
- "Epsilonbundleadjustgc_L2_FocalLengthPriorConstraintName_func"
- "Epsilonbundleadjustgc_L2_HeikkilaDistortionPriorConstraintName"
- "Epsilonbundleadjustgc_L2_HeikkilaDistortionPriorConstraintName_func"
- "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint"
- "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_func"
- "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstraint"
- "Epsilonbundleadjustgc_L2_HeikkilaReprojectionConstraint_func"
- "Epsilonbundleadjustgc_L2_KannalaDistortionPriorConstraintName"
- "Epsilonbundleadjustgc_L2_KannalaDistortionPriorConstraintName_func"
- "Epsilonbundleadjustgc_L2_KannalaReprojectionConstantPointConstraint"
- "Epsilonbundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_func"
- "Epsilonbundleadjustgc_L2_KannalaReprojectionConstraint"
- "Epsilonbundleadjustgc_L2_KannalaReprojectionConstraint_func"
- "Epsilonbundleadjustgc_L2_PrincipalPointPriorConstraintName"
- "Epsilonbundleadjustgc_L2_PrincipalPointPriorConstraintName_func"
- "EvaluateHpSFM_Huber_lossFactorGraph_func"
- "EvaluateHpSFM_L2_lossFactorGraph_func"
- "Evaluatebundleadjustgc_Huber_FocalLengthPriorConstraintName"
- "Evaluatebundleadjustgc_Huber_FocalLengthPriorConstraintName_func"
- "Evaluatebundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName"
- "Evaluatebundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName_func"
- "Evaluatebundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint"
- "Evaluatebundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_func"
- "Evaluatebundleadjustgc_Huber_HeikkilaReprojectionConstraint"
- "Evaluatebundleadjustgc_Huber_HeikkilaReprojectionConstraint_func"
- "Evaluatebundleadjustgc_Huber_KannalaDistortionPriorConstraintName"
- "Evaluatebundleadjustgc_Huber_KannalaDistortionPriorConstraintName_func"
- "Evaluatebundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint"
- "Evaluatebundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_func"
- "Evaluatebundleadjustgc_Huber_KannalaReprojectionConstraint"
- "Evaluatebundleadjustgc_Huber_KannalaReprojectionConstraint_func"
- "Evaluatebundleadjustgc_Huber_PrincipalPointPriorConstraintName"
- "Evaluatebundleadjustgc_Huber_PrincipalPointPriorConstraintName_func"
- "Evaluatebundleadjustgc_L2_FocalLengthPriorConstraintName"
- "Evaluatebundleadjustgc_L2_FocalLengthPriorConstraintName_func"
- "Evaluatebundleadjustgc_L2_HeikkilaDistortionPriorConstraintName"
- "Evaluatebundleadjustgc_L2_HeikkilaDistortionPriorConstraintName_func"
- "Evaluatebundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint"
- "Evaluatebundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_func"
- "Evaluatebundleadjustgc_L2_HeikkilaReprojectionConstraint"
- "Evaluatebundleadjustgc_L2_HeikkilaReprojectionConstraint_func"
- "Evaluatebundleadjustgc_L2_KannalaDistortionPriorConstraintName"
- "Evaluatebundleadjustgc_L2_KannalaDistortionPriorConstraintName_func"
- "Evaluatebundleadjustgc_L2_KannalaReprojectionConstantPointConstraint"
- "Evaluatebundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_func"
- "Evaluatebundleadjustgc_L2_KannalaReprojectionConstraint"
- "Evaluatebundleadjustgc_L2_KannalaReprojectionConstraint_func"
- "Evaluatebundleadjustgc_L2_PrincipalPointPriorConstraintName"
- "Evaluatebundleadjustgc_L2_PrincipalPointPriorConstraintName_func"
- "ExtractDiagJtJSFM_Huber_lossFactorGraph_func"
- "ExtractDiagJtJSFM_L2_lossFactorGraph_func"
- "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable0WithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable0_func"
- "GroupUpdatebundleadjustgc_Huber_FocalLengthVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_FocalLengthVariable_func"
- "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable0WithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable0_func"
- "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_HeikkilaDistortionVariable_func"
- "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable0WithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable0_func"
- "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_KannalaDistortionVariable_func"
- "GroupUpdatebundleadjustgc_Huber_PixelConstantWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_PixelConstant_func"
- "GroupUpdatebundleadjustgc_Huber_PointConstantWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_PointConstant_func"
- "GroupUpdatebundleadjustgc_Huber_PointVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_PointVariable_func"
- "GroupUpdatebundleadjustgc_Huber_PoseGroupVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_PoseGroupVariable_func"
- "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable0WithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable0_func"
- "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_PrincipalPointVariable_func"
- "GroupUpdatebundleadjustgc_Huber_RotationVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_RotationVariable_func"
- "GroupUpdatebundleadjustgc_Huber_TranslationVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_Huber_TranslationVariable_func"
- "GroupUpdatebundleadjustgc_L2_FocalLengthVariable0WithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_FocalLengthVariable0_func"
- "GroupUpdatebundleadjustgc_L2_FocalLengthVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_FocalLengthVariable_func"
- "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable0WithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable0_func"
- "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_HeikkilaDistortionVariable_func"
- "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable0WithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable0_func"
- "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_KannalaDistortionVariable_func"
- "GroupUpdatebundleadjustgc_L2_PixelConstantWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_PixelConstant_func"
- "GroupUpdatebundleadjustgc_L2_PointConstantWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_PointConstant_func"
- "GroupUpdatebundleadjustgc_L2_PointVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_PointVariable_func"
- "GroupUpdatebundleadjustgc_L2_PoseGroupVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_PoseGroupVariable_func"
- "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable0WithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable0_func"
- "GroupUpdatebundleadjustgc_L2_PrincipalPointVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_PrincipalPointVariable_func"
- "GroupUpdatebundleadjustgc_L2_RotationVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_RotationVariable_func"
- "GroupUpdatebundleadjustgc_L2_TranslationVariableWithOffsets_func"
- "GroupUpdatebundleadjustgc_L2_TranslationVariable_func"
- "Invalid he status."
- "MaterializeJacobiansbundleadjustgc_Huber_FocalLengthPriorConstraintName"
- "MaterializeJacobiansbundleadjustgc_Huber_FocalLengthPriorConstraintName_func"
- "MaterializeJacobiansbundleadjustgc_Huber_FocalLengthVariable"
- "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName"
- "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName_func"
- "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaDistortionVariable"
- "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint"
- "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint_func"
- "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstraint"
- "MaterializeJacobiansbundleadjustgc_Huber_HeikkilaReprojectionConstraint_func"
- "MaterializeJacobiansbundleadjustgc_Huber_KannalaDistortionPriorConstraintName"
- "MaterializeJacobiansbundleadjustgc_Huber_KannalaDistortionPriorConstraintName_func"
- "MaterializeJacobiansbundleadjustgc_Huber_KannalaDistortionVariable"
- "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint"
- "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint_func"
- "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstraint"
- "MaterializeJacobiansbundleadjustgc_Huber_KannalaReprojectionConstraint_func"
- "MaterializeJacobiansbundleadjustgc_Huber_PointVariable"
- "MaterializeJacobiansbundleadjustgc_Huber_PoseGroupVariable"
- "MaterializeJacobiansbundleadjustgc_Huber_PrincipalPointPriorConstraintName"
- "MaterializeJacobiansbundleadjustgc_Huber_PrincipalPointPriorConstraintName_func"
- "MaterializeJacobiansbundleadjustgc_Huber_PrincipalPointVariable"
- "MaterializeJacobiansbundleadjustgc_L2_FocalLengthPriorConstraintName"
- "MaterializeJacobiansbundleadjustgc_L2_FocalLengthPriorConstraintName_func"
- "MaterializeJacobiansbundleadjustgc_L2_FocalLengthVariable"
- "MaterializeJacobiansbundleadjustgc_L2_HeikkilaDistortionPriorConstraintName"
- "MaterializeJacobiansbundleadjustgc_L2_HeikkilaDistortionPriorConstraintName_func"
- "MaterializeJacobiansbundleadjustgc_L2_HeikkilaDistortionVariable"
- "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint"
- "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint_func"
- "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstraint"
- "MaterializeJacobiansbundleadjustgc_L2_HeikkilaReprojectionConstraint_func"
- "MaterializeJacobiansbundleadjustgc_L2_KannalaDistortionPriorConstraintName"
- "MaterializeJacobiansbundleadjustgc_L2_KannalaDistortionPriorConstraintName_func"
- "MaterializeJacobiansbundleadjustgc_L2_KannalaDistortionVariable"
- "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstantPointConstraint"
- "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstantPointConstraint_func"
- "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstraint"
- "MaterializeJacobiansbundleadjustgc_L2_KannalaReprojectionConstraint_func"
- "MaterializeJacobiansbundleadjustgc_L2_PointVariable"
- "MaterializeJacobiansbundleadjustgc_L2_PoseGroupVariable"
- "MaterializeJacobiansbundleadjustgc_L2_PrincipalPointPriorConstraintName"
- "MaterializeJacobiansbundleadjustgc_L2_PrincipalPointPriorConstraintName_func"
- "MaterializeJacobiansbundleadjustgc_L2_PrincipalPointVariable"
- "Materialized"
- "NormSqbundleadjustgc_Huber_FocalLengthVariable0_func"
- "NormSqbundleadjustgc_Huber_FocalLengthVariable_func"
- "NormSqbundleadjustgc_Huber_HeikkilaDistortionVariable0_func"
- "NormSqbundleadjustgc_Huber_HeikkilaDistortionVariable_func"
- "NormSqbundleadjustgc_Huber_KannalaDistortionVariable0_func"
- "NormSqbundleadjustgc_Huber_KannalaDistortionVariable_func"
- "NormSqbundleadjustgc_Huber_PixelConstant_func"
- "NormSqbundleadjustgc_Huber_PointConstant_func"
- "NormSqbundleadjustgc_Huber_PointVariable_func"
- "NormSqbundleadjustgc_Huber_PrincipalPointVariable0_func"
- "NormSqbundleadjustgc_Huber_PrincipalPointVariable_func"
- "NormSqbundleadjustgc_Huber_RotationVariable_func"
- "NormSqbundleadjustgc_Huber_TranslationVariable_func"
- "NormSqbundleadjustgc_L2_FocalLengthVariable0_func"
- "NormSqbundleadjustgc_L2_FocalLengthVariable_func"
- "NormSqbundleadjustgc_L2_HeikkilaDistortionVariable0_func"
- "NormSqbundleadjustgc_L2_HeikkilaDistortionVariable_func"
- "NormSqbundleadjustgc_L2_KannalaDistortionVariable0_func"
- "NormSqbundleadjustgc_L2_KannalaDistortionVariable_func"
- "NormSqbundleadjustgc_L2_PixelConstant_func"
- "NormSqbundleadjustgc_L2_PointConstant_func"
- "NormSqbundleadjustgc_L2_PointVariable_func"
- "NormSqbundleadjustgc_L2_PrincipalPointVariable0_func"
- "NormSqbundleadjustgc_L2_PrincipalPointVariable_func"
- "NormSqbundleadjustgc_L2_RotationVariable_func"
- "NormSqbundleadjustgc_L2_TranslationVariable_func"
- "SFM_Huber_lossFactorGraph.DenseLDLTSolver"
- "SFM_Huber_lossFactorGraphLinearizationPoint"
- "SFM_L2_lossFactorGraph.DenseLDLTSolver"
- "SFM_L2_lossFactorGraphLinearizationPoint"
- "SO3FromLogbundleadjustgc_Huber_RotationVariable_func"
- "SO3FromLogbundleadjustgc_L2_RotationVariable_func"
- "SchurBackSubstitute1SFM_Huber_lossFactorGraph_func"
- "SchurBackSubstitute1SFM_L2_lossFactorGraph_func"
- "SchurBackSubstitute2SFM_Huber_lossFactorGraph_func"
- "SchurBackSubstitute2SFM_L2_lossFactorGraph_func"
- "SchurInvertEliminatedAdditiveDampingSFM_Huber_lossFactorGraph_func"
- "SchurInvertEliminatedAdditiveDampingSFM_L2_lossFactorGraph_func"
- "SchurInvertEliminatedMultiplicativeDampingSFM_Huber_lossFactorGraph_func"
- "SchurInvertEliminatedMultiplicativeDampingSFM_L2_lossFactorGraph_func"
- "SchurInvertEliminatedSFM_Huber_lossFactorGraph_func"
- "SchurInvertEliminatedSFM_L2_lossFactorGraph_func"
- "SchurLinearizeSFM_Huber_lossFactorGraph_func"
- "SchurLinearizeSFM_L2_lossFactorGraph_func"
- "SchurOuterProductSFM_Huber_lossFactorGraph_func"
- "SchurOuterProductSFM_L2_lossFactorGraph_func"
- "Start handling request #%zu, identifier=%s, detail=%d"
- "SymmetrizeDiagJtJSFM_Huber_lossFactorGraph_func"
- "SymmetrizeDiagJtJSFM_L2_lossFactorGraph_func"
- "Unable to create execution stream op for : "
- "Updatebundleadjustgc_Huber_FocalLengthVariable0_func"
- "Updatebundleadjustgc_Huber_FocalLengthVariable_func"
- "Updatebundleadjustgc_Huber_HeikkilaDistortionVariable0_func"
- "Updatebundleadjustgc_Huber_HeikkilaDistortionVariable_func"
- "Updatebundleadjustgc_Huber_KannalaDistortionVariable0_func"
- "Updatebundleadjustgc_Huber_KannalaDistortionVariable_func"
- "Updatebundleadjustgc_Huber_PixelConstant_func"
- "Updatebundleadjustgc_Huber_PointConstant_func"
- "Updatebundleadjustgc_Huber_PointVariable_func"
- "Updatebundleadjustgc_Huber_PrincipalPointVariable0_func"
- "Updatebundleadjustgc_Huber_PrincipalPointVariable_func"
- "Updatebundleadjustgc_Huber_RotationVariable_func"
- "Updatebundleadjustgc_Huber_TranslationVariable_func"
- "Updatebundleadjustgc_L2_FocalLengthVariable0_func"
- "Updatebundleadjustgc_L2_FocalLengthVariable_func"
- "Updatebundleadjustgc_L2_HeikkilaDistortionVariable0_func"
- "Updatebundleadjustgc_L2_HeikkilaDistortionVariable_func"
- "Updatebundleadjustgc_L2_KannalaDistortionVariable0_func"
- "Updatebundleadjustgc_L2_KannalaDistortionVariable_func"
- "Updatebundleadjustgc_L2_PixelConstant_func"
- "Updatebundleadjustgc_L2_PointConstant_func"
- "Updatebundleadjustgc_L2_PointVariable_func"
- "Updatebundleadjustgc_L2_PrincipalPointVariable0_func"
- "Updatebundleadjustgc_L2_PrincipalPointVariable_func"
- "Updatebundleadjustgc_L2_RotationVariable_func"
- "Updatebundleadjustgc_L2_TranslationVariable_func"
- "[\n"
- "[]"
- "[json.exception."
- "\\u%04x"
- "\\u%04x\\u%04x"
- "\\ufffd"
- "],\n"
- "],\"subtype\":"
- "avg_edge_length"
- "avg_vertex_valence"
- "bbox_max_x"
- "bbox_max_y"
- "bbox_max_z"
- "bbox_min_x"
- "bbox_min_y"
- "bbox_min_z"
- "binary"
- "boolean"
- "build_ray_time"
- "bundleadjustgc_Huber_FocalLengthPriorConstraintName"
- "bundleadjustgc_Huber_FocalLengthPriorConstraintNameFactorJacobians"
- "bundleadjustgc_Huber_FocalLengthVariable"
- "bundleadjustgc_Huber_FocalLengthVariable0"
- "bundleadjustgc_Huber_HeikkilaDistortionPriorConstraintName"
- "bundleadjustgc_Huber_HeikkilaDistortionPriorConstraintNameFactorJacobians"
- "bundleadjustgc_Huber_HeikkilaDistortionVariable"
- "bundleadjustgc_Huber_HeikkilaDistortionVariable0"
- "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraint"
- "bundleadjustgc_Huber_HeikkilaReprojectionConstantPointConstraintFactorJacobians"
- "bundleadjustgc_Huber_HeikkilaReprojectionConstraint"
- "bundleadjustgc_Huber_HeikkilaReprojectionConstraintFactorJacobians"
- "bundleadjustgc_Huber_KannalaDistortionPriorConstraintName"
- "bundleadjustgc_Huber_KannalaDistortionPriorConstraintNameFactorJacobians"
- "bundleadjustgc_Huber_KannalaDistortionVariable"
- "bundleadjustgc_Huber_KannalaDistortionVariable0"
- "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraint"
- "bundleadjustgc_Huber_KannalaReprojectionConstantPointConstraintFactorJacobians"
- "bundleadjustgc_Huber_KannalaReprojectionConstraint"
- "bundleadjustgc_Huber_KannalaReprojectionConstraintFactorJacobians"
- "bundleadjustgc_Huber_PixelConstant"
- "bundleadjustgc_Huber_PointConstant"
- "bundleadjustgc_Huber_PointVariable"
- "bundleadjustgc_Huber_PrincipalPointPriorConstraintName"
- "bundleadjustgc_Huber_PrincipalPointPriorConstraintNameFactorJacobians"
- "bundleadjustgc_Huber_PrincipalPointVariable"
- "bundleadjustgc_Huber_PrincipalPointVariable0"
- "bundleadjustgc_Huber_RotationVariable"
- "bundleadjustgc_Huber_RotationVariablerectify_func"
- "bundleadjustgc_Huber_TranslationVariable"
- "bundleadjustgc_L2_FocalLengthPriorConstraintName"
- "bundleadjustgc_L2_FocalLengthPriorConstraintNameFactorJacobians"
- "bundleadjustgc_L2_FocalLengthVariable"
- "bundleadjustgc_L2_FocalLengthVariable0"
- "bundleadjustgc_L2_HeikkilaDistortionPriorConstraintName"
- "bundleadjustgc_L2_HeikkilaDistortionPriorConstraintNameFactorJacobians"
- "bundleadjustgc_L2_HeikkilaDistortionVariable"
- "bundleadjustgc_L2_HeikkilaDistortionVariable0"
- "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraint"
- "bundleadjustgc_L2_HeikkilaReprojectionConstantPointConstraintFactorJacobians"
- "bundleadjustgc_L2_HeikkilaReprojectionConstraint"
- "bundleadjustgc_L2_HeikkilaReprojectionConstraintFactorJacobians"
- "bundleadjustgc_L2_KannalaDistortionPriorConstraintName"
- "bundleadjustgc_L2_KannalaDistortionPriorConstraintNameFactorJacobians"
- "bundleadjustgc_L2_KannalaDistortionVariable"
- "bundleadjustgc_L2_KannalaDistortionVariable0"
- "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraint"
- "bundleadjustgc_L2_KannalaReprojectionConstantPointConstraintFactorJacobians"
- "bundleadjustgc_L2_KannalaReprojectionConstraint"
- "bundleadjustgc_L2_KannalaReprojectionConstraintFactorJacobians"
- "bundleadjustgc_L2_PixelConstant"
- "bundleadjustgc_L2_PointConstant"
- "bundleadjustgc_L2_PointVariable"
- "bundleadjustgc_L2_PrincipalPointPriorConstraintName"
- "bundleadjustgc_L2_PrincipalPointPriorConstraintNameFactorJacobians"
- "bundleadjustgc_L2_PrincipalPointVariable"
- "bundleadjustgc_L2_PrincipalPointVariable0"
- "bundleadjustgc_L2_RotationVariable"
- "bundleadjustgc_L2_RotationVariablerectify_func"
- "bundleadjustgc_L2_TranslationVariable"
- "cannot create object from initializer list"
- "cannot use operator[] with a numeric argument with "
- "chart_avg_stretch_ratio"
- "chart_max_stretch_ratio"
- "chart_number"
- "chart_self_intersect_ratio"
- "chart_time"
- "dense_backbone"
- "dense_transformer"
- "discarded"
- "edge_length_SD"
- "error_code"
- "face_angle_SD"
- "generate_ao_time"
- "generate_maps_time"
- "high_mesh_geometry"
- "high_mesh_topology"
- "images_320"
- "imaging_time"
- "incomplete UTF-8 string; last byte: 0x"
- "intersected_number_of_rays"
- "intersected_rate"
- "invalid UTF-8 byte at index "
- "load_high_time"
- "load_low_time"
- "low_mesh_geometry"
- "low_mesh_topology"
- "mesh_processing_time"
- "mesh_repair"
- "n_boundary_half_edges"
- "n_boundary_loops"
- "n_components"
- "n_half_edges"
- "n_vertices"
- "null"
- "null}"
- "number"
- "pack_img_number"
- "pack_time"
- "rasterize_time"
- "retrieve_attribute_time"
- "save_time"
- "simplify_time"
- "total_number_of_rays"
- "type_error"
- "vertex_valence_deviation"
- "{\"bytes\":["
- ""

```
